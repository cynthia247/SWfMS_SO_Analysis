Id,Title,Body,RatingsSentiCR,RatingsGPT35,RatingsGPTFineTuned
40510347,Can snakemake avoid ambiguity when two different rule paths can generate a given output?,"Initial workflow

I have a snakefile that can generate some output from paired-end data.

In this snakefile, I have a rule that ""installs"" the data given information stored in the config file (get_raw_data).

Then I have a rule that uses that data to generate intermediate files on which the rest of the workflow depends (run_tophat).

Here are the input and output of these rules (OPJ stands for os.path.join):

rule get_raw_data:
    output:
        OPJ(raw_data_dir, ""{lib}_1.fastq.gz""),
        OPJ(raw_data_dir, ""{lib}_2.fastq.gz""),


(More details on the implementation of this rule later)

rule run_tophat:
    input:
        transcriptome = OPJ(annot_dir, ""dmel-all-r5.9.gff""),
        fq1 = OPJ(raw_data_dir, ""{lib}_1.fastq.gz""),
        fq2 = OPJ(raw_data_dir, ""{lib}_2.fastq.gz""),
    output:
        junctions = OPJ(output_dir, ""{lib}"", ""junctions.bed""),
        bam = OPJ(output_dir, ""{lib}"", ""accepted_hits.bam""),


And (simplifying) my main rule would be something like that:

rule all:
    input:
        expand(OPJ(output_dir, ""{lib}"", ""junctions.bed""), lib=LIBS),


Extending the workflow to single-end data

I now have to run my workflow on data that is single-end.

I would like to avoid the final output having different name patterns depending on whether the data is single or paired end.

I can easily make variants of the above-mentioned two rules that should work with single-end data (get_raw_data_single_end and run_tophat_single_end), whose input and output are as follows:

rule get_raw_data_single_end:
    output:
        OPJ(raw_data_dir, ""{lib}.fastq.gz"")

rule run_tophat_single_end:
    input:
        transcriptome = OPJ(annot_dir, ""dmel-all-r5.9.gff""),
        fq = OPJ(raw_data_dir, ""{lib}.fastq.gz""),
    output:
        junctions = OPJ(output_dir, ""{lib}"", ""junctions.bed""),
        bam = OPJ(output_dir, ""{lib}"", ""accepted_hits.bam""),


How to provide snakemake with enough information to chose the correct rule path?

The config file contains the information about whether the lib wildcard is associated with single-end or paired-end data in the following manner: The library names are keys in either a lib2raw or a lib2raw_single_end dictionary (both dictionaries are read from the config file).

I'm not expecting the same library name to be a key in both dictionaries. Therefore, in a sense, it is not ambiguous whether I want the single-end or paired-end branch of the workflow to be executed.

A function lib2data (that uses these dictionaries) is used by both get_raw_data and get_raw_data_single_end to determine what shell command to run to ""install"" the data.

Here is a simplified version of this function (the actual one contains an extra branch to generate a command for data from a SRR identifier):

def lib2data(wildcards):
    lib = wildcards.lib
    if lib in lib2raw:
        raw = lib2raw[lib]
        link_1 = ""ln -s %s %s_1.fastq.gz"" % (raw.format(mate=""1""), lib)
        link_2 = ""ln -s %s %s_2.fastq.gz"" % (raw.format(mate=""2""), lib)
        return ""%s\n%s\n"" % (link_1, link_2)
    elif lib in lib2raw_single_end:
        raw = lib2raw_single_end[lib]
        return ""ln -s %s %s.fastq.gz\n"" % (raw, lib)
    else:
        raise ValueError(""Procedure to get raw data for %s unknown."" % lib)


Apart from their output, the two get_raw_data* rules are the same and work the following way:

params:
    shell_command = lib2data,
shell:
    """"""
    (
    cd {raw_data_dir}
    {params.shell_command}
    )
    """"""


Is snakemake able to determine the correct rule path given information that is not coded in rules input and output, but only in config files and functions?

It seems that it is not the case. Indeed, I'm trying to test my new snakefile (with the added *_single_end rules), but a KeyError occurs during the execution of the get_raw_data rule, whereas the library for which the rule is being executed is associated with single-end data.

How to achieve the desired behaviour (a two-branch workflow able to use the information in the configuration to chose the correct branch)?

Edit: The KeyError was due to an error in lib2data

After using the correct dictionary to get the data associated with the library name, I end up having the following error:

AmbiguousRuleException:
Rules run_tophat and run_tophat_single_end are ambiguous for the file tophat_junction_discovery_revision_supplement/HWT3/junctions.bed.
Expected input files:
        run_tophat: ./HWT3_1.fastq.gz ./HWT3_2.fastq.gz Annotations/dmel-all-r5.9.gff
        run_tophat_single_end: ./HWT3.fastq.gz Annotations/dmel-all-r5.9.gff


Edit 2: Adding input to the get_raw_data* rules

After reading this post on the snakemake mailing list, I tried to add some input to my rules to avoid ambiguity.

def lib2data_input(wildcards):
    lib = wildcards.lib
    if lib in lib2raw:
        raw = lib2raw[lib]
        return [raw.format(mate=""1""), raw.format(mate=""2"")]
    elif lib in lib2raw_single_end:
        raw = lib2raw_single_end[lib]
        return [raw]
    else:
        raise ValueError(""Procedure to get raw data for %s unknown."" % lib)

rule get_raw_data:
    input:
        lib2data_input
# [same output, params and shell as before]
# [same modification for the single-end case]


This results in a MissingInputException. Strangely, the reportedly missing file does exist. Is the trick supposed to work? (Can't reproduce this, now this results in:)

AmbiguousRuleException:
Rules run_tophat_single_end and run_tophat are ambiguous for the file tophat_junction_discovery_revision_supplement/HTW2/junctions.bed.
Expected input files:
        run_tophat_single_end: ./HTW2.fastq.gz Annotations/dmel-all-r5.9.gff
        run_tophat: ./HTW2_1.fastq.gz ./HTW2_2.fastq.gz Annotations/dmel-all-r5.9.gff


My way of specifying an input to the ""data installation"" rules is apparently not enough to guide snakemake to the correct rule.
",-1,1,-1.0
40936470,Snakemake complains it can't find the file it is supposed to create in a run instruction,"Consider the following simple snakefile, which is an attempt to write a file in a run instruction:

rule all:
    input:
        ""test.txt""

rule make_test:
    output:
        filename = ""test.txt""
    run:
        with open(output.filename) as f:
            f.write(""test"")


Running it results in the following:

Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   all
    1   make_test
    2
rule make_test:
    output: test.txt
Error in job make_test while creating output file test.txt.
RuleException:
FileNotFoundError in line 10 of /tmp/Snakefile:
[Errno 2] No such file or directory: 'test.txt'
  File ""/tmp/Snakefile"", line 10, in __rule_make_test
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message


I'm surprised of this FileNotFoundError. Obviously, I didn't find the correct way to tell snakemake that this is the file I want the rule make_test to create.

I also tried the following modification of the output syntax:

rule all:
    input:
        ""test.txt""

rule make_test:
    output:
        ""test.txt""
    run:
        with open(output[0]) as f:
            f.write(""test"")


The error is the same.

What's happening?
",-1,-1,-1.0
41121347,snakemake LSF issues,"I have a pipeline which works just fine in the command line

snakemake -l --snakefile snakemake_example/sankefile_test9.txt


I would like to be able to use it on the cluster. The pipeline takes samples (specified in the config file), and runs a few steps of processing - this is an RNA-Seq pipeline. I've tried to submit to the cluster using these two different ways

First try

snakemake --snakefile sankefile_test9_config.txt --jobs 999 --cluster 'bsub -q bio -R ""rusage[mem=4000]""'


Second try

snakemake --snakefile sankefile_test9_config.txt --cluster 'bsub -q bio' -j


which yielded the following output


  Provided cluster nodes: 48
  Job counts:
      count   jobs
      1   all
      2   collate_barcodes
      2   correct_counts
      2   count_reads
      2   dedup_counts
      2   extract_gz_samples
      2   mark_duplicaticates
      2   move_bc
      2   run_cutadapt
      2   star_mapping
      19
  rule extract_gz_samples:
      input: cluster_fastq/Zelzer_M_Spindle_M_1.R1.fastq.gz,   cluster_fastq/Zelzer_M_Spindle_M_1.R2.fastq.gz
      output: cluster_fastq/Zelzer_M_Spindle_M_1.R1.fastq,   cluster_fastq/Zelzer_M_Spindle_M_1.R2.fastq
      wildcards: sample=cluster_fastq/Zelzer_M_Spindle_M_1
  Memory reservation is (MB): 2048
  Memory Limit is (MB): 2048
  rule extract_gz_samples:
      input: cluster_fastq/WT_M_DT_T_393.R1.fastq.gz,   cluster_fastq/WT_M_DT_T_393.R2.fastq.gz
      output: cluster_fastq/WT_M_DT_T_393.R1.fastq,   cluster_fastq/WT_M_DT_T_393.R2.fastq
      wildcards: sample=cluster_fastq/WT_M_DT_T_393
  Memory reservation is (MB): 2048
  Memory Limit is (MB): 2048
  Waiting at most 5 seconds for missing files.
  Exception in thread Thread-1:
  Traceback (most recent call last):
    File ""/apps/RH6U4/python/3.5.2/lib/python3.5/site-packages/snakemake/dag.py"", line 257, in check_and_touch_output
      wait_for_files(expanded_output, latency_wait=wait)
    File ""/apps/RH6U4/python/3.5.2/lib/python3.5/site-packages/snakemake/io.py"", line 341, in wait_for_files
      latency_wait, ""\n"".join(get_missing())))
  OSError: Missing files after 5 seconds:
  cluster_fastq/Zelzer_M_Spindle_M_1.R1.fastq
  cluster_fastq/Zelzer_M_Spindle_M_1.R2.fastq  


During handling of the above exception, another exception occurred:  


  Traceback (most recent call last):
    File ""/apps/RH6U4/python/3.5.2/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
      self.run()
    File ""/apps/RH6U4/python/3.5.2/lib/python3.5/threading.py"", line 862, in run
      self._target(*self._args, **self._kwargs)
    File ""/apps/RH6U4/python/3.5.2/lib/python3.5/site-packages/snakemake/executors.py"", line 517, in _wait_for_jobs
      self.finish_job(active_job.job)
    File ""/apps/RH6U4/python/3.5.2/lib/python3.5/site-packages/snakemake/executors.py"", line 426, in finish_job
      super().finish_job(job, upload_remote=False)
    File ""/apps/RH6U4/python/3.5.2/lib/python3.5/site-packages/snakemake/executors.py"", line 153, in finish_job
      super().finish_job(job, upload_remote=upload_remote)
    File ""/apps/RH6U4/python/3.5.2/lib/python3.5/site-packages/snakemake/executors.py"", line 111, in finish_job
      self.dag.check_and_touch_output(job, wait=self.latency_wait)
    File ""/apps/RH6U4/python/3.5.2/lib/python3.5/site-packages/snakemake/dag.py"", line 259, in check_and_touch_output
      raise MissingOutputException(str(e), rule=job.rule)
  snakemake.exceptions.MissingOutputException: Missing files after 5 seconds:
  cluster_fastq/Zelzer_M_Spindle_M_1.R1.fastq
  cluster_fastq/Zelzer_M_Spindle_M_1.R2.fastq  
  
  ^C^H^CTerminating processes on user request.
  Will exit after finishing currently running jobs.
  Removing output files of failed job extract_gz_samples since they might be   corrupted:
  cluster_fastq/Zelzer_M_Spindle_M_1.R1.fastq,   cluster_fastq/Zelzer_M_Spindle_M_1.R2.fastq
  Removing output files of failed job extract_gz_samples since they might be corrupted:
  cluster_fastq/WT_M_DT_T_393.R1.fastq, cluster_fastq/WT_M_DT_T_393.R2.fastq  


At this point, the program seems to be stuck (see the ^C^H^C, my break) - and if I check the jobs (in another session) using bjobs, there are no jobs on the queue.

Any idea why this is happening and how to debug it?
",1,-1,-1.0
41465787,Using regex in snakemake wildcards,"I'm using regex in snakemake wildcards but I've come accross an error that I don't understand. 

In this shortened example it works:

rule graphviz:
    input: ""{graph}.dot""
    output: ""{graph}.{ext,(pdf|png)}""
    shell: ""dot -T{wildcards.ext} -o {output} {input}""


In this example, it doesn't:

## This is working
rule fastqc:
    input: ""{reads}.fastq""
    output: ""{reads}_fastqc/{sample}_fastqc.html""
    shell:""fastqc --format fastq {input}""

## This is not working
rule fastqc:
    input: ""{reads}.{ext,(bam|fastq)}""
    output: ""{reads}_fastqc/{sample}_fastqc.html""
    shell:""fastqc --format {wildcards.ext} {input}""


I'm attaching a screencap of the error message I'm getting. Thanks for your help.


",1,-1,-1.0
41090178,Snakemake + tmux,"My workflow often includes PBS job submissions to a shared cluster that need to either wait in the scheduling queue, take over 24hrs to run or both. I'd like to run snakemake in the 'background' and get my prompt back while these jobs are running. I know this can be done using tmux, screen, or &amp; but is there is a better way to do this? 

I guess submitting a bash wrapper script with the snakemake commands inside is an option but I think I'm lacking some understanding of the workflow.
",1,-1,-1.0
40769940,Snakemake cluster config in combination with DRMMA,"I have a question related to drmma and the cluster config file in snakemake.

Currently i have a pipeline and I submit jobs to the cluster using drmma with the following command:

snakemake --drmaa "" -q short.q -pe smp 8 -l membycore=4G"" --jobs 100 -p file1/out file2/out file3/out


The problem is that some of the rules/jobs require less or more resources. I though that if i used the json cluster file I would be able to submit the jobs with different resources. My json file looks like this:

{
    ""__default__"":
    {
        ""-q"":""short.q"",
        ""-pe"":""smp 1"",
        ""-l"":""membycore=4G""
    },
    ""job1"":
    {
        ""-q"":""short.q"",
        ""-pe"":""smp 8"",
        ""-l"":""membycore=4G""
    },
    ""job2"":
    {
        ""-q"":""short.q"",
        ""-pe"":""smp 8"",
        ""-l"":""membycore=4G""
    }
}


When I run the following command my jobs (job1 and job2) are submitted with default options and not with the custom ones:

snakemake --jobs 100 --cluster-config cluster.json --drmaa -p file1/out file2/out file3/out


What am I doing wrong? Is it that I cannot combine the drmaa option with the cluster-config file?
",-1,-1,-1.0
42629737,Snakemake: MissingInputException in snakemake pipeline,"I'm trying a SnakeMake pipeline and I'm stucked on an error I really don't understand.

I've got a directory (raw_data) in which I have the input files :

ll /home/nico/labo/etudes/Optimal/data/raw_data
total 41M
drwxrwxr-x  2 nico nico 4,0K mars   6 16:09 ./
drwxrwxr-x 11 nico nico 4,0K mars   6 16:14 ../
-rw-rw-r--  1 nico nico  15M févr. 27 12:21 sampleA_R1.fastq.gz
-rw-rw-r--  1 nico nico  19M févr. 27 12:22 sampleA_R2.fastq.gz
-rw-rw-r--  1 nico nico 3,4M févr. 27 12:21 sampleB_R1.fastq.gz
-rw-rw-r--  1 nico nico 4,3M févr. 27 12:22 sampleB_R2.fastq.gz


This directory contains 4 files for 2 samples.
I created a config json file for the SnakeMake pipeline named config_snakemake_Optimal_mapping_BaL.json:

{
    ""fastqExtension"": ""fastq.gz"",
    ""fastqDir"": ""/home/nico/labo/etudes/Optimal/data/raw_data"",
    ""outputDir"": ""/home/nico/labo/etudes/Optimal/data/mapping_BaL"",
    ""logDir"": ""logs"",
    ""reference"": {
        ""fasta"": ""/home/nico/labo/references/genomes/HIV1/BaL_AY713409/BaL_AY713409.fasta"",
        ""index"": ""/home/nico/labo/references/genomes/HIV1/BaL_AY713409/BaL_AY713409.fasta.bwt""
    }
}


And finally the SnakeMake file snakefile_bwa_samtools.py:

import subprocess
from os.path import join

### Globals ---------------------------------------------------------------------

# A Snakemake regular expression matching fastq files.

SAMPLES, = glob_wildcards(join(config[""fastqDir""], ""{sample}_R1.""+config[""fastqExtension""]))
print(SAMPLES)

### Rules -----------------------------------------------------------------------

# Pipeline output files
rule all:
    input: expand(join(config[""outputDir""], ""{sample}.bam.bai""), sample=SAMPLES)

# Reads alignment on reference genome and BAM file creation
rule bwa_mem_to_bam:
    input:
        index = config[""reference""][""index""],
        fasta = config[""reference""][""fasta""],
        fq1_ID = ""{sample}_R1.""+config[""fastqExtension""],
        fq2_ID = ""{sample}_R2.""+config[""fastqExtension""],
        fq1 = join(config[""fastqDir""], ""{sample}_R1.""+config[""fastqExtension""]),
        fq2 = join(config[""fastqDir""], ""{sample}_R2.""+config[""fastqExtension""])
    output:
        temp(join(config[""outputDir""], ""{sample}.bamUnsorted""))
    version:
        subprocess.getoutput(
        ""man bwa | tail -n 1 | cut -d ' ' -f 1 | cut -d '-' -f 2""
        )
    log:
        join(config[""outputDir""], config[""logDir""], ""{sample}.bwa_mem.log"")
    message:
        ""Alignment of {input.fq1_ID} and {input.fq2_ID} on {input.fasta} with BWA version {version}.""
    shell:
        ""bwa mem {input.fasta} {input.fq1} {input.fq2} 2&gt; {log} | samtools view -Sbh - &gt; {output}""

# Sorting the BAM files on genomic positions
rule bam_sort:
    input:
        join(config[""outputDir""], ""{sample}.bamUnsorted"")
    output:
        join(config[""outputDir""], ""{sample}.bam"")
    log:
        join(config[""outputDir""], config[""logDir""],  ""{sample}.samtools_sort.log"")
    version:
        subprocess.getoutput(
            ""samtools --version | ""
            ""head -1 | ""
            ""cut -d' ' -f2""
        )
    message:
        ""Genomic sorting of {input} with samtools version {version}.""
    shell:
        ""samtools sort -f {input} {output} 2&gt; {log}""

# Indexing the BAM files
rule bam_index:
    input:
        join(config[""outputDir""], ""{sample}.bam"")
    output:
        join(config[""outputDir""], ""{sample}.bam.bai"")
    message:
        ""Indexing {input}.""
    shell:
        ""samtools index {input}""


I run this pipeline:

snakemake --cores 3 --snakefile /home/nico/labo/scripts/pipeline_illumina/snakefile_bwa_samtools.py --configfile /home/nico/labo/etudes/Optimal/data/snakemake_config_files/config_snakemake_Optimal_mapping_BaL.json


and I've got the following error outputs:

['sampleB', 'sampleA']
MissingInputException in line 18 of /home/nico/labo/scripts/pipeline_illumina/snakefile_bwa_samtools.py:
Missing input files for rule bwa_mem_to_bam:
sampleB_R1.fastq.gz
sampleB_R2.fastq.gz


or depending the moment:

['sampleB', 'sampleA']
PeriodicWildcardError in line 40 of /home/nico/labo/scripts/pipeline_illumina/snakefile_bwa_samtools.py:
The value _unsorted in wildcard sample is periodically repeated (sampleB_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted_unsorted). This would lead to an infinite recursion. To avoid this, e.g. restrict the wildcards in this rule to certain values.


The samples are correctly detected as they appear in the list (first line of kind of outputs) and I'm surely messing around with the wildcards in the rule bwa_mem_to_bam, but I really don't get why..
Any clue?
",-1,-1,-1.0
42943795,`run` block in snakemake reset config value?,"For example, I have python script snakemake.py:

from snakemake import snakemake

cfg={'a':'aaaa', 'b':'bbbb', 'c': 'cccc'}
snakemake(
        'Snakefile',
        targets=['all'],
        printshellcmds=True,
        forceall=True,
        config=cfg,
        # configfile=config,
        keep_target_files=True,
        keep_logger=False)


Snakefile looks like this:

print(config)
print('------------------------------------------------------------------------------------------')
rule a:
    output:
        'a.out'
    shell:
        ""echo %s ; ""
        ""touch {output[0]}"" % config['a']
rule b:
    output:
        'b.out'
    shell:
        ""echo %s ; touch {output[0]}"" % config['b']
rule c:
    output:
        'c.out'
    run:
        print(config['c'])
        import os
        os.system('touch ' + output[0])

rule all:
    input:
        'a.out', 'b.out', 'c.out'


When I run python snakemake.py, I met an error:

{'a': 'aaaa', 'c': 'cccc', 'b': 'bbbb'}
------------------------------------------------------------------------------------------
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       a
        1       all
        1       b
        1       c
        4

rule c:
    output: c.out
    jobid: 1

{}
------------------------------------------------------------------------------------------
KeyError in line 8 of /Users/zech/Desktop/snakemake/Snakefile:
'a'
  File ""/Users/zech/Desktop/snakemake/Snakefile"", line 8, in &lt;module&gt;
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message


When I remove the c.out from rule all, then it runs perfectly fine. It looks like every run block in the rules reset config passed to snakemake function to empty? Isn't it a weird behavior? Is there any workaround? 

I am using snakemake version 3.11.2 (installed from bioconda channel of anaconda) on latest OSX.

NOTE: It runs fine when I run snakemake command line snakemake -p --keep-target-files all --config a=""aaaa"" b=""bbb"" c=""cccc"". So this looks like a problem for the API.
",-1,-1,-1.0
43006545,"Snakemake deletes the temp files prematurely, while running with --immediate-submit option","While I submit jobs with --immediate-submit and --dependency=afterok:{dependencies}, the temp files are getting deleted even before the rules that depend on temp files are started. It's working perfectly when running in normal way. Does any other came across this issue? 

Snakemake script

rule all:
    input: 'file2', 'file3'

rule one:
    input: 'file1'
    output: temp('file2')
    shell: ""touch {output}""

rule two:
    input: 'file2'
    output: 'file3'
    shell: ""touch {output}""


Submission command

snakemake -s sample.snake --cluster '../mybatch.py {dependencies}' -j 4 --immediate-submit


Snakemake output message

Provided cluster nodes: 4
Job counts:
        count   jobs
        1       all
        1       one
        1       two
        3

rule one:
    input: file1
    output: file2
    jobid: 1


rule two:
    input: file2
    output: file3
    jobid: 2


localrule all:
    input: file2, file3
    jobid: 0

Removing temporary output file file2.
Finished job 0.
1 of 3 steps (33%) done

Removing temporary output file file2.
Finished job 1.
2 of 3 steps (67%) done

localrule all:
    input: file2, file3
    jobid: 0

Removing temporary output file file2.
Finished job 0.
3 of 3 steps (100%) done
Error in job two while creating output file file3.
ClusterJobException in line 9 of /faststorage/home/veera/pipelines/ipsych-GATK/test/sample.snake:
Error executing rule two on cluster (jobid: 2, jobscript: /faststorage/home/veera/pipelines/ipsych-GATK/test/.snakemake/tmp.cmvmr3lz/snakejob.two.2.sh). For
detailed error see the cluster log.
Will exit after finishing currently running jobs.


Error message

Missing files after 5 seconds:
file2
/faststorage/home/veera/pipelines/ipsych-GATK/test/.snakemake/tmp.jqh2qz0n
touch: cannot touch `/faststorage/home/veera/pipelines/ipsych-GATK/test/.snakemake/tmp.jqh2qz0n/1.jobfailed': No such file or directory
Missing files after 5 seconds:
file2


The jobs are being submitted with proper dependency. The ../mybatch.py is custom wrapper script for sbatch. Is this a bug or error in my code? Thanks for the help in advance.
",-1,-1,-1.0
43073393,Implementation How to use expand in snakemake when some particular combinations of wildcards are not desired?,"I tried to implement How to use expand in snakemake when some particular combinations of wildcards are not desired? 

The goal is to only process crossed combinations between SUPERGROUPS:

from itertools import product

DOMAINS=[""Metallophos""]
SUPERGROUPS=[""2supergroups"",""5supergroups""]
SUPERGROUPS_INVERSED=[""5supergroups"",""2supergroups""]
CUTOFFS=[""0""]

def filter_combinator(combinator, blacklist):
    def filtered_combinator(*args, **kwargs):
        for wc_comb in combinator(*args, **kwargs):
            # Use frozenset instead of tuple
            # in order to accomodate
            # unpredictable wildcard order
            if frozenset(wc_comb) not in blacklist:
                yield wc_comb
    return filtered_combinator

# ""2supergroups/5supergroups"" and ""5supergroups/2supergroups"" are undesired
forbidden = {
    frozenset({(""supergroup"", ""2supergroups""), (""supergroup_other"", ""2supergroups"")}),
    frozenset({(""supergroup"", ""5supergroups""), (""supergroup_other"", ""5supergroups"")})}

filtered_product = filter_combinator(product, forbidden)

rule target :
    input:
        expand(expand(""results/{{domain}}/{supergroup}/{supergroup_other}/OGSmapping.txt.list.{{cutoff}}.statistics"", filtered_product, supergroup=SUPERGROUPS, supergroup_other = SUPERGROUPS_INVERSED), cutoff=CUTOFFS, domain = DOMAINS)

rule tree_measures:
    input:
        tree=""results/{domain}/{supergroup}/RAxML_bipartitionsBranchLabels.bbhlist.txt.{domain}.fa.aligned.rp.me-25.id.phylip.supergroups.for.notung"",
        list=""results/{domain}/{supergroup}/hmmer_search_bbh_1/bbhlist.txt.{domain}.fa.OGs.tbl.txt.0.list.txt.nh.OGs.txt"",
        mapping1=""results/{domain}/{supergroup_other}/{supergroup}/OGSmapping.txt.list"",
        categories=""results/{domain}/{supergroup}/{supergroup_other}/OGSmapping.txt.categories"",
        mapping2=""results/{domain}/{supergroup}/{supergroup_other}/OGSmapping.txt.list"",
        supergroups=""results/{domain}/{supergroup}/hmmer_search_2/{domain}.fa.OGs.tbl.txt.{cutoff}.supergroups.csv""
    output:
        ""results/{domain}/{supergroup}/{supergroup_other}/OGSmapping.txt.list.{cutoff}.statistics""
    shell:
        ""~/tools/Python-2.7.11/python scripts/tree_measures.py {input.tree} {input.list} {input.mapping1} {input.categories} {input.mapping2} {input.supergroups} {wildcards.cutoff} results/{wildcards.domain}/{wildcards.supergroup}/{wildcards.supergroup_other}/""


But I still get an error message:

Missing input files for rule tree_measures:
results/Metallophos/5supergroups/5supergroups/OGSmapping.txt.list
results/Metallophos/5supergroups/5supergroups/OGSmapping.txt.categories


What am I missing?
",-1,-1,-1.0
43373172,"Error: ""attribute ""m_numa_nodes"" is not a integer value."" when running a qsub snakemake","I am trying to run a snakemake with cluster submission for RNAseq analysis. Here is my script:



#path to gff 
GFF = ""RNASeq/data/ref_GRCh38.p2_top_level.gff3""

#sample names and classes
CTHP = 'CTHP1 CTHP2'.split()
CYP = 'CYP1 CYP2'.split()

samples = CTHP + CYP

rule all:
	input:
		'CTHP1/mapping_results/out_summary.gtf',
		'CTHP2/mapping_results/out_summary.gtf',
		'CYP2/mapping_results/out_summary.gtf',
		'CYP1/mapping_results/out_summary.gtf',
		
rule order_sam:
	input:
		'{samples}/mapping_results/mapped.sam'
	output:
		'{samples}/mapping_results/ordered.mapped.bam'
	threads: 12
	params: ppn=""nodes=1:ppn=12""
	shell:
		'samtools view -Su {input} | samtools sort &gt; {output}'

rule count_sam:
	input:
		bam='{samples}/mapping_results/ordered.mapped.bam'
	output:
		summary='{samples}/mapping_results/out_summary.gtf',
		abun='{samples}/mapping_results/abun_results.tab',
		cover='{samples}/mapping_results/coveraged.gtf'
	threads: 12
	params: ppn=""nodes=1:ppn=12""
	shell:
		'stringtie -o {output.summary} -G {GFF} -C {output.cover} '
		'-A {output.abun} -p {threads} -l {samples} {input.bam}'




```

I want to submit each rule to a cluster. So, in the Terminal from the working directory, I do this:


snakemake --cluster ""qsub -V -l {params.ppn}"" -j 6


However, the jobs are not submitted and I get following error:

Unable to run job: attribute ""m_numa_nodes"" is not a integer value.
Exiting.
Error submitting jobscript (exit code 1):

I have also tried to set the nodes variable directly when running the snake file like this:


snakemake --cluster ""qsub -V -l nodes=1:ppn=16"" -j 6

and as expected, it gave me the same error. At this point I am not sure if its the local cluster setup or something that I am not doing right in the snake file. Any help would be great.

Thanks
",-1,-1,-1.0
44452991,MissinOutputException in snakemake,"I'm planning to move my bioinformatics pipeline into snakemake as my current pipeline is a collection of multiple scripts that are increasingly hard to follow. On the basis of tutorials and documentation, snakemake seems to be very clear and interesting option for pipeline management. However, I'm not familiar with Python as I am mainly working with bash and R, so snakemake seems to be little harder to learn: I'm facing following problem.

I have two files, sampleA_L001_R1_001.fastq.gz and sampleA_L001_R2_001.fastq.gz, wchich are placed into same directory sampleA. I want to merge this files by using cat command. This is actually a test run: in real situation I would have eight separate FASTQ files per sample that should be merged in similar manner. Very simple job but something is wrong with my code.

snakemake --latency-wait 20 --snakefile /home/users/me/bin/snakefile.txt

rule mergeFastq:
    input:
        reads1='sampleA/sampleA_L001_R1_001.fastq.gz',
        reads2='sampleA/sampleA_L001_R2_001.fastq.gz'
    output:
        reads1='sampleA/sampleA_R1.fastq.gz',
        reads2='sampleA/sampleA_R2.fastq.gz'
    message:
        'Merging FASTQ files...'
    shell:
        'cat {input.reads1} &gt; {output.reads1}'
        'cat {input.reads2} &gt; {output.reads2}'

-------------------------------------------------------------

Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   mergeFastq
    1

Job 0: Merging FASTQ files...

Waiting at most 20 seconds for missing files.
Error in job mergeFastq while creating output files sampleA_R1.fastq.gz, sampleA_R2.fastq.gz.
MissingOutputException in line 5 of /home/users/me/bin/snakefile.txt:
Missing files after 20 seconds:
sampleA_R1.fastq.gz
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Removing output files of failed job mergeFastq since they might be corrupted: sampleA_R2.fastq.gz
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message.


As you can see, I already tried the --latency-wait option without any success. Do you have any ideas what could be the source of my problem? Paths to files are correct, files itself are non-corrupted and OK. I faced similar problem with wildcards as well, so there must be something that I don't understand in snakemake basics.
",-1,-1,-1.0
44561183,Multiple inputs and outputs in a single rule Snakemake file,"I am getting started with Snakemake and I have a very basic question which I couldnt find the answer in snakemake tutorial.

I want to create a single rule snakefile to download multiple files in linux one by one. 
The 'expand' can not be used in the output because the files need to be downloaded one by one and wildcards can not be used because it is the target rule.

The only way comes to my mind is something like this which doesnt work properly. I can not figure out how to send the downloaded items to specific directory with specific names such as 'downloaded_files.dwn' using {output} to be used in later steps:

links=[link1,link2,link3,....]
rule download:    
output: 
    ""outdir/{downloaded_file}.dwn""
params: 
    shellCallFile='callscript',
run: 
    callString=''
    for item in links:
        callString+='wget str(item) -O '+{output}+'\n'
    call('echo ""' + callString + '\n"" &gt;&gt; ' + params.shellCallFile, shell=True)
    call(callString, shell=True)


I appreciate any hint on how this should be solved and which part of snakemake I didnt understand well.
",1,-1,-1.0
44814874,Dynamic output in snakemake,"I'm using snakemake to develop a pipeline. I'm trying to create symbolic links for every file in a directory to a new target. I don't know ahead of time how many files there will be, so I'm trying to use dynamic output.

rule source:
    output: dynamic('{n}.txt')
    run:
        source_dir = config[""windows""]
        source = os.listdir(source_dir)
        for w in source:
            shell(""ln -s %s/%s source/%s"" % (source_dir, w, w))


This is the error I get:


  WorkflowError:
  ""Target rules may not contain wildcards. Please specify concrete files or a rule without wildcards.""


What is the issue?
",-1,-1,-1.0
44908069,snakemake - output one only file from multiple input files in one rule,"I'm using snakemake for the first time in order to build a basic pipeline using cutadapt, bwa and GATK (trimming ; mapping ; calling). I would like to run this pipeline on every fastq file contained in a directory, without having to specify their name or whatever in the snakefile or in the config file. I would like to succeed in doing this.

The first two steps (cutadapt and bwa / trimming and mapping) are running fine, but I'm encountering some problems with GATK.

First, I have to generate g.vcf files from bam files. I'm doing this using these rules:

configfile: ""config.yaml""

import os
import glob

rule all:
    input:
        ""merge_calling.g.vcf""

rule cutadapt:
    input:
        read=""data/Raw_reads/{sample}_R1_{run}.fastq.gz"", 
        read2=""data/Raw_reads/{sample}_R2_{run}.fastq.gz"" 
    output:
        R1=temp(""trimmed_reads/{sample}_R1_{run}.fastq.gz""),
        R2=temp(""trimmed_reads/{sample}_R2_{run}.fastq.gz"") 
    threads:
        10
    shell:
        ""cutadapt -q {config[Cutadapt][Quality_value]} -m {config[Cutadapt][min_length]} -a {config[Cutadapt][forward_adapter]} -A  {config[Cutadapt][reverse_adapter]} -o {output.R1} -p '{output.R2}' {input.read} {input.read2}""

rule bwa_map:
    input:
        genome=""data/genome.fasta"",
        read=expand(""trimmed_reads/{{sample}}_{pair}_{{run}}.fastq.gz"", pair=[""R1"", ""R2""]) 
    output:
        temp(""mapped_bam/{sample}_{run}.bam"")
    threads:
        10
    params:
        rg=""@RG\\tID:{sample}\\tPL:ILLUMINA\\tSM:{sample}""
    shell:
        ""bwa mem -t 2 -R '{params.rg}' {input.genome} {input.read} | samtools view -Sb - &gt; {output}""

rule picard_sort:
    input:
        ""mapped_bam/{sample}.bam""
    output:
        ""sorted_reads/{sample}.bam""
    shell:
        ""java -Xmx4g -jar /home/alexandre/picard-tools/picard.jar SortSam I={input} O={output} SO=coordinate VALIDATION_STRINGENCY=SILENT""

rule picard_rmdup:
    input:
        bam=""sorted_reads/{sample}.bam""
    output:
        ""rmduped_reads/{sample}.bam"",
        ""picard_stats/{sample}.bam""
    params:
        reads=""rmduped_reads/{sample}.bam"",
        stats=""picard_stats/{sample}.bam"",
    shell:
        ""java -jar -Xmx2g /home/alexandre/picard-tools/picard.jar MarkDuplicates ""
        ""I={input.bam} ""
        ""O='{params.reads}' ""
        ""VALIDATION_STRINGENCY=SILENT ""
        ""MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1000 ""
        ""REMOVE_DUPLICATES=TRUE ""
        ""M='{params.stats}'""

rule samtools_index:
    input:
        ""rmduped_reads/{sample}.bam""
    output:
        ""rmduped_reads/{sample}.bam.bai""
    shell:
        ""samtools index {input}""

rule GATK_raw_calling:
    input:
        bam=""rmduped_reads/{sample}.bam"",
        bai=""rmduped_reads/{sample}.bam.bai"",
        genome=""data/genome.fasta""
    output:
        ""Raw_calling/{sample}.g.vcf"",
    shell:
        ""java -Xmx4g -jar /home/alexandre/GenomeAnalysisTK-3.7/GenomeAnalysisTK.jar -ploidy 2 --emitRefConfidence GVCF -T HaplotypeCaller -R {input.genome} -I {input.bam} --genotyping_mode DISCOVERY -o {output}""


These rules work fine. For example, if I have the files :
    Cla001d_S281_L001_R1_001.fastq.gz
    Cla001d_S281_L001_R2_001.fastq.gz

I can create one bam file (Cla001d_S281_L001_001.bam) and from that bam file create a GVCF file (Cla001d_S281_L001_001.g.vcf). I have a lot of sample like this one, and I need to create one GVCF file for each, and then merge these GVCF files into one file. The problem is that I'm unable to give the list of the file to merge to the following rule:

rule GATK_merge:
    input:
        ???
    output:
        ""merge_calling.g.vcf""
    shell:
        ""java -Xmx4g -jar /home/alexandre/GenomeAnalysisTK-3.7/GenomeAnalysisTK.jar ""
        ""-T CombineGVCFs ""
        ""-R data/genome.fasta ""
        ""--variant {input} ""
        ""-o {output}""


I tried several things in order to do that, but cannot succeed. The problem is the link between the two rules (GATK_raw_calling and GATK_merge that is supposed to merge the output of GATK_raw_calling). I can't output one single file if I'm specifying the output of GATK_raw_calling as the input of the following rule (Wildcards in input files cannot be determined from output files), and  I'm unable to make a link between the two rules if I'm not specifying these files as an input...

Is there a way to succeed in doing that? The difficulty is that I'm not defining a list of names or whatever, I think.

Thanks you in advance for your help.
",1,1,-1.0
44702238,No values given for wildcard error' in snakemake,"I am trying to make a simple pipeline using snakemake to download two files from the web and then merge them into a single output.

What I thought would work is the following code:

dwn_lnks = {
    '1': 'https://molb7621.github.io/workshop/_downloads/sample.fa',
    '2': 'https://molb7621.github.io/workshop/_downloads/sample.fa'       
    }
import os

# association between chromosomes and their links
def chromo2link(wildcards):
    return dwn_lnks[wildcards.chromo]

rule all:
    input:
        os.path.join('genome_dir', 'human_en37_sm.fa')

rule download:
    output:
        expand(os.path.join('chr_dir', '{chromo}')),
    params:
        link=chromo2link,
    shell:
        ""wget {params.link} -O {output}""


rule merger:
    input:
        expand(os.path.join('chr_dir', ""{chromo}""), chromo=dwn_lnks.keys())
    output:
        os.path.join('genome_dir', 'human_en37_sm.fa')
    run:
        txt = open({output}, 'a+')
        with open (os.path.join('chr_dir', ""{chromo}"") as file:
                    line = file.readline()
                    while line:
                        txt.write(line)
                        line = file.readline()
        txt.close()


This code returns the error:
No values given for wildcard 'chromo'. in line 20

Also, in the merger rule, the python code within the run does not work.

The tutorial in the snakemake package does not cover enough examples to learn the details for non-computer scientists. If anybody knows a good resource to learn how to work with snakemake, I would appreciate if they could share :).     
",-1,-1,-1.0
44616073,Thread.py error snakemake,"I am trying to run a simple one-rule snakemake file as following:

resources_dir='resources'

rule downloadReference:
    output:
        fa = resources_dir+'/human_g1k_v37.fasta',
        fai = resources_dir+'/human_g1k_v37.fasta.fai',
    shell:
        ('mkdir -p '+resources_dir+'; cd '+resources_dir+'; ' +
        'wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz; gunzip human_g1k_v37.fasta.gz; ' +
        'wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.fai;')


But I get an error as :

    Error in job downloadReference while creating output files 
    resources/human_g1k_v37.fasta, resources/human_g1k_v37.fasta.fai.
    RuleException:
    CalledProcessError in line 10 of 
    /lustre4/home/masih/projects/NGS_pipeline/snake_test:
    Command 'mkdir -p resources; cd resources; wget ftp://ftp-
  trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz; gunzip human_g1k_v37.fasta.gz; wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.fai;' returned non-zero exit status 2.
      File ""/lustre4/home/masih/projects/NGS_pipeline/snake_test"", line 10, in __rule_downloadReference
      File ""/home/masih/miniconda3/lib/python3.6/concurrent/futures/thread.py"", line 55, in run
    Removing output files of failed job downloadReference since they might be corrupted:
    resources/human_g1k_v37.fasta
    Will exit after finishing currently running jobs.
    Exiting because a job execution failed. Look above for error message


I am not using the threads option in snakemake. I can not figure out how this is related with thread.py. Anybody has experience with this error?
",-1,-1,-1.0
44316075,Using pyenv in a snakemake rule,"I am using snakemake for a long and complicated pipeline which involves some externally written python2 scripts. When I try to specify python2 with pyenv, pyenv shell command fails while pyenv global and pyenv local have no effect. I have two questions.


Why is the shell command absent? Is it because snakemake runs a non-login non-interactive shell?
Is it possible at all to switch to python2 in a snakemake rule using pyenv?


Here is an example Snakefile.

rule aaa:
   output:
      ""aaa.txt""
   shell:
      """"""
      pyenv versions
      python --version
      echo ""global""
      pyenv global 2.7.12
      python --version
      echo ""local""
      pyenv local 2.7.12
      python --version
      echo ""shell""
      pyenv shell 2.7.12
      python --version
      echo ""pa-pa, zlyj svite"" &gt; aaa.txt
      """"""


That produces the following output.

...
rule aaa:
    output: aaa.txt
    jobid: 0

  system
  2.7.12
  3.5.2
* 3.6.1 (set by PYENV_VERSION environment variable)
Python 3.6.1
global
Python 3.6.1
local
Python 3.6.1
shell
pyenv: no such command `shell'
Error in job aaa while creating output file aaa.txt.
...

",-1,-1,-1.0
45442962,snakemake merge bam file after bwa alignment from 2 or more lanes,"I try to use snakemake for map and merge some data obtaine from many lanes.
I have some problems.  What I want to do is this:


  *.gz> 432_L001.sam, 432_L002.sam  > 432_L001.sorted.bam,432_L002.sorted.bam> 432.bam


So starting from fastq from units create a unic bamfile with  the name of the key of samples.

config.yaml

samples:
    ""432"": [""432_L001"", ""432_L002""]
    ""433"": [""433_LOO1"",""433_L002""]


units:

  ""432_L001"": [ ""RAW/432_CGATGT_L001_R1_001.fastq.gz"", ""RAW/432_CGATGT_L001_R2_001.fastq.gz""]
  ""432_L002"": [""RAW/432_CGATGT_L002_R1_001.fastq.gz"",""RAW/432_CGATGT_L002_R2_001.fastq.gz""]
  ""433_L001"": [""RAW/433_CAGATC_L001_R1_001.fastq.gz"",""RAW/433_CAGATC_L001_R2_001.fastq.gz""]
  ""433_L002"": [""RAW/433_CAGATC_L002_R1_001.fastq.gz"",""RAW/433_CAGATC_L002_R2_001.fastq.gz""]


snakemake

rule all:
    input: expand(""mapped_reads/merged_samples/{A}.bam"", A=config[""samples""]),
           expand(""mapped_reads/bam/{unit}_sorted.bam"",unit=config['units'])


include_prefix=""rules""


include:
    include_prefix + ""/bwa_mem.rules""
include:
    include_prefix + ""/samfiles.rules""
include:
    include_prefix + ""/picard.rules""


rules

    from snakemake.exceptions import MissingInputException

    rule bwa_mem:
        input:
            lambda wildcards: config[""units""][wildcards.unit]
        output:
            temp(""mapped_reads/sam/{unit}.sam"")
        params:
            #sample=lambda wildcards, UNIT_TO_SAMPLE[wildcards.unit]
            #sample=lambda wildcards: units[wildcards.unit],
            genome= config[""reference""]['genome_fasta']
        log:
            ""mapped_reads/log/{unit}_bwa_mem.log""
        benchmark:
            ""benchmarks/bwa/mem/{unit}.txt""
        threads: 8
        shell:
            '/illumina/software/PROG2/bwa-0.7.15/bwa mem '\
                    '-t {threads} {params.genome} {input} 2&gt; {log} &gt; {output}'
rule picard_SortSam:
   input:
       ""mapped_reads/sam/{unit}.sam""
   output:
       temp(""mapped_reads/bam/{unit}_sorted.bam"")
   benchmark:
       ""benchmarks/picard/SortSam/{unit}.txt""
   shell:
       ""picard  SortSam I={input} O={output} SO=coordinate""

rule samtools_merge_bam:
    """"""
    Merge bam files for multiple units into one for the given sample.
    If the sample has only one unit, files will be copied.
    """"""
    input:
        lambda wildcards: expand(""mapped_reads/bam/{unit}_sorted.bam"",unit=config[""samples""][wildcards.sample])
    output:
        ""mapped_reads/merged_samples/{sample}.bam""
    benchmark:
        ""benchmarks/samtools/merge/{sample}.txt""
    run:
        if len(input) &gt; 1:
            shell(""samtools merge {output} {input}"")
        else:
            shell(""cp {input} {output} &amp;&amp; touch -h {output}"")


If I use this code  I have always this error:

InputFunctionException in line 50 of /home/maurizio/Desktop/TEST_exome/rules/bwa_mem.rules:
KeyError: '433_LOO1'
Wildcards:
unit=433_LOO1


How can resolve? 

What it is wrong in this wildcard..??:


  lambda wildcards:
  expand(""mapped_reads/bam/{unit}_sorted.bam"",unit=config[""samples""][wildcards.sample])

",-1,-1,-1.0
45573725,Snakemake: Error while loading shared libraries - msub,"I have a problem when using Snakemake to submit jobs via msub on our MOAB cluster. Every time I run the test job using msub, the job fails and outputs the following error in the log file:


  /opt/***/common/devel/python/3.5.0/bin/python3.5: error while loading shared libraries: libpython3.5m.so.1.0: cannot open shared object file: No such file or directory


The command I am using is

snakemake -j 10 --cluster-config cluster.json --cluster ""msub -q singlenode -l nodes={cluster.nodes}:ppn={cluster.ppn} -l pmem={cluster.pmem} -l walltime={cluster.time}"" DECOMPRESS


The error is weird, as the required library is present in the path denoted in $LD_LIBRARY_PATH:

$ $LD_LIBRARY_PATH
-bash: /opt/***/common/devel/python/3.5.2/lib: is a directory
$ ls $LD_LIBRARY_PATH
libpython3.5m.so  libpython3.5m.so.1.0  libpython3.so  pkgconfig  python3.5


When running the same test job directly on the login node (without using msub) the job also goes through perfectly, indicating that Snakemake is able to run in principle. 

Do you think this is a problem of Snakemake or rather on the side of the cluster?

Cheers,
zuup
",-1,-1,-1.0
45588311,snakemake understand yaml interpretation on alignment command,"I have this rule on snakemake file. When I launch the input file are populated from all the input present on my  yaml file. I expect to populate one units key for each process of bwa.
Here you have the rules and Yaml file (not complete) and the  dry run results.

rule bwa_mem:
    input:
        dt=expand(""trim/{sample}/"",sample=config['units']),
        forward_paired=expand(""trim/{sample}/{sample}_forward_paired.fq.gz"",sample=config['units']),
        reverse_paired=expand(""trim/{sample}/{sample}_reverse_paired.fq.gz"",sample=config['units']),
        forward_unpaired=expand(""trim/{sample}/{sample}_forward_unpaired.fq.gz"",sample=config['units']),
        reverse_unpaired=expand(""trim/{sample}/{sample}_reverse_unpaired.fq.gz"",sample=config['units']),

    output:
        temp(""mapped_reads/sam/{unit}.sam"")
    params:
        genome= config[""reference""]['genome_fasta']
    log:
        ""mapped_reads/log/{unit}_bwa_mem.log""
    benchmark:
        ""benchmarks/bwa/mem/{unit}.txt""
    threads: 8
    shell:
        '/illumina/software/PROG2/bwa-0.7.15/bwa mem '\
                '-t {threads} {params.genome}  {input.forward_paired} {input.reverse_paired} {input.forward_unpaired} {input.reverse_unpaired} 2&gt; {log} &gt; {output}'


And this yaml file configuration:

  'samples':
  '432':
  - '432_L001'
  - '432_L002'
  '433':
  - '433_L002'
  - '433_L001'
  '434':
  - '434_L001'
  - '434_L002'
  '435':
  - '435_L002'
  - '435_L001'
....
'units':
  '432_L001':
  - '/illumina/runs/FASTQ/RAW/432_CGATGT_L001_R1_001.fastq.gz'
  - '/illumina/runs/FASTQ/RAW/432_CGATGT_L001_R2_001.fastq.gz'
  '432_L002':
  - '/illumina/runs/FASTQ/RAW/432_CGATGT_L002_R1_001.fastq.gz'
  - '/illumina/runs/FASTQ/RAW/432_CGATGT_L002_R2_001.fastq.gz'
  '433_L001':
  - '/illumina/runs/FASTQ/RAW/433_CAGATC_L001_R1_001.fastq.gz'
  - '/illumina/runs/FASTQ/RAW/433_CAGATC_L001_R2_001.fastq.gz'
  '433_L002':
  - '/illumina/runs/FASTQ/RAW/433_CAGATC_L002_R1_001.fastq.gz'
  - '/illumina/runs/FASTQ/RAW/433_CAGATC_L002_R2_001.fastq.gz'
  '434_L001':
  - '/illumina/runs/FASTQ/RAW/434_GTGAAA_L001_R1_001.fastq.gz'
  - '/illumina/runs/FASTQ/RAW/434_GTGAAA_L001_R2_001.fastq.gz'
  '434_L002':
  - '/illumina/runs/FASTQ/RAW/434_GTGAAA_L002_R1_001.fastq.gz'
  - '/illumina/runs/FASTQ/RAW/434_GTGAAA_L002_R2_001.fastq.gz'
  '435_L001':
  - '/illumina/runs/FASTQ/RAW/435_ACAGTG_L001_R1_001.fastq.gz'
  - '/illumina/runs/FASTQ/RAW/435_ACAGTG_L001_R2_001.fastq.gz'


when I try to run he bwa command gave this results

rule bwa_mem:
    input: trim/432_L001/432_L001_reverse_unpaired.fq.gz, trim/432_L002/4
32_L002_reverse_unpaired.fq.gz, trim/433_L001/433_L001_reverse_unpaired.f
q.gz, trim/433_L002/433_L002_reverse_unpaired.fq.gz, trim/434_L001/434_L0
01_reverse_unpaired.fq.gz, trim/434_L002/434_L002_reverse_unpaired.fq.gz,
 trim/435_L001/435_L001_reverse_unpaired.fq.gz, trim/435_L002/435_L002_re
verse_unpaired.fq.gz, trim/436_L001/436_L001_reverse_unpaired.fq.gz, trim
/436_L002/436_L002_reverse_unpaired.fq.gz, trim/437_L001/437_L001_reverse
_unpaired.fq.gz, trim/437_L002/437_L002_reverse_unpaired.fq.gz, trim/438_
L003/438_L003_reverse_unpaired.fq.gz, trim/438_L004/438_L004_reverse_unpa
ired.fq.gz,  trim/lane1_L001/lane1_L
001_reverse_paired.fq.gz, trim/lane2_L002/lane2_L002_reverse_paired.fq.gz
, trim/lane8_L008/
    output: mapped_reads/sam/441_L004.sam
    log: mapped_reads/log/441_L004_bwa_mem.log
    jobid: 208
    benchmark: benchmarks/bwa/mem/441_L004.txt
    wildcards: unit=441_L004


For any element on units report all the input files... Where I made the mistake? 
",-1,1,-1.0
45744998,Snakemake and cloud formation cluster error with local scratch space,"I am having a problem using local scratch space on cfncluster and snakemake at the same time. My strategy is to write data  to local scratch for each node in the cluster and then move the data to the NFS partition.
Unfortunately I am getting the following error:

snakemake 4.0.0, cfncluster

 /shared/bin/bin/snakemake --rerun-incomplete  -s /shared/scripts/sra_to_fa_cluster.py -j 1 -p --latency-wait 20  -k  -c "" qsub -cwd -V"" -F 

/shared/dbGAP/sra_toolkit/sratoolkit.2.8.2-1-ubuntu64/bin/fastq-dump  --split-files --gzip  --outdir /scratch/   /shared/dbGAP/sras2/test/SRR2135300.sra
Waiting at most 20 seconds for missing files.
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/shared/bin/lib/python3.6/site-packages/snakemake/dag.py"", line 319, in check_and_touch_output
    wait_for_files(expanded_output, latency_wait=wait)
  File ""/shared/bin/lib/python3.6/site-packages/snakemake/io.py"", line 395, in wait_for_files
    latency_wait, ""\n"".join(get_missing())))
OSError: Missing files after 20 seconds:
/scratch/SRR2135300_2.fastq.gz

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/shared/bin/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/shared/bin/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/shared/bin/lib/python3.6/site-packages/snakemake/executors.py"", line 647, in _wait_for_jobs
    active_job.callback(active_job.job)
  File ""/shared/bin/lib/python3.6/site-packages/snakemake/scheduler.py"", line 287, in _proceed
    self.get_executor(job).handle_job_success(job)
  File ""/shared/bin/lib/python3.6/site-packages/snakemake/executors.py"", line 549, in handle_job_success
    super().handle_job_success(job, upload_remote=False)
  File ""/shared/bin/lib/python3.6/site-packages/snakemake/executors.py"", line 178, in handle_job_success
    ignore_missing_output=ignore_missing_output)
  File ""/shared/bin/lib/python3.6/site-packages/snakemake/dag.py"", line 323, in check_and_touch_output
    ""wait time with --latency-wait."", rule=job.rule)
snakemake.exceptions.MissingOutputException: Missing files after 20 seconds:
/scratch/SRR2135300_2.fastq.gz
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.


This is similar to the error reported here: 
https://bitbucket.org/snakemake/snakemake/issues/462/unhandled-missingoutputexception-in

Snakemake script is as follows:

   rule all:
           input:expand(""/shared/dbGAP/sras2/fastq.gz/{sample}_{end}.fastq.gz"", 
  sample=SAMPLES, end=END)

rule move:
    input: left=""/scratch/{sample}_1.fastq.gz"", right=""/scratch/{sample}_2.fastq.gz""
    output: left=""/shared/dbGAP/sras2/fastq.gz/{sample}_1.fastq.gz"", right=""/shared/dbGAP/sras2/fastq.gz/{sample}_2.fastq.gz""
    shell: ""rsync --remove-source-files  -av {input.left} {output.left}; rsync --remove-source-files  -av {input.right} {output.right};""


rule get_fastq_files_from_sra_file:
    input: sras=""/shared/dbGAP/sras2/test/{sample}.sra""
    output: left=""/scratch/{sample}_1.fastq.gz"", right=""/scratch/{sample}_2.fastq.gz""
    shell: ""/shared/dbGAP/sra_toolkit/sratoolkit.2.8.2-1-ubuntu64/bin/fastq-dump  --split-files --gzip  --outdir /scratch/   {input}""


My feeling is that snakemake cannot ""see"" the scratch on the nodes, so it returns it as missing, but I am not sure how to solve this issue.
",-1,-1,-1.0
45765403,Bracket issue in snakemake cluster command,"I am using snakemake 4.0.0 on aws cfncluster with the following commands.

rule fastq_to_counts:
input: fastql=""/shared/dbGAP/sras2/fastq.gz/{sample}_1.fastq.gz"", fastqr=""/shared/dbGAP/sras2/fastq.gz/{sample}_2.fastq.gz""
output: ""/shared/counts/{sample}""
shell: '/shared/packages/sailfish-master/bin/sailfish quant -i /shared/packages/gencode26/gencode26 -l IU -p 1  -1 &lt;(zcat {input.fastql})  -2 &lt;(zcat {input.fastqr})       --output  {output}'


It runs fine on the head node but when I submit to head node i get the following error.

    /shared/packages/sailfish-master/bin/sailfish quant -i /shared/packages/gencode26/gencode26 -l IU -p 1  -1 &lt;(zcat /shared/dbGAP/sras2/fastq.gz/xxx.fastq.gz)  -2 &lt;(zcat /shared/dbGAP/sras2/fastq.gz/xxx.fastq.gz)       --output  /shared/counts/SRR1075530 
/bin/sh: 1: Syntax error: ""("" unexpected
Error in job fastq_to_counts while creating output file /shared/counts/xxx.


RuleException:

The issue is that the ""sh"" command does not support ""bashism"" of the ""("" 
is there any way to force snakemake to use /bin/bash ?
Thanks
",-1,-1,-1.0
46072567,Snakemake touch command,"I use snakemake and I tried to write a work-flow on alignment and create bigwig.
I would like to introduce a creation after star alignment on a file to use for  gave me a way to run the  wig generation only after all the samples are aligned.

I have this error:

  snakemake --core 3 --configfile config_tardis.yml  -np
RuleException in line 40 of /home/centos/rna_test/rules/star2.rules:
Could not resolve wildcards in rule star_map:
sample


I tried to use this code:

rule star_map:
    input:
        dt=""trim/{sample}/"",
        forward_paired=""trim/{sample}/{sample}_forward_paired.fq.gz"",
        reverse_paired=""trim/{sample}/{sample}_reverse_paired.fq.gz"",
        forward_unpaired=""trim/{sample}/{sample}_forward_unpaired.fq.gz"",
        reverse_unpaired=""trim/{sample}/{sample}_reverse_unpaired.fq.gz"",
        t1p=""database.done"",
    output:
        out1=""ALIGN/{sample}/Aligned.sortedByCoord.out.bam"",
        out2=touch(""Star.align.done"")
    params:
        genomedir = config['references']['basepath'],
        sample=config[""samples""],
        platform_unit=config['platform'],
        cente=config['center']
    threads: 12
    log: ""ALIGN/log/{params.sample}_star.log""
    shell:
        'STAR --runMode alignReads  --genomeDir {params.genomedir} '
        r' --outSAMattrRGline  ID:{params.sample} SM:{params.sample} PL:{config[platform]}  PU:{params.platform_unit} CN:{params.cente} '
        '--readFilesIn   {input.forward_paired} {input.reverse_paired} {input.forward_unpaired} {input.reverse_unpaired} \
       --readFilesCommand zcat \
       --outStd Log \
       --outSAMunmapped Within \
       --outSAMtype BAM SortedByCoordinate \
       --runThreadN  {threads} --outFileNamePrefix  {output.out1};{output.out2}  2&gt; {log} '




rule star_wigg_file:
    input:
        f1= ""ALIGN/{sample}/Aligned.sortedByCoord.out.bam"",
        t1p=""Star.align.done"",
    output:
        ""ALIGN/{sample}/wiggle/""
    threads: 12

    shell:
       'STAR --runMode inputAlignmentsFromBAM -inputBAMfile {input.f1} --outWigType wiggle \
  --outWigStrand Stranded '


So, the problems seem associated on the introduce of touch
",-1,1,-1.0
46398826,Snakemake absolute file paths not recognized in rule when file path specified as target,"Snakefile:


rule A:
    output: ""/Volumes/rnaseq/x.txt""


Command:

snakemake -s t1 /Volumes/rnaseq/x.txt


Output:

MissingRuleException:
No rule to produce ../../../../../../../Volumes/rnaseq/x.txt (if you use input functions make sure that they don't raise unexpected exceptions).


Why is this happening?  Is this a SnakeMake bug?  Does it have a problem matching the file with the rule because it puts the ../..... into the filename?
",-1,-1,-1.0
46399060,Snakemake rules,"I want to use snakemake for making a bioinformatics pipeline and I googled it and read documents and other stuff, but I still don't know how to get it works.

Here are some of my raw data files.

Rawdata/010_0_bua_1.fq.gz, Rawdata/010_0_bua_2.fq.gz
Rawdata/11_15_ap_1.fq.gz, Rawdata/11_15_ap_2.fq.gz

...they are all paired files.)

Here is my align.snakemake

from os.path import join


STAR_INDEX = ""/app/ref/ensembl/human/idx/""
SAMPLE_DIR = ""Rawdata""
SAMPLES, = glob_wildcards(SAMPLE_DIR + ""/{sample}_1.fq.gz"")

R1 = '{sample}_1.fq.gz'
R2 = '{sample}_2.fq.gz'


rule alignment:
    input:
        r1 = join(SAMPLE_DIR, R1),
        r2 = join(SAMPLE_DIR, R2),
    params:
        STAR_INDEX = STAR_INDEX
    output:
        ""Align/{sample}.bam""
    message:
        ""--- mapping STAR---""
    shell:""""""
        mkdir -p Align/{wildcards.sample}
        STAR --genomeDir {params.STAR_INDEX} --readFilesCommand zcat  --readFilesIn {input.r1} {input.r2}  --outFileNamePrefix Align/{wildcards.sample}/log
""""""


This is it. I run this file by ""snakemake -np -s align.snakemake"" and I got this error.

WorkflowError:
Target rules may not contain wildcards. Please specify concrete files or a rule without wildcards.

I am sorry that I ask this question, there are many people using it pretty well though. any help would be really appriciated. Sorry for my English.

P.S. I read the official document and tutorial but still have no idea. 
",1,-1,-1.0
46428054,Snakemake : CalledProcessError when running BWA on multiple files,"I have a folder with multiple sub-folders that each contain .fastq files(s) that I would like to align to a genome. I am trying to create a snakemake workflow for it. First I access each sub-directory and the files in them using wildcards. Then I use the expand function to store all the paths to the files and write a rule to map the files to the genome. The code is as follows:

    from snakemake.io import glob_wildcards, expand
    import sys
    import os

    directories, files = glob_wildcards(""data/samples/{dir}/{file}.fastq"")
    print(directories, files)

    rule all:
        input:
             expand(""data/samples/{dir}/{file}.fastq"", zip, dir=directories, 
    file=files)

    rule bwa_map:
        input:
            G = ""data/genome.fa"",
            r1 = expand(""data/samples/{dir}/{file}.fastq"", zip, 
    dir=directories, file=files)
        output:
            r2 = expand(""data/results/{dir}/{file}.bam"", zip, dir=directories, 
    file=files)
        shell:
           ""./bwa mem {input.G} {input.r1} | ./samtools sort -o - &gt; {output.r2}""


However, when I execute this code as ""snakemake bwa_map"", I get the following error:

Error in job bwa_map while creating output files data/results/SRR5923/A.bam, data/results/SRR5924/B.bam, data/results/SRR5925/C.bam.
RuleException:
CalledProcessError in line 19 of /Users/rewatitappu/PycharmProjects/RNA-seq_Snakemake/Snakefile:
Command './bwa mem data/genome.fa data/samples/SRR5923/A.fastq data/samples/SRR5924/B.fastq data/samples/SRR5925/C.fastq | ./samtools sort -o - &gt; data/results/SRR5923/A.bam data/results/SRR5924/B.bam data/results/SRR5925/C.bam' returned non-zero exit status 1.
  File ""/Users/rewatitappu/PycharmProjects/RNA-seq_Snakemake/Snakefile"", line 19, in __rule_bwa_map
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/thread.py"", line 55, in run
Removing output files of failed job bwa_map since they might be corrupted:
data/results/SRR5923/A.bam
Will exit after finishing currently running jobs.


Am I wrongly executing the snakemake command or could there be a problem with the code?
",-1,-1,-1.0
46508335,Input Wildcard Constraints Snakemake,"I am new to snakemake, and I am using it to merge steps from two pipelines into a single larger pipeline. The issue that several steps create similarly named files, and I cannot find a way to limit the wildcards, so I am getting a Missing input files for rule error on one step that I just cannot resolve.

The full snakefile is long, and is available here: https://mdd.li/snakefile 

The relevant sections are (sections of the file are missing below):

wildcard_constraints:
    sdir=""[^/]+"",
    name=""[^/]+"",
    prefix=""[^/]+""

# Several mapping rules here

rule find_intersecting_snps:
    input:
        bam=""hic_mapping/bowtie_results/bwt2/{sdir}/{prefix}_hg19.bwt2pairs.bam""
    params:
        hornet=os.path.expanduser(config['hornet']),
        snps=config['snps']
    output:
        ""hic_mapping/bowtie_results/bwt2/{sdir}/{prefix}_hg19.bwt2pairs.remap.fq1.gz"",
        ""hic_mapping/bowtie_results/bwt2/{sdir}/{prefix}_hg19.bwt2pairs.remap.fq2.gz"",
        ""hic_mapping/bowtie_results/bwt2/{sdir}/{prefix}_hg19.bwt2pairs.keep.bam"",
        ""hic_mapping/bowtie_results/bwt2/{sdir}/{prefix}_hg19.bwt2pairs.to.remap.bam"",
        ""hic_mapping/bowtie_results/bwt2/{sdir}/{prefix}_hg19.bwt2pairs.to.remap.num.gz""
    shell:
        dedent(
            """"""\
            python2 {params.hornet}/find_intersecting_snps.py \
            -p {input.bam} {params.snps}
            """"""
        )

# Several remapping steps, similar to the first mapping steps, but in a different directory

rule wasp_merge:
    input:
        ""hic_mapping/bowtie_results/bwt2/{sdir}/{prefix}_hg19.bwt2pairs.keep.bam"",
        ""hic_mapping/wasp_results/{sdir}_{prefix}_filt_hg19.remap.kept.bam""
    output:
        ""hic_mapping/wasp_results/{sdir}_{prefix}_filt_hg19.bwt2pairs.filt.bam""
    params:
        threads=config['job_cores']
    shell:
        dedent(
            """"""\
            {module}
            module load samtools
            samtools merge --threads {params.threads} {output} {input}
            """"""
        )

# All future steps use the name style wildcard, like below

rule move_results:
    input:
        ""hic_mapping/wasp_results/{name}_filt_hg19.bwt2pairs.filt.bam""
    output:
        ""hic_mapping/wasp_results/{name}_filt_hg19.bwt2pairs.bam""
    shell:
        dedent(
            """"""\
            mv {input} {output}
            """"""
        )


This pipeline is essentially doing some mapping steps in one directory structure that looks like hic_mapping/bowtie_results/bwt2/&lt;subdir&gt;/&lt;file&gt;, (where subdir is three different directories) then filtering the results, and doing another almost identical mapping step in hic_remap/bowtie_results/bwt2/&lt;subdir&gt;/&lt;file&gt;, before merging the results into an entirely new directory and collapsing the subdirectories into the file name: hic_mapping/wasp_results/&lt;subdir&gt;_&lt;file&gt;.

The problem I have is that the wasp_merge step breaks the find_intersecting_snps step if I collapse the subdirectory name into the filename. If I just maintain the subdirectory structure, everything works fine. Doing this would break future steps of the pipeline though.

The error I get is:

MissingInputException in line 243 of /godot/quertermous/PooledHiChip/pipeline/Snakefile:
Missing input files for rule find_intersecting_snps:
hic_mapping/bowtie_results/bwt2/HCASMC5-8_HCASMC-8-CAGAGAGG-TACTCCTT_S8_L006/001_hg19.bwt2pairs.bam


The correct file is:
hic_mapping/bowtie_results/bwt2/HCASMC5-8/HCASMC-8-CAGAGAGG-TACTCCTT_S8_L006_001_hg19.bwt2pairs.bam

But it is looking for:
hic_mapping/bowtie_results/bwt2/HCASMC5-8_HCASMC-8-CAGAGAGG-TACTCCTT_S8_L006/001_hg19.bwt2pairs.bam

Which is not created anywhere, nor defined by any rule. I think it is somehow getting confused by the existence of the file created by the wasp_merge step:
hic_mapping/wasp_results/HCASMC5-8_HCASMC-8-CAGAGAGG-TACTCCTT_S8_L006_001_filt_hg19.bwt2pairs.filt.bam

Or possibly a downstream file (after the target that creates this error):
hic_mapping/wasp_results/HCASMC5-8_HCASMC-8-CAGAGAGG-TACTCCTT_S8_L006_001_filt_hg19.bwt2pairs.bam

However, I have no idea why either of those files would confuse the find_intersecting_snps rule, because the directory structures are totally different.

I feel like I must be missing something obvious, because this error is so absurd, but I cannot figure out what it is.
",-1,-1,-1.0
46555679,Snakemake MissingOutputException latency-wait ignored,"I am attempting to run some picard tools metrics collection in snakemake. A --dryrun works fine with no errors. When I actually run the snake file I receive an MissingOutputException for reasons I do not understand.

First here is my rule

rule CollectAlignmentSummaryMetrics:
    input:
        ""bam_input/final/{sample}/{sample}.ready.bam""
    output:
        ""bam_input/final/{sample}/metrics/{reference}/alignment_summary.metrics""
    params:
        reference=config['reference']['file'],
        memory=""10240m""
    run:
        ""java -Xmx{params.memory} -jar $HOME/software/picard/build/libs/picard.jar CollectAlignmentSummaryMetrics R={params.reference} I={input} O={output}""


Now the error.

snakemake --latency-wait 120 -s metrics.snake -p
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        38      CollectAlignmentSummaryMetrics
        1       all
        39

rule CollectAlignmentSummaryMetrics:
    input: bam_input/final/TB5173-T14/TB5173-T14.ready.bam
    output: bam_input/final/TB5173-T14/metrics/GRCh37/alignment_summary.metrics
    jobid: 7
    wildcards: reference=GRCh37, sample=TB5173-T14

Error in job CollectAlignmentSummaryMetrics while creating output file bam_input/final/TB5173-T14/metrics/GRCh37/alignment_summary.metrics.
MissingOutputException in line 21 of/home/bwubb/projects/PD1WES/metrics.snake:
Missing files after 5 seconds:
bam_input/final/TB5173-T14/metrics/GRCh37/alignment_summary.metrics
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Exiting because a job execution failed. Look above for error message
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message


The --latency-wait is completely ignored. I have even tried bumping it up to 84600. If I am to run the intended picard java command, it executes no problem. Ive made several snakemake pipelines without any mysterious issues, so this is driving me quite mad. Thank you for any insight!
",-1,-1,-1.0
46569236,Handling SIGPIPE error in snakemake,"The following snakemake script:

rule all:
    input:
        'test.done'

rule pipe:
   output:
       'test.done'
   shell:
        """"""
        seq 1 10000 | head &gt; test.done
        """"""


fails with the following error:

snakemake -s test.snake

Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   all
    1   pipe
    2

rule pipe:
    output: test.done
    jobid: 1

Error in job pipe while creating output file test.done.
RuleException:
CalledProcessError in line 9 of /Users/db291g/Tritume/test.snake:
Command '
        seq 1 10000 | head &gt; test.done
        ' returned non-zero exit status 141.
  File ""/Users/db291g/Tritume/test.snake"", line 9, in __rule_pipe
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/thread.py"", line 55, in run
Removing output files of failed job pipe since they might be corrupted:
test.done
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message


The explanation returned non-zero exit status 141 seems to say that snakemake has caught the SIGPIPE fail sent by head. I guess strictly speaking snakemake is doing the right thing in catching the fail, but I wonder if it would be possible to ignore some types of errors like this one. I have a snakemake script using the head command and I'm trying to find a workaround this error.   
",-1,-1,-1.0
46636844,Is Snakemake params function evaluated before input file existence?,"Consider this snakefile:

def rdf(fn):
    f = open(fn, ""rt"")
    t = f.readlines()
    f.close()
    return t

rule a:
    output: ""test.txt""
    input: ""test.dat""
    params: X=lambda wildcards, input, output, threads, resources: rdf(input[0])
    message: ""X is {params.X}""
    shell: ""cp {input} {output}""

rule b:
    output: ""test.dat""
    shell: ""echo 'hello world' &gt;{output}""


When run and neither test.txt nor test.dat exists, it gives this error:

InputFunctionException in line 7 of /Users/tedtoal/Documents/BioinformaticsConsulting/Mars/Cacao/Pipeline/SnakeMake/t2:
FileNotFoundError: [Errno 2] No such file or directory: 'test.dat'


However, if test.dat exists, it runs fine.  Why?

I would have expected params not be be evaluated until snakemake was ready to run rule 'a'.  Instead, it must call the params function rdf() above during DAG phase prior to running rule 'a'.  And yet the following works, even when test.dat does not exist initially:

import os

def rdf(fn):
    if not os.path.exists(fn): return """"
    f = open(fn, ""rt"")
    t = f.readlines()
    f.close()
    return t

rule a:
    output: ""test.txt""
    input: ""test.dat""
    params: X=lambda wildcards, input, output, threads, resources: rdf(input[0])
    message: ""X is {params.X}""
    shell: ""cp {input} {output}""

rule b:
    output: ""test.dat""
    shell: ""echo 'hello world' &gt;{output}""


This implies that the params are evaluated twice, once during DAG phase and once during rule execution phase.  Why?

This is a problem for me.  I need to be able to read data from an input file to the rule, to formulate arguments for the program to be executed.  The command does not receive the input filename itself, instead it gets arguments derived from the contents of the input file.  I can handle it as above, but this seems klugey, and I wonder if there is a bug or I'm missing something?
",-1,-1,-1.0
46855983,Snakemake: merging inputs with different suffixes to same-suffix output,"Okay, I've been trying all day to solve this, to no avail... I am working with downloading and analysing RNA-sequencing data, and my analysis incorporates public datasets that come in two flavours: single-end reads and paired-end reads. In essence, every raw file that my workflow start to work on can either be a single file named {sample}.fastq.gz or two files, named {sample}_1.fastq.gz and {sample}_2.fastq.gz, respectively.

I have all the samples and their read layouts (and some other info) in a metadata file which I parse with pandas into a dataframe. I need to be able to give parameters to my scripts (here simply abstracted to touch {output}) in order for them to perform their function depending on the read layout (they are all bash scripts using command line software like sratools and STAR). What I want to achieve is something along the following snakemake pseudocode:

# Metadata in a pandas dataframe
metadata = data.frame(SAMPLES, LAYOUTS, ...)

# Function for retrieving metadata
def get_metadata(sample, column):
    result = metadata.loc[metadata['sample'] == sample][column].values[0]
    return result

# Rules
rule all:
    input:
        expand('{sample}.bam', sample = SAMPLES)

rule: download:
    output:
        '{sample}.fastq.gz' for 'SINGLE' in metadata[LAYOUT],
        '{sample}_1.fastq.gz' for 'PAIRED' in metadata[LAYOUT]
    params:
        layout = lambda wildcards:
            get_metadata(wildcards.sample, layout_col)
    shell:
        'touch {output}'

rule align:
    input:
        '{sample}.fastq.gz' for 'SINGLE' in metadata[LAYOUT],
        '{sample}_1.fastq.gz' for 'PAIRED' in metadata[LAYOUT]
    params:
        layout = lambda wildcards:
            get_metadata(wildcards.sample, layout_col)
    output:
        '{sample}.bam'
    shell:
        'touch {output}'


In all code variations I have tried so far I either create ambiguous rules, create single-end reads for paired-end IDs (and vice versa) or it all just fails. I have come up with two very unsatisfactory solutions:


Have two entirely separate workflows, one working on the single-end inputs and the other for the paired-end, requiring the user to manually start both
Have a single workflow that separates the read layouts by adding a prefix 'single'/'paired' for every file in the workflow (i.e. single/{sample}.bam, etc.)


The first is unsatisfactory because the user has to start two different workflows, and the second because it adds a level of input data abstraction that is not present in the output data (since the output .bam-files are created regardless of the input read layouts through options in the sub-scripts I have).

Does somebody have a better idea as to how to achieve this? If it's unclear as to what I'm after I'd be happy to elaborate.
",1,-1,-1.0
46856698,Can't get this regex to work for wildcard_constraints in snakemake,"I have a workflow written in Snakemake for analyzing biological sequencing data. The workflow expects all the data files to be organized so that each raw read file begins with the type of assay (RNASeq, DNaseSeq, etc.) and this filename convention is maintained throughout all the files the workflow produces.

I have a rule to align the reads for data from every assay except RNASeq, and a different rule that should only be applied to RNASeq data. I'm been having trouble getting these rules set up so that snakemake knows which to use for which files.

In the RNASeq rule, I have this:

wildcard_constraints: library='RNASeq_.+'


and this works to make sure the RNASeq libraries use that rule. I'm still getting an error about ambiguous rules for other assays, though, so I think I need to constrain the wildcards in the other rules. I've tried this:

wildcard_constraints: library='(!?RNASeq)_.+'


to say match anything that doesn't have RNASeq, but while this works if I try it in the python interpreter, snakemake seems to not be able to match anything to this regex. I've tried it other ways, such as '[^R][^N][^A]' but can't get anything to work.

Since these regexes work when I try them manually against strings, I think there's either a bug with how snakemake applies regular expressions, or I don't understand something about how they are used by snakemake. I was assuming it was simply ""If this regex matches the wildcard string, use this rule. If it doesn't, don't use this rule.""
",-1,-1,-1.0
46948494,How to use environment per rule in snakemake?,"I have a workflow with several rules. One rule needs a py2 dependent tool. Thus I would like to specify a certain conda environment for this specific rule but I get the an error for this.

Environment: Debian 8, Conda 4.3.25 and snakemake 3.11.2

Here is how I run my workflow:


Create default environment with conda env create --name myworkflow --file environment.yml
source activate myworkflow
snakemake -ps workflow.snake --use-conda


It is trying to create the environment, but then runs into an error.
If I run 

   conda create env --name python2_tool --file environment2.yaml


from within the activated environment there is no error. But with the --prefix it is not working. 

So I am wondering what is going wrong here? 

The error looks as followed: 

ERROR conda.core.link:_execute_actions(337): An error occurred while installing package 'defaults::icu-54.1-0'
OSError(95, 'Operation not supported')            
Attempting to roll back.                 
OSError(95, 'Operation not supported') 


The rule causing problems looks like that:

rule annotation:
  input: ""{condition}.bed""
  output: ""{condition}_annotated.bed""
  threads: 8
  conda: 
    ""envs/environment2.yml""
  shell:
    ""wrapper.sh -i {input} -o {output} -c {threads}""

",-1,-1,-1.0
46605231,Time latency for snakemake,"I have this rule:

rule Bam_coverage:
    input: ""ALIGN/result/{case}_filter_dup.bam""
    output: ""result/{case}.bw""
    params: genome=config['reference']['genome_fasta'],
            name=lambda wildcards, input: input[0][:-4]
    threads: 6
    log: ""log/{case}.bamcoverage.report.txt""
    conda:
         ""envs/deeptools.yaml""
    shell:
        """"""
         bamCoverage -b  {input}  -o {output} -of bigwig -bs 50 -p {threads}  &amp;2&gt; {log}
        """"""


When I launch snakemake the program runs, but snakemake gives me this error:

Error in job Bam_coverage while creating output file result/MB.neg.27ac_27ac.bw.

MissingOutputException in line 21 of /home/maurizio/Desktop/TEST_chip/rules/deeptools.rules:
Missing files after 60 seconds:
result/MB.neg.27ac_27ac.bw
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Will exit after finishing currently running jobs.


However, the program is in background and after some time writes the file.
What is wrong?
",-1,-1,-1.0
47215594,How can Snakemake be made to update files in a hierarchical rule-based manner when a new file appears at the bottom of the hierarchy?,"I have a snakefile with dozens of rules, and it processes thousands of files.  This is a bioinformatics pipeline for DNA sequencing analysis.  Today I added two more samples to my set of samples, and I expected to be able to run snakemake and it would automatically determine which rules to run on which files to process the new sample files and all files that depend on them on up the hierarchy to the very top level.  However, it does nothing.  And the -R option doesn't do it either.

The problem is illustrated with this snakefile:

&gt; cat tst
rule A:
    output: ""test1.txt""
    input: ""test2.txt""
    shell: ""cp {input} {output}""

rule B:
    output: ""test2.txt""
    input: ""test3.txt""
    shell: ""cp {input} {output}""

rule C:
    output: ""test3.txt""
    input: ""test4.txt""
    shell: ""cp {input} {output}""

rule D:
    output: ""test4.txt""
    input: ""test5.txt""
    shell: ""cp {input} {output}""


Execute it as follows:

&gt; rm test*.txt
&gt; touch test2.txt
&gt; touch test1.txt
&gt; snakemake -s tst -F


Output is:

Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   A
    1

rule A:
    input: test2.txt
    output: test1.txt
    jobid: 0

Finished job 0.
1 of 1 steps (100%) done


Since test5.txt does not exist, I expected an error message to that effect, but it did not happen. And of course, test3.txt and test4.txt do not exist.

Furthermore, using -R instead of -F results in ""Nothing to be done.""
Using ""-R A"" runs rule A only.

This relates to my project in that it shows that Snakemake does not analyze the entire dependent tree if you tell it to build a rule at the top of the tree and that rule's output and input files already exist.  And the -R option does not force it either.  When I tried -F on my project, it started rebuilding the entire thing, including files that did not need to be rebuilt.

It seems to me that this is fundamental to what Snakemake should be doing, and I just don't understand it.  The only way I can see to get my pipeline to analyze the new samples is to individually invoke each rule required for the new files, in order.  And that is way too tedious and is one reason why I used Snakemake in the first place.

Help!
",1,-1,-1.0
47645990,How to keep the snakemake shell file while running in cluster,"While running my snakemake file in cluster I keep getting an error,

snakemake -j 20 --cluster ""qsub -o out.txt -e err.txt -q debug"" -s 
seadragon/scripts/viral_hisat.snake --config json=""&lt;input file&gt;"" 
output=""&lt;output file&gt;""


Now this gives me the follwing error, 

Error in job run_salmon while creating output file 
/gpfs/home/user/seadragon/output/quant_v2_4/test.
ClusterJobException in line 58 of seadragon/scripts/viral_hisat.snake
:
Error executing rule run_salmon on cluster (jobid: 1, external: 156618.sn-mgmt.cm.cluster, jobscript: /gpfs/home/user/.snakemake/tmp.j9nb0hyo/snakejob.run_salmon.1.sh). For detailed error see the cluster log.
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message


Now I don't find any way to track the error, since my cluster does not give me an way to store the log files, on the other hand /gpfs/home/user/.snakemake/tmp.j9nb0hyo/snakejob.run_salmon.1.sh file is deleted immediately after finishing. 
Please let me know if there is an way to keep this shell file even if the snakemake fails. 
",-1,-1,-1.0
48009091,Can't import python module using a conda environment in Snakemake rule,"I created a Conda environment with python 3.5, in order to run Snakemake workflows. I'm using separate Conda environments in Snakemake rules. I would like to run one of them using python2 libs, however here I can't seem to import a specific module. 

This is my rule's environment:

channels:
  - conda-forge
dependencies:
  - zeep


My rule is calling a python script using python2:

python2 my_script.py


And the python script contains this import:

from zeep import Client


When I run the workflow I get this error:

ImportError: No module named zeep


Could this be an issue related to python versions? 
",-1,-1,-1.0
48263541,"How to use expand in snakemake when some combinations of wildcards are not desired (missing input files), with a ""merge"" rule?","this is a slightly more complicated case than the one reported here.
My input files are the following:

ont_Hv1_2.5+.fastq                                                                                                                                                                              
ont_Hv2_2.5+.fastq                                                                                                                                                                              
pacBio_Hv1_1-1.5.fastq                                                                                                                                                                          
pacBio_Hv1_1.5-2.5.fastq                                                                                                                                                                        
pacBio_Hv1_2.5+.fastq                                                                                                                                                                           
pacBio_Hv2_1-1.5.fastq                                                                                                                                                                          
pacBio_Hv2_1.5-2.5.fastq
pacBio_Hv2_2.5+.fastq
pacBio_Mv1_1-1.5.fastq
pacBio_Mv1_1.5-2.5.fastq
pacBio_Mv1_2.5+.fastq


I would like to process only  existing input files, i.e. automatically skip those wildcard combinations that correspond to non-existing input files.

My Snakefile looks like this:

import glob
import os.path
from itertools import product

#make wildcards regexps non-greedy:
wildcard_constraints:
    capDesign = ""[^_/]+"",
    sizeFrac = ""[^_/]+"",
    techname = ""[^_/]+"",

# get TECHNAMES (sequencing technology, i.e. 'ont' or 'pacBio'), CAPDESIGNS (capture designs, i.e. Hv1, Hv2, Mv1) and SIZEFRACS (size fractions) variables from input FASTQ file names:
(TECHNAMES, CAPDESIGNS, SIZEFRACS) = glob_wildcards(""{techname}_{capDesign}_{sizeFrac}.fastq"")
# make lists non-redundant:
CAPDESIGNS=set(CAPDESIGNS)
SIZEFRACS=set(SIZEFRACS)
TECHNAMES=set(TECHNAMES)

# make list of authorized wildcard combinations (based on presence of input files)
AUTHORIZEDCOMBINATIONS = []
for comb in product(TECHNAMES,CAPDESIGNS,SIZEFRACS):
    if(os.path.isfile(comb[0] + ""_"" + comb[1] + ""_"" + comb[2] + "".fastq"")):
        tup=((""techname"", comb[0]),(""capDesign"", comb[1]),(""sizeFrac"", comb[2]))
        AUTHORIZEDCOMBINATIONS.append(tup)

# Function to create filtered combinations of wildcards, based on the presence of input files.
# Inspired by:
# https://stackoverflow.com/questions/41185567/how-to-use-expand-in-snakemake-when-some-particular-combinations-of-wildcards-ar
def filter_combinator(whitelist):
    def filtered_combinator(*args, **kwargs):
        for wc_comb in product(*args, **kwargs):
            for ac in AUTHORIZEDCOMBINATIONS:
                if(wc_comb[0:3] == ac):
                    print (""SUCCESS"")
                    yield(wc_comb)
                    break
    return filtered_combinator

filtered_product = filter_combinator(AUTHORIZEDCOMBINATIONS)

rule all:
    input:
        expand(""{techname}_{capDesign}_all.readlength.tsv"", filtered_product, techname=TECHNAMES, capDesign=CAPDESIGNS, sizeFrac=SIZEFRACS)

#get read lengths for all FASTQ files:
rule getReadLength:
    input: ""{techname}_{capDesign}_{sizeFrac}.fastq""
    output: ""{techname}_{capDesign}_{sizeFrac}.readlength.tsv""
    shell: ""fastq2tsv.pl {input} | awk -v s={wildcards.sizeFrac} '{{print s\""\\t\""length($2)}}' &gt; {output}"" #fastq2tsv.pl converts each FASTQ record into a tab-separated line, with the sequence in second field

#combine read length data over all sizeFracs of a given techname/capDesign combo:
rule aggReadLength:
    input: expand(""{{techname}}_{{capDesign}}_{sizeFrac}.readlength.tsv"", sizeFrac=SIZEFRACS)
    output: ""{techname}_{capDesign}_all.readlength.tsv""
    shell: ""cat {input} &gt; {output}""


Rule getReadLength collects read lengths for each input FASTQ (i.e. for each techname, capDesign, sizeFrac combo). 

Rule aggReadLength merges read length statistics generated by getReadLength, for each techname, capDesign combo.

The workflow fails with the following message:

Missing input files for rule getReadLength:
ont_Hv1_1-1.5.fastq


So it seems that the wildcard combination filtering step applied to the target is not propagated to all upstream rules it depends on. Anyone knows how to make it so?

(Using Snakemake version 4.4.0.)

Thanks a lot in advance
",-1,the,-1.0
48414742,Snakemake in cluster mode with --no-shared-fs: How to set cluster-status,"I'm running Snakemake in a cluster environment and would like to use S3 as shared file system for writing output files.

Options --default-remote-provider, --default-remote-prefix and --no-shared-fs are set accordingly. The cluster uses UGE as scheduler, so setting --cluster is straightforward, but how do I set --cluster-status, whose use is enforced when using --no-shared-fs?

My best guess was a naive --cluster-status ""qstat -j"" which resulted in 

subprocess.CalledProcessError: Command 'qstat Your job 2 (""snakejob.bwa_map.1.sh"") has been submitted' returned non-zero exit status 1. 


So I guess my question is, how do I get the actual jobid in there?

Thanks!

Andreas

EDIT 1:
I found https://groups.google.com/forum/#!topic/snakemake/7cyqAIfgeq4, so cluster-status has to be a script. So I wrote a Python script that is able to parse the above line, however snakemake still fails with:

/bin/sh: -c: line 0: syntax error near unexpected token `('
/bin/sh: -c: line 0: `/home/ec2-user/clusterstatus.py Your job 2 (""snakejob.bwa_map.1.sh"") has been submitted'
...
subprocess.CalledProcessError: Command '/home/ec2-user/clusterstatus.py 
Your job 2 (""snakejob.bwa_map.1.sh"") has been submitted' returned non-zero exit status 1.

",-1,-1,-1.0
48440439,Snakemake: R script fails (almost) immediately,"Once more I'm encountering an error in my snakemake workflow that doesn't make any sense to me.

This is the error I get:

[Thu Jan 25 10:47:00 2018] Building DAG of jobs...
[Thu Jan 25 10:47:01 2018] Provided cores: 24
[Thu Jan 25 10:47:01 2018] Rules claiming more threads will be scaled down.
[Thu Jan 25 10:47:01 2018] Job counts:
[Thu Jan 25 10:47:01 2018]  count   jobs
[Thu Jan 25 10:47:01 2018]  1   merging_seurat
[Thu Jan 25 10:47:01 2018]  1

[Thu Jan 25 10:47:01 2018] Job 0: --- Merging samples using seurat.

Error in setClass(""Snakemake"", slots = c(input = ""list"", output = ""list"",  : 
  unused argument(s) (slots = c(input = ""list"", output = ""list"", params = ""list"", wildcards = ""list"", threads = ""numeric"", log = ""list"", resources = ""list"", config = ""list"", rule = ""character""))
Execution halted
[Thu Jan 25 10:47:02 2018] Error in rule merging_seurat:
[Thu Jan 25 10:47:02 2018]     jobid: 0
[Thu Jan 25 10:47:02 2018]     output: merging_seurat/12_top10_heatmap_all_wilcox.pdf, merging_seurat/13_top10_heatmap_all_roc.pdf, merging_seurat/merging_seurat.RData

[Thu Jan 25 10:47:02 2018] RuleException:
[Thu Jan 25 10:47:02 2018] CalledProcessError in line 372 of .../snakemake_pipeline/Snakefile:
[Thu Jan 25 10:47:02 2018] Command ' set -euo pipefail;  Rscript .../snakemake_pipeline/scripts/.snakemake.jv8ijpiw.merging_seurat.R ' returned non-zero exit status 1
[Thu Jan 25 10:47:02 2018]   File "".../snakemake_pipeline/Snakefile"", line 372, in __rule_merging_seurat
[Thu Jan 25 10:47:02 2018]   File "".../tools/anaconda3/envs/Seurat/lib/python3.5/concurrent/futures/thread.py"", line 55, in run
[Thu Jan 25 10:47:02 2018] Will exit after finishing currently running jobs.
[Thu Jan 25 10:47:02 2018] Exiting because a job execution failed. Look above for error message
[Thu Jan 25 10:47:02 2018] Complete log: .../snakemake_6/.snakemake/log/2018-01-25T104700.498155.snakemake.log


This is the rule in question:

rule merging_seurat:
  input:   expand(""{sample}/molecule_count/counts_wide.tsv"", sample=config[""samples""]),
  output:  ""merging_seurat/12_top10_heatmap_all_wilcox.pdf"",
           ""merging_seurat/13_top10_heatmap_all_roc.pdf"",
           ""merging_seurat/merging_seurat.RData""
  message: ""--- Merging samples using seurat.""
  script:   ""scripts/merging_seurat.R""


Here we have the top of the R script that is still executed:

sink('merging_seurat/output.txt')
print(installed.packages())
print(sessionInfo())
sink()


And these are the following lines in the R script that are not executed anymore:

library('Seurat')
library('dplyr')
library('org.Hs.eg.db')


Because I logged the packages that are available, I know that the three packages that should be loaded are also installed so it shouldn't fail because of that. In fact, commenting these lines out doesn't change anything, the script still breaks and the log message that should come right after loading the packages is not written to the log file.

Finally, this is the command that I use to run snakemake:

snakemake --use-conda \
--latency-wait 90 \
--rerun-incomplete \
--keep-going \
--timestamp \
--cluster-config SGE.json \
--cluster ""qsub -cwd -N {cluster.name} -l h_vmem={cluster.h_vmem},h_stack=256M -o {cluster.stdout}{cluster.name}.o -e {cluster.stderr}{cluster.name}.e -m {cluster.mailtype} -M {cluster.mailuser}"" \
-j 8 \
--directory .../snakemake_6


Does anybody have a clue what the error message could mean? unused argument of the snakemake R object is strange because I'm calling multiple parameters later on in the script.

What is even more strange is that I have another R script that works with different packages but has very similar first lines runs fine. I remember that in the beginning I had the same problem with that script (same error message) but don't remember how I solved it.

Any help is well appreciated.
",1,-1,-1.0
48443572,Using multiple filenames as wildcards in Snakemake,"I am trying to create a rule to implement bedtools in snakemake, which will closest a file with bunch of files in another directory.

What I have is, under /home/bedfiles directory, 20 bed files:

1A.bed , 2B_83.bed , 3f_33.bed ...


What I want is, under /home/bedfiles directory, 20 modified bed files:

1A_modified,  2B_83_modified , 3f_33_modified ...


So the bash command would be :

filelist='/home/bedfiles/*.bed'
for mfile in $filelist;
do
bedtools closest -a /home/other/merged.txt -b ${mfile} &gt; ${mfile}_modified


So this command would make files with _modified extension, in /home/bedfiles directory.

I want to implement this with Snakemake, however I keep having a syntax error, that I have no idea of how to fix. My trial is:

Step1:Getting the first part of bed files in the directory

FIRSTPART = [f.split(""."")[0] for f in os.listdir(""/home/bedfiles"") if f.endswith('.bed')]


Step2: Defining the output name and folder

MODIFIED = expand(""/home/bedfiles/{first}_modified"", first=FIRSTPART)


Step3: Writing this in rule all:

rule all:
   input: MODIFIED


Step4: Making a specific rule to implement 'bedtools closest'

rule closest:

    input:
        input1 = ""/home/other/merged.txt"" , \
        input2 = expand(""/home/bedfiles/{first}.bed"", first=FIRSTPART) 

    output:
        expand(""/home/bedfiles/{first}_modified"", first=FIRSTPART)  

    shell:
        """""" bedtools closest -a {input.input1} -b {input.input2} &gt; {output} """"""


And it throws me the error at the line for rule all,input:

invalid syntax


Do you know how to overpass this error or any other way to implement it?

PS : Writing the names of the files one by one is not possible.
",-1,1,-1.0
48488641,sh: command not found when using Snakemake profiles,"I'm trying to create a new profile for snakemake to easy run workflows on our cluster system. I followed the examples available in the Snakemake-Profiles repo, and have the following directory structure:

project/
- my_cluster/
-- config.yaml
-- cluster-submit.py
-- jobscript.sh
- Snakefile
- (other files)


Thus, the my_cluster directory contains the cluster profile. Excerpt of config.yaml:

cluster: ""cluster-submit.py {dependencies}""
jobscript: ""jobscript.sh""
jobs: 1000
immediate-submit: true
notemp: true


If I try to run my workflow as follows:

snakemake --profile my_cluster target


I get the following error: sh: command not found: cluster-submit.py, for each rule snakemake tries to submit. Which may not be super surprising considering the cluster-submit.py script lives in a different directory. I'd have thought that snakemake would handle the paths when using profiles. So am I forgetting something? Did I misconfigure something? The cluster-submit.py file has executable permissions and when using absolute paths in my config.yaml this works fine.
",-1,-1,-1.0
48554664,Using other files based on dynamic outputs with mixed non-dynamic wildcards in snakemake,"I am trying to use a Snakefile that does something like this:

rule split_files:
    input:
        '{pop}.bam'
    output:
        dynamic('{pop}_split/{chrom}.sam')
    shell:
        ""something""

rule work:
    input:
        sam='{pop}_split/{chrom}.sam',
        snps='snps/{chrom}_snps'
    output:
        '{pop}_split/{chrom}_parsed.sam'
    shell:
        ""something""

rule combine:
    input:
        dynamic('{pop}_split/{chrom}_parsed.sam')
    output:
        '{pop}_merged.sam'
    shell:
        ""something""


This results in the error:

Missing input files for rule work:
snps/__snakemake_dynamic___snps


Adding dynamic to both of the inputs for the work rule results in the same error.

I need to do this because some populations have chrY and others do not, so I can't just expand with a chromosome list (actually I can with another work around that I am currently using, but it is cumbersome).
",-1,-1,-1.0
48118174,Missing wildcards in S4 snakemake Object in R,"I'm running a workflow with a main Snakefile including rules from the rules folder and calling rscripts from those included rules.

Here are a few lines and their specific files:

Snakefile:

samples = pd.read_table(""samples.csv"", header=0, sep=',', index_col=0)
rule extract:
    input:
        'summary/umi_expression_matrix.tsv'
include: ""rules/extract_expression_single.smk""


rules/extract_expression_single.smk:

rule merge_umi:
    input:
        expand('summary/{sample}_umi_expression_matrix.tsv', sample=samples.index)
    output:
        'summary/umi_expression_matrix.tsv'
    script:
        ""../scripts/merge_counts_single.R""


scripts/merge_counts_single.R:

samples = read.csv('samples.csv', header=TRUE, stringsAsFactors=FALSE)$samples
read_list = c()
for (i in 1:length(samples)){
    temp_matrix = read.table(snakemake@input[[i]][1], header=T, stringsAsFactors = F)
    cell_barcodes = colnames(temp_matrix)[-1]
    colnames(temp_matrix) = c(""GENE"",paste(samples[i], cell_barcodes, sep = ""_""))
    read_list=c(read_list, list(temp_matrix))
}

# Little function that allows to merge unequal matrices
merge.all &lt;- function(x, y) {
  merge(x, y, all=TRUE, by=""GENE"")
}

read_counts &lt;- Reduce(merge.all, read_list)
read_counts[is.na(read_counts)] = 0
rownames(read_counts) = read_counts[,1]
read_counts = read_counts[,-1]
write.table(read_counts, file=snakemake@output[[1]], sep='\t')


The ""clean"" way to do it would be to call snakemake@wildcard.sample to attribute sample names to the script. But for some reason snakemake@wildcards is an empty vector.
In python:

print(type(snakemake.wildcards))
print(snakemake.wildcards)
print('done')


gives:

&lt;class 'snakemake.io.Wildcards'&gt;

done


which means it's also empty.
So right now I have to rely on getting back to the samples.csv file and getting the sample names there. I will also have to double check matching indexes maybe using greps, don't want the samples and the files to get mixed up.

Any idea why this is happening?

Update:

I've tried adding the sample_name as params to see if this would work and it actually does.

rule merge_umi:
    input:
        expand('summary/{sample}_umi_expression_matrix.tsv', sample=samples.index)
    params:
        sample_name = lambda wildcards: samples.index
    output:
        'summary/umi_expression_matrix.tsv'
    script:
        ""../scripts/merge_counts_single.R""


I'm gonna use this for now, but my guess is there is still an issue with the scope of wildcards in included rules. Or maybe I'm doing it wrong.
",-1,1,-1.0
48671421,Q : Target rules may not contain wildcards Error in Snakemake - No wildcards in Target?,"I'm trying to create a snakemake pipeline whose outputs are determined by the set of sequencing files present in a particular folder. The structure of my filepath here is something like:

project_dir
&gt;    Snakefile
&gt;    code
&gt;        python_scripts
&gt;            ab1_to_fastq.py
&gt;    data
&gt;        1.ab1_files
&gt;            A.ab1
&gt;            B.ab1
&gt;            C.ab1
&gt;        2.fastq_files


Here's the code for my actual Snakefile

import glob
import os

def collect_reads():
    ab1_files = glob.glob(""data/1.ab1_files/*.ab1"")
    ab1_files.sort()
    ab1_reads = [ab1_file.split('/')[-1].replace('.ab1', '') for ab1_file in ab1_files]
    return ab1_reads

READS = collect_reads()
print(expand(""data/2.fastq_files/{read}.fastq"", read=READS))

rule convert_ab1_to_fastq:
    input:
        ab1=""data/1.ab1_files/{read}.ab1""
    output:
        fastq=""data/2.fastq_files/{read}.fastq""
    shell:
        ""python code/python_scripts/ab1_to_fastq.py --ab1 {input.ab1} --fastq {output.fastq}""

rule all:
    input:
        fastq=expand(""data/2.fastq_files/{read}.fastq"", read=READS)


My understanding is that all should be my target rule, and that the input variable of fastq in that rule evaluates to

['data/2.fastq_files/A.fastq', 'data/2.fastq_files/B.fastq', 'data/2.fastq_files/C.fastq']


And this seems to be confirmed by the print output in the pipeline when I run my script. However, I get the error WorkflowError: Target rules may not contain wildcards. Please specify concrete files or a rule without wildcards. whenever I run this script. 

Strangely, I can copy one of the paths from the list generated by expand to call snakemake directly, e.g. snakemake data/2.fastq_files/A.fastq and the pipeline completes successfully. 

What am I missing?
",-1,-1,-1.0
48747790,Snakemake: Error with unexisting wildcard,"
Edit 2 : I figure it out. I posted my answer as reply.
Edit 1 : I added a beginning of solution at the end of the question following @bli advices and https://stackoverflow.com/a/41185568/1025741

I'm writing a snakemake file where I parse a samplesheet file (defined in the yaml configuration file) in order to concatenate files listed in this samplesheet.
Samplesheet looks like:
sample  unit    fq1 fq2
A   lane1   A.l1.1.R1.txt   A.l1.1.R2.txt
A   lane1   A.l1.2.R1.txt   A.l1.2.R2.txt
A   lane2   A.l2.R1.txt A.l2.R2.txt
B   lane1   B.l1.R1.txt B.l1.R2.txt

The idea is to concatenate files (listed in fq1 and fq2) from the same sample and sample unit. In this case:

A.l1.1.R1.txt and A.l2.2.R1.txt will be concatenated
A.l1.1.R2.txt and A.l2.2.R2.txt will be concatenated

The other files will not be concatenated but will also be reported in this directory structure:
{sample}/
    {sample}_{unit}_merged_R1.txt
    {sample}_{unit}_merged_R2.txt

So at the end for this example I should have:
A/
  A_lane1_merged_R1.txt
  A_lane1_merged_R2.txt
  A_lane2_merged_R1.txt
  A_lane2_merged_R2.txt
B/
  B_lane1_merged_R1.txt
  B_lane1_merged_R2.txt

Here's my snakemake file to execute such task:
import pandas as pd
shell.executable(&quot;bash&quot;)

configfile: &quot;config.yaml&quot;

# open samplesheet
units = pd.read_table(config[&quot;units&quot;], dtype=str)
units = units.set_index([&quot;sample&quot;, &quot;unit&quot;])


rule all:
    input:
        expand(&quot;{sample}/{sample}_{unit}_merge_R1.txt&quot;,
            sample=units.index.get_level_values('sample').unique(),
            unit=units.index.get_level_values('unit').unique()),
        expand(&quot;{sample}/{sample}_{unit}_merge_R2.txt&quot;,
            sample=units.index.get_level_values('sample').unique(),
            unit=units.index.get_level_values('unit').unique())


def get_fastq_r1(wildcards):
    return units.loc[(wildcards.sample, wildcards.unit), [&quot;fq1&quot;]].dropna().values.flatten()

def get_fastq_r2(wildcards):
    return units.loc[(wildcards.sample, wildcards.unit), [&quot;fq2&quot;]].dropna().values.flatten()


rule merge:
    input:
        r1 = get_fastq_r1,
        r2 = get_fastq_r2
    output:
        &quot;{sample}/{sample}_{unit}_merge_R1.txt&quot;,
        &quot;{sample}/{sample}_{unit}_merge_R2.txt&quot;
    shell:
        &quot;&quot;&quot;
        echo {input.r1} &gt; {sample}/{sample}_{unit}_merge_R1.txt
        echo {input.r2} &gt; {sample}/{sample}_{unit}_merge_R2.txt
        &quot;&quot;&quot;

and the config.yaml :
units: units.tsv

But I have an error as I don't have a sample B with unit = lane2:
InputFunctionException in line 29 of /home/nrosewick/Documents/analysis/pilot_data_ADX17009/workflow/test_snakemake/Snakefile:
KeyError: ('B', 'lane2')
Wildcards:
sample=B
unit=lane2

Is there a way/trick to avoid this kind of error ?
Thanks

Beginning of solution

Following @bli advice I used a filtered version of itertools.product by wrapping it in a higher-order generator that checks that the yielded combination of wildcards is among a pre-established list:
import pandas as pd
shell.executable(&quot;bash&quot;)

configfile: &quot;config.yaml&quot;

### 
from itertools import product

def filter_combinator(combinator, inlist):
    def filtered_combinator(*args, **kwargs):
        for wc_comb in combinator(*args, **kwargs):
            # Use frozenset instead of tuple
            # in order to accomodate
            # unpredictable wildcard order
            if frozenset(wc_comb) in inlist:
                yield wc_comb
    return filtered_combinator

# open samplesheet
units = pd.read_table(config[&quot;units&quot;], dtype=str)

# list of pair sample-unit included in the samplesheet
inList={
    frozenset({(&quot;sample&quot;, &quot;A&quot;), (&quot;unit&quot;, &quot;lane1&quot;)}),
    frozenset({(&quot;sample&quot;, &quot;A&quot;), (&quot;unit&quot;, &quot;lane2&quot;)}),
    frozenset({(&quot;sample&quot;, &quot;B&quot;), (&quot;unit&quot;, &quot;lane1&quot;)})}

# set df index
units = units.set_index([&quot;sample&quot;, &quot;unit&quot;])

# build new iterator
filtered_product = filter_combinator(product, inList)

rule all:
    input:
        expand(&quot;{sample}/{sample}_{unit}_merge_R1.txt&quot;,
            filtered_product,
            sample=units.index.get_level_values('sample').unique().values,
            unit=units.index.get_level_values('unit').unique().values),
        expand(&quot;{sample}/{sample}_{unit}_merge_R2.txt&quot;,
            filtered_product,
            sample=units.index.get_level_values('sample').unique().values,
            unit=units.index.get_level_values('unit').unique().values)


def get_fastq_r1(wildcards):
    return units.loc[(wildcards.sample, wildcards.unit), [&quot;fq1&quot;]].dropna().values.flatten()

def get_fastq_r2(wildcards):
    return units.loc[(wildcards.sample, wildcards.unit), [&quot;fq2&quot;]].dropna().values.flatten()

rule merge:
    input:
        r1 = get_fastq_r1,
        r2 = get_fastq_r2
    output:
        &quot;{sample}/{sample}_{unit}_merge_R1.txt&quot;,
        &quot;{sample}/{sample}_{unit}_merge_R2.txt&quot;
    message:
        &quot;test&quot;
    shell:
        &quot;&quot;&quot;
        cat {input.r1} &gt; {sample}/{sample}_{unit}_merge_R1.txt
        cat {input.r2} &gt; {sample}/{sample}_{unit}_merge_R2.txt
        &quot;&quot;&quot;

But it returns me an error when running snakemake -n:
Job 1: test

RuleException in line 53 of /home/nrosewick/Documents/analysis/pilot_data_ADX17009/workflow/test_snakemake/Snakefile:
NameError: The name 'sample' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}

Any clue?
",1,-1,-1.0
48784360,Making snakemake use as input 2 outputs of a rule with different names,"Making snakemake use as input 2 outputs of a rule with different names

I am making a snakemake pipeline in which I use strelka to compare a tumor and a normal sample. In this case I want to compare the first element of  GERMLINE = (""PT1"", ""S6"", ""S1”) to the first one of tumor TUMOR = (""T5"", ""T7"", ""T20"")


PT1 vs. T5 
S6 vs. T7
S1 vs. T20


The pipeline works well for the initial rules: folders, strelkaconfig and strelkarun. The issue is on the postprocessing of the output of strelka as I want to do the same for both outputs:


somatic.snvs.vcf
somatic.indels.vcf


However, I don't know how to make snakemake understand that it should do the same for both without repeating the rule. I have tried to do the following:

GERMLINE = (""PT1"", ""S6"", ""S1"")
TUMOR = (""T5"", ""T7"", ""T20"")
ANALYSIS = ""OUTPUT_PATH""
TYPEVAR = [""snvs"",""indels""]
INDGATK = ""ref""

rule all:
    input:
        [ANALYSIS +""/{}_vs_{}/Stelka/results/variants/somatic.snvs.vcf"".format(sample_g, sample_t)
         for (sample_g, sample_t) in zip(GERMLINE, TUMOR)],
        [ANALYSIS +""/{}_vs_{}/Stelka/runWorkflow.py"".format(sample_g, sample_t)
         for (sample_g, sample_t) in zip(GERMLINE, TUMOR)],
        [ANALYSIS +""/{}_vs_{}/Stelka/results/variants/somatic.{}_Filtered"".format(sample_g, sample_t,typevar)
         for (sample_g, sample_t,typevar)
         in zip(GERMLINE*len(TUMOR), TUMOR*len(TUMOR),sorted(TYPEVAR*len(TUMOR)))]


# Make folders
rule folders:
    input:
        g = ""{samples_g}.bam"",
        t = ""{samples_t}.bam""
    output: 
        gen = ""/{samples_g}_vs_{samples_t}"",
        strelka = ""/{samples_g}_vs_{samples_t}/Stelka/""
    run: 
        '''mkdir {output.gen}
        mkdir {output.strelka}'''

# Strelka configuration
rule strelkaconfig:
    input:
        g = ""{samples_g}.bam"",
        t = ""{samples_t}.bam"",
        out_dir = ANALYSIS + ""/{samples_g}_vs_{samples_t}/Stelka/""
    output:
        wfs = ANALYSIS + ""/{samples_g}_vs_{samples_t}/Stelka/runWorkflow.py""
    params:
        ref = INDGATK
    shell:
        ""python configureStrelkaSomaticWorkflow.py --normalBam {input.g} --tumorBam {input.t} --referenceFasta {params.ref} --runDir {input.out_dir} ""

# Strelka run
rule strelkarun:
    input:
        wfs = ANALYSIS + ""/{samples_g}_vs_{samples_t}/Stelka/runWorkflow.py""
    output:
        outsnvs = ANALYSIS + ""/{samples_g}_vs_{samples_t}/Stelka/results/variants/somatic.snvs.vcf"",
        outindels = ANALYSIS + ""/{samples_g}_vs_{samples_t}/Stelka/results/variants/somatic.indels.vcf""
    shell:
        ""python {input.wfs}""

# POSTPROCESSING
rule vcfp:
    input: ANALYSIS + ""/{samples_g}_vs_{samples_t}/Stelka/results/variants/somatic.{typevar}.vcf""
    output: ANALYSIS + ""/{samples_g}_vs_{samples_t}/Stelka/results/variants/somatic.{typevar}_Filtered.vcf""
    shell:
        ""java -jar StrelkaVCFParser -v {input} ""


But when I dry run I get this error:

MissingInputException in line 15 of pipe:
Missing input files for rule folders:
T7/Stelka/results/variants/somatic.indels_Filtered.bam

",-1,-1,-1.0
48932439,MissingRuleException in Snakemake after code changes,"I have these two rules:

all_participants = ['01','03','04','05','06','07','08']
rule all:
    input: expand(""data/interim/tables/screen/p{participant_id}.csv"",participant_id=all_participants)

rule extract_screen_table:
    output: ""data/interim/tables/screen/p{participant_id}.csv""
    shell: ""python src/data/sql_table_to_csv.py --table screen""


If I execute snakemake everything works, but if I change the code and execute: snakemake -n -R 'snakemake --list-code-changes' I get this error:

Building DAG of jobs...
MissingRuleException:
No rule to produce snakemake --list-code-changes (if you use input functions make sure that they don't raise unexpected exceptions).


The output of snakemake --list-code-change is: 

Building DAG of jobs...
data/interim/tables/screen/p03.csv


which I reckon it shouldn't be, and I should get the python script instead.
",-1,-1,-1.0
48993241,Varying (known) number of outputs in Snakemake,"I have a Snakemake rule that works on a data archive and essentially unpacks the data in it. The archives contain a varying number of files that I know before my rule starts, so I would like to exploit this and do something like

rule unpack:
    input: '{id}.archive'
    output: 
        lambda wildcards: ARCHIVE_CONTENTS[wildcards.id]


but I can't use functions in output, and for good reason. However, I can't come up with a good replacement. The rule is very expensive to run, so I cannot do

rule unpack:
    input: '{id}.archive'
    output: '{id}/{outfile}'


and run the rule several times for each archive. Another alternative could be

rule unpack:
    input: '{id}.archive'
    output: '{id}/{outfile}'
    run:
        if os.path.isfile(output[0]):
            return
        ...


but I am afraid that would introduce a race condition.

Is marking the rule output with dynamic really the only option? I would be fine with auto-generating a separate rule for every archive, but I haven't found a way to do so.
",1,-1,-1.0
49139395,Set cluster core per rule in Snakemake,"I need to download hundreds of large files, and run each of them through my snakemake pipeline. The file download is fast compared to my downstream pipeline. I'd like to limit the number of parallel downloads to 5, but allow the downsteam processing to use 100 cores. In snakemake, is there a way to limit the number of cores used by a certain rule? I picture 5 cores constantly grabbing data, while my other cores are working on the data I've already downloaded. If I run snakemake as usual with 100 cores, it will try to download all files at once, and overload the server. 
I already tried to do it by adding 'threads:1' to the rule but It does not work as it is supposed to. I suppose that by adding 'threads:1' to the rule, it should return the same results as when '-j 1' option is used in the command line for that rule, but they return different results.  
",-1,1,-1.0
49178033,Snakemake: Avoid removing output files before executing the shell command,"Is there a possibility to avoid that the output files defined in a snakemake rule are deleted before executing the shell command? I found a description of this behaviour here: http://snakemake.readthedocs.io/en/stable/project_info/faq.html#can-the-output-of-a-rule-be-a-symlink

What I am trying to do is definining a rule for a list of input and a list of output files (N:M relation). This rule should be triggered if one of the input files has changed. The python script which is called in the shell command then creates only those output which do not exist or whose content has changed in comparison to the already existing files (i.e. a change detection is implemented inside the python script). I expected that something like the following rule should solve this, but as the output.jsons are deleted before running the python script, all output.jsons will be created with a new timestamp instead of only those which have changed.

rule jsons:
""Create transformation files out of landmark correspondences.""
input:
    matchfiles = [""matching/%04i-%04i.h5"" % (SECTIONS[i], SECTIONS[i+1]) for i in range(len(SECTIONS)-1)]
output:
    jsons = [""transformation/{section}_transformation.json"".format(section=s) for s in SECTIONS]
shell:
    ""python create_transformation_jsons.py --matchfiles {input.matchfiles} --outfiles {output.jsons}""


If there is no possibility to avoid the deletion of output files in Snakemake, does anybody has another idea how to map this workflow into a snakemake rule without updating all output files?

Update: 

I tried to solve this problem by changing the Snakemake source code. I removed the line self.remove_existing_output() in jobs.py to avoid removing output files before executing a rule. Furthermore, I added the parameter no_touch=True when self.dag.check_and_touch_output() is called in executors.handle_job_success. This worked great as the output files now were neither removed before nor touched after the rule is executed. But following rules with json files as input are still triggered for each json file (even if it did not change) as Snakemake recognizes that the json file was defined as an output before and theremore must have been changed.
So I think avoiding the deletion of output files does not solve my problem, maybe a workaround - if existing - is the only way...

Update 2: 

I also tried to find a workaround without changing the Snakemake source code by changing the output path of the above defined jsons rule to transformation/tmp/... and adding the following rule:

def cmp_jsons(wildcards):
    section = int(wildcards.section)
    # compare json for given section in transformation/ with json in transformation/tmp/
    # return [] if json did not change
    # return path to tmp json filename if json has changed
rule copy:
    input:
        json_tmp = cmp_jsons
    output:
        jsonfile = ""transformation/B21_{section,\d+}_affine_transformation.json""
    shell:
        ""cp {input.json_tmp} {output.jsonfile}""


But as the input function is evaluated before the workflow starts, the tmp-jsons are either not yet existing or not yet updated by the jsons rule and therefore the comparison won't be correct.
",-1,-1,-1.0
49390202,Snakemake - Wildcards in input files cannot be determined from output files,"I am very new to snakemake and also not so fluent in python (so apologies this might be a very basic stupid question):

I am currently building a pipeline to analyze a set of bamfiles with atlas. These bamfiles are located in different folders and should not be moved to a common one. Therefore I decided to provide a samplelist looking like this (this is just an example, in reality samples might be on totaly different drives):

Sample     Path
Sample1    /some/path/to/my/sample/
Sample2    /some/different/path/


And load it in my config.yaml with:

sample_file: /path/to/samplelist/samplslist.txt


Now to my Snakefile:

import pandas as pd

#define configfile with paths etc.
configfile: ""config.yaml""

#read-in dataframe and define Sample and Path
SAMPLES = pd.read_table(config[""sample_file""])
BAMFILE = SAMPLES[""Sample""]
PATH = SAMPLES[""Path""]

rule all:
    input:
        expand(""{path}{sample}.summary.txt"", zip, path=PATH, sample=BAMFILE)

#this works like a charm as long as I give the zip-function in the rules 'all' and 'summary':

rule indexBam:
    input: 
        ""{path}{sample}.bam""
    output:
        ""{path}{sample}.bam.bai""
    shell:
        ""samtools index {input}""

#this following command works as long as I give the specific folder for a sample instead of {path}.
rule bamdiagnostics:
    input:
        bam=""{path}{sample}.bam"",
        bai=expand(""{path}{sample}.bam.bai"", zip, path=PATH, sample=BAMFILE)
    params:
        prefix=""analysis/BAMDiagnostics/{sample}""   
    output:
        ""analysis/BAMDiagnostics/{sample}_approximateDepth.txt"",
        ""analysis/BAMDiagnostics/{sample}_fragmentStats.txt"",
        ""analysis/BAMDiagnostics/{sample}_MQ.txt"",
        ""analysis/BAMDiagnostics/{sample}_readLength.txt"",
        ""analysis/BAMDiagnostics/{sample}_BamDiagnostics.log""
    message:
        ""running BamDiagnostics...{wildcards.sample}""
    shell:
        ""{config[atlas]} task=BAMDiagnostics bam={input.bam} out={params.prefix} logFile={params.prefix}_BamDiagnostics.log verbose""

rule summary:
    input:
        index=expand(""{path}{sample}.bam.bai"", zip, path=PATH, sample=BAMFILE),
        bamd=expand(""analysis/BAMDiagnostics/{sample}_approximateDepth.txt"", sample=BAMFILE)
    output:
        ""{path}{sample}.summary.txt""
    shell:
        ""echo -e '{input.index} {input.bamd}""


I get the error


  WildcardError in line 28 of path/to/my/Snakefile:
     Wildcards in input files cannot be determined from output files:
     'path'


Can anyone help me? 
- I tried to solve this problem with join, or creating input functions but I think I am just not skilled enough to see my error... 
- I guess the problem is, that my summary-rule does not contain the tuplet with the {path} for the bamdiagnostics-output (since the output is somewhere else) and cannot make the connection to the input file or so... 
- Expanding my input on bamdiagnostics-rule makes the code work, but of course takes every samples input to every samples output and creates a big mess:
In this case, both bamfiles are used for the creation of each outputfile. This is wrong as the samples AND the output are to be treated independently.
",1,-1,-1.0
49435692,Snakemake: missing file operand when using cluster,"I am deploying a snakemake workflow on a PBS cluster (PBSpro). I'm running into a problem where shell commands run on cluster nodes are failing due to missing arguments/operands to the shell command. Below is a minimal example that can reproduce the behavior I'm seeing:

rule all:
    input: 'foo.txt'

rule run_foo:
    output: 'foo.txt'
    shell: 'touch {output}'


Run from the command line as:

snakemake all


The workflow runs to completion without any errors. However, run from the command line as:

snakemake all --jobs 1 --cluster ""qsub -l select=1:ncpus=1 -l walltime=00:05:00 -A $PROJECT -q share -j oe""


The workflow fails and the produces a cluster log such as this:

Error: Image not found
Error in job run_foo while creating output file foo.txt.
RuleException:
CalledProcessError in line 7 of /glade2/scratch2/jhamman/Snakefile:
Command 'touch foo.txt' returned non-zero exit status 1.
  File ""/glade2/scratch2/jhamman/Snakefile"", line 7, in __rule_run_foo
  File ""/glade/u/home/jhamman/anaconda/envs/storylines/lib/python3.6/concurrent/futures/thread.py"", line 56, in run
Exiting because a job execution failed. Look above for error message


What appears to be happening is that the arguments to the command (in this case touch) are not applied, despite being listed traceback.

Is there a trick to passing shell commands to a cluster via snakemake that I am missing?
",-1,-1,-1.0
49574969,Snakemake: unknown output/input files after splitting by chromosome,"To speed up a certain snakemake step I would like to:


split my bamfile per chromosome using
bamtools split -in sample.bam --reference
this results in files named as sample.REF_{chromosome}.bam  
perform variant calling on each resulting in e.g. sample.REF_{chromosome}.vcf
recombine the obtained vcf files using vcf-concat (VCFtools) using
vcf-concat file1.vcf file2.vcf file3.vcf &gt; sample.vcf


The problem is that I don't know a priori which chromosomes may be in my bam file. So I cannot specify accurately the output of bamtools split. Furthermore, I'm not sure how to make the input of vcf-concat to take all vcf files.

I thought of using a samples.fofn and do something like

rule split_bam:
    input:
        bam = ""alignment/{sample}.bam"",
        pattern = ""alignment/{sample}.REF_""
    output:
        alignment/anon.splitbams.fofn
    log:
        ""logs/bamtools_split/{sample}.log""
    shell:
        ""bamtools split -in {input.bam} -reference &amp;&amp; \
         ls alignment/{input.pattern}*.bam | sed 's/.bam/.vcf/' &gt; {output}""


And use the same fofn for concatenating the obtained vcf files. But this feels like a very awkward hack and I'd appreciate your suggestions.



EDIT 20180409

As suggested by @jeeyem I tried the dynamic() functions, but I can't figure it out. 

My complete snakefile is on GitHub, the dynamic part is at lines 99-133.

The error I get is:

InputFunctionException in line 44 of /home/wdecoster/DR34/SV-nanopore.smk:
KeyError: 'anon___snakemake_dynamic'
Wildcards:
sample=anon___snakemake_dynamic

(with anon an anonymized {sample} identifier)

Running with --debug-dag gives (last parts before erroring):

candidate job cat_vcfs
    wildcards: sample=anon
candidate job nanosv
    wildcards: sample=anon___snakemake_dynamic, chromosome=_
candidate job samtools_index
    wildcards: aligner=split_ngmlr, sample=anon___snakemake_dynamic.REF__
candidate job split_bam
    wildcards: sample=anon___snakemake_dynamic, chromosome=_
InputFunctionException in line 44 of /home/wdecoster/DR34/SV-nanopore.smk:
KeyError: 'anon___snakemake_dynamic'
Wildcards:
sample=anon___snakemake_dynamic


Which shows that the wildcard is misinterpreted?



Cheers,
Wouter
",-1,-1,-1.0
49680732,Snakemake: I keep getting The 'conda' command is not available in $PATH. when running on SGE cluster,"I'm tearing my hair out here, hopefully someone can help me.

Running snakemake 4.8.0

I have a snakemake pipeline, which I run with two conda envs and --use-conda and it works fine when run as a standalone pipeline.

However, when I run on our cluster, I get the error:

""The 'conda' command is not available in $PATH.""

Now.  Anaconda is installed on our cluster, but we need to activate it on nodes with:

module load anaconda


Also, module is defined as a function, so I have source a couple of things first.  Therefore, at the top of my snakefile, I have:

shell.prefix(""source $HOME/.bashrc; source /etc/profile; module load anaconda; )


This doesn't solve the problem.

I even put module load anaconda in my .bashrc, and that still doesn't work.  Only on cluster execution, I get the error about conda not being found.

Other changes to my .bashrc are picked up and are picked up by snakemake, so I have no idea why it is having problems with conda.

I even created a conda env, loaded snakemake and conda into that env, activate the env in the submission script and in the Snakefile:

shell.prefix(""source $HOME/.bashrc; source /etc/profile; module load anaconda; source activate MAGpy-3.5; "")


And it still says ""The 'conda' command is not available in $PATH.""

Literally tearing my hair out.

As an aside, I submit using qsub -S /bin/bash and also use shell.executable(""/bin/bash"") but the temp shell scripts created in .snakemake are run by /bin/sh - is that expected?

Please help me!
",-1,-1,-1.0
49702056,Singularity & Snakemake - MissingOutputException,"I am combining Singularity &amp; Snakemake to create a workflow for some sequencing data. I modeled my pipeline after this git project https://github.com/sci-f/snakemake.scif. The version of the pipeline that does not use Singularity runs absolutely fine. The version that uses Singularity always stops after the first rule with the following error:

$ singularity run --bind data/raw_data/:/scif/data/ /gpfs/data01/heinzlab/home/cag104/bin/chip-seq-pipeline/chip-seq-pipeline-hg38.simg run snakemake all
[snakemake] executing /bin/bash /scif/apps/snakemake/scif/runscript all
Copying Snakefile to /scif/data
Copying config.yaml to /scif/data
Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       all
        1       bowtie2_mapping
        1       create_bigwig
        1       create_tag_directories
        1       fastp
        1       fastqc
        1       quality_metrics
        1       samtools_index
        8

rule fastp:
    input: THP-1_PU1-cMyc_PU1_sc_S40_R1_001.fastq.gz
    output: fastp/THP-1_PU1-cMyc_PU1_sc_S40_R1_001.fastp.fastq.gz, fastp_report/THP-1_PU1-cMyc_PU1_sc_S40_R1_001.html, fastp_report/THP-1_PU1-cMyc_PU1_sc_S40_R1_001.json
    log: logs/fastp/THP-1_PU1-cMyc_PU1_sc_S40_R1_001.log
    jobid: 7
    wildcards: sample=THP-1_PU1-cMyc_PU1_sc_S40_R1_001

usage: scif run [-h] [cmd [cmd ...]]

positional arguments:
  cmd         app and optional arguments to target for the entry

optional arguments:
  -h, --help  show this help message and exit
Waiting at most 5 seconds for missing files.
MissingOutputException in line 16 of /scif/data/Snakefile:
Missing files after 5 seconds:
fastp/THP-1_PU1-cMyc_PU1_sc_S40_R1_001.fastp.fastq.gz
fastp_report/THP-1_PU1-cMyc_PU1_sc_S40_R1_001.html
fastp_report/THP-1_PU1-cMyc_PU1_sc_S40_R1_001.json
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Will exit after finishing currently running jobs.
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /scif/data/.snakemake/log/2018-04-06T224320.958604.snakemake.log


The directory however does create the fastp and fastp_report directories as well as the logs directory. I tried increasing the latency to 50 seconds, but I still get the same error.

Any ideas on what to try here?
",-1,-1,-1.0
49738091,Snakemake does not recognize symlinked files as input files,"I am running a snakemake pipeline on a couple of files that I symlinked from another directory.

However, it seems that the snakemake file does not recognize the input files when they are symlinked. Is this supposed to happen, or is there someway around this? I am getting a 'missing input files' error.

When I cp my input file into the exact same directory and run the script, it works fine. It's not a big deal to have to cp stuff around, but with a ton of data this might be a little more trouble than it's worth.

Version is 4.6.0
",-1,-1,-1.0
49804372,Snakemake 'MissingOutputException',"Input: A Snakefile that uses the SSP software to calculate various quality metrics for sequencing data. The input to SSP is a BAM file.

sample1.sorted.bam


Output: Various files, but the only one I care about is a file named {prefix}.stats.txt.

sample1.stats.txt


Snakefile: ($SCIF_DATA = /scif/data)

configfile: ""config.yaml""
workdir: ""/scif/data""

# define samples
SAMPLES, = glob_wildcards(""raw_data/{sample}.fastq.gz"")

rule all:
    input:
        expand(""processed_data/qc/{sample}/{sample}.stats.txt"", sample=SAMPLES),

rule quality_metrics:
    input:
        ""processed_data/{sample}.sorted.bam""
    params:
        prefix=""{sample}"",
        gt=""raw_data/hg38.chrom.sizes""
    output:
        ""processed_data/qc/{sample}/{sample}.stats.txt""
    shell:
        ""scif run ssp '-i $SCIF_DATA/{input} -o {params.prefix} --gt {params.gt} -p 50 --odir $SCIF_DATA/{params.prefix}'""


When I run ssp '-i sample1.sorted.bam -o sample1 --gt {params.gt} -p 50 --odir sample1 on the terminal, I get the correct output:

{path}/sample1/sample1.stats.txt


However when I run my snakemake workflow, I am getting the following error:

Waiting at most 5 seconds for missing files.
MissingOutputException in line 58 of /scif/data/Snakefile:
Missing files after 5 seconds:
processed_data/qc/THP-1_PU1-cMyc_PU1_sc_S40_R1_001/THP-1_PU1-cMyc_PU1_sc_S40_R1_001.stats.txt
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Will exit after finishing currently running jobs.
Shutting down, this might take some time.


Increasing the latency wait time does not help.

Any ideas?
",-1,-1,-1.0
50034797,Can Snakemake work if a rule's shell command is a cluster job?,"In below example, if shell script shell_script.sh sends a job to cluster, is it possible to have snakemake aware of that cluster job's completion? That is, first, file a should be created by shell_script.sh which sends its own job to the cluster, and then once this cluster job is completed, file b should be created. 

For simplicity, let's assume that snakemake is run locally meaning that the only cluster job originating is from shell_script.sh and not by snakemake .

localrules: that_job

rule all:
    input:
        ""output_from_shell_script.txt"",
        ""file_after_cluster_job.txt""

rule that_job:
    output:
        a = ""output_from_shell_script.txt"",
        b = ""file_after_cluster_job.txt""
    shell:
        """"""
        shell_script.sh {output.a}
        touch {output.b}
        """"""


PS - At the moment, I am using sleep command to give it a waiting time before the job is ""completed"". But this is an awful workaround as this could give rise to several problems.
",-1,1,-1.0
50497442,Conda Snakemake installation issue: UnsatisfiableError,"I am trying to install snakemake on linux machine. When running conda install -c bioconda snakemake I am getting the error:


  -bash-4.2$ conda install -c bioconda snakemake
  Solving environment: failed
  UnsatisfiableError: The following specifications were found to be in conflict:
    - backports.functools_lru_cache
    - snakemake
  Use ""conda info "" to see the dependencies for each package.


Python version is 3.4.5. OS info:

lsb_release -a
LSB Version:    :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1- 
noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1- 
amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch
Distributor ID: CentOS
Description:    CentOS Linux release 7.3.1611 (Core) 
Release:    7.3.1611
Codename:   Core


Any suggestions would be greatly appreciated


  Update


I tried conda install -c conda-forge backports, it installed successfully, but did not help.


  Update


I tried before also using the long command: conda install -c bioconda -c conda-forge snakemake but it failed for some dependencies conflicts. Right now I tried it again and got unexpected error of a different kind:

-bash-4.2$ conda install -c bioconda -c conda-forge snakemake
Solving environment: failed

# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ERROR REPORT &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;

Traceback (most recent call last):
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/exceptions.py"", line 819, in __call__
    return func(*args, **kwargs)
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/cli/main.py"", line 78, in _main
    exit_code = do_call(args, p)
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/cli/conda_argparse.py"", line 77, in do_call
    exit_code = getattr(module, func_name)(args, parser)
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/cli/main_install.py"", line 11, in execute
    install(args, parser, 'install')
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/cli/install.py"", line 235, in install
    force_reinstall=context.force,
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/solve.py"", line 505, in solve_for_transaction
    force_remove, force_reinstall)
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/solve.py"", line 438, in solve_for_diff
    final_precs = self.solve_final_state(deps_modifier, prune, ignore_pinned, force_remove)
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/solve.py"", line 179, in solve_final_state
    index, r = self._prepare(prepared_specs)
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/solve.py"", line 560, in _prepare
    self.subdirs, prepared_specs)
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/index.py"", line 215, in get_reduced_index
    new_records = query_all(spec)
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/index.py"", line 184, in query_all
    return tuple(concat(future.result() for future in as_completed(futures)))
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/subdir_data.py"", line 94, in query
    self.load()
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/subdir_data.py"", line 148, in load
    _internal_state = self._load()
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/subdir_data.py"", line 222, in _load
    mod_etag_headers.get('_mod'))
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/subdir_data.py"", line 269, in _read_local_repdata
    _internal_state = self._process_raw_repodata_str(raw_repodata_str)
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/site-packages/conda/core/subdir_data.py"", line 306, in _process_raw_repodata_str
    json_obj = json.loads(raw_repodata_str or '{}')
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/json/__init__.py"", line 339, in loads
    return _default_decoder.decode(s)
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/json/decoder.py"", line 364, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/sfs/nfs/blue/nv4e/anaconda2/lib/python2.7/json/decoder.py"", line 380, in raw_decode
    obj, end = self.scan_once(s, idx)
ValueError: Expecting : delimiter: line 1 column 8388609 (char 8388608)

`$ /sfs/nfs/blue/nv4e/anaconda2/bin/conda install -c bioconda -c conda-forge snakemake`

environment variables:
             CIO_TEST=&lt;not set&gt;
           CONDA_ROOT=/sfs/nfs/blue/nv4e/anaconda2

LD_INCLUDE_PATH=/opt/slurm/17.02.1b/include:/share/rci_apps/common/inc

LD_LIBRARY_PATH=/opt/slurm/17.02.1b/lib:/share/rci_apps/common/lib64
              MANPATH=/usr/share/lmod/lmod/share/man::

 MODULEPATH=/apps/modulefiles/standard/core:/apps/modulefiles/standard/
toolchains



PATH=/scratch/nv4e/cellranger:/scratch/nv4e/bcl2fastq/build/bin:
/scratch/nv
                      4e/spark/bin:/scratch/nv4e/scala/bin:/sfs/nfs/blue/nv4e/private/bin:/s
                      fs/nfs/blue/nv4e/anaconda2/bin:/sfs/nfs/blue/nv4e/.local/bin:/usr/lib6
                      4/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/sl
                      urm/current/bin:/opt/slurm/current/sbin:/opt/singularity/current/bin:/
                      opt/rci/bin:/opt/rci/sbin:/opt/nhc/current/sbin:/share/rci_apps/common
                      /bin:/share/resources/HPCtools/
       QT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
   REQUESTS_CA_BUNDLE=&lt;not set&gt;
        SSL_CERT_FILE=&lt;not set&gt;

 active environment : None
   user config file : /home/nv4e/.condarc
 populated config files : 
      conda version : 4.5.4
 conda-build version : 3.0.27
     python version : 2.7.14.final.0
   base environment : /sfs/nfs/blue/nv4e/anaconda2  (writable)
       channel URLs : https://conda.anaconda.org/bioconda/linux-64
                      https://conda.anaconda.org/bioconda/noarch
                      https://conda.anaconda.org/conda-forge/linux-64
                      https://conda.anaconda.org/conda-forge/noarch
                      https://repo.anaconda.com/pkgs/main/linux-64
                      https://repo.anaconda.com/pkgs/main/noarch
                      https://repo.anaconda.com/pkgs/free/linux-64
                      https://repo.anaconda.com/pkgs/free/noarch
                      https://repo.anaconda.com/pkgs/r/linux-64
                      https://repo.anaconda.com/pkgs/r/noarch
                      https://repo.anaconda.com/pkgs/pro/linux-64
                      https://repo.anaconda.com/pkgs/pro/noarch
      package cache : /sfs/nfs/blue/nv4e/anaconda2/pkgs
                      /home/nv4e/.conda/pkgs
   envs directories : /sfs/nfs/blue/nv4e/anaconda2/envs
                      /home/nv4e/.conda/envs
           platform : linux-64
         user-agent : conda/4.5.4 requests/2.18.4 CPython/2.7.14 Linux/3.10.0-514.21.2.el7.x86_64 centos/7 glibc/2.17
            UID:GID : 1096641:100
         netrc file : None
       offline mode : False


An unexpected error has occurred. Conda has prepared the above report.
If submitted, this report will be used by core maintainers to improve
future releases of conda.
Would you like conda to send this report to the core maintainers?
[y/N]: 

",-1,-1,-1.0
50526689,Snakemake run docker container command,"I have this problem I have a docker image of a program and I want to run some commands using that code.
I have rules that trasfrom input to comand script

rule create_maf:
    input:
        read=""/data3/CALL/{sample}.muetect2.vt.clean.vcf.gz""
    output:
        out=""/data3/MAF/{sample}.muetect2.vt.vep.maf""
   params:
         db_ens= ""/mnt/mpwork/vep/homo_sapiens:/mnt/homo_sapiens"",
         fst= ""/mnt/homo_sapiens/87_GRCh37/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa"",
         tumor_id=""{sample}"",
         data=""/home/data/TEST/:/data3""
    script:
        ""scripts/report.py""


this is the report.py

from subprocess import run

if len(snakemake.input) &gt; 1:
    cmd = ['docker','container','run', '-it','-v', snakemake.params['db_ens'], '-v', snakemake.params['data'],'vcf2maf_87','perl','vcf2maf.pl','--input-vcf',snakemake.input['read'],snakemake.output['out'],'--vep-data', '/mnt/','--ref-fasta',snakemake.params['fst'],'--tumor-id',snakemake.params['tumor_id'],'--normal-id','NORMAL']
    run(cmd)


Inside /home/data/TEST/ I have CALL with all the samples... The problem they don't found....
",-1,-1,-1.0
50891407,Snakemake: How to dynamically set memory resource based on input file size,"I'm trying to base my cluster memory allocation for a given rule on the file size of an input file. Is this possible in snakemake and if so how? 

So far I have tried specifying it in the resource: section like so:

rule compute2:
    input: ""input1.txt""
    output: ""input2.txt""
    resources:
        mem_mb=lambda wildcards, input, attempt: int(os.path.getsize(str(input))/(1024*1024))
    shell: ""touch input2.txt""


But it seems snakemake attempts to calculate this upfront before the file gets created as I'm getting this error:

InputFunctionException in line 35 of test_snakemake/Snakefile:
FileNotFoundError: [Errno 2] No such file or directory: 'input1.txt'


Im running my snakemake with the following command:

snakemake --verbose -j 10 --cluster-config cluster.json --cluster ""sbatch -n {cluster.n} -t {cluster.time} --mem {resources.mem_mb}""

",-1,-1,-1.0
50911726,Snakemake: How to prevent ambiguous rules from being executed both?,"I am using snakemake in a machine-learning context. I have two rules (process_x_only, and process_x_and_y) that have processed_x.txt as target output, and are thus ambiguous. See following code:

rule process_x_only:
    input:
        'x.txt',
    output:
        'processed_x.txt'

rule process_x_and_y:
    input:
        'x.txt',
        'y.txt'
    output:
        'processed_x.txt',
        'processed_y.txt'

ruleorder: process_x_only &gt; process_x_and_y

rule refit_model:
    input:
        'processed_x.txt',
        'processed_y.txt'
    output:
        'predictions_refit.txt'

rule predict_model:
    input:
        'processed_x.txt'
    output:
        'predictions.txt'


Following snakemake's documentation, I use a ruleorder statement to specify that preferably only x should be processed ( I.e. only when y needs to be processed process_x_and_y should be run, otherwise it is sufficient to just process x and process_x_only can be run.). This resolves the ambiguity problem, but introduces another problem. When I execute: 

snakemake predictions_refit.txt


snakemake will first execute process_x_only, and then process_x_and_y, whereas in this case I only want process_x_and_y to be executed. How do I make snakemake build a DAG where only process_x_and_y is executed?

To clarify: This is a big simplification of my actual problem. I am aware that changing the restrictions of my problem statement will solve the problem, but I am interested in how to solve that snakemake is executing both rules in the ruleorder.

To add: While executing rule refit model the following warning is displayed:

Warning: the following output files of rule process_x_only were not 
present when the DAG was created:

{'processed_x.txt'}

",-1,-1,-1.0
51044361,Syntax error in snakemake snakefile,"I tried to run snakemake to test a small job. The code is the following:

rule kallisto_quant:
    input:
        idx='/fullpath/snakemake-example/Kallisto_test/Arabidopsis_thaliana.fa.index'
        fwd='/fullpath/snakemake-example/Kallisto_test/Condition1_R1_008.trimmed.fastq.gz'
        rvs='/fullpath/snakemake-example/Kallisto_test/Condition1_R2_008.trimmed.fastq.gz'
    output:
        '/Condition1'
    threads: 10
    shell:
        'kallisto quant -i {input.idx} -o {output} -b 100 {input.fwd} {input.rvs}'


When I run this, I get syntax error:

SyntaxError in line 4 of /fullpath/snakemake-example/Snakefile:
invalid syntax


By referring to snakemake manual, I am unable to see any syntax error. What would be the problem here ?

Thanks in advance.
",-1,-1,-1.0
51047574,snakemake parameter function lambda,"I want to use a function on params.

Snakemake:
def mitico(x):
 res =int(x)+1
 return res


I I have a wildcard {sample} that are integer. And I want to use {sample}+1
How can do this inside the snakemake params?

In the  function:

rule create_pt:
    input:
        read=""CALL2/{sample}.vcf"",
    output:
        out=""OUT/{sample}.txt
    conda:
         ""envs/mb.yml""
    params:
         db_ens = ""/mnt/mpwor2k/"",
         fst = ""/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa"",
         tumor_id=""{sample}"",
         normal_id=lambda wildcards: mitico('{sample}')



    shell:


I have this error

ValueError: invalid literal for int() with base 10: '{sample}'
Wildcards:
sample=432

",-1,1,-1.0
50688922,"""No such file or directory"" error when generating a report with Snakemake","I'm trying to generate a report automatically as described here using Snakemake's new feature. I get that the RST file should be created prior to workflow execution and located relatively to the Snakefile location (that is to say, not the workdir location).  

Since my Snakefile is located in a subdirectory of the workdir, I defined the RST file like this:

report: ""../../../../results/reports/peak-calling_workflow.rst""


Although the file does exist, I run into a ""No such file or directory"" issue: 

Traceback (most recent call last):
  File ""/home/rioualen/miniconda3/lib/python3.5/site-packages/snakemake/__init__.py"", line 541, in snakemake
    report=report)
  File ""/home/rioualen/miniconda3/lib/python3.5/site-packages/snakemake/workflow.py"", line 495, in execute
    auto_report(dag, report)
  File ""/home/rioualen/miniconda3/lib/python3.5/site-packages/snakemake/report/__init__.py"", line 374, in auto_report
    with open(dag.workflow.report_text) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/rioualen/ChIP-seq_GSE20870/SnakeChunks/scripts/snakefiles/workflows/../../../../results/reports/peak-calling_workflow.rst'


Any idea how I should deal with this?

Thanks!
",-1,-1,-1.0
51946235,Snakemake: Error when trying to generate multiple output files,"I'm writing a snakemake pipeline to take publicly available sra files, convert them to fastq files then run them through alignment, peak calling and LD score regression.

I'm having an issue in the rule called SRA2fastq below in which I use parallel-fastq-dump to convert SRA files to paired end fastq files. This rule generates two outputs for each SRA file, SRRXXXXXXX_1, and SRRXXXXXXX_2. 

Here is my config file:

samples:
    fullard2018_NpfcATAC_1: SRR5367824
    fullard2018_NpfcATAC_2: SRR5367798
    fullard2018_NpfcATAC_3: SRR5367778
    fullard2018_NpfcATAC_4: SRR5367754
    fullard2018_NpfcATAC_5: SRR5367729


And here are the first few rules of my Snakefile:

# read config info into this namespace
configfile: ""config.yaml""
print (config['samples'])

rule all:
    input:
        expand(""fastq_files/{SRA}_{num}.fastq.gz"", SRA=[config['samples'][x] for x in config['samples']], num=[1,2]),
        expand(""FastQC/{SRA}_{num}_fastqc.html"", SRA=[config['samples'][x] for x in config['samples']], num=[1,2]),
        ""FastQC/fastq_multiqc.html"",
        expand(""peak_files/{sample}_peaks.blrm.narrowPeak"", sample=config['samples']),
        ""peak_files/Fullard2018_peaks.mrgd.blrm.narrowPeak"",
        expand(""LD_annotation_files/Fullard_2018.{chr}.l2.ldscore.gz"", chr=range(1,23))

rule SRA_prefetch:
    params:
        SRA=""{SRA}""
    output:
        ""/home/c1477909/ncbi/public/sra/{SRA}.sra""
    log:
        ""logs/prefetch/{SRA}.log""
    shell:
        ""prefetch {params.SRA}""

rule SRA2fastq:
    input:
        ""/home/c1477909/ncbi/public/sra/{SRA}.sra""
    output:
        ""fastq_files/{SRA}_1.fastq.gz"",
        ""fastq_files/{SRA}_2.fastq.gz""
    log:
        ""logs/SRA2fastq/{SRA}.log""
    shell:
        """"""
        parallel-fastq-dump --sra-id {input} --threads 8 \
        --outdir fastq_files --split-files --gzip
        """"""

rule fastqc:
    input:
        rules.SRA2fastq.output
    output:
        # Output needs to end in '_fastqc.html' for multiqc to work
        html=""FastQC/{SRA}_{num}_fastqc.html""
    log:
        ""logs/FASTQC/{SRA}_{num}.log""
    wrapper:
        ""0.27.1/bio/fastqc""

rule multiqc_fastq:
    input:
        lambda wildcards: expand(""FastQC/{SRA}_{num}_fastqc.html"", SRA=[config['samples'][x] for x in config['samples']], num=[1,2])
    output:
        ""FastQC/fastq_multiqc.html""
    wrapper:
        ""0.27.1/bio/multiqc""

rule bowtie2:
    input:
        sample=lambda wildcards: expand(""fastq_files/{SRA}_{num}.fastq.gz"", SRA=config['samples'][wildcards.sample], num=[1,2])
    output:
        ""bam_files/{sample}.bam""
    log:
        ""logs/bowtie2/{sample}.txt""
    params:
        index=config[""index""],  # prefix of reference genome index (built with bowtie2-build),
        extra=""""
    threads: 8
    wrapper:
       ""0.27.1/bio/bowtie2/align""


However, when I run the Snakefile I get the following error:

Error in job SRA2fastq while creating output files fastq_files/SRR5367754_1.fastq.gz, fastq_files/SRR5367754_2.fastq.gz


I've seen this error many times before and it's usually caused when the name of output file generated by the program does not exactly match the output file name you specify in the corresponding snakemake rule. However, this is not the case here as if I run the command snakemake generates for this particular rule separately the files are created as expected and the file names match. Here is an example of one instance of the rule taken after running snakemake -np:

rule SRA2fastq:
    input: /home/c1477909/ncbi/public/sra/SRR5367779.sra
    output: fastq_files/SRR5367779_1.fastq.gz, fastq_files/SRR5367779_2.fastq.gz
    log: logs/SRA2fastq/SRR5367779.log
    jobid: 18
    wildcards: SRA=SRR5367779

    parallel-fastq-dump --sra-id /home/c1477909/ncbi/public/sra/SRR5367779.sra --threads 8 --outdir fastq_files --split-files --gzip


Note the output files generated by the parallel-fastq-dump command run separately (i.e. not using snakemake) are named as specified in the SRA2fastq rule:

ls fastq_files
SRR5367729_1.fastq.gz  SRR5367729_2.fastq.gz


I'm a bit stumped by this as this error is usually easily rectified but I can't work out what the issue is. I've tried changing the output section of the SRA2fastq to:

    output:
        file1=""fastq_files/{SRA}_1.fastq.gz"",
        file2=""fastq_files/{SRA}_2.fastq.gz""


However, this throws the same error. I've also tried just specifying one output file but this affects the bowtie2 rule later on as I get an input files missing error. 

Any ideas what's going on here? Is there something I'm missing when trying to look for multiple output files in a single rule?

Many Thanks
",-1,-1,-1.0
51965825,Snakemake: exporting d3dag - MissingInputException,"I want to export the DAG of my workflow in D3.js compatible JSON format: 

snakemake.snakemake(snakefile=smfile,
                    dryrun=True,
                    forceall=True,
                    printdag=False,
                    printd3dag=True,
                    keepgoing=True,
                    cluster_config=cluster_config,
                    configfile=configfile,
                    targets=targetfiles)


Unfortunately, it complains about missing input files. 

It is right about the fact that the files are missing but I had hoped that it would run anyways, especially after setting the keepgoing option to True. 

Is there a smart way to export the DAG without the input files?

Thanks, 
Jan
",0,-1,-1.0
51977436,Restrict number of jobs by a rule in snakemake,"Is it possible to restrict number of jobs to run by a particular rule in snakemake? --jobs controls globably how many jobs are allowed to run at a time, but I would like to restrict by a specific rule. 

This is because, I have a particular rule that can be used at most for two jobs in parallel. However, if I set --jobs to 20, this results in tool in that particular rule crashing. I use snakemake v5.2.0 in LSF cluster.
",0,-1,-1.0
52261082,"Snakemake, trouble with ""lambda wildcards, attempt, threads""","I was using snakemake to implement a new tool in a pipeline and i had some trouble with those lines: 

resources:
    # Samtools sort requires by default 768M per threads
    # Here, be set the maximum amount of memory are 1.5Go per threads
    mem_mb = (
        lambda wildcards, attempt, threads: min(
        attempt * 250 + threads * 768,
        1536 * threads)
    )


the error is :

TypeError: &lt;lambda&gt;() missing 1 required positional argument: 'threads'
Wildcards:
sample=test_GATK
genome=fakefile


To make those lines i used the documentation on this page : 
snakemake documentation

To fix this issue i use this code instead:

resources:
    # Samtools sort requires by default 768M per threads
    # Here, be set the maximum amount of memory are 1.5Go per threads
    mem_mb = (
        lambda wildcards, attempt: min(
        attempt * 250 + config[""threads""] * 768,
        1536 * config[""threads""])
    )


I don't know why the previous one is not working, can you help me figure it out ? 

Thanks for the help :)
",1,-1,-1.0
52273322,Snakemake and pandas syntax,"I have a input file as follow 

SampleName Run Read1 Read2
A run1 test/true_data/4k_R1.fq test/true_data/4k_R2.fq
A run2 test/samples/A.fastq test/samples/A2.fastq
B run1 test/samples/B.fastq test/samples/B2.fastq
C run1 test/samples/C.fastq test/samples/C5.fastq
D


So I am getting all indexs in an array:

sample_table    = pd.read_table('samples.tsv', sep=' ', lineterminator='\n')
sample_table    = sample_table.drop_duplicates(subset='SampleName', keep='first', inplace=False)
sample_table    = sample_table.dropna()
sample_table.set_index('SampleName',inplace=True)
sample_ID=sample_table.index.values


At this point sample_ID=['A' 'B' 'C'] which is what I want.
Then I want to set a variable r1 that will correspond to the Read1 and r2 for the Read2 of each samples.

rule all:
    input:
        expand(""test/fltr/{ID_sample}.fq"", ID_sample=sample_ID)

rule send_reads:
    input:
        #Tried both way but it does not work 
        r1=sample_table.loc[""{ID_sample}"",'Read1']
        r2=sample_table.Read2[""{ID_sample}""]
    output:
       ""test/fltr/{ID_sample}{input.r1}.fq""
    shell:
       ""touch {output}""


I get the error 


  the label [{ID_sample}] is not in the [index]


Is it a syntax error or a bigger mistake ? 

I am just starting to use Snakemake, I thought I had understood it after the tutorial but obviously I did not.

Thanks a lot,
Cheers
",-1,1,-1.0
52500725,Snakemake hangs when cluster (slurm) cancelled a job,"Maybe the answer is obvious for many, but I am quite surprised I could not find a question regarding this topic, which represents a major problem for me.
I would greatly appreciate a hint!

When submitting a job on a cluster managed by slurm, if the queue manager cancels the job (e.g. for insufficient resources or time), snakemake seems to not receive any signal, and hangs forever. On the other hand, when the job fails, also snakemake fails, as expected. Is this behavior normal/wanted? How can I have snakemake to fail also when a job gets cancelled? I had this problem with snakemake version 3.13.3 and it remained updating to 5.3.0.

For example in this case I launch a simple pipeline, with insufficient resources for the rule pluto:

$ snakemake -j1 -p --cluster 'sbatch --mem {resources.mem}' pluto.txt
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cluster nodes: 1
Unlimited resources: mem
Job counts:
    count   jobs
    1       pippo
    1       pluto
    2

[Tue Sep 25 16:04:21 2018]
rule pippo:
    output: pippo.txt
    jobid: 1
    resources: mem=1000

seq 1000000 | shuf &gt; pippo.txt
Submitted job 1 with external jobid 'Submitted batch job 4776582'.
[Tue Sep 25 16:04:31 2018]
Finished job 1.
1 of 2 steps (50%) done

[Tue Sep 25 16:04:31 2018]
rule pluto:
    input: pippo.txt
    output: pluto.txt
    jobid: 0
    resources: mem=1

sort pippo.txt &gt; pluto.txt
Submitted job 0 with external jobid 'Submitted batch job 4776583'.


Here it hangs. And here is the content of the job accounting:

$ sacct -S2018-09-25-16:04 -o jobid,JobName,state,ReqMem,MaxRSS,Start,End,Elapsed
       JobID    JobName      State     ReqMem     MaxRSS               Start                 End    Elapsed
------------ ---------- ---------- ---------- ---------- ------------------- ------------------- ----------
4776582      snakejob.+  COMPLETED     1000Mn            2018-09-25T16:04:22 2018-09-25T16:04:27   00:00:05
4776582.bat+      batch  COMPLETED     1000Mn      1156K 2018-09-25T16:04:22 2018-09-25T16:04:27   00:00:05
4776583      snakejob.+ CANCELLED+        1Mn            2018-09-25T16:04:32 2018-09-25T16:04:32   00:00:00
4776583.bat+      batch  CANCELLED        1Mn      1156K 2018-09-25T16:04:32 2018-09-25T16:04:32   00:00:00

",-1,-1,-1.0
52754456,snakemake --list-code-changes does not propagate into subworkflows,"I use 

snakemake -R `snakemake --list-code-changes`


to rerun rules in which the code has changed. However, this does not seem to work when using subworkflows, for example:

Main Snakefile:

rule all:
  input: ""out""

subworkflow subworkflow:
  workdir: ""subworkflow""

rule run_subworkflow:
  input: subworkflow(""subworkflow_output"")
  output: ""out""
  shell: ""cat {input} &gt; {output}""


Subworkflow Snakefile:

rule all:
  input: ""subworkflow_output""

rule generate_b:
  output: ""subworkflow_output""
  shell: ""echo 'subworkflow' &gt; {output}""


Changes made in the shell command of the subworkflow are not detected.

Is this expected behaviour?
",-1,-1,-1.0
52967024,snakemake substitution issue,"I am new to snakemake and have an issue with the following code that should take 9 fastq files one after another and apply fastqc.

smp should take the values:

UG1_S12
UG2_S13
UG3_S14
UR1_S1
UR2_S2
UR3_S3
UY1_S6
UY2_S7
UY3_S8

Which works when I run

SAMPLES, = glob_wildcards(""reads/merged_s{smp}_L001.fastq.gz"")
NB_SAMPLES = len(SAMPLES)

for smp in SAMPLES:
  message(""Sample "" + smp + "" will be processed"")
message(""N= "" + str(NB_SAMPLES))


The problem is the replacement of {smp} below which is first replaced by UY2_S7 then by UY3_S8 in the mv commands.

How should I make sure that the same substitution is used in both subcommands of the same rule?

my current code (inspired by):

SAMPLES, = glob_wildcards(""reads/merged_s{smp}_L001.fastq.gz"")

rule all: 
  input: 
        expand(""reads/merged_s{smp}_L001.fastq.gz"", smp=SAMPLES),
        ""results/multiqc.html""

rule fastqc:
    """"""
    Run FastQC on each FASTQ file.
    """"""
    input:
        ""reads/merged_s{smp}_L001.fastq.gz""
    output:
        ""results/{smp}_fastqc.html"",
        ""intermediate/{smp}_fastqc.zip""
    version: ""1.0""
    shadow: ""minimal""
    threads: 8
    shell:
        """"""
        # Run fastQC and save the output to the current directory
        fastqc {input} -t {threads} -q -d . -o .

        # Move the files which are used in the workflow
        mv merged_s{smp}_L001_fastqc.html {output[0]}
        mv merged_s{smp}_L001_fastqc.zip {output[1]}
        """"""


the error:

Error in rule fastqc:
    jobid: 0
    output: results/UY2_S7_fastqc.html, intermediate/UY2_S7_fastqc.zip

RuleException:
CalledProcessError in line 60 of Snakefile:
Command ' set -euo pipefail;  
        # Run fastQC and save the output to the current directory
        fastqc reads/merged_sUY2_S7_L001.fastq.gz -t 8 -q -d . -o .

        # Move the files which are used in the workflow
        mv merged_sUY3_S8_L001_fastqc.html results/UY2_S7_fastqc.html
        mv merged_sUY3_S8_L001_fastqc.zip intermediate/UY2_S7_fastqc.zip ' returned non-zero exit status 130.
  File ""Snakefile"", line 60, in __rule_fastqc
  File ""/opt/biotools/miniconda2/envs/snakemake-tutorial/lib/python3.6/concurrent/futures/thread.py"", line 56, in run

",-1,-1,-1.0
53088506,import matplotlib error in snakemake-tutorial environment,"I am trying to get familiar with snakemake and started with the tutorial. All works fine until Step 6: Using custom scripts. Apparently the python code crashes when importing matplotlib.

I run the following command in the tutorial home folder:

snakemake


And get the following error message:

Traceback (most recent call last):
File ""/scratch/users/xxx/snakemake/tutorial/.snakemake/scripts/tmpfg_qprf9.plot-quals.py"", line 5, in &lt;module&gt;
import matplotlib
File ""/home/xxx/tools/miniconda3/envs/snakemake-tutorial/lib/python3.6/site-packages/matplotlib/__init__.py"", line 141, in &lt;module&gt;
from . import cbook, rcsetup
File ""/home/xxx/tools/miniconda3/envs/snakemake-tutorial/lib/python3.6/site-packages/matplotlib/cbook/__init__.py"", line 33, in &lt;module&gt;
import numpy as np
File ""/n/sw/centos6/numpy-1.7.1_python-2.7.3/lib/python2.7/site-packages/numpy/__init__.py"", line 128, in &lt;module&gt;
from version import git_revision as __git_revision__
ModuleNotFoundError: No module named 'version'


In my understanding the idea is to provide an environment where everything is setup. And indeed conda list shows matplotlib 2.2.3 is installed. I assume I do something wrong, as I could not find any other user asking this question, however, I also failed in solving it myself. Help is highly appreciated.

Thanks
",1,-1,-1.0
53105643,RSeQC with multiQC and snakemake,"I want to include RSeQC results using multiQC in a snakemake workflows.
I have the issue that one of the RSeQC tool only reports a .r and a .pdf while it seems that multiQC requires a .txt input to create a plot.
Has anyone working code for snakemake that recover info from RSeQC into a multiQC rule.
As this is a combination of three tools, it is difficult to get support.

My code here of which only the geneBodyCoverage.txt RSeQC output is used (not the two .r outputs and especially junctionSaturation_plot.r of which there is nothing else than the .r and the png picture)
rule multiqc_global:
&quot;&quot;&quot;
Aggregate all MultiQC reports
&quot;&quot;&quot;
  input:
    expand(&quot;intermediate/{smp}_fastqc.zip&quot;, smp=SAMPLES),
    expand(&quot;intermediate/merged_{smp}_fastqc.zip&quot;, smp=SAMPLES),
    expand(&quot;logs/star/{smp}_Log.final.out&quot;, smp=SAMPLES),
    expand(&quot;intermediate/{smp}.geneBodyCoverage.txt&quot;, smp=SAMPLES),
    expand(&quot;intermediate/{smp}.geneBodyCoverage.r&quot;, smp=SAMPLES),
    expand(&quot;intermediate/{smp}.junctionSaturation_plot.r&quot;, smp=SAMPLES),
  output:
    html = &quot;results/global_multiqc.html&quot;,
    stats = &quot;intermediate/global_multiqc_general_stats.txt&quot;
  log:
    &quot;logs/multiqc/global_multiqc.log&quot;
  version: &quot;1.0&quot;
  shadow: &quot;minimal&quot;
  shell: 
    &quot;&quot;&quot;
    # Run multiQC and keep the html report
    multiqc -n multiqc.html {input} 2&gt; {log}
    mv multiqc.html {output.html}
    mv multiqc_data/multiqc_general_stats.txt {output.stats}
    &quot;&quot;&quot;

",-1,-1,-1.0
53152063,snakemake: MissingOutputException within docker,"I am trying to run a pipeline within a docker using snakemake. I am having problem using the sortmerna tool to produce {sample}_merged_sorted_mRNA and {sample}_merged_sorted output from control_merged.fq and treated_merged.fq input files.

Here my Snakefile: 

   SAMPLES = [""control"",""treated""]
   for smp in SAMPLES:
       print(""Sample "" + smp + "" will be processed"")
  rule final:
       input:
          expand('/output/{sample}_merged.fq', sample=SAMPLES),
          expand('/output/{sample}_merged_sorted', sample=SAMPLES),
          expand('/output/{sample}_merged_sorted_mRNA', sample=SAMPLES),

  rule sortmerna:
       input: '/output/{sample}_merged.fq',

       output: merged_file='/output/{sample}_merged_sorted_mRNA', merged_sorted='/output/{sample}_merged_sorted',

   message: """"""---SORTING---""""""
   shell:
      '''
         sortmerna --ref /usr/share/sortmerna/rRNA_databases/silva-bac-23s-id98.fasta,/    usr/share/sortmerna/rRNA_databases/index/silva-bac-23s-id98: --reads {input} --paired_in     -a 16 --log --fastx --aligned {output.merged_file} --other {output.merged_sorted} -v
     '''


When runnig this I get:

Waiting at most 5 seconds for missing files.                                                 
 MissingOutputException in line 57 of /input/Snakefile:                                       
 Missing files after 5 seconds:
/output/control_merged_sorted_mRNA
/output/control_merged_sorted  

 This might be due to filesystem latency. If that is the case, consider to increase the wait $ime with --latency-wait.

 Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /input/.snakemake/log/2018-11-05T091643.911334.snakemake.log


I tried to increase the latency with --latency-wait but I get the same result. Funny thing is that two output files control_merged_sorted_mRNA.fq and control_merged_sorted.fq are produced but the program fails and exits. The version of snakemake is 5.3.0. Any help?    
",-1,-1,-1.0
52741062,snakemake download files from sourceforce,"I try to use the  utility of snakemake for download some data (snakemake 5.2.1):

rule down_data2:
    input:

        HTTP.remote(""https://sourceforge.net/projects/fusioncatcher/files/data/human_v90.tar.gz.ac"", keep_local=True,allow_redirects=True),

    output:
        ""
        ""human_v90.tar.gz.ac"",

    run:
        outputName = os.path.basename(input[0])
        shell(""mv {input} {outputName}"")


I have a problem on  mv comand.. Seems they save on another directory.
",-1,-1,-1.0
52598637,unknown output in snakemake,"I'm working on implementing a very simple pipeline in snakemake in hopes of replacing a chain of annoying bash scripts with one cohesive Snakefile. 

I'm having trouble writing a rule that splits a file into smaller pieces (using GNU split), and then leads to a second rule where the output is concatenated together. 

I don't know what to write for the input in the concat step, since I don't know how to define all the files fitting the pattern bam_files/test*. I tried with glob, but that decidedly doesn't seem to work (it seems like it's actually skipping split altogether with the glob included). Is there any better way that I could be doing this? 

# test snakemake pipeline
import glob


SAMPLE_IDS = [""test""]

rule all: 
    input: 
        expand(""bam_files/{FASTQ}.out"", FASTQ=SAMPLE_IDS)


rule split: 
    input: 
        expand(""{FASTQ}.txt"", FASTQ=SAMPLE_IDS)
    output: 
        ""bam_files/{FASTQ}.""
    shell:
        ""cat {input} | split -l 1000 -d - {output}.""


rule concat: 
    input:
        split_files = glob.glob(""bam_files/{FASTQ}.*"")
    output: 
        ""bam_files/{FASTQ}.out""
    shell: 
        ""cat {input} &gt; {output}""

",1,1,-1.0
53234045,Snakemake to --export-cwl conversion requires static config/inputs,"So I am currently interesting in using Snakemake to convert some snakemake pipelines into CWL using the --export-cwl function, but have come to the realization that this requires that all inputs filenames be predefined in the config file. This means that this cwl workflow is only useful for the current pipeline run and is not capable of being exported as a more general use pipeline that can be adapted to varying config parameters (e.g. I would have to re-run the conversion tool for every single run that ever goes through it). Is this a limitation of the Snakemake conversion tool or CWL? I am not very familiar with CWL so I know very little about it's inner workings. Or is there some workaround that I am not seeing?

Thanks for any help!
",-1,-1,-1.0
53267414,Getting data from iRODS server from snakemake,"I am trying to download a file from an iRODS server using snakemake.

I have followed the instructions here.

I am working on a GNU/Linux computer cluster.

I have got this at the top of my Snakefile:

from snakemake.remote.iRODS import RemoteProvider

irods = RemoteProvider(irods_env_file='/nfs/users/nfs_c/username/.irods/irods_environment.json',
                       timezone=""Europe/London"") # all parameters are optional


This code runs smoothly in the Python interpreter without causing an error.

I then have this rule:

rule iget:
    input:
        irodsBam = irods.remote('/seq/path/to/my/file.bam')
    output:
        lustreBam = 'new/file/location/on/lustre.bam'
    conda:
        'envs/iget.yaml'
    shell:
        'iget -K -f {input.irodsBam} {output.lutreBam}'


In the yaml file, I've got this:

channels:
  - bioconda
  - r
dependencies:
  - boto
  - moto
  - filechunkio
  - pysftp
  - dropbox
  - requests
  - ftputil
  - XRootD
  - biopython


And I ran snakemake with:

snakemake --use-conda new/file/location/on/lustre.bam


I have got an error:

Building DAG of jobs...
MissingInputException in line 146 of /lustre/projects/main/Snakefile:
Missing input files for rule igetPacbio:
/seq/path/to/my/file.bam


However, I checked with ils and my BAM file is on iRODS at the specified location.

Can anyone help me, please?

Update 13th November 2018

I have also tried this rule:

rule iget:
    input:
        irodsBam = irods.remote('/seq/path/to/my/file.bam')
    output:
        'new/file/location/on/lustre.bam'
    conda:
        'envs/iget.yaml'
    shell:
        r""""""
        touch {output}
        """"""


but the error is the same:

MissingInputException


Update 24th November 2018

ils /seq/pacbio/r54097_20170511_114349/1_A01/


gives

/seq/pacbio/r54097_20170511_114349/1_A01:
  m54097_170511_115331.adapters.fasta
  m54097_170511_115331.scraps.bam
  m54097_170511_115331.scraps.bam.pbi
  m54097_170511_115331.sts.xml
  m54097_170511_115331.subreads.bam
  m54097_170511_115331.subreads.bam.pbi
  m54097_170511_115331.subreadset.xml


And removing the '/' in '/seq/path/to/my/file.bam' does not change the error. It still is a MissingInputException.
",-1,-1,-1.0
53305892,Install older version of snakemake,"I need to install version 4.7.0 of snakemake, but I cannot find it. Could you please tell me where can I get old versions of the software? I tried the bitbucket repository but I could not find any other than the latest version.
",-1,-1,-1.0
53749519,snakemake define folder as output,"I try to run prokka using snakemake and rule all. In the latter I define all output folders which will be produced by prokka to write the results. Prokka requires a folder to be supplied as an output rather than a file.

A simplified version of what I have is here:

PATIENTID_ls = range(2)
rule all:
input:
    expand(""results_{subjectID}_outputfolder"",subjectID=PATIENTID_ls), 

rule prokka:
    input:
        ""contigs/subject_{subjectID}/contigs.fasta"",
    output:
        ""results/subject_{subjectID}_outputfolder"",
    shell:
        ""prokka --cpus 1 --proteins ../GCF_000009645.1_ASM964v1_genomic.gbff --outdir {output} --prefix contigs500_anno9ref {input} ""


When running:

$snakemake -p
Building DAG of jobs...
MissingInputException in line 2 of Snakefile:
Missing input files for rule all:
results_1_outputfolder
results_0_outputfolder


It works however when specifying the output explicitly:

snakemake -p results/subject_1_outputfolder


I am sure that is noob mistake on my side, but after hours of playing around I could not solve the issue. Help is highly appreciated. 
Thank you
",-1,-1,-1.0
53852874,Snakemake - Many to one using an expand exception,"I have a functioning Snakefile for Partitioned Heritability using multiple bed files. This produces a perfect list of jobs using snakemake -np so this file only needs a minor tweak (I hope!). 

My issue occurs in the merge_peaks rule below. 

At this stage, I have 25 bed files and need to run 5 calls of merge_peaks, one call for each extension ext=[100,200,300,400,500], so I need only the 5 bed files containing the relevant extension to be called each time. 

For example for the following merge_peaks output file peak_files/Fullard2018_peaks.mrgd.blrm.100.bed, I only need the following 5 bed files with ext=100 to be used as input:

peak_files/fullard2018_NpfcATAC_1.blrm.100.bed
peak_files/fullard2018_NpfcATAC_2.blrm.100.bed
peak_files/fullard2018_NpfcATAC_3.blrm.100.bed
peak_files/fullard2018_NpfcATAC_4.blrm.100.bed
peak_files/fullard2018_NpfcATAC_5.blrm.100.bed


Here is my config file:

samples:
    fullard2018_NpfcATAC_1:
    fullard2018_NpfcATAC_2:
    fullard2018_NpfcATAC_3:
    fullard2018_NpfcATAC_4:
    fullard2018_NpfcATAC_5:
index: /home/genomes_and_index_files/hg19.chrom.sizes


Here is the Snakefile:

# read config info into this namespace
configfile: ""config.yaml""
print (config['samples'])

rule all:
    input:
        expand(""peak_files/{sample}.blrm.{ext}.bed"", sample=config['samples'], ext=[100,200,300,400,500]),
        expand(""LD_annotation_files/Fullard2018.{ext}.{chr}.l2.ldscore.gz"", sample=config['samples'], ext=[100,200,300,400,500], chr=range(1,23))

rule annot2bed:
    input:
        folder = ""Reference/baseline""
    params:
        file = ""Reference/baseline/baseline.{chr}.annot.gz""
    output:
        ""LD_annotation_files/baseline.{chr}_no_head.bed""
    shell:
        ""zcat {params.file} | tail -n +2 |awk -v OFS=\""\t\"" '{{print \""chr\""$1, $2-1, $2, $3, $4}}' ""
        ""| sort -k1,1 -k2,2n &gt; {output}""

rule extend_bed:
    input:
        ""peak_files/{sample}_peaks.blrm.narrowPeak""
    output:
        ""peak_files/{sample}.blrm.{ext}.bed""
    params:
        ext = ""{ext}"",
        index = config[""index""]
    shell:
        ""bedtools slop -i {input} -g {params.index} -b {params.ext} &gt; {output}""

rule merge_peaks:
    input:
        expand(""peak_files/{sample}.blrm.{ext}.bed"", sample = config['samples'], ext=[100,200,300,400,500])
    output:
        ""peak_files/Fullard2018_peaks.mrgd.blrm.{ext}.bed""
    shell:
        ""cat {input} | bedtools sort -i stdin | bedtools merge -i stdin &gt; {output}""
rule intersect_mybed:
    input:
        annot = rules.annot2bed.output,
        mybed = rules.merge_peaks.output
    output:
        ""LD_annotation_files/Fullard2018.{ext}.{chr}.annot.gz""
    params:
        uncompressed = ""LD_annotation_files/Fullard2018.{ext}.{chr}.annot""
    shell:
        ""echo -e \""CHR\tBP\tSNP\tCM\tANN\"" &gt; {params.uncompressed}; ""
        ""/share/apps/bedtools intersect -a {input.annot} -b {input.mybed} -c | ""
        ""sed 's/^chr//g' | awk -v OFS=\""\t\"" '{{print $1, $2, $4, $5, $6}}' &gt;&gt; {params.uncompressed}; ""
        ""gzip {params.uncompressed}""

rule ldsr:
    input:
        annot = ""LD_annotation_files/Fullard2018.{ext}.{chr}.annot.gz"",
        bfile_folder = ""Reference/1000G_plinkfiles"",
        snps_folder = ""Reference/hapmap3_snps""
    output:
        ""LD_annotation_files/Fullard2018.{ext}.{chr}.l2.ldscore.gz""
    conda:
        ""envs/p2-ldscore.yaml""
    params:
        bfile = ""Reference/1000G_plinkfiles/1000G.mac5eur.{chr}"",
        ldscores = ""LD_annotation_files/Fullard2018.{ext}.{chr}"",
        snps = ""Reference/hapmap3_snps/hm.{chr}.snp""
    log:
        ""logs/LDSC/Fullard2018.{ext}.{chr}_ldsc.txt""
    shell:
        ""export MKL_NUM_THREADS=2;"" # Export arguments are  workaround as ldsr uses all available cores
        ""export NUMEXPR_NUM_THREADS=2;"" # Numbers must match cores parameter in cluster config
        ""Reference/ldsc/ldsc.py --l2 --bfile {params.bfile} --ld-wind-cm 1 ""
        ""--annot {input.annot} --out {params.ldscores} --print-snps {params.snps} 2&gt; {log}""


What is currently happening is all 25 bed files are being fed to the merge peaks rule for each call, whereas I only need the 5 with the relevant extension to be fed in each time. I'm struggling to work out how to use the expand function correctly, or an alternate method, to include and merge only the relevant bed files in each call of the rule.

This question asks something similar, I think, but is not quite what I'm looking for as it doesn't use a config file - Snakemake: rule for using many inputs for one output with multiple sub-groups

I love Snakemake but my python is a bit dicey.

Many Thanks.
",-1,1,-1.0
53853194,Parallelise output of input function in Snakemake,"Hello Snakemake community,

I am having quite some troubles to define correctly a function in Snakemake and call it in the params section. The output of the function is a list and my aim is to use each item of the list as a parameter of a shell command. In other words, I would like to run multiple jobs in parallel of the same shell command with a different parameter.

This is the function:

import os, glob
def get_scontigs_names(wildcards):
   scontigs = glob.glob(os.path.join(""reference"", ""Supercontig*""))
   files = [os.path.basename(s) for s in scontigs]
   return name


The output is a list that looks like:

['Supercontig0', 'Supercontig100', 'Supercontig2', ...]

The Snakemake rules are:

rule all:
    input:
        ""updated/all_supercontigs.sorted.vcf.gz""
rule update_vcf:
    input:
        len=""genome/genome_contigs_len_cumsum.txt"",
        vcf=""filtered/all.vcf.gz""
    output:
        cat=""updated/all_supercontigs.updated.list""
    params:
        scaf=get_scontigs_names
    shell:
        """"""
        python 3.7 scripts/update_genomic_reg.py -len {input.len} -vcf {input.vcf} -scaf {params.scaf}
        ls updated/*.updated.vcf.gz &gt; {output.cat}
        """"""


This code is incorrect because all the items of the list are loaded into the shell command when I call {params.scaf}. The current shell commands looks like:

python 3.7 scripts/update_genomic_reg.py -len genome/genome_contigs_len_cumsum.txt -vcf filtered/all.vcf.gz -scaf Supercontig0 Supercontig100 Supercontig2 ...

What I would like to get is:*

python 3.7 scripts/update_genomic_reg.py -len genome/genome_contigs_len_cumsum.txt -vcf filtered/all.vcf.gz -scaf Supercontig0

python 3.7 scripts/update_genomic_reg.py -len genome/genome_contigs_len_cumsum.txt -vcf filtered/all.vcf.gz -scaf Supercontig100

and so on.

I have tried to use wildcards inside the function but I am failing to give it the correct attribute.

There are several posts about input functions and wildcards plus the snakemake docs but I could not really apply them to my case.
Can somebody help me with this, please?
",-1,-1,-1.0
53906974,How does Snakemake parse slurm jobid with --cluster-status and sacct used,"I am running a large snakemake (v5.3.0) pipeline using a slurm scheduler (v14.11.4). Unfortunately ~1/1000 jobs crash with a NODE_FAILED (ExitCode 0) which snakemake does not recognise, leading to half finished output files.

In order to make snakemake aware of the incident I figured out that --cluster-status and a script that parses the jobid using sacct should do the trick. To do the job I modified a script I found online, which now looks like that:

#!/usr/bin/env python3
import os
import sys
import warnings
import subprocess

jobid = sys.argv[1]
state = subprocess.run(['sacct','-j',jobid,'--format=State'],stdout=subprocess.PIPE).stdout.decode('utf-8')
state = state.split('\n')[2].strip()

map_state={""PENDING"":'running',
       ""RUNNING"":'running', 
       ""SUSPENDED"":'running', 
       ""CANCELLED"":'failed', 
       ""COMPLETING"":'running', 
       ""COMPLETED"":'success', 
       ""CONFIGURING"":'running', 
       ""FAILED"":'failed',
       ""TIMEOUT"":'failed',
       ""PREEMPTED"":'failed',
       ""NODE_FAIL"":'failed',
       ""REVOKED"":'failed',
       ""SPECIAL_EXIT"":'failed',
       """":'success'}

print(map_state[state])


The script works fine in the command line. However, when initiating snakemake as followed:

SM_ARGS=""--cpus-per-task {cluster.cpus-per-task} --mem-per-cpu {cluster.mem-per-cpu-mb} --job-name {cluster.job-name} --ntasks {cluster.ntasks} --partition {cluster.partition} --time {cluster.time} --mail-user {cluster.mail-user} --mail-type {cluster.mail-type} --error {cluster.error} --output {cluster.output}""

snakemake -p \
$* \
 --latency-wait 120 \
-j 600 \
--cluster-config $(dirname $0)/cluster.slurm.json \
--cluster ""sbatch $SM_ARGS"" \
--cluster-status ~/scripts/snakemake/slurm_status.py


It starts to submit the first batch of 600 jobs and basically stalls right afterwards with no additional jobs submitted. However, all initially submitted jobs finish successfully. The snakemake log produces after all jobs are submitted a single error:

sacct: error: slurmdbd: Getting response to message type 1444
sacct: error: slurmdbd: DBD_GET_JOBS_COND failure: No error


I assume my command does not parse the jobid correctly to slurm_status.py. However, I do not know how snakemake parses the jobid to the slurm_status.py and google could not answer this question (neither the sparse info obtained via snakemake --help).

Thanks for your support.
",1,-1,-1.0
54095096,Snakemake and Pandas syntax: Getting sample specific parameters from the sample table,"First off all, this could be a duplicate of Snakemake and pandas syntax. However, I'm still confused so I'd like to explain again.

In Snakemake I have loaded a sample table with several columns. One of the columns is called 'Read1', it contains sample specific read lengths. I would like to get this value for every sample separately as it may differ.

What I would expect to work is this:

rule mismatch_profile:
    input:
        rseqc_input_bam
    output:
        os.path.join(rseqc_dir, '{sample}.mismatch_profile.xls')
    conda:
        ""../envs/rseqc.yaml""  
    params:
        read_length = samples.loc['{sample}']['Read1']
    shell:
        '''
        #!/bin/bash
        mismatch_profile.py -i {input} -o {rseqc_dir}/{wildcards.sample} -l {params.read_length}


However, that does not work. For some reason I am not allowed to use {sample} inside standard Pandas syntax and I get this error:

KeyError in line 41 of /rst1/2017-0205_illuminaseq/scratch/swo-406/test_snakemake_full/rules/rseqc.smk:
'the label [{sample}] is not in the [index]'


I don't understand why this does not work. I read that I can also use lambda functions but I don't really understand exactly how since they still need {sample} as input.

Could anyone help me?
",1,-1,-1.0
54024862,Snakemake conda env parameter is not taken from config.yaml file,"I use a conda env that I create manually, not automatically using Snakemake. I do this to keep tighter version control.

Anyway, in my config.yaml I have the following line:

conda_env: '/rst1/2017-0205_illuminaseq/scratch/swo-406/snakemake'


Then, at the start of my Snakefile I read that variable (reading variables from config in your shell part does not seem to work, am I right?):

conda_env = config['conda_env']


Then in  a shell part I hail said parameter like this:

rule rsem_quantify:
    input:
        os.path.join(fastq_dir, '{sample}_R1_001.fastq.gz'),
        os.path.join(fastq_dir, '{sample}_R2_001.fastq.gz')
    output:
        os.path.join(analyzed_dir, '{sample}.genes.results'),
        os.path.join(analyzed_dir, '{sample}.STAR.genome.bam')
    threads: 8
    shell:
        '''
        #!/bin/bash
        source activate {conda_env}

        rsem-calculate-expression \
        --paired-end \
        {input} \
        {rsem_ref_base} \
        {analyzed_dir}/{wildcards.sample} \
        --strandedness reverse \
        --num-threads {threads} \
        --star \
        --star-gzipped-read-file \
        --star-output-genome-bam
        '''


Notice the {conda_env}. Now this gives me the following error:

Could not find conda environment: None
You can list all discoverable environments with `conda info --envs`.


Now, if I change {conda_env} for its parameter directly /rst1/2017-0205_illuminaseq/scratch/swo-406/snakemake, it does work! I don't have any trouble reading other parameters using this method (like rsem_ref_base and analyzed_dir in the example rule above.

What could be wrong here?

Highest regards,

Freek.
",-1,-1,-1.0
54422967,Parallel execution of a snakemake rule with same input and a range of values for a single parameter,"I am transitioning a bash script to snakemake and I would like to parallelize a step I was previously handling with a for loop. The issue I am running into is that instead of running parallel processes, snakemake ends up trying to run one process with all parameters and fails.

My original bash script runs a program multiple times for a range of values of the parameter K.

for num in {1..3}
do
  structure.py -K $num --input=fileprefix --output=fileprefix
done


There are multiple input files that start with fileprefix. And there are two main outputs per run, e.g. for K=1 they are fileprefix.1.meanP, fileprefix.1.meanQ. My config and snakemake files are as follows.

Config:

cat config.yaml

infile: fileprefix
K:
  - 1
  - 2
  - 3


Snakemake:

configfile: 'config.yaml'

rule all:
    input:
        expand(""output/{sample}.{K}.{ext}"",
            sample = config['infile'],
            K = config['K'],
            ext = ['meanQ', 'meanP'])

rule structure:
    output:
        ""output/{sample}.{K}.meanQ"",
        ""output/{sample}.{K}.meanP""
    params:
        prefix = config['infile'],
        K = config['K']
    threads: 3
    shell:
        """"""
        structure.py -K {params.K} \
        --input=output/{params.prefix} \
        --output=output/{params.prefix}
        """"""


This was executed with snakemake --cores 3. The problem persists when I only use one thread.

I expected the outputs described above for each value of K, but the run fails with this error:

RuleException:
CalledProcessError in line 84 of Snakefile:
Command ' set -euo pipefail;  structure.py -K 1 2 3 --input=output/fileprefix \
--output=output/fileprefix ' returned non-zero exit status 2.
  File ""Snakefile"", line 84, in __rule_Structure
  File ""snake/lib/python3.6/concurrent/futures/thread.py"", line 56, in run


When I set K to a single value such as K = ['1'], everything works. So the problem seems to be that {params.K} is being expanded to all values of K when the shell command is executed. I started teaching myself snakemake today, and it works really well, but I'm hitting a brick wall with this.
",-1,-1,-1.0
54584678,Snakemake rule is run only for one file,"I have a rule in snakemake that runs HDBSCAN clustering. Previously it was regular DBSCAN and was working fine, but after I modified it, somehow the problem started (I modified also Snakemake file for other reasons, so hard to say what is to blame). So, I started seeing such a picture when only one file is run through HDBSCAN and results are generated. It gives no errors, just that next rules say that they are waiting for the missing files (that were not generated by the rule that runs HDBSCAN). Here is how the relevant parts of the Snakemake file look like:

configfile: ""config.yml""

samples,=glob_wildcards('data_files/normalized/{sample}.hdf5')
rule all:
    input:
        expand('results/tsne/{sample}_tsne.csv', sample=samples),
        expand('results/umap/{sample}_umap.csv', sample=samples),
        expand('results/umap/img/{sample}_umap.png', sample=samples),
        expand('results/tsne/img/{sample}_tsne.png', sample=samples),
        expand('results/clusters/umap/{sample}_umap_clusters.csv', sample=samples),
        expand('results/clusters/tsne/{sample}_tsne_clusters.csv', sample=samples),
        expand('results/neo4j/{sample}/{file}', sample=samples,
          file=['cells.csv', 'genes.csv', 'cl_contains.csv', 'cl_isin.csv', 'cl_nodes.csv', 'expr_by.csv', 'expr_ess.csv']),
        'results/neo4j/db_command'

rule cluster:
    input:
        script = 'python/dbscan.py',
        umap   = 'results/umap/{sample}_umap.csv'
    output:
        umap = 'results/umap/img/{sample}_umap.png',
        clusters_umap = 'results/clusters/umap/{sample}_umap_clusters.csv'
    shell:
        ""python {input.script} -umap_data {input.umap} -min_cluster_size {config[dbscan][min_cluster_size]} -img_umap {output.umap} -clusters_umap {output.clusters_umap}""


Here is how the dbscan.py looks like:

import numpy as np
import matplotlib.pyplot as plt
plt.switch_backend('agg')
from hdbscan import HDBSCAN
import pathlib
import os
import nice_service as ns

def run_dbscan(args):
    print('running HDBSCAN')

    path_to_img = args['-img_umap']
    path_to_clusters = args['-clusters_umap']
    path_to_data = args['-umap_data']

    # If folders in paths do not exist, create them
    for path_to_save in path_to_img:
        img_dir = os.path.dirname(path_to_save)
        pathlib.Path(img_dir).mkdir(parents=True, exist_ok=True) 

    for path_to_save in path_to_clusters:
        cluster_dir = os.path.dirname(path_to_save)
        pathlib.Path(cluster_dir).mkdir(parents=True, exist_ok=True) 

    #for idx, path_to_data in enumerate(data_arr):
    data = np.loadtxt(open(path_to_data, ""rb""), delimiter="","")
    db = HDBSCAN(min_cluster_size=int(args['-min_cluster_size'])).fit(data)

    # 'TRUE' where the point was assigned to cluster, 'FALSE' where not assigned
    # aka 'noise'
    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    core_samples_mask[db.labels_ != -1] = True
    labels = db.labels_

    # Number of clusters in labels, ignoring noise if present.
    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
    print('Estimated number of clusters: %d' % n_clusters_)
    unique_labels = set(labels)
    colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
    for k, col in zip(unique_labels, colors):
        if k == -1:
            # Black used for noise.
            col = [0, 0, 0, 1]
        class_member_mask = (labels == k)
        xy = data[class_member_mask &amp; core_samples_mask]
        plt.plot(xy[:, 0], xy[:, 1], '.', color=tuple(col), markersize=1)
            #plt.legend()

    plt.title('Estimated number of clusters: %d' % n_clusters_)
    plt.savefig(path_to_img, dpi = 500)
    np.savetxt(path_to_clusters, labels.astype(int), fmt='%i', delimiter="","")
    print('Finished running HDBSCAN algorithm')

if __name__ == '__main__':
    from sys import argv
    myargs = ns.getopts(argv)
    print(myargs)
    run_dbscan(myargs)


The input files for the rule cluster are all present and are correct. Somehow the rule is just skipped for all other files but one.
",-1,-1,-1.0
54665024,snakemake temp() causing unnecessary rerun of rules,"I'm using snakemake v 5.4.0, and I'm running into a problem with temp(). In a hypothetical scenario:

Rule A --&gt; Rule B1 --&gt; Rule C1
     |
      --&gt; Rule B2 --&gt; Rule C2 

where Rule A generates temp() files used by both pathways 1 (B1 + C1) and 2 (B2 + C2).


If I run the pipeline, the temp() files generated by RuleA are deleted by after they are used in both pathways, which is what I expect. However, if I then want to re-run Pathway 2, the temp() files for RuleA must be recreated which triggers the re-run of the entire pipeline, not just Pathway2. This becomes very computationally expensive for long pipelines. Is there a good way to prevent this besides not using temp(), which in my case would require many TB of extra hard drive space?
",-1,-1,-1.0
54901886,"snakemake + singularity, getting rid of python dependency","I have a slightly weird question: consider the case of a containerized workflow where the entire workflow is executed using a single singularity container.
This still requires python to be available on the system to invoke snakemake --use-singularity, right? 
How can I get rid of this dependency as well? Can I just install python, snakemake, and singularity in my container and execute

singularity exec my-container snakemake --use-singularity


Is this safe to use with cluster configurations?

Edit: does not seem to work at all - I could not get snakemake with singularity to work from within another container...
",0,-1,-1.0
55007705,"Snakemake refuses to unpack input function when rule A is a dependency of rule B, but accepts it when rule A is the final rule","I have a snakemake workflow for a metagenomics project. At a point in the workflow, I map DNA sequencing reads (either single or paired-end) to metagenome assemblies made by the same workflow. I made an input function conform the Snakemake manual to map both single end and paired end reads with one rule. like so

import os.path
def get_binning_reads(wildcards):
    pathpe=(""data/sequencing_binning_signals/"" + wildcards.binningsignal + "".trimmed_paired.R1.fastq.gz"")
    pathse=(""data/sequencing_binning_signals/"" + wildcards.binningsignal + "".trimmed.fastq.gz"")
    if os.path.isfile(pathpe) == True :
      return {'reads' : expand(""data/sequencing_binning_signals/{binningsignal}.trimmed_paired.R{PE}.fastq.gz"", PE=[1,2],binningsignal=wildcards.binningsignal) }
    elif os.path.isfile(pathse) == True :
      return {'reads' : expand(""data/sequencing_binning_signals/{binningsignal}.trimmed.fastq.gz"", binningsignal=wildcards.binningsignal) }

rule backmap_bwa_mem:
  input:
    unpack(get_binning_reads),
    index=expand(""data/assembly_{{assemblytype}}/{{hostcode}}/scaffolds_bwa_index/scaffolds.{ext}"",ext=['bwt','pac','ann','sa','amb'])
  params:
    lambda w: expand(""data/assembly_{assemblytype}/{hostcode}/scaffolds_bwa_index/scaffolds"",assemblytype=w.assemblytype,hostcode=w.hostcode)
  output:
    ""data/assembly_{assemblytype}_binningsignals/{hostcode}/{binningsignal}.bam""
  threads: 100
  log:
    stdout=""logs/bwa_backmap_samtools_{assemblytype}_{hostcode}.stdout"",
    samstderr=""logs/bwa_backmap_samtools_{assemblytype}_{hostcode}.stdout"",
    stderr=""logs/bwa_backmap_{assemblytype}_{hostcode}.stderr""
  shell:
    ""bwa mem -t {threads} {params} {input.reads} 2&gt; {log.stderr} | samtools view -@ 12 -b -o {output}  2&gt; {log.samstderr} &gt; {log.stdout}""


When I make an arbitrary 'all-rule' like this, the workflow runs successfully.

rule allbackmapped:
  input:
    expand(""data/assembly_{assemblytype}_binningsignals/{hostcode}/{binningsignal}.bam"",       binningsignal=BINNINGSIGNALS,assemblytype=ASSEMBLYTYPES,hostcode=HOSTCODES)


However, when the files created by this rule are required for subsequent rules like so:

rule backmap_samtools_sort:
  input:
    ""data/assembly_{assemblytype}_binningsignals/{hostcode}/{binningsignal}.bam""
  output:
    ""data/assembly_{assemblytype}_binningsignals/{hostcode}/{binningsignal}.sorted.bam""
  threads: 6
  resources:
    mem_mb=5000
  shell:
""samtools sort -@ {threads} -m {mem_mb}M -o {output} {input}""
rule allsorted:
  input:
    expand(""data/assembly_{assemblytype}_binningsignals/{hostcode}/{binningsignal}.sorted.bam"",binningsignal=BINNINGSIGNALS,assemblytype=ASSEMBLYTYPES,hostcode=HOSTCODES)


The workflow closes with this error


  WorkflowError in line 416 of
  /stor/azolla_metagenome/Azolla_genus_metagenome/Snakefile: Can only
  use unpack() on list and dict


To me, this error suggests the input function for the former rule is faulty. This however, seems not to be the case for it ran successfully when no subsequent processing was queued.

The entire project is hosted on github. The entire Snakefile and a github issue.
",-1,1,-1.0
55029406,Snakemake rule to write a new text file from input variables (Snakemake syntax),"I've got a fully functional Snakemake workflow, but I'd like to add a rule where the input variables are written out as new lines in a newly generated output text file. To briefly summarize, I've included relevant code below:

OUTPUTDIR = config[""outputDIR""] 
SAMPLEID = list(SAMPLE_TABLE.Sample_Name)
# Above 2 lines are functional in other parts of script.

rule all:
  input:
    manifest = OUTPUTDIR + ""/manifest.txt""

rule write_manifest:
  input:
    sampleid = SAMPLEID,
    loc_r1 = expand(""{base}/trimmed/{sample}_1.trimmed.fastq.gz"", base = OUTPUTDIR, sample = SAMPLELIST),
    loc_r2 = expand(""{base}/trimmed/{sample}_2.trimmed.fastq.gz"", base = OUTPUTDIR, sample = SAMPLELIST)
  output:
    OUTPUTDIR + ""/manifest.txt""
  shell:
    """"""
    echo ""{input.sampleid},{input.loc_r1},forward"" &gt;&gt; {output}
    echo ""{input.sampleid},{input.loc_r2},reverse"" &gt;&gt; {output}
    """"""


My issue is that Snakemake is reading in files, and I need it to print the file path or sample id that is it detecting instead.
Help with syntax?

Desired output file needs to look like this:

depth1,$PWD/raw_seqs_dir/Test01_full_L001_R1_001.fastq.gz,forward
depth1,$PWD/raw_seqs_dir/Test01_full_L001_R2_001.fastq.gz,reverse
depth2,$PWD/raw_seqs_dir/Test02_full_L001_R1_001.fastq.gz,forward
depth2,$PWD/raw_seqs_dir/Test02_full_L001_R2_001.fastq.gz,reverse


Trying to write this using echo.

Error message:

Building DAG of jobs...
MissingInputException in [write_manifest]:
Missing input files for rule write_manifest:
sample1
sample2
sample3


UPDATE:
by adding sampleid to params:

rule write_manifest:
  input:
    loc_r1 = expand(""{base}/trimmed/{sample}_{suf}_1.trimmed.fastq.gz"", base = SCRATCHDIR, sample = SAMPLE$
    loc_r2 = expand(""{base}/trimmed/{sample}_{suf}_2.trimmed.fastq.gz"", base = SCRATCHDIR, sample = SAMPLE$
  output:
    OUTPUTDIR + ""/manifest.txt""
  params:
    sampleid = SAMPLEID
  shell:
    """"""
    echo ""{params.sampleid},{input.loc_r1},forward"" &gt;&gt; {output}
    echo ""{params.sampleid},{input.loc_r2},reverse"" &gt;&gt; {output}
    """"""


My output looked like this (which is incorrect)

sample1 sample2 sample3,$PWD/tmp/dir/sample1.fastq $PWD/tmp/dir/sample2.fastq $PWD/tmp/dir/sample3.fastq,forward
sample1 sample2 sample3,$PWD/tmp/dir/sample1.fastq $PWD/tmp/dir/sample2.fastq $PWD/tmp/dir/sample3.fastq,reverse


This is still not what I want, I need it to look like the below desired output. Can I write it so Snakemake loops through each sample/input/params?
Desired output file needs to look like this:

depth1,$PWD/raw_seqs_dir/Test01_full_L001_R1_001.fastq.gz,forward
depth1,$PWD/raw_seqs_dir/Test01_full_L001_R2_001.fastq.gz,reverse
depth2,$PWD/raw_seqs_dir/Test02_full_L001_R1_001.fastq.gz,forward
depth2,$PWD/raw_seqs_dir/Test02_full_L001_R2_001.fastq.gz,reverse

",-1,1,-1.0
54946274,A curious case of snakemake,"I have a similar goal as in Snakemake: unknown output/input files after splitting by chromosome , however, as pointed out, I do know in advance that I have e.g., 5 chromosomes in my sample.bam file.  Using as a toy example:

$ cat sample.bam 
chromosome 1
chromosome 2
chromosome 3
chromosome 4
chromosome 5


I wish to ""split"" this bam file, and then do a bunch of per chromosome downstream jobs on the resulting chromosomes.  The simplest solution I could conjure up was:

chromosomes = '1 2 3 4 5'.split()

rule master :
    input :
        expand('sample.REF_{chromosome}.bam',
            chromosome = chromosomes)


rule chromosome :
    output :
        touch('sample.REF_{chromosome}.bam')

    input : 'split.done'


rule split_bam :
    output :
        touch('split.done')

    input : 'sample.bam'

    run :
        print('splitting bam..')
        chromosome = 1
        for line in open(input[0]) :
            outfile = 'sample.REF_{}.bam'.format(chromosome)
            print(line, end = '', file = open(outfile, 'w'))
            chromosome += 1


results in empty sample_REF_{chromosome}.bam files.  I understand why this happens, and indeed snakemake even warns, e.g.,

Warning: the following output files of rule chromosome were not present when the DAG was created:
{'sample.REF_3.bam'}
Touching output file sample.REF_3.bam.


that is, these files were not in the DAG to begin with, and snakemake touches them with empty versions, erasing what was put there.  I guess I am surprised by this behavior, and wonder if there is a good reason for this.  Note that this behavior is not limited to snakemake's touch(), since, should I replace touch('sample.REF_{chromosome}.bam') with simply 'sample.REF_{chromosome}.bam', and then have a shell :touch {output}`, I get the same result.  Now, of course, I have found a perfectly acceptable workaround:

chromosomes = '1 2 3 4 5'.split()

rule master :
    input :
        expand('sample.REF_{chromosome}.bam',
            chromosome = chromosomes)


rule chromosome :
    output : 'sample.REF_{chromosome}.bam'

    input : 'split_dir'

    shell : 'mv {input}/{output} {output}'


rule split_bam :
    output :
        temp(directory('split_dir'))

    input : 'sample.bam'

    run :
        print('splitting bam..')
        shell('mkdir {output}')
        chromosome = 1
        for line in open(input[0]) :
            outfile = '{}/sample.REF_{}.bam'.format(output[0], chromosome)
            print(line, end = '', file = open(outfile, 'w'))
            chromosome += 1


but I am surprised I have to go though these gymnastics for a seemingly simple task.  For this reason, I wonder if there is a better design, or if I am not asking the right question.  Any advice/ideas are most welcome.
",1,1,-1.0
54922713,Snakemake processing large workflow slow due to lengthy sequential checking of job completion? >100x speed reduction,"I am working on a rather complex snakemake workflow that spawns several hundreds of thousands of jobs. Everything works... The workflow executes, DAG gets created (thanks to the new checkpoint implementation), but is unbearably slow. I think the bottleneck is in checking off the successfully completed jobs before continuing to the next. More annoyingly it does this sequentially for all the jobs that are started in a batch, and the next batch is only executed when all checks are successful. The execution time of each job takes about 1 or two seconds and is done in parallel (1 core per job), but snakemake then cycles through the job completion check one at a time taking 5 to 10 seconds per job. So the whole process per batch takes minutes. See part of the log below that shows the different timestamps of consecutive jobs run in the same batch. There is about 11 seconds difference between the timestamps

Finished job 42227.
5853 of 230419 steps (3%) done
[Thu Feb 28 09:41:09 2019]
Finished job 119645.
5854 of 230419 steps (3%) done
[Thu Feb 28 09:41:21 2019]
Finished job 161354.
5855 of 230419 steps (3%) done
[Thu Feb 28 09:41:32 2019]
Finished job 160224.
5856 of 230419 steps (3%) done
[Thu Feb 28 09:41:42 2019]
Finished job 197063.
5857 of 230419 steps (3%) done
[Thu Feb 28 09:41:53 2019]
Finished job 200063.
5858 of 230419 steps (3%) done
[Thu Feb 28 09:42:04 2019]
Finished job 45227.
5859 of 230419 steps (3%) done
[Thu Feb 28 09:42:15 2019]
Finished job 44097.
5860 of 230419 steps (3%) done
[Thu Feb 28 09:42:26 2019]
Finished job 5387.
5861 of 230419 steps (3%) done
[Thu Feb 28 09:42:37 2019]
Finished job 158354.
5862 of 230419 steps (3%) done
[Thu Feb 28 09:42:48 2019]

So for 20 parallel processes 2 seconds are used for computation, but then goes idle for 20x11 = 220 seconds before continuing processing the next 20 jobs.

In the run above I had about 200K+ jobs. If I scale down the time between logs becomes much much shorter:

Finished job 2630.
5 of 25857 steps (0.02%) done
[Thu Feb 28 10:14:31 2019]
Finished job 886.
6 of 25857 steps (0.02%) done
[Thu Feb 28 10:14:31 2019]
Finished job 9606.
7 of 25857 steps (0.03%) done
[Thu Feb 28 10:14:31 2019]
Finished job 4374.
8 of 25857 steps (0.03%) done
[Thu Feb 28 10:14:32 2019]
Finished job 6118.
9 of 25857 steps (0.03%) done
[Thu Feb 28 10:14:32 2019]
Finished job 7862.
10 of 25857 steps (0.04%) done
[Thu Feb 28 10:14:32 2019]
Finished job 278.
11 of 25857 steps (0.04%) done
[Thu Feb 28 10:14:32 2019]
Finished job 2022.
12 of 25857 steps (0.05%) done
[Thu Feb 28 10:14:33 2019]

With 25K jobs the checking time is now &lt; 1 second.

I hope I am missing an argument here, or some parameter, but I am fearing that it maybe something non-trivial.

I run snakemake with the following flags:
snakemake --keep-going --snakefile My.Snakefile --configfile config.yaml -j 23 --max-jobs-per-second 23

I am running my workflow locally on a 24 core system with 256 GB ram.

Any help to speed things up would greatly be appreciated!

John
",1,-1,-1.0
55272746,Running snakemake with several cores,"I am trying to run a Snakefile using snakemake, which works fine when I go into the right directory and type

conda create --name durp python=3.5
snakemake


However, if I now use the -j switch to use more than 1 core, I get an error

snakemake -j 8
ERROR: Running in parallel is not supported on Python 2


I am specifying python3.5 when I create a conda environment though…I am not sure why I get this error. Any ideas?
",-1,-1,-1.0
55300845,"Snakemake : ""wildcards in input files cannot be determined from output files""","I use Snakemake to execute some rules, and I've a problem with one:

rule filt_SJ_out:
input: 
    ""pass1/{sample}SJ.out.tab""

output:
    ""pass1/SJ.db""

shell:''' 
gawk '$6==1 || ($6==0 &amp;&amp; $7&gt;2)' {input} &gt;&gt; {output};


'''


Here, I just want to merge some files into a general file, but by searching on google I've see that wildcards use in inputs must be also use in output.

But I can't find a solution to work around this problem ..

Thank's by advance 
",-1,-1,-1.0
55576386,NameError in Snakemake dryrun mode,"I am new to Snakemake and I am trying to develop some pipelines. I am encountering some problems when I use wildcards, trying to automate my bioinformatic analyses as much as possible. I run into troubles when the pipeline becomes more complex (as shown below). It looks like Snakemake does not resolve the wildcards correctly. During a dry run of the Snakefile, the wildcards values look correct in the executions of some rules. However, the same wildcards lead to an error in a different step(rule) of the pipeline, and I cannot figure out why. Below I provide the code and the output message of a dry run. 

num=[""327905-LR-41624_normal"",""327907-LR-41624_tumor""]
num_normal=[""327905-LR-41624""]
num_tumor=[""327907-LR-41624""]

path=""/path/to/Snakemake/""
genome=""/path/to/references_genome/Mus_musculus.GRCm38.dna_rm.toplevel.fa""

rule all:
    input:  
    expand(""/path/to/Snakemake/AS-{num_tum}_tumor_no_dupl_sort_RG_LB.bam"",num_tum=num_tumor),
    expand(""/path/to/Snakemake/AS-{num_norm}_normal_no_dupl_sort_RG_LB.bam"",num_norm=num_normal)
ruleorder: samtools_sort &gt; remove_duplicates &gt;  samtools_index #&gt;     add_readgroup_tumor &gt; add_readgroup_normal

rule trim_galore:
    input:
        r1=""/path/to/Snakemake/AS-{num}_R1.fastq"",
        r2=""/path/to/Snakemake/AS-{num}_R2.fastq""
    output:
        ""/path/to/Snakemake/AS-{num }_R1_val_1.fq"",
        ""/path/to/Snakemake/AS-{num }_R2_val_2.fq""
    shell:
        ""module load trim-galore/0.5.0 ; module load pypy/2.7-6.0.0 ; trim_galore  --output_dir /path/to/Snakemake/  --paired {input.r1} {input.r2}  ""  

rule bwa_mem:
    input:
        R1=""/path/to/Snakemake/AS-{num}_R1_val_1.fq"",
        R2=""/path/to/Snakemake/AS-{num}_R2_val_2.fq""
    output:
        ""/path/to/Snakemake/AS-{num}.bam""
    shell:
        ""module load samtools/default ; module load bwa/0.7.8 ; bwa mem  {genome}  {input.R1} {input.R2} | samtools view -h -b  &gt; {output} ""

rule samtools_sort:
    input:
        ""/path/to/Snakemake/AS-{num}.bam""
    output:
        ""/path/to/Snakemake/AS-{num}_sort.bam""
    shell:
        ""module load samtools/default ; samtools sort -n  -O BAM {input} &gt; {output} ""

rule remove_duplicates:
    input:
        ""/path/to/Snakemake/AS-{num}_sort.bam""
    output:
        outbam=""/path/to/Snakemake/AS-{num}_no_dupl_sort.bam"",
        metrics=""/path/to/Snakemake/AS-{num}_dupl_metrics.txt""
    shell:
        ""module load gatk/4.0.9.0 ; gatk MarkDuplicates -I {input}  -O {output.outbam} -M {output.metrics}  --REMOVE_DUPLICATES=true ""

rule samtools_index:
    input:
        ""/path/to/Snakemake/AS-{num}_no_dupl_sort.bam""
    output:
        ""/path/to/Snakemake/AS-{num}_no_dupl_sort.bam.bai""
    shell:
        ""module load samtools/default ; samtools index  {input} ""

rule add_readgroup_normal:
    input:
    ""/path/to/Snakemake/AS-{num_normal}_normal_no_dupl_sort.bam""
output:
    ""/path/to/Snakemake/AS-{num_normal}_normal_no_dupl_sort_RG_LB.bam""
shell:
    ""module load gatk/4.0.9.0 ;  gatk AddOrReplaceReadGroups   -PL Illumina -LB  { num_normal }   -PU  { num_normal }  -SM  NORMAL  -I  { input }    -O  {output} ""

rule add_readgroup_tumor:
    input:
        ""/path/to/Snakemake/AS-{num_tumor}_tumor_no_dupl_sort.bam""
    output:
        ""/path/to/Snakemake/AS-{num_tumor}_tumor_no_dupl_sort_RG_LB.bam""
    shell:
        ""module load gatk/4.0.9.0 ;  gatk AddOrReplaceReadGroups   -PL Illumina -LB  { num_tumor }   -PU  { num_tumor }  -SM  TUMOR     -I  { input }    -O  {output} ""


When I test the Snakefile with the command:
    .local/bin/snakemake  -s Snakefile_pipeline  --dryrun 

I get the following:

**Building DAG of jobs...**


**Job counts:**
    **count jobs
    1   add_readgroup_normal
    1   add_readgroup_tumor
    1   all
    2   bwa_mem
    2   remove_duplicates
    2   samtools_sort
    2   trim_galore
    11**

**[Mon Apr  8 16:14:27 2019]
rule trim_galore:
    input: /path/to/Snakemake/AS-327907-LR-41624_tumor_R1.fastq, /path/to/Snakemake/AS-327907-LR-41624_tumor_R2.fastq
    output: /path/to/Snakemake/AS-327907-LR-41624_tumor_R1_val_1.fq, /path/to/Snakemake/AS-327907-LR-41624_tumor_R2_val_2.fq
    jobid: 9
    wildcards: num=327907-LR-41624_tumor**


**[Mon Apr  8 16:14:27 2019]
rule trim_galore:
    input: /path/to/Snakemake/AS-327905-LR-41624_normal_R1.fastq, /path/to/Snakemake/AS-327905-LR-41624_normal_R2.fastq
    output: /path/to/Snakemake/AS-327905-LR-41624_normal_R1_val_1.fq, /path/to/Snakemake/AS-327905-LR-41624_normal_R2_val_2.fq
    jobid: 10
    wildcards: num=327905-LR-41624_normal**


**[Mon Apr  8 16:14:27 2019]
rule bwa_mem:
    input: /path/to/Snakemake/AS-327905-LR-41624_normal_R1_val_1.fq, /path/to/Snakemake/AS-327905-LR-41624_normal_R2_val_2.fq
    output: /path/to/Snakemake/AS-327905-LR-41624_normal.bam
    jobid: 8
    wildcards: num=327905-LR-41624_normal**


**[Mon Apr  8 16:14:27 2019]
rule bwa_mem:
    input: /path/to/Snakemake/AS-327907-LR-41624_tumor_R1_val_1.fq, /path/to/Snakemake/AS-327907-LR-41624_tumor_R2_val_2.fq
    output: /path/to/Snakemake/AS-327907-LR-41624_tumor.bam
    jobid: 7
    wildcards: num=327907-LR-41624_tumor**


**[Mon Apr  8 16:14:27 2019]
rule samtools_sort:
    input: /path/to/Snakemake/AS-327907-LR-41624_tumor.bam
    output: /path/to/Snakemake/AS-327907-LR-41624_tumor_sort.bam
    jobid: 5
    wildcards: num=327907-LR-41624_tumor**


**[Mon Apr  8 16:14:27 2019]
rule samtools_sort:
    input: /path/to/Snakemake/AS-327905-LR-41624_normal.bam
    output: /path/to/Snakemake/AS-327905-LR-41624_normal_sort.bam
    jobid: 6
    wildcards: num=327905-LR-41624_normal**


**[Mon Apr  8 16:14:27 2019]
rule remove_duplicates:
    input: /path/to/Snakemake/AS-327907-LR-41624_tumor_sort.bam
    output: /path/to/Snakemake/AS-327907-LR-41624_tumor_no_dupl_sort.bam, /path/to/Snakemake/AS-327907-LR-41624_tumor_dupl_metrics.txt
    jobid: 3
    wildcards: num=327907-LR-41624_tumor**


**[Mon Apr  8 16:14:27 2019]
rule remove_duplicates:
    input: /path/to/Snakemake/AS-327905-LR-41624_normal_sort.bam
    output: /path/to/Snakemake/AS-327905-LR-41624_normal_no_dupl_sort.bam, /path/to/Snakemake/AS-327905-LR-41624_normal_dupl_metrics.txt
    jobid: 4
    wildcards: num=327905-LR-41624_normal**


**[Mon Apr  8 16:14:27 2019]
rule add_readgroup_normal:
    input: /path/to/Snakemake/AS-327905-LR-41624_normal_no_dupl_sort.bam
    output: /path/to/Snakemake/AS-327905-LR-41624_normal_no_dupl_sort_RG_LB.bam
    jobid: 2
    wildcards: num_normal=327905-LR-41624**

**RuleException in line 93 of /home/l136n/Snakefile_mapping_snv_call_pipeline2:
NameError: The name ' num_normal ' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}**


I have googled the error but found little help. Also, I double checked the pipeline for any incosistency. What I expect as output is indicated in the rule ""all"".  The rules ""add_readgroup_normal"" and ""add_readgroup_tumor"" are supposed to take different subsets of input files, generated by the previous steps, which are run on all files. I wonder if the problem arises somehow because of this separation into 2 subsets. 
I repeat that I am quite new to Snakemake, so I might be missing something silly somewhere! Any help would be really appreciated, as I am completely stuck! 
Thank you so much in advance!

num=[""327905-LR-41624_normal"",""327907-LR-41624_tumor""]
normal=[""327905-LR-41624_normal""]
num_tumor=[""327907-LR-41624_tumor""]

path=""/path/to/Snakemake/""
genome=""/icgc/dkfzlsdf/analysis/B210/references_genome/Mus_musculus.GRCm38.dna_rm.toplevel.fa""

rule all:
    input:  
        ""/path/to/Snakemake/AS-327905-LR-41624_normal_R1_val_1.fq"",
        ""/path/to/Snakemake/AS-327905-LR-41624_normal_R2_val_2.fq"",
        ""/path/to/Snakemake/AS-327907-LR-41624_tumor_R1_val_1.fq"",
        ""/path/to/Snakemake/AS-327907-LR-41624_tumor_R2_val_2.fq"",
        ""/path/to/Snakemake/AS-327905-LR-41624_normal_no_dupl_sort.bam.bai"",
        ""/path/to/Snakemake/AS-327907-LR-41624_tumor_no_dupl_sort.bam.bai"",
        ""/path/to/Snakemake/AS-327905-LR-41624_normal_RG.bam""
        ""/path/to/Snakemake/AS-327907-LR-41624_tumor_RG.bam""


rule trim_galore:
    input:
        r1=""/path/to/Snakemake/AS-{num}_R1.fastq"",
        r2=""/path/to/Snakemake/AS-{num}_R2.fastq""
    output:
        ""/path/to/Snakemake/AS-{num }_R1_val_1.fq"",
        ""/path/to/Snakemake/AS-{num }_R2_val_2.fq""
    shell:
        ""module load trim-galore/0.5.0 ; module load pypy/2.7-6.0.0 ; trim_galore  --output_dir /path/to/Snakemake/  --paired {input.r1} {input.r2}  ""  

rule bwa_mem:
    input:
        R1=""/path/to/Snakemake/AS-{num}_R1_val_1.fq"",
        R2=""/path/to/Snakemake/AS-{num}_R2_val_2.fq""
    output:
        ""/path/to/Snakemake/AS-{num}.bam""
    shell:
        ""module load samtools/default ; module load bwa/0.7.8 ; bwa mem  {genome}  {input.R1} {input.R2} | samtools view -h -b  &gt; {output} ""

rule samtools_sort:
    input:
        ""/path/to/Snakemake/AS-{num}.bam""
    output:
        ""/path/to/Snakemake/AS-{num}_sort.bam""
    shell:
        ""module load samtools/default ; samtools sort -n  -O BAM {input} &gt; {output} ""

rule remove_duplicates:
    input:
        ""/path/to/Snakemake/AS-{num}_sort.bam""
    output:
        outbam=""/path/to/Snakemake/AS-{num}_no_dupl_sort.bam"",
        metrics=""/path/to/Snakemake/AS-{num}_dupl_metrics.txt""
    shell:
        ""module load gatk/4.0.9.0 ; gatk MarkDuplicates -I {input}  -O {output.outbam} -M {output.metrics}  --REMOVE_DUPLICATES=true ""

rule samtools_index:
    input:
        ""/path/to/Snakemake/AS-{num}_no_dupl_sort.bam""
    output:
        ""/path/to/Snakemake/AS-{num}_no_dupl_sort.bam.bai""
    shell:
        ""module load samtools/default ; samtools index  {input} ""

rule add_readgroup_normal:
    input:
        ""/path/to/Snakemake/AS-{normal}_no_dupl_sort.bam""
    output:
        ""/path/to/Snakemake/AS-{normal}_RG.bam""
    shell:
        ""module load gatk/4.0.9.0 ;  gatk AddOrReplaceReadGroups   -PL Illumina -LB  { wildcards.normal }   -PU  { wildcards.normal }  -SM  NORMAL  -I  { input }    -O  {output} ""

rule add_readgroup_tumor:
    input:
        ""/path/to/Snakemake/AS-{num}_no_dupl_sort.bam""
    output:
        ""/path/to/Snakemake/AS-{num_,'.*tumor.*'}_RG.bam""
    shell:
        ""module load gatk/4.0.9.0 ;  gatk AddOrReplaceReadGroups   -PL Illumina -LB  { wildcards.num }   -PU  { wildcards.num }  -SM  TUMOR     -I  { input }    -O  {output} ""


Error:

Building DAG of jobs...
MissingInputException in line 37 of /home/l136n/Snakefile_mapping_snv_call_pipeline2b1:
Missing input files for rule trim_galore:
/path/to/Luca/Snakemake/AS-327905-LR-41624_normal_RG.bam/path/to/Luca/Snakemake/AS-327907-LR-41624_tumor_RG_R1.fastq
/path/to/Snakemake/AS-327905-LR-41624_normal_RG.bam/path/to/Luca/Snakemake/AS-327907-LR-41624_tumor_RG_R2.fastq

",-1,-1,-1.0
55776952,snakemake: write files from an array,"I have an array xx = [1,2,3] and I want to use Snakemake to create a list of (empty) files 1.txt, 2.txt, 3.txt.

This is the Snakefile I use:

xx = [1,2,3]
rule makefiles:
    output: expand(""{f}.txt"", f=xx)
    run:
        with open(output, 'w') as file:
            file.write('blank')


However instead of having three new shiny text files in my folder I see an error message:

expected str, bytes or os.PathLike object, not OutputFiles


Not sure what I am doing wrong.
",-1,-1,-1.0
55799280,Can snakemake create job error and output files automatically on a SLURM cluster?,"I work on GNU/Linux Ubuntu 16.04.5.

I have the following rule in a Snakefile:

rule cutadapt:
    input:
        reads = '{path2reads}/raw/reads.fq
    output:
        trimmed = '{path2reads}/trimmed/reads.fq
    shell:
        ""cutadapt -q 20 --minimum-length 40 --output {output.trimmed} {input.reads}""


Then, in my slurm.json file, I have:

...
    ""output"": ""output/log/job/output/{rule}%A.o"",
    ""error"": ""output/log/job/error/{rule}%A.e"",
...


In the Snakefile, I create the folders output/log/job/{error,output}.

This works fine, presumably because snakemake does not have to create new folders in order to store the error and output from the job, which I run like this:

snakemake output/reads/trimmed/reads.fq --cluster-config slurm.json --cluster ""sbatch ... --output {cluster.output} --error {cluster.error} ...""

So path2reads will be evaluated to output/reads.

Note that I have omitted parameters which I deemed irrelevant for this discussion.

However, I would like SLURM to store my results in the folders output/lob/job/error/{rule}{wildcards}.e and output/lob/job/output/{rule}{wildcards}.o. If I put those folders in my slurm.json file, the job fails. This structure ...{rule}{wildcards}... worked for other rules which did not need to create new folders (because the wildcards did not contain a folder path`. 

How can I get round this problem? I know I could figure out all the folders beforehand and create them before running snakemake but this seems inefficient. Isn't there a feature in snakemake that does this for me? After all, snakemake creates all output, benchmark and log folders if they don't exist. Why doesn't it do it for SLURM error and output files?

24th April 2019 update based on Johannes Koester's reply:

I have changed my rule to:

rule cutadapt:
    input:
        reads = '{path2reads}/raw/reads.fq
    output:
        trimmed = '{path2reads}/trimmed/reads.fq
    log:
        output = 'output/log/snakemake/output/cutadapt/path2reads={path2reads}.o',
        error = 'output/log/snakemake/error/cutadapt/path2reads={path2reads}.e',
        jobError = 'output/log/job/error/cutadapt/path2reads={path2reads}.e',
        jobOutput = 'output/log/job/output/cutadapt/path2reads={path2reads}.o',
    shell:
        'cutadapt -q 20 --minimum-length 40 --output {output.trimmed} {input.reads} &gt; {log.output} 2&gt; {log.error}'


and run the following snakemake command:

snakemake paths/2/reads/trimmed/reads.fq --cluster-config slurm.json --cluster ""sbatch ... --output {cluster.output} --error {cluster.error} ...""


My jobs fail and some log directories are missing. I see the directories output/log/job/{error,output} but they are empty. I do not see the directory output/log/snakemake. However, if I first create the directories output/log/{job,snakemake}/{error,output}/cutadapt/path2reads=path/2/reads/, then the jobs are successful.

If I run snakemake on the head node, it also works. Note my slurm.json has the following parameters for fastqc (default parameters not shown):

""fastqc"" :
      {
          ""output"" : ""output/log/job/output/{rule}/{wildcards}.o"",
          ""error""  : ""output/log/job/error/{rule}/{wildcards}.e""
      },


Do you know what could be going wrong?
",-1,-1,-1.0
55850808,always report errors: 'str' object is not callable in my snakemake for RNA-seq workflow,"I want to use snakemake to write my RNA-seq pipeline,but it always report same errors.It annoys me !

The following shows whole file at present folder. 

|-- 01_raw
|   |-- epcr1_1.fastq
|   |-- epcr1_2.fastq
|   |-- epcr2_1.fastq
|   |-- epcr2_2.fastq
|   |-- wt1_1.fastq
|   |-- wt1_2.fastq
|   |-- wt2_1.fastq
|   `-- wt2_2.fastq
|-- 02_clean
|   `-- id.txt
|-- Snakefile
`-- Snakemake2.py


there is my whole content in Snakefile

SBT=[""wt1"",""wt2"",""epcr1"",""epcr2""]


rule all:
    input:
        expand(""02_clean/{nico}_1.paired.fq.gz"",""02_clean/{nico}_2.paired.fq.gz"",nico=SBT)

rule trim_galore:
    input:
        ""01_raw/{nico}_1.fastq"",
        ""01_raw/{nico}_2.fastq""
    output:
        ""02_clean/{nico}_1.paired.fq.gz"",
        ""02_clean/{nico}_1.unpaired.fq.gz"",
        ""02_clean/{nico}_2.paired.fq.gz"",
        ""02_clean/{nico}_2.unpaired.fq.gz"",
    log:
        ""02_clean/{nico}_qc.log""
    shell:
        ""Trimmomatic PE -threads 16 {input[0]} {input[1]} {output[0]} {output[1]} {output[2]} {output[3]} ILLUMINACLIP:/software/Trimmomatic-0.36/adapters/TruSeq3-PE-2.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 &amp;""


when I use command ""snakemake -np"" to dry_run it, I hope it can run smoothly,but It always report same errors:

TypeError in line 6 of /root/s/r/snakemake/my_rnaseq_data/Snakefile:
'str' object is not callable
  File ""/root/s/r/snakemake/my_rnaseq_data/Snakefile"", line 6, in &lt;module&gt;


and the line6 is 

expand(""02_clean/{nico}_1.paired.fq.gz"",""02_clean/{nico}_2.paired.fq.gz"",nico=SBT)


I dont't know what's wrong with it. It annoys me whole day! Hope somebody can help me.Thanks advance!
",-1,-1,-1.0
55877593,Snakemake subworkflow result is not found if I specify a target in subfolder,"Let's regard two snakefiles, one main file and one subworkflow:

./Snakefile:

subworkflow sub:
    workdir: "".""
    snakemake: ""subworkflow/Snakefile""

rule all:
    input: sub(""subresult"")


./subworkflow/Snakefile:

rule sub_all:
    output: ""subresult""
    shell: ""touch {output}""


This code works pretty well. Now let's introduce a small change: substitute ""subresult"" with ""./subresult"" in the main file:
output: ""./subresult""
That still works, but if I make the same change in the subworkflow, I get the exception:

MissingRuleException:
No rule to produce subresult


The same exception is thrown if I specify any other subfolder in the output of the subworkflow's rule:

subworkflow sub:
    workdir: "".""
    snakemake: ""subworkflow/Snakefile""

rule all:
    input: sub(""ANY_PATH/subresult"")


rule sub_all:
    output: ""ANY_PATH/subresult""
    shell: ""touch {output}""


I guess this is not a normal behavior. Is there anything wrong in my code? Is there a way to specify subworkflow's target in a subfolder?

OS: Windows + MinGW

Python 3.6.5

Snakemake 5.4.5, 5.2

Update:

I tried the example provided by @JeeYem, and even data subdirectory didn't work on my system. After some investigation I found that this is a platform-specific problem for Windows or Windows/MinGW combination. Below is the code that works and shows the problem (I left the original code commented for comparison):

File Snakefile:

subworkflow otherworkflow:
    workdir:
        "".""
    snakefile:
        ""kingmaker.Snakefile""

rule all:
    input:
        otherworkflow('data/a.txt')


Subworkflow file kingmaker.Snakefile:

rule write_file:
    output:
        #'data/a.txt'
        'data\\a.txt'
    shell:
        #'touch {output}'
        'touch data/a.txt'


Note that I even cannot use {output} variable in the shell section.

I will submit a ticket to the Snakemake repository.
",-1,-1,-1.0
55878044,"snakemake always report "" MissingOutputException in line 44, Missing files after 5 seconds:","I always get the same error report in my RNAs-seq pipeline by snakemake:

MissingOutputException in line 44 of /root/s/r/snakemake/my_rnaseq_data/Snakefile:
Missing files after 5 seconds:
03_align/wt2.bam
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.


Here is my Snakefile:

SBT=[""wt1"",""wt2"",""epcr1"",""epcr2""]

rule all:
    input:
        expand(""02_clean/{nico}_1.paired.fq"", nico=SBT),
        expand(""02_clean/{nico}_2.paired.fq"", nico=SBT),
        expand(""03_align/{nico}.bam"", nico=SBT)

rule trim:
    input:
        ""01_raw/{nico}_1.fastq"",
        ""01_raw/{nico}_2.fastq""
    output:
        ""02_clean/{nico}_1.paired.fq.gz"",
        ""02_clean/{nico}_1.unpaired.fq.gz"",
        ""02_clean/{nico}_2.paired.fq.gz"",
        ""02_clean/{nico}_2.unpaired.fq.gz"",
    shell:
        ""java -jar /software/Trimmomatic-0.36/trimmomatic-0.36.jar PE -threads 16 {input[0]} {input[1]} {output[0]} {output[1]} {output[2]} {output[3]} ILLUMINACLIP:/software/Trimmomatic-0.36/adapters/TruSeq3-PE-2.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 &amp;""

rule gzip:
    input:
        ""02_clean/{nico}_1.paired.fq.gz"",
        ""02_clean/{nico}_2.paired.fq.gz""
    output:
        ""02_clean/{nico}_1.paired.fq"",
        ""02_clean/{nico}_2.paired.fq""
    run:
        shell(""gzip -d {input[0]} &gt; {output[0]}"")
        shell(""gzip -d {input[1]} &gt; {output[1]}"")

rule map:
    input:
        ""02_clean/{nico}_1.paired.fq"",
        ""02_clean/{nico}_2.paired.fq""
    output:
        ""03_align/{nico}.sam""
    log:
        ""logs/map/{nico}.log""
    threads: 40
    shell:
        ""hisat2 -p 20 --dta -x /root/s/r/p/A_th/WT-Al_VS_WT-CK/index/tair10 -1 {input[0]} -2 {input[1]} -S {output} &gt;{log} 2&gt;&amp;1 &amp;""

rule sort2bam:
    input:
        ""03_align/{nico}.sam""
    output:
        ""03_align/{nico}.bam""
    threads:30
    shell:
        ""samtools sort -@ 20 -m 20G -o {output} {input} &amp;""


everything is fine until I add ""rule sort2bam"" part.

When I dry-run ,it goes fine. But when I execute it,it report error as the question describe. And Surprisely it run the task where it report it stuck in the background.But it always run the one task.like these:

rule sort2bam:
    input: 03_align/epcr1.sam
    output: 03_align/epcr1.bam
    jobid: 11
    wildcards: nico=epcr1

Waiting at most 5 seconds for missing files.
MissingOutputException in line 45 of /root/s/r/snakemake/my_rnaseq_data/Snakefile:
Missing files after 5 seconds:
03_align/epcr1.bam
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message


[Sat Apr 27 06:10:22 2019]
rule sort2bam:
    input: 03_align/wt1.sam
    output: 03_align/wt1.bam
    jobid: 9
    wildcards: nico=wt1

Waiting at most 5 seconds for missing files.
MissingOutputException in line 45 of /root/s/r/snakemake/my_rnaseq_data/Snakefile:
Missing files after 5 seconds:
03_align/wt1.bam
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message



[Sat Apr 27 06:23:13 2019]
rule sort2bam:
    input: 03_align/wt2.sam
    output: 03_align/wt2.bam
    jobid: 6
    wildcards: nico=wt2

Waiting at most 5 seconds for missing files.
MissingOutputException in line 44 of /root/s/r/snakemake/my_rnaseq_data/Snakefile:
Missing files after 5 seconds:
03_align/wt2.bam
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message


I don't know what's wrong with my code? Any ideals? Thanks in advance!
",-1,-1,-1.0
56175151,Snakemake DAG not working because of literal dot being encountered - possible Graphiz dot issue?,"I am trying to use the snakemake --dag or snakemake --rulegraph piped to  dot but I end up getting errors because I assume graphviz is not able to handle the literal dot '.' 

I am using snakemake 5.4.5 in a conda environment in linux.

I end up getting errors that keep saying ""syntax ambiguity - badly delimited number"" and I believe it is more of a graphviz issue than snakemake.

Has anybody else in the snakemake community experienced something like this and come up with a workaround?

Thanks in advance.

I happened to see something similar listed here - https://github.com/influxdata/kapacitor/issues/1068

snakemake --forceall --rulegraph | dot -Tpdf &gt; dag.pdf

Error messages

Warning: syntax ambiguity - badly delimited number '.11.' in line 1 of  splits into two tokens
Warning: syntax ambiguity - badly delimited number '.11.' in line 3 of  splits into two tokens
Warning: syntax ambiguity - badly delimited number '0.13.' in line 4 of  splits into two tokens
Warning: syntax ambiguity - badly delimited number '3.5.' in line 6 of  splits into two tokens
Building DAG of jobs...
",-1,-1,-1.0
56177987,"How to fix this ""IndexError: list index out of range"" in snakemake","I am setting up a new snakemake pipeline for the first time and running into an issue with the code.

I have tried to make it really simple in the beginning.


configfile: ""config.yaml""
SAMPLES, = glob_wildcards(""data/{sample}_L008_R1_001.fastq.gz"")

rule all:
    input:
        expand(""umi_labeled_fastq/{sample}.umi-extract.fq.gz"", sample=SAMPLES)
rule umi_tools_extract:
    input:
        ""data/{sample}_L008_R1_001.fastq.gz""
    output:
        ""umi_labeled_fastq/{sample}.umi-extract.fq.gz""
    shell:
        ""umi_tools extract --extract-method=regex --bc-pattern=”(?P&lt;umi_1&gt;.{6})(?P&lt;discard_1&gt;.{4}).*” -I {input} -S {output}""


here is the output I receive:

Job counts:
    count   jobs
    1   all
    6   umi_tools_extract
    7

[Thu May 16 16:55:05 2019]
rule umi_tools_extract:
    input: data/YL5_S221_L008_R1_001.fastq.gz
    output: umi_labeled_fastq/YL5_S221.umi-extract.fq.gz
    jobid: 3
    wildcards: sample=YL5_S221

RuleException in line 9 of /home/ryan/lexogen/test2.snakefile:
IndexError: list index out of range


If I remove this part from the regex pattern then I get no error:

--bc-pattern=”(?P&lt;umi_1&gt;.{6})(?P&lt;discard_1&gt;.{4}).*”

then I get no error.  How do I get around this?
",-1,-1,-1.0
56230133,problem with loop through a list in snakemake R script,"I tried to loop through a list in a R script in a snakemake rule as in the following snakefile, but got errors.

from snakemake.utils import R

rule test:
    run:
        R(""""""
            print(""hello!"")
            a = c(1, 2, 3)
            for (i in a)
            {
                print(i)
            }
        """""")


Here are the errors.

RuleException:
NameError in line 12 of Snakefile:
The name '\n    print(i)\n' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}
File ""Snakefile"", line 12, in __rule_test
File ""~/miniconda/envs/py36/lib/python3.6/concurrent/futures/thread.py"", line 56, in run
Exiting because a job execution failed. Look above for error message
Shutting down, this might take some time.


The code gave no errors when I ran it directly in R. Anyone have any ideas what's wrong?  Thanks.
",-1,-1,-1.0
56241962,Snakemake Getting Checkpoint and Aggregate Function to Work,"I'm having issue getting my snakemake aggregate command to work. My hope is to take a given GTF file, look for separate regions within the GTF and if found write these regions to a separate file. Thus, I'm unsure the number of output GTF files each input GTF file will create. In order to solve this problem I'm attempting to use a snakemake checkpoint.

To do this I wrote a brief script called collapse_gtf_file.py which simply takes in a GTF file, and generates N number of files corresponding to the number of individual regions found. So if given the file test_regions.gtf which had three regions, it would generate test_regions_1.gtf, test_regions_2.gtf test_regions_3.gtf respectivly. 

After said seperation, all GTF files should be converted to fasta files, and the aggregated.

However, I have not been able to get my checkpoint command to work. I can get the example cases to work, yet when I try and build a larger pipeline around this checkpoint it breaks. 

So far I've tried following the checkpoint tutorial found here https://snakemake.readthedocs.io/en/stable/snakefile/rules.html#dynamic-files 

sample=config[""samples""]
reference=config[""reference""]

rule all:
    input:
    ¦   expand(""string_tie_assembly/{sample}.gtf"", sample=sample),
    ¦                                                                                                 expand(""string_tie_assembly_merged/merged_{sample}.gtf"",sample=sample),
    ¦   #expand(""split_gtf_file/{sample}"", sample=sample),
    ¦   #expand(""lncRNA_fasta/{sample}.fa"", sample=sample),
    ¦   ""combined_fasta/all_fastas_combined.fa"",
    ¦   ""run_CPC2/cpc_calc_output.txt""

rule samtools_sort:
    input:
    ¦   ""mapped_reads/{sample}.bam""
    output:
    ¦   ""sorted_reads/{sample}.sorted.bam""
    shell:
    ¦   ""samtools sort -T sorted_reads/{wildcards.sample} {input} &gt; {output}""

rule samtools_index:
    input:
        ""sorted_reads/{sample}.sorted.bam""
    output:
        ""sorted_reads/{sample}.sorted.bam.bai""
    shell:
        ""samtools index {input}""

rule generate_fastq:
    input:
        ""sorted_reads/{sample}.sorted.bam""
    output:
        ""fastq_reads/{sample}.fastq""
    shell:
        ""samtools fastq {input} &gt; {output}""

rule string_tie_assembly:
    input:
        ""sorted_reads/{sample}.sorted.bam""
    output:
        ""string_tie_assembly/{sample}.gtf""
    shell:
        ""stringtie {input} -f 0.0 -a 0 -m 50 -c 3.0 -f 0.0 -o {output}""

rule merge_gtf_file_features:
    input:
    ¦   ""string_tie_assembly/{sample}.gtf""
    output:
    ¦   ""string_tie_assembly_merged/merged_{sample}.gtf""
    shell:
    ¦   #prevents errors when there's no sequence
    ¦   """"""
    ¦   set +e
    ¦   stringtie --merge -o {output} -m 25 -c 3.0 {input}
    ¦   exitcode=$?
    ¦   if [ $exitcode -eq 1 ]
    ¦   then
    ¦   ¦   exit 0
    ¦   else
    ¦   ¦   exit 0
    ¦   fi
    ¦   """"""

#This is where the issue seems to arise from. Modeled after https://snakemake.readthedocs.io/en/stable/snakefile/rules.html#dynamic-files 

checkpoint clustering:
    input:
    ¦   ""string_tie_assembly_merged/merged_{sample}.gtf""
    output:
    ¦   clusters = directory(""split_gtf_file/{sample}"")
    shell:
    ¦   """"""
    ¦   mkdir -p split_gtf_file/{wildcards.sample} ;

    ¦   python collapse_gtf_file.py -gtf {input} -o split_gtf_file/{wildcards.sample}/{wildcards.sample}
    ¦   """"""

rule gtf_to_fasta:
    input:
    ¦   ""split_gtf_file/{sample}/{sample}_{i}.gtf""
    output:
    ¦   ""lncRNA_fasta/{sample}/canidate_{sample}_{i}.fa""
    wildcard_constraints:
    ¦   i=""\d+""
    shell:
    ¦   ""gffread -w {output} -g {reference} {input}""


def aggregate_input(wildcards):
    checkpoint_output = checkpoints.clustering.get(**wildcards).output[0]
    x = expand(""lncRNA_fasta/{sample}/canidate_{sample}_{i}.fa"",
    ¦   sample=wildcards.sample,
    ¦   i=glob_wildcards(os.path.join(checkpoint_output, ""{i}.fa"")).i)
    return x

rule combine_fasta_file:
    input:
    ¦   aggregate_input
    output:
    ¦   ""combined_fasta/all_fastas_combined.fa""
    shell:
    ¦   ""cat {input} &gt; {output}""




Error Message:

InputFunctionException in combine_fasta_file:
WorkflowError: Missing wildcard values for sample
Wildcards:


What this seems to me is indicating that something is wrong with the way I've called wildcards above in the aggregate command, but I cannot figure out what. Any pointers would be very much appreciated.
",-1,-1,-1.0
56288018,snakemake PICARD merge bam files,"I am new in using snakemake, I have an issue when using PICARD MergeSamFiles to merge bam files into one bam files. I would like to merge 1_sorted.bam 2_sorted.bam ...10_sorted.bam into one bam file with directory name.

import snakemake.io 
import os.path

PICARD=""/data/src/picard.jar""
(SAMPLES,)=glob_wildcards(""bam/{sample}_sorted.bam"")
NAME=os.path.dirname

def bam_inputs(wildcards):
    files = expand(""bam/{sample}_sorted.bam"", sample=SAMPLES)
    INPUT = ""I=""+files 
    return INPUT

rule all:
    input: ""bam/{NAME}.bam""

rule merge_bams:
    input: bam_inputs
    output: ""bam/{NAME}.bam""
    params: mrkdup_jar=""/data/src/picard.jar""
    shell: ""java -Xmx16G -jar {params.mrkdup_jar} MergeSamFiles \
    {input} \
    O={output} \
    SORT_ORDER=coordinate \
    ASSUME_SORTED=false \
    USE_THREADING=true""


Error:

Building DAG of jobs...
WildcardError in line 12 of /data/data/Samples/snakemake-example/WGS-test/step3.smk:
Wildcards in input files cannot be determined from output files:
'NAME'


I don't know how to merge all bam files into one and don't know how to set the directory name as a variable to the final bam file. Please advice.

UPDATE:

import snakemake.io

PICARD=""/data/src/picard.jar""
(SAMPLES,)=glob_wildcards(""bam/{sample}_sorted.bam"")
#NAME=os.path.dirname
NAME=""test""

rule all:
    input: ""bam/{name}.bam"".format(name=NAME)

rule merge_bams:
    input: expand(""bam/{sample}_sorted.bam"",sample=SAMPLES)
    output: ""bam/{name}.bam"".format(name=NAME)
    params: mrkdup_jar=""/data/src/picard.jar""
    shell: """"""java -Xmx16G -jar {params.mrkdup_jar} MergeSamFiles \
    {""I="" + input} \
    O={output} \
    SORT_ORDER=coordinate \
    ASSUME_SORTED=false \
    USE_THREADING=true """"""

ERROR:

RuleException in line 11 of /data/data/Samples/snakemake-example/WGS-test/step3.smk:
NameError: The name '""I="" + input' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}

MergeSamFiles \
I= sub1_sorted.bam I=sub2_sorted.bam I=sub3_sorted.bam \
O= sub.bam \
SORT_ORDER=coordinate \
        ASSUME_SORTED=false \
        USE_THREADING=true

",-1,-1,-1.0
56346416,NameError Snakemake when I want to run my code,"I have a snakemake problem! I want to download fastq file by giving the SRR accesion number as sample. But when I run my code it says name sample is unknown. But it is defined! 

config.yaml

samples: [""SRR8181579""]
SRAToolKit: sratoolkit.2.9.6-centos_linux64/bin/fastq-dump


snakefile:

configfile: ""config.yaml""

rule all:
    input:
        expand(""{sample}.fastq"", sample = config[""samples""])

rule get_fastq:
    output:
        '{sample}.fastq'
    wildcard_constraints:
        sample=""SRR\d+""
    message:
      ""Downloading the fastq files""
    shell:
        '{config[SRAToolKit]} {sample}'


The error I get:


  RuleException in line 7 of /data/storix2/student/Thema11/dme/projectThema11/fastq.Pipeline:
  NameError: The name 'sample' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}

",-1,-1,-1.0
56374443,Snakemake on gatk VariantRecalibrator,"I am new in using snakemake, I have an issue when doing the step gatk VariantRecalibrator on snakemake, it generated error but the script can run without error when not in snakemake format.

import snakemake.io
import os

REF=""/data/data/reference/refs/ucsc.hg19.fasta""
HM=""/data/data/variant_call/hapmap_3.3.hg19.sites.vcf""
OMNI=""/data/data/variant_call/1000G_omni2.5.hg19.sites.vcf""
SNPS=""/data/data/variant_call/1000G_phase1.snps.high_confidence.hg19.sites.vcf""
DBSNP=""/data/data/variant_call/dbsnp_138.hg19.vcf""

NAME=""CHS""

rule all:
  input:  ""VCFs/{name}.recal.vcf"".format(name=NAME),
          ""VCFs/{name}.output.tranches"".format(name=NAME)

rule vqsr:
  input:  vcf=""VCFs/SRS008640.raw.vcf"",
          ref=REF,
          hm=HM,
          omni=OMNI,
          snps=SNPS,
          dbsnp=DBSNP
  output: recal=""VCFs/{name}.recal.vcf"".format(name=NAME),
          tranches=""VCFs/{name}.output.tranches"".format(name=NAME),
          rscript=""VCFs/{name}.output.plots.R"".format(name=NAME)
  params: java_opts=""-Xmx16g""
  shell: ""gatk --java-options -Xmx16g VariantRecalibrator \
  -R {input.ref} \
  -V {input.vcf} \
  --resource:hapmap,known=false,training=true,truth=true,prior=15.0 {input.hm} \
  --resource:omni,known=false,training=true,truth=false,prior=12.0 {input.omni} \
  --resource:1000G,known=false,training=true,truth=false,prior=10.0 {input.snps} \
  --resource:dbsnp,known=true,training=false,truth=false,prior=2.0 {input.dbsnp} \
  -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR \
  -mode SNP \
  -O {output.recal} \
  --tranches-file {output.tranches} \
  --rscript-file {output.rscript}""


Error:
Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to p rint the stack trace.
    [Thu May 30 08:05:30 2019]
    Error in rule vqsr:
        jobid: 1
        output: VCFs/CHS.recal.vcf, VCFs/CHS.output.tranches, VCFs/CHS.output.plots.R

RuleException:
CalledProcessError in line 28 of /data/data/Samples/snakemake-example/WGS-test/step7.smk:
    Command ' set -euo pipefail;  gatk --java-options -Xmx16g VariantRecalibrator   -R /data/data/reference/refs/ucsc.hg19.fas ta   -V VCFs/SRS008640.raw.vcf   --resource:hapmap,known=false,training=true,truth=true,prior=15.0 /data/data/variant_call /hapmap_3.3.hg19.sites.vcf   --resource:omni,known=false,training=true,truth=false,prior=12.0 /data/data/variant_call/1000 G_omni2.5.hg19.sites.vcf   --resource:1000G,known=false,training=true,truth=false,prior=10.0 /data/data/variant_call/1000G _phase1.snps.high_confidence.hg19.sites.vcf   --resource:dbsnp,known=true,training=false,truth=false,prior=2.0 /data/data/ variant_call/dbsnp_138.hg19.vcf   -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR   -mode SNP   -O VCFs/CHS. recal.vcf   --tranches-file VCFs/CHS.output.tranches   --rscript-file VCFs/CHS.output.plots.R ' returned non-zero exit sta tus 2.
      File ""/data/data/Samples/snakemake-example/WGS-test/step7.smk"", line 28, in __rule_vqsr
      File ""/root/miniconda3/envs/bioinfo/lib/python3.6/concurrent/futures/thread.py"", line 56, in run
    Removing output files of failed job vqsr since they might be corrupted:
    VCFs/CHS.recal.vcf, VCFs/CHS.output.tranches, VCFs/CHS.output.plots.R
    Shutting down, this might take some time.
    Exiting because a job execution failed. Look above for error message
    Complete log: /data/data/Samples/snakemake-example/WGS-test/.snakemake/log/2019-05-30T065011.676785.snakemake.log


If I use the same code, I can run to create the recal file and tranches and can go to the next step applyvqsr, however if I put it in the snakemake it has error and line 27 is gatk --java-options -Xmx16g VariantRecalibrator is error but I don't know what error is it. Please advice. 
",-1,-1,-1.0
56454248,Unable to install Snakemake in either command prompt nor anaconda prompt,"I'm trying to install Snakemake.

When I try to use pip, I receive the following error message:

ERROR: Complete output from command 'C:\Python\Python37\python.exe' -u -c 'import setuptools, tokenize;__file__='""'""'C:\\Users\\ANGELA~1\\AppData\\Local\\Temp\\pip-install-1kbp7voq\\datrie\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\ANGELA~1\AppData\Local\Temp\pip-record-_eliva3i\install-record.txt' --single-version-externally-managed --compile:
ERROR: running install
running build
running build_clib
building 'libdatrie' library
creating build
creating build\temp.win-amd64-3.7
creating build\temp.win-amd64-3.7\libdatrie
creating build\temp.win-amd64-3.7\libdatrie\datrie
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.21.27702\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Ilibdatrie ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.21.27702\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.21.27702\include"" /Tclibdatrie\datrie\alpha-map.c /Fobuild\temp.win-amd64-3.7\libdatrie\datrie\alpha-map.obj
alpha-map.c
libdatrie\datrie\alpha-map.c(27): fatal error C1083: Cannot open include file: 'ctype.h': No such file or directory
error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.21.27702\\bin\\HostX86\\x64\\cl.exe' failed with exit status 2
----------------------------------------
ERROR: Command ""'C:\Python\Python37\python.exe' -u -c 'import setuptools, tokenize;__file__='""'""'C:\\Users\\ANGELA~1\\AppData\\Local\\Temp\\pip-install-1kbp7voq\\datrie\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\ANGELA~1\AppData\Local\Temp\pip-record-_eliva3i\install-record.txt' --single-version-externally-managed --compile"" failed with error code 1 in C:\Users\ANGELA~1\AppData\Local\Temp\pip-install-1kbp7voq\datrie\


I've downloaded C++ and python build tools for MS Visual studio 2019. I've also upgraded setup tools.

When I try to install using the anaconda prompt, I get this:

PackagesNotFoundError: The following packages are not available from current channels:

 &gt; - snakemake -&gt; pygraphviz


conda install pygraphviz also yields the same message, and I've appended conda-forge to the channels.
",-1,-1,-1.0
56535751,Snakemake - How to use every line of input file as wildcard,"I am pretty new to using Snakemake and I have looked around on SO to see if there is a solution for the below - I am almost very close to a solution, but not there yet. 

I have a single column file containing a list of SRA ids and I want to use snakemake to define my rules such that every SRA id from that file becomes a parameter on command line.

#FileName = Samples.txt
Samples
SRR5597645
SRR5597646
SRR5597647


Snakefile below:

from pathlib import Path
shell.executable(""bash"")
import pandas as pd
import os
import glob
import shutil

configfile: ""config.json""

data_dir=os.getcwd()

units_table = pd.read_table(""Samples.txt"")
samples= list(units_table.Samples.unique())

#print(samples)

rule all:
    input:
           expand(""out/{sample}.fastq.gz"",sample=samples)

rule clean:
     shell: ""rm -rf .snakemake/""

include: 'rules/download_sample.smk'


download_sample.smk

rule download_sample:
    """"""
    Download RNA-Seq data from SRA.
    """"""
    input: ""{sample}""
    output: expand(""out/{sample}.fastq.gz"", sample=samples)
    params:
        outdir = ""out"",
        threads = 16
    priority:85
    shell: ""parallel-fastq-dump --sra-id {input} --threads {params.threads} --outdir {params.outdir}  --gzip ""


I have tried many different variants of the above code, but somewhere I am getting it wrong.

What I want: For every record in the file Samples.txt, I want the parallel-fastq-dump command to run. Since I have 3 records in Samples.txt, I would like these 3 commands to get executed 

parallel-fastq-dump --sra-id SRR5597645 --threads 16 --outdir out --gzip


parallel-fastq-dump --sra-id SRR5597646 --threads 16 --outdir out --gzip


parallel-fastq-dump --sra-id SRR5597647 --threads 16 --outdir out --gzip


This is the error I get 

snakemake -np
WildcardError in line 1 of rules/download_sample.smk:
Wildcards in input files cannot be determined from output files:
'sample'


Thanks in advance
",-1,-1,-1.0
56693699,"Snakemake claims rule exits with non-zero exit code, even with ""|| true""?","My snakemake pipeline asserts that my code raises a non-zero exit code whenever I run any rule, even though my code returns an error code of 0 if I manually run the same exact code, and it works perfectly normally when ran in Snakemake.

As per the advice of this question, I tried appending || true to the shell command in the snakemake rule, changing my rule from looking like

rule rulename:
    input:
        ""input/file""
    output:
        ""output/file""
    shell:
        ""python3.7 scripts/script.py {input} {output}""


to

rule rulename:
    input:
        ""input/file""
    output:
        ""output/file""
    shell:
        ""python3.7 scripts/script.py {input} {output} || true""


However, when I re-run the pipeline, snakemake still errors and says, (exited with non-zero exit code), even though the || true at the end will ensure that this command always returns an exit code of 0.

What is snakemake doing to cause that? For reference, I am using snakemake 5.5.0 with python 3.7.0, and the server I'm using has Ubuntu 16.04.5, if that's relevant.
",1,-1,-1.0
56700602,Can a snakemake rule allow empty output files?,"I have a rule in a snakemake workflow that might produce an empty output file, depending on the input. That output file is then used in another rule whose command will be totally ok with an empty file as input, as long as the path exists for that input file, and still complete successfully with the correct result. Is there a way I can write the workflow so that snakemake doesn't consider the first rule to have failed because it produced an empty file?

Edit with some output from snakemake:

RuleException:
CalledProcessError in line 123 of /home/ckern/All_Species_Chromatin_Model/Snakefile:
Command ' set -euo pipefail;  grep E6$ Model_10/Cattle_Cortex_10_segments.bed &gt; Model_10/Cattle_Cortex_10_E6.bed ' returned non-zero exit status 1.
  File ""/home/ckern/All_Species_Chromatin_Model/Snakefile"", line 123, in __rule_split_states
  File ""/share/apps/conda3/miniconda3/lib/python3.6/concurrent/futures/thread.py"", line 56, in run
Removing output files of failed job split_states since they might be corrupted:
Model_10/Cattle_Cortex_10_E6.bed
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

",-1,-1,-1.0
56833233,Error submitting jobs on a condor cluster with snakemake,"I am trying to sumbit some snakemake jobs to a condor cluster with the following command.

snakemake -f TestJob --cluster-config cluster.json -j 30 --cluster condor_submit


where the TestJob rule is specified in the SnakeFile

rule TestJob:
    input:
    output: ""test.txt""
    shell:
        ""touch test.txt;""


and the cluster-configuration file is the following:

{
    ""__default__"" :
    {
        ""output""    : ""workdir/logs/cluster/{rule}.{wildcards}.out"",
        ""error""     : ""workdir/logs/cluster/{rule}.{wildcards}.err"",
        ""log""       : ""workdir/logs/cluster/{rule}.{wildcards}.log""
    },

}


When I do this I obtain the following error:

Building DAG of jobs...
Using shell: /bin/bash
Provided cluster nodes: 30
Job counts:
    count   jobs
    1   TestJob
    1

[Mon Jul  1 11:18:04 2019]
rule TestJob:
    output: test.txt
    jobid: 0


ERROR: on Line 11 of submit file: 

ERROR: Failed to parse command file (line 11).
Error submitting jobscript (exit code 1):
Submitting job(s)
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message


Has anyone any idea of what might be happening? 
Thanks
",-1,-1,-1.0
56837811,Local singularity image in snakemake workflow on AWS eks with --kubernetes option,"I am trying to use Snakemake on AWS EKS, running shell commands in singularity containers. If I understood correctly, Snakemake is then run as a container itself by kubernetes, so I would run a singularity container inside a Pod.
I have a ubuntu image in the same folder of the Snakefile.
How do I tell Singularity the path where the image is stored?

I have tried to run the workflow specifying an image in the DockerHub and it works perfectly.
However I cannot make it work with images stored in my EC2 instance, if I specify --kubernetes flag (if I run it without --kubernetes, it runs perfectly also with local images). I tried specifying the path of the image on the EC2 instance, but it is not working.

My Snakefile looks something like that:

rule test:
    singularity:
        ""ubuntu.simg""
    output:
        ""out.txt""
    shell:
        ""cat /etc/os-release &gt; {output}""


and I run Snakemake with the command:

snakemake --kubernetes --default-remote-provider S3 --default-remote-prefix S3-bucket-name --use-singularity


Desired output is the file ""out.txt"" created in the specified s3 bucket with the info about the os release of the ""ubuntu.sigm"" container.
The actual result is a failed job. If I inspect with

kubectl logs snakejob-c25eaf1f-6ad4-505e-94a0-646543a59d33


I get the error:

ERROR  : Image path ubuntu.simg doesn't exist: No such file or directory
ABORT  : Retval = 255


Anyone who had the same issue?
",-1,-1,-1.0
56932937,Snakemake: Is it possible to use directories as wildcards?,"Hi I´m new in Snakemake and have a question. I want to run a tool to multiple data sets. One data set represents one tissue and for each tissue exists fastq files, which are stored in the respective tissue directory. The rough command for the tools is:

  python TEcount.py -rosette rosettefile -TE te_references -count result/tissue/output.csv -RNA &lt;LIST OF FASTQ FILE FOR THE RESPECTIVE SAMPLE&gt;          


The tissues shall be the wildcards. How can I do this? Below I have a first try that did not work.

import os                                                                        

#collect data sets                                                               
SAMPLES=os.listdir(""data/rnaseq/"")                                               


rule all:                                                                        
    input:                                                                       
        expand(""results/{sample}/TEtools.{sample}.output.csv"", sample=SAMPLES)                   

rule run_TEtools:                                                                
    input:                                                                       
        TEcount='scripts/TEtools/TEcount.py',                                    
        rosette='data/prepared_data/rosette/rosette',                            
        te_references='data/prepared_data/references/all_TE_instances.fa'        
    params:
        #collect the fastq files in the tissue directory                                                              
        fastq_files = os.listdir(""data/rnaseq/{sample}"")                         
    output:                                                                      
        'results/{sample}/TEtools.{sample}.output.csv'                           
    shell:                                                                       
        'python {input.TEcount} -rosette {input.rosette} -TE                     
{input.te_references} -count {output} -RNA {params.fastq_files}'


In the rule run_TEtools it does not know what the {sample} is.
",-1,-1,-1.0
57071860,Snakemake: MissingOutputException when output does exist,"I'm trying to make a snakemake pipeline and it complains about a MissingOutputException:

Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cluster nodes: 8
Job counts:
        count   jobs
        1       add_replace_readgroups
        1       all
        1       annotate_nirvana
        1       apply_bqsr
        1       base_recalibrator
        1       bwa_aln1
        1       bwa_aln2
        1       bwa_sampe
        1       flagstats
        1       haplotypecaller
        1       intervals
        1       isize
        1       json2vcf
        1       mark_dups
        1       mod_sam
        1       region_coverage
        1       sam2bam
        17

[Wed Jul 17 09:03:04 2019]
rule bwa_aln2:
    input: C052007W-B.2.fq.gz, /mnt/storage/projects/hiv_data/refs/PKD_GANAB/PKD.fasta
    output: tmp/C052007W-B.2.sai
    log: logs/bwa_aln2/C052007W-B.log
    jobid: 8
    wildcards: sample=C052007W-B
    threads: 8

bwa aln -t 8 -q 15 -e 50 -f tmp/C052007W-B.2.sai /mnt/storage/projects/hiv_data/refs/PKD_GANAB/PKD.fasta C052007W-B.2.fq.gz 2&gt; logs/bwa_aln2/C052007W-B.log
Submitted job 8 with external jobid 'Submitted batch job 283790'.

[Wed Jul 17 09:03:04 2019]
rule bwa_aln1:
    input: C052007W-B.1.fq.gz, /mnt/storage/projects/hiv_data/refs/PKD_GANAB/PKD.fasta
    output: tmp/C052007W-B.1.sai
    log: logs/bwa_aln1/C052007W-B.log
    jobid: 7
    wildcards: sample=C052007W-B
    threads: 8

bwa aln -t 8 -q 15 -e 50 -f tmp/C052007W-B.1.sai /mnt/storage/projects/hiv_data/refs/PKD_GANAB/PKD.fasta C052007W-B.1.fq.gz 2&gt; logs/bwa_aln1/C052007W-B.log
Submitted job 7 with external jobid 'Submitted batch job 283791'.
Waiting at most 30 seconds for missing files.
MissingOutputException in line 63 of /mnt/storage/home/kimy/projects/automate_CP/scripts/Snakefile:
Missing files after 30 seconds:
tmp/C052007W-B.2.sai
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Waiting at most 30 seconds for missing files.
[Wed Jul 17 09:04:04 2019]
Finished job 7.
1 of 17 steps (6%) done
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /mnt/storage/home/kimy/projects/automate_CP/CP0340/.snakemake/log/2019-07-17T090304.049684.snakemake.log


I'm running this using this command line:

/mnt/storage/home/kimy/software/miniconda3/bin/snakemake \
    -s /mnt/storage/home/kimy/projects/automate_CP/scripts/Snakefile \
    -j 8 \
    -p \
    --latency-wait 30 \
    --configfile ./${sample}_config.yaml \
    --cluster ""sbatch --job-name=NILES --ntasks=1 --partition=long --time=12:00:00 --mem=25000 --reservation=compute006""


However the files that Snakemake complains about do exist and if I rerun the same snakefile, the next rule is run but it complains about the output of that next rule.
Another detail: I ran the pipeline for a whole folder (where I have a bunch of fastqs and using another config file of course) and it worked fine for every sample except for 2 of them, prompting me to investigate the sample.

Do you know why I get this error?
",-1,-1,-1.0
57090794,Put optional input files for rule all in Snakemake,"In my Snakemake project, I have a config.yaml file which allows users to run certain steps of the pipeline or not, for example:

DEG : 
   exec : True


And so, in the Snakefile, I include the rules associate with the DEG:

if config[""DEG""][""exec""]:
   include: ""rules/classic_mapping.smk""
   include: ""rules/counts.smk""
   include: ""rules/run_DESeq2.smk""


The problem is, now I would like to dynamically specify the output files in the ""all"" rule, so that Snakemake knows which files to generate based on the parameters entered by the user. For example I had in mind to proceed as follows:

rule all:   
   input:
       if config[""DEG""][""exec""]:
          ""DEG/DEG.txt""
       if config[""DTU""][""exec""]:
          ""DTU/DTU.txt"" 


but it doesn't work:
    SyntaxError in line 58 of Unexpected keyword if in rule definition (Snakefile, line 58)

I would need an outside point of view to find an alternative because Snakemake should not work on this way

Thank's by advance
",-1,-1,-1.0
57418589,Bare bones necessary to run pipeline on snakemake?,"I have to perform imputation of genetic information, but scripts and files are located on different locations. There are some parallel jobs for could (cluster) involved as well. Too much of a manual running-waiting is necessary to get job done.

In order to streamline the mess, i want to put these bash scripts in Snakemake pipeline. The expected outcome would be a Snakefile with rules consisting only of shell scripts as input/output files are already specified.

For that i basically have to run this simple example:

Let's say we have input called test.txt containting list:

13 234 34 4


And bash script for ordering, called test.sh:

sort -n test.txt &gt; test1.txt


In order to run this script through Snakemake, i made Snakefile:

rule sort:
    shell:
            ""path/to/test.sh""


When i run it using command 

snakemake Snakefile 


i get following error:

/usr/bin/bash: path/to/test.sh: Permission denied


Could you guys point me on why such syntax would not work with snakemake?
",-1,-1,-1.0
57432036,Snakemake checkpoint (exited with non-zero exit code),"I need to make a checkpoint in Snakemake at the step where chromosomes are scattered to call copy number variants with GATK:

rule all:
    input:
        'aggregated/chr1'

# step that gives non-zero exit code error
checkpoint scattering:
    input:
        interval = 'gcfiltered_{chr}.interval_list'
    output:
        directory('scatter_{chr}')
    shell:
        'mkdir -p {output} &amp;&amp; '
        'gatk --java-options ""-Xmx8G"" IntervalListTools '
        '--INPUT {input.interval} '
        '--SUBDIVISION_MODE INTERVAL_COUNT '
        '--SCATTER_CONTENT 600 '
        '--OUTPUT {output}'

def aggregate_scatter(wildcards):
    checkpoint_output = checkpoints.scattering.get(**wildcards).output[0]
    return expand('scatter_{chr}/{i}/scattered.interval_list',
            chr=wildcards.chr,
            i=glob_wildcards(os.path.join(checkoint_output, '{i}/scattered.interval_list')).i)

# dummy rule to check if scattered files will be aggregated:
rule aggregate:
    input:
        aggregate_scatter
    output:
        ""aggregated/{chr}""
    shell:
        ""cat {input} &gt; {output}""




But the scattering rule fails. It gives an error (exited with non-zero exit code) and removes the output, although the output is produced correctly. I tried adding || true, it did not work.

However, when I run the command outside of snakemake, it runs fine with 0 exit status:

mkdir -p scatter_chr1 &amp;&amp; gatk --java-options ""-Xmx8G"" IntervalListTools --INPUT gcfiltered_chr1.interval_list --SUBDIVISION_MODE INTERVAL_COUNT --SCATTER_CONTENT 600 --OUTPUT scatter_chr1
echo $?


I use Snakemake 5.5.4 and GATK 4.1.2.0. Input file example (gcfiltered_chr1.interval_list):

@HD VN:1.6
@SQ SN:chr1 LN:122678785    UR:file:canFam3.fa  M5:e4671b339daa96b7f11eb0b68fd999d8
chr1    100000  100999  +   .
chr1    101000  101999  +   .
chr1    102000  102999  +   .
chr1    103000  103999  +   .
chr1    104000  104999  +   .
chr1    105000  105999  +   .
chr1    106000  106999  +   .
chr1    107000  107999  +   .
chr1    108000  108999  +   .
chr1    109000  109999  +   .
chr1    110000  110999  +   .
chr1    111000  111999  +   .
chr1    112000  112999  +   .
chr1    113000  113999  +   .
chr1    114000  114999  +   .
chr1    115000  115999  +   .
chr1    116000  116999  +   .
chr1    117000  117999  +   .
chr1    118000  118999  +   .
chr1    119000  119999  +   .
chr1    120000  120999  +   .

",-1,1,-1.0
57656699,Snakemake doesn't recognize wildcard,"I try to run snakemake but I have this error message:


  Missing input files for rule mapping_SE:
  Trimming/hira_2..trim.fastq


As you can see, the problem is that Snakemake expect two dots after the name of the sample, here ""hira_2""

Actually, the file in the folder have this name :
hira_2.trim.fastq

I have define this rule within my snakefile :

rule mapping_SE:        
    input:
        r = 'Trimming/{sample}.trim.fastq'


Normally, the wildcards for 'sample'. should be ""hira_1"", ""hira_2"", etc

So I don't understand why Snakemake add another dot after the name of the sample ..
",-1,-1,-1.0
57702048,"Snakemake, pandas, and NCBI: how do I combine a pandas dataframe with the remote NCBI search?","I'm still pretty new to Snakemake, and I've been having trouble with a rule I'm trying to write. 

I've been trying to combine using snakemake.remote.NCBI with accessing a pandas dataframe and using a wildcard, but I can't seem to make it work. 

I have a tsv file called genomes.tsv with several columns, where each row is one species. One is ""id"" and has the genbank id for the species's genomes. Another ""species"" has a short string unique for each species. In my Snakefile, genomes.tsv is imported as genomes, with only the id and species column, then species is set as genomes index and dropped from genome.

I want to use the values in ""species"" as values for the wildcard {species} in my workflow, and I want my rule to use snakemake.remote.NCBI to download each species's genome sequence in fasta format and then output it to a file ""{species}_gen.fa""

from snakemake.remote.NCBI import RemoteProvider as NCBIRemoteProvider
import pandas as pd

configfile: ""config.yaml""

NCBI = NCBIRemoteProvider(email=config[""email""]) # email required by NCBI to prevent abuse

genomes = pd.read_table(config[""genomes""], usecols=[""species"",""id""]).set_index(""species"")

SPECIES = genomes.index.values.tolist()

rule all:
    input: expand(""{species}_gen.fasta"",species=SPECIES)

rule download_and_count:
    input:
        lambda wildcards: NCBI.remote(str(genomes[str(wildcards.species)]) + "".fasta"", db=""nuccore"")
    output:
        ""{species}_gen.fasta""
    shell:
        ""{input} &gt; {output}""


Currently, trying to run my code results in a key error, but it says that the key is a value from species, so it should be able to get the corresponding genbank id from genomes.

EDIT: here is the error

InputFunctionException in line 18 of /home/sjenkins/work/olflo/Snakefile:
KeyError: 'cappil'
Wildcards:
species=cappil


cappil is a valid value for {species}, and it should be usable as an index, I think. Here are the first few rows of genomes, for reference:

species id  accession   name    assembly
cappil  8252558 GCA_004027915.1 Capromys_pilorides_(Desmarest's_hutia)  CapPil_v1_BIUU
cavape  1067048 GCA_000688575.1 Cavia_aperea_(Brazilian_guinea_pig) CavAp1.0
cavpor  175118  GCA_000151735.1 Cavia_porcellus_(domestic_guinea_pig)   Cavpor3.0


Update:

I tried changing the the input line to:

lambda wildcards: NCBI.remote(str(genomes[genomes['species'] == wildcards.species].iloc[0]['id']) + "".fasta"", db=""nuccore"")


but that gives me the error message:

Traceback (most recent call last):
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/init.py"", line 547, in snakemake
    export_cwl=export_cwl)
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/workflow.py"", line 421, in execute
    dag.init()
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/dag.py"", line 122, in init
    job = self.update([job], progress=progress)
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/dag.py"", line 603, in update
    progress=progress)
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/dag.py"", line 666, in update_
    progress=progress)
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/dag.py"", line 603, in update
    progress=progress)
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/dag.py"", line 655, in update_
    missing_input = job.missing_input
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/jobs.py"", line 398, in missing_input
    for f in self.input
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/jobs.py"", line 399, in 
    if not f.exists and not f in self.subworkflow_input)
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/io.py"", line 208, in exists
    return self.exists_remote
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/io.py"", line 119, in wrapper
    v = func(self, *args, **kwargs)
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/io.py"", line 258, in exists_remote
    return self.remote_object.exists()
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/remote/NCBI.py"", line 72, in exists
    likely_request_options = self._ncbi.guess_db_options_for_extension(self.file_ext, db=self.db, rettype=self.rettype, retmode=self.retmode)
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/remote/NCBI.py"", line 110, in file_ext
    accession, version, file_ext = self._ncbi.parse_accession_str(self.local_file())
  File ""/home/sjenkins/miniconda3/envs/olflo/lib/python3.7/site-packages/snakemake/remote/NCBI.py"", line 366, in parse_accession_str
    assert file_ext, ""file_ext must be defined: {}.{}.. Possible values include: {}"".format(accession,version,"", "".join(list(self.valid_extensions)))
AssertionError: file_ext must be defined: ... Possible values include: est, ssexemplar, gb.xml, docset, fasta.xml, fasta, fasta_cds_na, abstract, txt, gp, medline, chr, flt, homologene, alignmentscores, gbwithparts, seqid, fasta_cds_aa, gpc, uilist, uilist.xml, rsr, xml, gb, gene_table, gss, ft, gp.xml, acc, asn1, gbc
",-1,-1,-1.0
57977484,"Snakemake shell command should only take one file at a time, but it's trying to do multiple files at once","First off, I'm sorry if I'm not explaining my problem clearly, English is not my native language.

I'm trying to make a snakemake rule that takes a fastq file and filters it with a program called Filtlong. I have multiple fastq files on which I want to run this rule and it should output a filtered file per fastq file but apparently it takes all of the fastq files as input for a single Filtlong command.

The fastq files are in separate directories and the snakemake rule should write the filtered files to separate directories aswell.

This is how my code looks right now:

from os import listdir

configfile: ""config.yaml""

DATA = config[""DATA""]
SAMPLES = listdir(config[""RAW_DATA""])
RAW_DATA = config[""RAW_DATA""]
FILT_DIR = config[""FILTERED_DIR""]

rule all:
    input:
        expand(""{FILT_DIR}/{sample}/{sample}_filtered.fastq.gz"", FILT_DIR=FILT_DIR, sample=SAMPLES)

rule filter_reads:
    input:
        expand(""{RAW_DATA}/{sample}/{sample}.fastq"", sample=SAMPLES, RAW_DATA=RAW_DATA)
    output:
        ""{FILT_DIR}/{sample}/{sample}_filtered.fastq.gz""
    shell:
        ""filtlong --keep_percent 90 --target_bases 300000000 {input} | gzip &gt; {output}""


And this is the config file:

DATA:
  all_samples

RAW_DATA:
  all_samples/raw_samples

FILTERED_DIR:
  all_samples/filtered_samples


The separate directories with the fastq files are in RAW_DATA and the directories with the filtered files should be in FILTERED_DIR,

When I try to run this, I get an error that looks something like this:

Error in rule filter_reads:
    jobid: 30
    output: all_samples/filtered_samples/cell_18-07-19_barcode10/cell_18-07-19_barcode10_filtered.fastq.gz
    shell:
        filtlong --keep_percent 90 --target_bases 300000000 all_samples/raw_samples/cell3_barcode11/cell3_barcode11.fastq all_samples/raw_samples/barcode01/barcode01.fastq all_samples/raw_samples/barcode03/barcode03.fastq all_samples/raw_samples/barcode04/barcode04.fastq all_samples/raw_samples/barcode05/barcode05.fastq all_samples/raw_samples/barcode06/barcode06.fastq all_samples/raw_samples/barcode07/barcode07.fastq all_samples/raw_samples/barcode08/barcode08.fastq all_samples/raw_samples/barcode09/barcode09.fastq all_samples/raw_samples/cell3_barcode01/cell3_barcode01.fastq all_samples/raw_samples/cell3_barcode02/cell3_barcode02.fastq all_samples/raw_samples/cell3_barcode03/cell3_barcode03.fastq all_samples/raw_samples/cell3_barcode04/cell3_barcode04.fastq all_samples/raw_samples/cell3_barcode05/cell3_barcode05.fastq all_samples/raw_samples/cell3_barcode06/cell3_barcode06.fastq all_samples/raw_samples/cell3_barcode07/cell3_barcode07.fastq all_samples/raw_samples/cell3_barcode08/cell3_barcode08.fastq all_samples/raw_samples/cell3_barcode09/cell3_barcode09.fastq all_samples/raw_samples/cell3_barcode10/cell3_barcode10.fastq all_samples/raw_samples/cell3_barcode12/cell3_barcode12.fastq all_samples/raw_samples/cell_18-07-19_barcode01/cell_18-07-19_barcode01.fastq all_samples/raw_samples/cell_18-07-19_barcode02/cell_18-07-19_barcode02.fastq all_samples/raw_samples/cell_18-07-19_barcode03/cell_18-07-19_barcode03.fastq all_samples/raw_samples/cell_18-07-19_barcode04/cell_18-07-19_barcode04.fastq all_samples/raw_samples/cell_18-07-19_barcode05/cell_18-07-19_barcode05.fastq all_samples/raw_samples/cell_18-07-19_barcode06/cell_18-07-19_barcode06.fastq all_samples/raw_samples/cell_18-07-19_barcode07/cell_18-07-19_barcode07.fastq all_samples/raw_samples/cell_18-07-19_barcode08/cell_18-07-19_barcode08.fastq all_samples/raw_samples/cell_18-07-19_barcode09/cell_18-07-19_barcode09.fastq all_samples/raw_samples/cell_18-07-19_barcode10/cell_18-07-19_barcode10.fastq all_samples/raw_samples/cell_18-07-19_barcode11/cell_18-07-19_barcode11.fastq all_samples/raw_samples/cell_18-07-19_barcode12/cell_18-07-19_barcode12.fastq all_samples/raw_samples/cell_18-07-19_barcode13/cell_18-07-19_barcode13.fastq all_samples/raw_samples/cell_18-07-19_barcode14/cell_18-07-19_barcode14.fastq all_samples/raw_samples/cell_18-07-19_barcode15/cell_18-07-19_barcode15.fastq all_samples/raw_samples/cell_18-07-19_barcode16/cell_18-07-19_barcode16.fastq all_samples/raw_samples/cell_18-07-19_barcode17/cell_18-07-19_barcode17.fastq all_samples/raw_samples/cell_18-07-19_barcode18/cell_18-07-19_barcode18.fastq all_samples/raw_samples/cell_18-07-19_barcode19/cell_18-07-19_barcode19.fastq | gzip &gt; all_samples/filtered_samples/cell_18-07-19_barcode10/cell_18-07-19_barcode10_filtered.fastq.gz
        (exited with non-zero exit code)



As far as I can tell, the rule takes all of the fastq files as input for a single Filtlong command, but I don't quite understand why
",-1,-1,-1.0
58199730,Snakemake ambiguity,"I have an ambiguity error and I can't figure out why and how to solve it.

Defining the wildcards:

rule all:
    input:
        xls = expand(""reports/{sample}.xlsx"", sample = config[""samples""]),
        runfolder_xls = expand(""{runfolder}.xlsx"", runfolder = config[""runfolder""])


Actual rules:

rule sample_report:
    input:
        vcf = ""vcfs/{sample}.annotated.vcf"",
        cov = ""stats/{sample}.coverage.gz"",
        mod_bed = ""tmp/mod_ref_{sample}.bed"",
        nirvana_g2t = ""/mnt/storage/data/NGS/nirvana_genes2transcripts""
    output:
        ""reports/{sample}.xlsx""
    params:
        get_nb_samples()
    log:
        ""logs/{sample}.log""
    shell: """"""
        python /mnt/storage/home/kimy/projects/automate_CP/niles/NILES_create_sample_report.py -v {input.vcf} -c {input.cov} -r {input.mod_bed} -n {input.nirvana_g2t} -r {rule};
        exitcode=$? ;

        if [[ {params} &gt; 1 ]]
        then
            python /mnt/storage/home/kimy/projects/automate_CP/niles/NILES_check_exitcode.py -e $exitcode -r {rule} -n {wildcards.sample}
        elif [[ {params} == 1 ]]
        then 
            python /mnt/storage/home/kimy/projects/automate_CP/niles/NILES_check_exitcode.py -e $exitcode -r sample_mode -n {wildcards.sample}
        else
            python /mnt/storage/home/kimy/projects/automate_CP/niles/NILES_check_exitcode.py -e 1 -r {rule} -n {wildcards.sample}
        fi
    """"""


rule runfolder_report:
    input:
        sample_sheet = ""SampleSheet.csv""
    output:
        ""{runfolder}.xlsx""
    log:
        ""logs/{runfolder}.log""
    shell: """"""
        python /mnt/storage/home/kimy/projects/automate_CP/niles/NILES_create_runfolder_report.py -run {wildcards.runfolder} -s {input.sample_sheet} -r {rule} ;
        exitcode=$? ;
        python /mnt/storage/home/kimy/projects/automate_CP/niles/NILES_check_exitcode.py -e $exitcode -r {rule} -n {wildcards.runfolder}
    """"""


Config file:

runfolder: ""CP0340""

samples: ['C014044p', 'C130157', 'C014040p', 'C014054b-1', 'C051198-A', 'C014042p', 'C052007W-C', 'C051198-B', 'C014038p', 'C052004-B', 'C051198-C', 'C052004-C', 'C052003-B', 'C052003-A', 'C052004-A', 'C052002-C', 'C052005-C', 'C052002-A', 'C130157N', 'C052006-B', 'C014063pW', 'C014054b-2', 'C052002-B', 'C052006-C', 'C052007W-B', 'C052003-C', 'C014064bW', 'C052005-B', 'C052006-A', 'C052005-A']


Error:

$ snakemake -n -s ../niles/Snakefile --configfile logs/CP0340_config.yaml
Building DAG of jobs...
AmbiguousRuleException:
Rules runfolder_report and sample_report are ambiguous for the file reports/C014044p.xlsx.
Consider starting rule output with a unique prefix, constrain your wildcards, or use the ruleorder directive.
Wildcards:
        runfolder_report: runfolder=reports/C014044p
        sample_report: sample=C014044p
Expected input files:
        runfolder_report: SampleSheet.csv
        sample_report: vcfs/C014044p.annotated.vcf stats/C014044p.coverage.gz tmp/mod_ref_C014044p.bed /mnt/storage/data/NGS/nirvana_genes2transcriptsExpected output files:
        runfolder_report: reports/C014044p.xlsx
        sample_report: reports/C014044p.xlsx


If I understand Snakemake correctly, the wildcards in the rules are defined in my all rule so I don't understand why the runfolder_report rule tries to put reports/C014044p.xlsx as an output + how the output has a sample name instead of the runfolder name (as defined in the config file).
",-1,-1,-1.0
58233532,Working directory when using include in snakemake for rules that use the report() function,"I am using snakemake to program my workflows. In order to reuse code, the simplest way is to use the statement
include: ""path/to/other/Snakefile""

This works fine for most cases but fails when creating the reports via the report() function. The problem is that it does not find the .rst file that is specified for the caption.

Thus it seems that report() has the working directory in which the other Snakefile is located and not the one of the main Snakefile.

Is there a flexible workaround for this, so that it behaves as just being loaded into the Snakefile and then being executed as it were in the main Snakefile?

This is an example rule in another Snakemake file:

rule evaluation:
    input:
        ""data/final_feature.model""
    output:
        report(""data/results.txt"",caption=""report/evaluation.rst"",category =""Evaluation"")
    shell:
        ""Rscript {scripts}/evaluation.R {input}""


This is included in the main Snakefile via:

include: ""../General/subworkflows/evaluation.snakemake""


This is the error message showing that the file is not present:

WorkflowError:
Error loading caption file of output marked for report.
FileNotFoundError: [Errno 2] No such file or directory: '.../workflows/General/subworkflows/report/evaluation.rst'


Thank you for any help in advance!
",-1,-1,-1.0
58454538,Where can I find an example of a file workflow.rst for generating snakemake reports?,"I am trying to use the reporting function of snakemake which is documented here:
https://snakemake.readthedocs.io/en/stable/snakefiles/reporting.html

However I am finding it hard to find a meaningful example of what the workflow.rst file should look like. 

I have looked at the output example
https://koesterlab.github.io/resources/report.html

but I can't find the code to generate this report.
",-1,-1,-1.0
58597047,Using subprocess.run to run Snakemake,"I am trying to run sankemake within python script using cluster configuration where myoutput/output.txt one of my output results:

cluster = """""" ""qsub -A {cluster.account}  -l walltime={cluster.time} -q {cluster.queue} -l nodes=1:ppn={cluster.nCPUs} -l mem={cluster.memory}"" """"""  

subprocess.run(['snakemake',  '-p', ""myoutput/output.txt"", '-j 200', '--cluster-config', cluster_config, '--cluster', cluster])


I have this error:

/bin/sh: qsub -A proj_A  -l walltime=72:00:00 -q analysis -l nodes=1:ppn=20,mem=30G: command not found


I see that snakemake submitting to /bin/sh where qsub actually in /usr/bin/qsub.

Any Idea how to solve it or is there a better implementation?

Thanks
",1,-1,-1.0
58703001,running nanofilt with snakemake,"I am new to using snakemake. I want to run my fastq files that are in one folder with nanofilt. I want to run this with snakemake because I need it to create a pipeline. This is my snake make script: 

rule NanoFilt:
    input:
        ""data/samples/{sample}.fastq""
    output:
        ""nanofilt_out.gz""
    shell:
        ""gunzip -c {input} | NanoFilt -q 8 | gzip &gt; {output}""


When I run it I get the following error message:

WildcardError in line 2:
Wildcards in input files cannot be determined from output files:
'sample'


EDIT

I tried searching the error message but still couldnt figure out how to make it work. Can anyone help me?

So I tried what people on here told me so the new script is this: 

samples = ['fastqrunid4d89b52e7b9734bd797205037ef201a30be415c8293','fastqrunid4d89b52e7b9734bd797205037ef201a30be415c8292','fastqrunid4d89b52e7b9734bd797205037ef201a30be415c8291','fastqrunid4d89b52e7b9734bd797205037ef201a30be415c8290']
rule all:
    input:
        [f""nanofilt_out_{sample}.gz"" for sample in samples]

rule NanoFilt:
    input:
        ""zipped/zipped{sample}.gz""
    output:
        ""nanofilt_out_{sample}.gz""
    shell:
        ""gunzip -c {input} | NanoFilt -q 8 | gzip &gt; {output}"" 


but when I run this I get the following error message: 

Error in rule NanoFilt:
Removing output files of failed job NanoFilt since they might be corrupted:
nanofilt_out_fastqrunid4d89b52e7b9734bd797205037ef201a30be415c8292.gz
    jobid: 4
    output: nanofilt_out_fastqrunid4d89b52e7b9734bd797205037ef201a30be415c8290.gz
    shell:
        gunzip -c zipped/zippedfastqrunid4d89b52e7b9734bd797205037ef201a30be415c8290.gz | NanoFilt -q 8 | gzip &gt; nanofilt_out_fastqrunid4d89b52e7b9734bd797205037ef201a30be415c8290.gz
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)


Does anyone know how to fix this?
",-1,-1,-1.0
58848521,How to use dict value and key in Snakemake rules,"I stuck in a Snakemake issue for a lone while.

Propose I have a dict like:

dict_A = {A:""id1"",""id2"",""id3"", B:""id2"",""id3"",""id4"",""id5"", C:""id1"",""id4"",""id5""} 


and I want to write rules like:

input:
    ""{dict_A.keys()}/{dict_A[key]}_R1.txt""
output:
    ""{dict_A.keys()}/{dict_A[key]}_R1_filter.txt""
shell:
    ""XXX {input} &gt; {output}""


While, I try to search on google and StackOverflow but I can't figure out this problem. Really hope there is someone can help me !

Really thanks!
",-1,-1,-1.0
58875726,Wildcard / regular expression not working in snakemake?,"I'm a beginner with snakemake. I'm trying to understand how to use variables and regular expressions. I've tried to follow tutorials and...I still need help. I the following example, what I'd like my code to do is ""run this, for files in directory 'data' that are named gene_dna and have any extension"".
I would like to understand why this code doesn't work:

rule test:
    input:
        ""data/gene_dna.{.+}""
    output:
        ""test.txt""
    shell:
        ""echo 1 &gt; {output}""


...but this one does (at least during a dry-run). The difference is "".txt."" instead if {.+}:

rule test:
    input:
        ""data/gene_dna.txt""
    output:
        ""test.txt""
    shell:
        ""echo 1 &gt; {output}""


Please help?
",-1,-1,-1.0
59002807,snakemake expansion no input files,"I have some different configurations and I need to get the combination of them all to run a python script

versions = ['lg', 'sm']
start_time = ['0', '1']
end_time = ['2']


What I want is snakemake to do this for me:

python my_script.py -v lg -s 0 -e 2 &gt; lg_0_2.out
python my_script.py -v lg -s 1 -e 2 &gt; lg_1_2.out
python my_script.py -v sm -s 0 -e 2 &gt; sm_0_2.out
python my_script.py -v sm -s 1 -e 2 &gt; sm_1_2.out


but I can't seem to figure out how to do this in snakemake. Any ideas?
",-1,-1,-1.0
59085429,Missing input files for rule all in snakemake,"I am trying to construct a snakemake pipeline for biosynthetic gene cluter detection but am struggling with the error:

Missing input files for rule all:
antismash-output/Unmap_09/Unmap_09.txt
antismash-output/Unmap_12/Unmap_12.txt
antismash-output/Unmap_18/Unmap_18.txt


And so on with more files. As far as I can see the file generation in the snakefile should be working:

    workdir: config[""path_to_files""]
wildcard_constraints:
    separator = config[""separator""],
    extension = config[""file_extension""],
    sample = config[""samples""]

rule all:
    input:
        expand(""antismash-output/{sample}/{sample}.txt"", sample = config[""samples""])

# merging the paired end reads (either fasta or fastq) as prodigal only takes single end reads
rule pear:
    input:
        forward = ""{sample}{separator}1.{extension}"",
        reverse = ""{sample}{separator}2.{extension}""

    output:
        ""merged_reads/{sample}.{extension}""

    conda:
        ""~/miniconda3/envs/antismash""

    shell:
        ""pear -f {input.forward} -r {input.reverse} -o {output} -t 21""

# If single end then move them to merged_reads directory
rule move:
    input:
        ""{sample}.{extension}""

    output:
        ""merged_reads/{sample}.{extension}""

    shell:
        ""cp {path}/{sample}.{extension} {path}/merged_reads/""

# Setting the rule order on the 2 above rules which should be treated equally and only one run.
ruleorder: pear &gt; move
# annotating the metagenome with prodigal#. Can be done inside antiSMASH but prefer to do it out
rule prodigal:
    input:
        ""merged_reads/{sample}.{extension}""

    output:
        gbk_files = ""annotated_reads/{sample}.gbk"",
        protein_files = ""protein_reads/{sample}.faa""

    conda:
        ""~/miniconda3/envs/antismash""

    shell:
        ""prodigal -i {input} -o {output.gbk_files} -a {output.protein_files} -p meta""

# running antiSMASH on the annotated metagenome
rule antiSMASH:
    input:
        ""annotated_reads/{sample}.gbk""

    output:
        touch(""antismash-output/{sample}/{sample}.txt"")

    conda:
        ""~/miniconda3/envs/antismash""

    shell:
        ""antismash --knownclusterblast --subclusterblast --full-hmmer --smcog --outputfolder antismash-output/{wildcards.sample}/ {input}""


This is an example of what my config.yaml file looks like:

file_extension: fastq
path_to_files: /home/lamma/ABR/Each_reads
samples:
- Unmap_14
- Unmap_55
- Unmap_37
separator: _


I can not see where i am going wrong within the snakefile to produce such an error. Apologies for the simple question, I am new to snakemake.
",-1,-1,-1.0
59107413,Activating existing conda enviornments in snakemake,"How do I get snakemake to activate a conda environment that already exists in my environment list?
I know you can use the --use-conda with a .yaml environment file but that seems to generate a new environment which is just annoying when the environment already exists. Any help with this would be much appreciated.
I have tried using the:
conda:
    path/to/some/yamlFile

but it just returns command not found errors for packages in the environment
",1,-1,-1.0
59191168,"Snakemake ""Missing files after X seconds"" error","I am getting the following error every time I try to run my snakemake script:

    Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 16
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       pear
        1

[Wed Dec  4 17:32:54 2019]
rule pear:
    input: Unmap_41_1.fastq, Unmap_41_2.fastq
    output: merged_reads/Unmap_41.fastq
    jobid: 0
    wildcards: sample=Unmap_41, extension=fastq

Waiting at most 120 seconds for missing files.
MissingOutputException in line 14 of /faststorage/project/ABR/scripts/antismash.smk:
Missing files after 120 seconds:
merged_reads/Unmap_41.fastq
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message


The snakefile is the following:

 workdir: config[""path_to_files""]
wildcard_constraints:
    separator = config[""separator""],
    extension = config[""file_extension""],
    sample = '|' .join(config[""samples""])

rule all:
    input:
        expand(""antismash-output/{sample}/{sample}.txt"", sample = config[""samples""])

# merging the paired end reads (either fasta or fastq) as prodigal only takes single end reads
rule pear:
    input:
        forward = f""{{sample}}{config['separator']}1.{{extension}}"",
        reverse = f""{{sample}}{config['separator']}2.{{extension}}""

    output:
        ""merged_reads/{sample}.{extension}""

    #conda:
        #""/home/lamma/env-export/antismash.yaml""

    run:
        """"""
        set+u; source activate antismash; set -u ;
        pear -f {input.forward} -r {input.reverse} -o {output} -t 21
        """"""

# If single end then move them to merged_reads directory
rule move:
    input:
        ""{sample}.{extension}""

    output:
        ""merged_reads/{sample}.{extension}""

    shell:
        ""cp {path}/{sample}.{extension} {path}/merged_reads/""

# Setting the rule order on the 3 above rules which should be treated equally and only one run.
ruleorder: pear &gt; move
# annotating the metagenome with prodigal#. Can be done inside antiSMASH but prefer to do it out
rule prodigal:
    input:
        f""merged_reads/{{sample}}.{config['file_extension']}""

    output:
        gbk_files = ""annotated_reads/{sample}.gbk"",
        protein_files = ""protein_reads/{sample}.faa""

    #conda:
        #""/home/lamma/env-export/antismash.yaml""

    shell:
        """"""
        set+u; source activate antismash; set -u ;
        prodigal -i {input} -o {output.gbk_files} -a {output.protein_files} -p meta
        """"""

# running antiSMASH on the annotated metagenome
rule antiSMASH:
    input:
        ""annotated_reads/{sample}.gbk""

    output:
        touch(""antismash-output/{sample}/{sample}.txt"")

    #conda:
        #""/home/lamma/env-export/antismash.yaml""

    shell:
        """"""
        set+u; source activate antismash; set -u ;
        antismash --knownclusterblast --subclusterblast --full-hmmer --smcog --outputfolder antismash-output/{wildcards.sample}/ {input}
        """"""


I am running the pipeline on only one file at the moment but the yaml file looks like this if it is of intest:

file_extension: fastq
path_to_files: /home/lamma/ABR/Each_reads
samples:
- Unmap_41
separator: _


I know the error can occure when you use certain flags in snakemake but I dont believe I am using those flags. The command being submited to run the snakefile is:

snakemake --latency-wait 120 --rerun-incomplete --keep-going --jobs 99 --cluster-status 'python /home/lamma/ABR/scripts/slurm-status.py' --cluster 'sbatch  -t {cluster.time} --mem={cluster.mem} --cpus-per-task={cluster.c} --error={cluster.error}  --job-name={cluster.name} --output={cluster.output}' --cluster-config antismash-config.json --configfile yaml-config-files/antismash-on-rawMetagenome.yaml -F --snakefile antismash.smk


I have tried to -F flag to force a rerun but this seems to do nothing, as does increasing the --latency-wait number. Any help would be appriciated :)
",-1,-1,-1.0
59198122,"""one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode"", I am not sure why I am getting this error","I am getting the following error:

one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!


My snakefile looks like the following:

#!/miniconda/bin/python

workdir: config[""path_to_files""]
wildcard_constraints:
    separator = config[""separator""],
    extension = config[""file_extension""],
    sample = '|' .join(config[""samples""])

rule all:
    input:
        expand(""antismash-output/{sample}/{sample}.txt"", sample = config[""samples""])

# merging the paired end reads (either fasta or fastq) as prodigal only takes single end reads
rule pear:
    input:
        forward = f""{{sample}}{config['separator']}1.{{extension}}"",
        reverse = f""{{sample}}{config['separator']}2.{{extension}}""

    output:
        ""merged_reads/{sample}.{extension}""

    #conda:
        #""/home/lamma/env-export/antismash.yaml""

    shell:
        """"""
        set+u;source ~/miniconda3/etc/profile.d/conda.sh; set -u ;
        set+u;conda activate antismash; set -u ;
        pear -f {input.forward} -r {input.reverse} -o {output} -t 21
        """"""

# If single end then move them to merged_reads directory
rule move:
    input:
        ""{sample}.{extension}""

    output:
        ""merged_reads/{sample}.{extension}""

    shell:
        ""cp {path}/{sample}.{extension} {path}/merged_reads/""

# Setting the rule order on the 3 above rules which should be treated equally and only one run.
ruleorder: pear &gt; move
# annotating the metagenome with prodigal#. Can be done inside antiSMASH but prefer to do it out
rule prodigal:
    input:
        f""merged_reads/{{sample}}.{config['file_extension']}""

    output:
        gbk_files = ""annotated_reads/{sample}.gbk"",
        protein_files = ""protein_reads/{sample}.faa""

    #conda:
        #""/home/lamma/env-export/antismash.yaml""

    shell:
        """"""
        set+u;source ~/miniconda3/etc/profile.d/conda.sh; set -u ;
        set+u;conda activate antismash; set -u ;
        prodigal -i {input} -o {output.gbk_files} -a {output.protein_files} -p meta
        """"""

# running antiSMASH on the annotated metagenome
rule antiSMASH:
    input:
        ""annotated_reads/{sample}.gbk""

    output:
        touch(""antismash-output/{sample}/{sample}.txt"")

    #conda:
        #""/home/lamma/env-export/antismash.yaml""

    shell:
        """"""
        set+u;source ~/miniconda3/etc/profile.d/conda.sh; set -u ;
        set+u;conda activate antismash; set -u ;
        antismash --knownclusterblast --subclusterblast --full-hmmer --smcog --outputfolder antismash-output/{wildcards.sample}/ {input}
        """"""


Any help as to why I am getting this error would be appriciated. I think it is something to do with how I am activating the conda environments, using the conda: path/to/yaml has not worked for me as I get version errors, where it seems snakemake can't satisfy the version requierments set out by the yaml file.

Edit:

The full error message is as follows:

Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 16
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       pear
        1

[Thu Dec  5 16:05:37 2019]
rule pear:
    input: Unmap_41_1.fastq, Unmap_41_2.fastq
    output: merged_reads/Unmap_41.fastq
    jobid: 0
    wildcards: sample=Unmap_41, extension=fastq

/usr/bin/bash: set+u: command not found
[Thu Dec  5 16:05:37 2019]
Error in rule pear:
    jobid: 0
    output: merged_reads/Unmap_41.fastq
    shell:

        set+u;source ~/miniconda3/etc/profile.d/conda.sh; set -u ;
        set+u;conda activate antismash; set -u ;
        pear -f Unmap_41_1.fastq -r Unmap_41_2.fastq -o merged_reads/Unmap_41.fastq -t 21

        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)


It also seems set+u; is an unknow command but to my understanding it is a snakemake command.
",-1,-1,-1.0
59287774,Snakemake “Missing files after X seconds” error,"I am getting the following error every time I try to run my snakemake script:

Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cluster nodes: 99
Job counts:
        count   jobs
        1       all
        1       antiSMASH
        1       pear
        1       prodigal
        4

[Wed Dec 11 14:59:43 2019]
rule pear:
    input: Unmap_41_1.fastq, Unmap_41_2.fastq
    output: merged_reads/Unmap_41.fastq
    jobid: 3
    wildcards: sample=Unmap_41, extension=fastq

Submitted job 3 with external jobid 'Submitted batch job 4572437'.
Waiting at most 120 seconds for missing files.
MissingOutputException in line 14 of /faststorage/project/ABR/scripts/antismash.smk:
Missing files after 120 seconds:
merged_reads/Unmap_41.fastq
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Job failed, going on with independent jobs.
Exiting because a job execution failed. Look above for error message


It would seem that the first rule is not executing, but I am unsure as to why as from what I can see all the syntax is correct. Anyone have some advice?

The snakefile is the following:

#!/miniconda/bin/python

workdir: config[""path_to_files""]
wildcard_constraints:
    separator = config[""separator""],
    extension = config[""file_extension""],
    sample = '|' .join(config[""samples""])

rule all:
    input:
        expand(""antismash-output/{sample}/{sample}.txt"", sample = config[""samples""])

# merging the paired end reads (either fasta or fastq) as prodigal only takes single end reads
rule pear:
    input:
        forward = f""{{sample}}{config['separator']}1.{{extension}}"",
        reverse = f""{{sample}}{config['separator']}2.{{extension}}""

    output:
        ""merged_reads/{sample}.{extension}""

    #conda:
        #""/home/lamma/env-export/antismash.yaml""

    run:
        shell(""set +u"")
        shell(""source ~/miniconda3/etc/profile.d/conda.sh"")
        shell(""conda activate antismash"")
        shell(""pear -f {input.forward} -r {input.reverse} -o {output} -t 21"")


# If single end then move them to merged_reads directory
rule move:
    input:
        ""{sample}.{extension}""

    output:
        ""merged_reads/{sample}.{extension}""

    shell:
        ""cp {path}/{sample}.{extension} {path}/merged_reads/""

# Setting the rule order on the 3 above rules which should be treated equally and only one run.
ruleorder: pear &gt; move
# annotating the metagenome with prodigal#. Can be done inside antiSMASH but prefer to do it out
rule prodigal:
    input:
        f""merged_reads/{{sample}}.{config['file_extension']}""

    output:
        gbk_files = ""annotated_reads/{sample}.gbk"",
        protein_files = ""protein_reads/{sample}.faa""

    #conda:
        #""/home/lamma/env-export/antismash.yaml""

    run:
        shell(""set +u"")
        shell(""source ~/miniconda3/etc/profile.d/conda.sh"")
        shell(""conda activate antismash"")
        shell(""prodigal -i {input} -o {output.gbk_files} -a {output.protein_files} -p meta"")


# running antiSMASH on the annotated metagenome
rule antiSMASH:
    input:
        ""annotated_reads/{sample}.gbk""

    output:
        touch(""antismash-output/{sample}/{sample}.txt"")

    #conda:
        #""/home/lamma/env-export/antismash.yaml""

    run:
        shell(""set +u"")
        shell(""source ~/miniconda3/etc/profile.d/conda.sh"")
        shell(""conda activate antismash"")
        shell(""antismash --knownclusterblast --subclusterblast --full-hmmer --smcog --outputfolder antismash-output/{wildcards.sample}/ {input}"")


I am running the pipeline on only one file at the moment but the yaml file looks like this if it is of intest:

file_extension: fastq
path_to_files: /home/lamma/ABR/Each_reads
samples:
- Unmap_41
separator: _


I know the error can occure when you use certain flags in snakemake but I dont believe I am using those flags. The command being submited to run the snakefile is:

snakemake --latency-wait 120 --rerun-incomplete --keep-going --jobs 99 --cluster-status 'python /home/lamma/ABR/scripts/slurm-status.py' --cluster 'sbatch  -t {cluster.time} --mem={cluster.mem} --cpus-per-task={cluster.c} --error={cluster.error}  --job-name={cluster.name} --output={cluster.output}' --cluster-config antismash-config.json --configfile yaml-config-files/antismash-on-rawMetagenome.yaml --snakefile antismash.smk


I have tried to -F flag to force a rerun but this seems to do nothing, as does increasing the --latency-wait number. Any help would be appriciated :)

I think it is likely to be something involving the way I am calling the conda environment in the run commands but using the conda: option with a yaml files returns version not found style errors.
",-1,-1,-1.0
59288002,MissingOutputException and latency-wait error with snakemake,"I am trying to makeblastdb database in snakemake:

workdir: ""/path/to/workdir/""

(SAMPLES,) =glob_wildcards('/path/to/workdir/{sample}.fasta')

rule all:
    input: 
        expand(""{sample}.fasta.{ext}"", sample=SAMPLES, ext=[""nhr"", ""nin"", ""nsq""])

rule makeblastdb:
    input:
        reference = ""/path/to/workdir/{sample}.fasta""
    output:
        out = ""{sample}.fasta.{ext}""
    shell:
        /Tools/ncbi-blast-2.9.0+/bin/makeblastdb -in {input.reference} -out {output.out} -dbtype nucl""


I get this error:

MissingOutputException in line 11:
Missing files after 10 seconds:
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.


What is the problem?
",-1,-1,-1.0
59388961,Downloading large files from S3 with snakemake,"I am making a workflow in snakemake 5.8.2 which takes as an input huge files from S3 (280Gb each of 4 files). The first rule just concatenates the files.
When I run the workflow, it seems to download only 5GB of some split version of each file, delete the file and fails to concatenate. I know aws transfers files in 5GB batches but I expected snakemake to handle this in the background.
Am I missing something? Is this a bug?
Thanks,
Ilya.
",0,-1,-1.0
59493422,snakemake cluster script ImportError snakemake.utils,"I have a strange issue that comes and goes randomly and I really can't figure out when and why.
I am running a snakemake pipeline like this:

conda activate $myEnv    
snakemake -s $snakefile --configfile test.conf.yml --cluster ""python $qsub_script"" --latency-wait 60 --use-conda -p -j 10 --jobscript ""$job_script""


I installed snakemake 5.9.1 (also tried downgrading to 5.5.4) within a conda environment.
This works fine if I just run this command, but when I qsub this command to the PBS cluster I'm using, I get an error. My qsub script looks like this:

#PBS stuff...

source ~/.bashrc
hostname
conda activate PGC_de_novo

cd $workDir
snakefile=""...""
qsub_script=""pbs_qsub_snakemake_wrapper.py""
job_script=""...""
snakemake -s $snakefile --configfile test.conf.yml --cluster ""python $qsub_script"" --latency-wait 60 --use-conda -p -j 10 --jobscript ""$job_script"" &gt;out 2&gt;err


And the error message I get is:

...
Traceback (most recent call last):
  File ""/path/to/pbs_qsub_snakemake_wrapper.py"", line 6, in &lt;module&gt;
    from snakemake.utils import read_job_properties
ImportError: No module named snakemake.utils
Error submitting jobscript (exit code 1):
...


So it looks like for some reason my cluster script doesn't find snakemake, although snakemake is clearly installed. As I said, this problem keeps coming and going. It'd stay for a few hours, then go away for now aparent reason. I guess this indicates an environment problem, but I really can't figure out what, and ran out of ideas. I've tried:


different conda versions
different snakemake versions
different nodes on the cluster
ssh to the node it just failed on and try to reproduce the error


but nothing. Any ideas where to look? Thanks!
",-1,-1,-1.0
59616894,Snakemake syntax for multiple outputs with the use of checkpoint,"I'm using snakemake to build a pipeline. I have a checkpoint that should produce multiple output files. These output files are later used in my rule all within expand. The thing is that I don't know the amount of files that will be produced and therefore can't specify a dataset in expand.

The files will be produced in a R-script.

Example:

rule all:
    input:
        expand([""results/{output}],
               output=????)



checkpoint rscript:
    input:
        ""foo.input""
    output:
        report(""somedir/{output}""),
    script:
        ""../scripts/foo.R"" 


Of course this is only a small part but I basically have a loop in my R-script to output multiple files in the somedir. But since I don't know how many and because they are firstly evaluated in the R script I can't set output in expand.

Maybe this is a really trivial question to some of you, or even a stupid question and there are better ways to do this. If that's the case I'd still be thankful cause I had problems understanding most of the snakemake functions because of my ability to comprehend the functions in english.

If there are more questions I'd gladly answer. (The best case for me would be to let output have names that I could specify in runtime within the R script)

(I also can't aggregate the created files in another rule, because each file will show a different plot)

Edit: The main problem still seems to be that checkpoint rscript is not able to create multiple {output} files in ""somedir/"". The attempt with touch(""rscript_finish.flag"") seems to output only the svg File as ""rscript_finish.flag"" or seems to override ""rscript_finish.flag"" each time the loop in my rscript writes into snakemake@output[[1]].
",-1,-1,-1.0
59753180,Various iterations of snakefile give the same error,"I am trying to use snakemake for a bioinformatics pipeline. To test it, I made the following snakefile. 

I have a directory with various fastq.gz files (e.g. K1_R1.fastq.gz, K1_R2.fastq.gz, K2_R1.fastq.gz etc) and I am running fastqc on them. 

SAMPLES = [ ""K1"", ""K2"", ""W1"", ""W2"" ]
REPLICATE = ['R1', 'R2']

PathtoWD = '/home/hbanduk/scratch/working_projects_2020/'

rule fastqc:
    input:
        ""expand(PathtoWD + ""/raw_data/{sample}_{replicate}.fastq.gz"", sample=SAMPLES, replicate=REPLICATE)""
    output:
         PathtoWD + ""/fastqc_raw_reads/{sample}_{replicate}_fastqc.zip"",
         PathtoWD + ""/fastqc_raw_reads/{sample}_{replicate}_fastqc.html""
    shell:
        ""fastqc {input} -o /home/hbanduk/scratch/working_projects_2020/fastqc_raw_reads""


when I run, 

snakemake -np snakefile


I get the following error:

SyntaxError in line 9 of 
/scratch/hbanduk/working_projects_2020/Snakefile:
invalid syntax


I have tried many permutations of this file but I keep getting the same error (othertimes for other lines). 

Any input will be greatly appreciated. 
",-1,-1,-1.0
59825151,Snakemake live user input,"I have an a bunch of R scripts that follow one another and I wanted to connect them using Snakemake. But I’m running in a problem.

One of my R scripts shows two images and asks a user’s input on how many cluster there are present. The R function for this is [readline]

This query on how many clusters is asked but directly after the next line of code is run. Without an opportunity  to input a number. the rest of the program crashes, since trying to calculate (empty number) of clusters doesn’t really work. Is there a way around this. By getting the values via a function/rule from Snakemake

or is there a other way to work around this issue?
",-1,-1,-1.0
59826772,How to define 'output directory' instead of 'output file' in snakemake,"I am trying to run a shell command with snakemake that requires an output directory defined in the command. Specifically, I am trying to use fastqc which has the following required parameters:

fastqc {input} -o {output.dir}



Below are the three versions of my snakefile, none of which worked.

Version 1:

rule QC_rawdata:
    input:
         expand(""raw_reads/{sample}_{replicate}.fastq.gz"", sample=SAMPLE, replicate=REPLICATE)
    output:
         directory(""fastqc_raw_reads/"")
    shell:
        ""fastqc {input} -o {output}""


Version 2:

rule QC_rawdata:
    input:
         expand(""raw_reads/{sample}_{replicate}.fastq.gz"", sample=SAMPLE, replicate=REPLICATE)
    output:
         zip=expand(""fastqc_raw_reads/{sample}_{replicate}_fastqc.zip"", sample=SAMPLE, replicate=REPLICATE),
         html=expand(""AA-041_Mus_Sat_Cells_Nelfb-{sample}_{replicate}_fastqc.html"", sample=SAMPLE, replicate=REPLICATE)
    shell:
        ""fastqc {input} -o {output}""


Error: Syntax error

Version 3:

rule QC_rawdata:
    input:
         expand(""raw_reads/{sample}_{replicate}.fastq.gz"", sample=SAMPLE, replicate=REPLICATE)
    output:
         directory(""fastqc_raw_reads/""),
         zip=expand(""fastqc_raw_reads/{sample}_{replicate}_fastqc.zip"", sample=SAMPLE, replicate=REPLICATE),
         html=expand(""AA-041_Mus_Sat_Cells_Nelfb-{sample}_{replicate}_fastqc.html"", sample=SAMPLE, replicate=REPLICATE)
    shell:
        ""fastqc {input} -o {output}""


Error: Syntax error

I was using https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#directories-as-outputs as a reference.

I think I am missing some fundamental way in which output directories are defined in snakemake. If I can't point to a specific output file in my shell command then how do I write my snakemake file?
",1,-1,-1.0
60041193,Snakemake skips multiple of the rules with same input,"Let's say I have 3 rules with the same input, snakemake skips 2 of them and only runs one of the rules. Is there a workaround to force all 3 rules to execute, since I need all 3 of them? I could add some other files as input to the existing input, but I feel like that's somewhat cheated and probably confusing to other people looking at my code, since I declare an input that is not used at all.
",-1,-1,-1.0
60044919,Snakemake with Singularity,"I'm trying to use Singularity within one of my Snakemake rules. This works as expected when running my Snakemake pipeline locally. However, when I try to submit using sbatch onto my computing cluster, I run into errors. I'm wondering if you have any suggestions about how to translate the local pipeline to one that can work on the cluster. Thank you in advance!

The rule which causes errors uses Singularity to call variants with DeepVariant:


# Call variants with DeepVariant.
rule deepvariant_call:
  input:
    ref_path='/labs/jandr/walter/varcal/data/refs/{ref}.fa',
    bam='results/{samp}/bams/{samp}_{mapper}_{ref}.rmdup.bam'
  params:
    nshards='1',
    version='0.7.0'
  threads: 8
  output:
    vcf='results/{samp}/vars/{samp}_{mapper}_{ref}_deep.g.vcf.gz'
  shell:
    'singularity exec --bind /srv/gsfs0 --bind /labs/jandr/walter/ /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:0.7.0.simg \
    /labs/jandr/walter/tb/test/scripts/call_deepvariant.sh {input.ref_path} {input.bam} {params.nshards} {params.version} {output.vcf} '
#
# Error in rule deepvariant_call:
#     jobid: 17
#     output: results/T1-XX-2017-1068_S51/vars/T1-XX-2017-1068_S51_bowtie2_H37Rv_deep.g.vcf.gz
#     shell:
#         singularity exec --bind /srv/gsfs0 --bind /labs/jandr/walter/ /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:0.7.0.simg;     /labs/jandr/walter/tb/test/scripts/call_deepvariant.sh /labs/jandr/walter/varcal/data/refs/H37Rv.fa results/T1-XX-2017-1068_S51/bams/T1-XX-2017-1068_S51_bowtie2_H37Rv.rmdup.bam 1 0.7.0 results/T1-XX-2017-1068_S51/vars/T1-XX-2017-1068_S51_bowtie2_H37Rv_deep.g.vcf.gz 
#         (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)



I submit jobs to the cluster with the following:

snakemake -j 128 --cluster-config cluster.json --cluster ""sbatch -A {cluster.account} --mem={cluster.mem} -t {cluster.time} -c {threads}""


",-1,-1,-1.0
60085294,snakemake - checkpoint and wildcards,"I have a snakemake workflow, which fails, because the last job creates either two output files, or none. I tried solving it with checkpoint, but I think I am stuck with the wildcards when trying to collate the outfiles in the aggregate function.

The workflow (1) creates a fasta file from a biom community profile. Then runs an in silico PCR (2) on the fasta file, which creates a txt file as output.

The last step is the parser (3), which outputs a csv and a fasta file. However if there are no matches in the txt file (aka the insilico PCR yielded no results), then it doesn't create a csv or fasta file.

SAMPLES, = glob_wildcards(""input/metaphlan/{sample}.biom"")
ID = ""0 1 2 3 4"".split()

TARGETS = expand(""output/metaphlan/isPCR/final/{id}_mismatch_{sample}.fasta"", sample = SAMPLES, id = ID)

rule all:
    input:
        TARGETS

rule getgenome:
    input:
        ""input/metaphlan/{sample}.biom""
    output:
        csv=""output/metaphlan/fasta_dump/{sample}.csv"",
        fas=""output/metaphlan/fasta_dump/{sample}_dump.fasta""
    conda:
        ""envs/synth_genome.yaml""
    shell:
        ""python scripts/get_genomes_noabund_Snakemake.py {input} 1 {output.fas} {output.csv}""

rule PCR:
    input:
        ""output/metaphlan/fasta_dump/{sample}_dump.fasta""
    output:
        ""output/metaphlan/isPCR/raw/{id}_mismatch/{sample}.txt""
    params:
        id = ""{id}""
    shell:
        ""software/exonerate-2.2.0-x86_64/bin/ipcress --products --mismatch {params.id} scripts/primers-miseq.txt {input} &gt; {output}""

rule parse:
    input:
        ""output/metaphlan/isPCR/raw/{id}_mismatch/{sample}.txt""
    output:
        ""output/metaphlan/isPCR/final/{id}_mismatch_{sample}.csv"",
        ""output/metaphlan/isPCR/final/{id}_mismatch_{sample}.fasta""
    shell:
        ""python scripts/iPCRess_parser_v2.py {input} {output}""


The dry run is fine - no errors. But if I do the proper run, snakemake aborts it saying a job execution failed:

Waiting at most 5 seconds for missing files.
MissingOutputException in line 31 of snakeflow/Snakefile:
Missing files after 5 seconds:
output/metaphlan/isPCR/final/2_mismatch_metaphlan_rectal_SRR5907487.csv
output/metaphlan/isPCR/final/2_mismatch_metaphlan_rectal_SRR5907487.fasta
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.


I know I could either change the parser script to just create two empty files, but I don't want to create unnecessary files.
I looked into dynamic, but that doesn't work with two potential output files, so  I had a look at checkpoint. As I understand it, that should help me solve the problem. 

Here is my attempt at using checkpoints:

SAMPLES, = glob_wildcards(""input/metaphlan/{sample}.biom"")
ID = ""0 1 2 3 4"".split()

TARGETS = expand(""output/metaphlan/isPCR/final/{id}_mismatch_{sample}n.txt"", sample = SAMPLES, id = ID)
print(TARGETS)

rule all:
    input:
        TARGETS

rule getgenome:
    input:
        ""input/metaphlan/{sample}.biom""
    output:
        csv=""output/metaphlan/fasta_dump/{sample}.csv"",
        fas=""output/metaphlan/fasta_dump/{sample}_dump.fasta""
    conda:
        ""envs/synth_genome.yaml""
    shell:
        ""python scripts/get_genomes_noabund_Snakemake.py {input} 1 {output.fas} {output.csv}""

rule PCR:
    input:
        ""output/metaphlan/fasta_dump/{sample}_dump.fasta""
    output:
        ""output/metaphlan/isPCR/raw/{id}_mismatch/{sample}.txt""
    params:
        id = ""{id}""
    shell:
        ""software/exonerate-2.2.0-x86_64/bin/ipcress --products --mismatch {params.id} scripts/primers-miseq.txt {input} &gt; {output}""

checkpoint parse:
    input:
        ""output/metaphlan/isPCR/raw/{id}_mismatch/{sample}.txt""
    output:
        ""output/metaphlan/isPCR/final/{id}_mismatch_{sample}.csv"",
        ""output/metaphlan/isPCR/final/{id}_mismatch_{sample}.fasta""
    shell:
        ""python scripts/iPCRess_parser_v2.py {input} {output}""

def aggregate_input(wildcards):
    checkpoint_output = checkpoints.parse.get(**wildcards).output[0,1]
    return expand('output/metaphlan/isPCR/final/{id}_mismatch_{sample}.csv','output/metaphlan/isPCR/final/{id}_mismatch_{sample}.fasta', sample = wildcards.SAMPLES, id=wildcards.ID)

rule collect:
    input:
        aggregate_input
    output:
        ""output/metaphlan/isPCR/final/{id}_mismatch_{sample}n.txt""
    shell:
        ""cat {input} &gt;&gt; {output}""



and the error


&lt;function aggregate_input at 0x7f63eade2158&gt;
SyntaxError:
Input and output files have to be specified as strings or lists of strings.
  File ""snakeflow/Snakefile"", line 52, in &lt;module&gt;


I believe this is because there is something wrong with how I am using wildcards in the aggregate function, but I can't figure it out. I have tried various versions that I have found in the checkpoint tutorials to no avail.

Any help much appreciated,
thanks!
",1,-1,-1.0
60099074,Snakemake - parameter file treated as a wildcard,"I have written a pipeline in Snakemake. It's an ATAC-seq pipeline (bioinformatics pipeline to analyze genomics data from a specific experiment). Basically, until merging alignment step I use {sample_id} wildcard, to later switch to {sample} wildcard (merging two or more sample_ids into one sample). 

working DAG here (for simplicity only one sample shown; orange and blue {sample_id}s are merged into one green {sample}

Tha all rule looks as follows:

configfile: ""config.yaml""
SAMPLES_DICT = dict()

with open(config['SAMPLE_SHEET'], ""r+"") as fil:
    next(fil)
    for lin in fil.readlines():
        row = lin.strip(""\n"").split(""\t"")
        sample_id = row[0]
        sample_name = row[1]
        if sample_name in SAMPLES_DICT.keys():
            SAMPLES_DICT[sample_name].append(sample_id)
        else:
            SAMPLES_DICT[sample_name] = [sample_id]

SAMPLES = list(SAMPLES_DICT.keys())
SAMPLE_IDS = [sample_id for sample in SAMPLES_DICT.values() for sample_id in sample]
rule all:
    input:
        # FASTQC output for RAW reads
        expand(os.path.join(config['FASTQC'], '{sample_id}_R{read}_fastqc.zip'),
               sample_id = SAMPLE_IDS,
               read = ['1', '2']),

        # Trimming
        expand(os.path.join(config['TRIMMED'],
                            '{sample_id}_R{read}_val_{read}.fq.gz'),
               sample_id = SAMPLE_IDS,
               read = ['1', '2']),

        # Alignment
        expand(os.path.join(config['ALIGNMENT'], '{sample_id}_sorted.bam'),
               sample_id = SAMPLE_IDS),

        # Merging
        expand(os.path.join(config['ALIGNMENT'], '{sample}_sorted_merged.bam'),
               sample = SAMPLES),

        # Marking Duplicates
        expand(os.path.join(config['ALIGNMENT'], '{sample}_sorted_md.bam'),
               sample = SAMPLES),

        # Filtering
        expand(os.path.join(config['FILTERED'],
                            '{sample}.bam'),
               sample = SAMPLES),
        expand(os.path.join(config['FILTERED'],
                            '{sample}.bam.bai'),
               sample = SAMPLES),

        # multiqc report
        ""multiqc_report.html""

    message:
        '\n#################### ATAC-seq pipeline #####################\n'
        'Running all necessary rules to produce complete output.\n'
        '############################################################'


I know it's too messy, I should only leave the necessary bits, but here my understanding of snakemake fails cause I don't know what I have to keep and what I should delete.

This is working, to my knowledge exactly as I want. 

However, I added a rule:

rule hmmratac:
    input:
        bam = os.path.join(config['FILTERED'], '{sample}.bam'),
        index = os.path.join(config['FILTERED'], '{sample}.bam.bai')
    output:
        model = os.path.join(config['HMMRATAC'], '{sample}.model'),
        gappedPeak = os.path.join(config['HMMRATAC'], '{sample}_peaks.gappedPeak'),
        summits = os.path.join(config['HMMRATAC'], '{sample}_summits.bed'),
        states = os.path.join(config['HMMRATAC'], '{sample}.bedgraph'),
        logs = os.path.join(config['HMMRATAC'], '{sample}.log'),
        sample_name = '{sample}'
    log:
        os.path.join(config['LOGS'], 'hmmratac', '{sample}.log')
    params:
        genomes = config['GENOMES'],
        blacklisted = config['BLACKLIST']
    resources:
        mem_mb = 32000
    message:
        '\n######################### Peak calling ########################\n'
        'Peak calling for {output.sample_name}\n.'
        '############################################################'
    shell:
        'HMMRATAC -Xms2g -Xmx{resources.mem_mb}m '
        '--bam {input.bam} --index {input.index} '
        '--genome {params.genome} --blacklist {params.blacklisted} '
        '--output {output.sample_name} --bedgraph true &amp;&gt; {log}'


And into the rule all, after filtering, before multiqc, I added:

    # Peak calling
    expand(os.path.join(config['HMMRATAC'], '{sample}.model'),
           sample = SAMPLES),


Relevant config.yaml fragments:

# Path to blacklisted regions
BLACKLIST: ""/mnt/data/.../hg38.blacklist.bed""

# Path to chromosome sizes
GENOMES: ""/mnt/data/.../hg38_sizes.genome""

# Path to filtered alignment
FILTERED: ""alignment/filtered""

# Path to peaks
HMMRATAC: ""peaks/hmmratac""


This is the error* I get (It goes on for every input and output of the rule). *Technically it's a warning but it halts execution of snakemake so I am calling it an error.

File path alignment/filtered//mnt/data/.../hg38.blacklist.bed.bam contains double '/'. This is likely unintended. It can also lead to inconsistent results of the file-matching approach used by Snakemake.
WARNING:snakemake.logging:File path alignment/filtered//mnt/data/.../hg38.blacklist.bed.bam contains double '/'. This is likely unintended. It can also lead to inconsistent results of the file-matching approach used by Snakemake.


It isn't actually ... - I just didn't feel safe providing an absolute path here.

For a couple of days, I have struggled with this error. Looked through the documentation, listened to the introduction. I understand that the above description is far from perfect (it is huge bc I don't even know how to work it down to provide minimal reproducible example...) but I am desperate and hope you can be patient with me.

Any suggestions as to how to google it, where to look for an error would be much appreciated.
",-1,1,-1.0
60118536,MissingOutputException in Snakemake workflow,"So I have the following snakemake rule:

SAMPLES=[""A"", ""B""]
READS=[""R1"", ""R2""]

rule fastqc_check:
    input:
      r1 = expand(""../../data/raw/{sample}_{read}.fastq.gz"",sample=SAMPLES, read=READS )
    output:
      html=expand(""../../data/interim/{sample}_{read}.html"", sample=SAMPLES, read=READS)
    conda:
      ""../../environment.yml""
    shell: ""fastqc --outdir='../../data/interim/'  {input.r1}""


When I run it, it starts executing successfully, generates the HTML files and then the workflow throws the following error

 MissingOutputException in line 5 of rules/minimap2_freebayes.smk:
    Missing files after 5 seconds:
    ../../data/interim/A_R1.html
    ../../data/interim/A_R2.html
    ../../data/interim/B_R1.html
    ../../data/interim/B_R2.html
    This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.


I can actually see the files and even open them, also tried --latency-wait 120 but it does not make any difference, keeps throwing the error. I am very new to Snakemake so I am not sure what else to do with it. 
",-1,-1,-1.0
60127866,Using bash functions in snakemake,"I am trying to download some files with snakemake. The files (http://snpeff.sourceforge.net/SnpSift.html#dbNSFP) I would like to download are on a google site/drive and my usual wget approach does not work. I found a bash function that does the job (https://www.zachpfeffer.com/single-post/wget-a-Google-Drive-file):

function gdrive_download () {  CONFIRM=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate ""https://docs.google.com/uc?export=download&amp;id=$1"" -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')  wget --load-cookies /tmp/cookies.txt ""https://docs.google.com/uc?export=download&amp;confirm=$CONFIRM&amp;id=$1"" -O $2  rm -rf /tmp/cookies.txt }

gdrive_download 120aPYqveqPx6jtssMEnLoqY0kCgVdR2fgMpb8FhFNHo test.txt


I have tested this function with my ids in a plain bash script and was able to download all the files. To add a bit to the complexity, I must use a workplace template, and incorporate the function into it.

rule dl: 
    params:
        url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_{genome}/{afile}'
    output:
        'data/{genome}/{afile}'
    params:
        id1 = '0B7Ms5xMSFMYlOTV5RllpRjNHU2s',
        f1 = 'dbNSFP.txt.gz'
    shell:
        """"""CONFIRM=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate ""https://docs.google.com/uc?export=download&amp;id={{params.id1}}"" -O- | sed -rn ""s/.*confirm=([0-9A-Za-z_]+).*/\1\n/p"") &amp;&amp; wget --load-cookies /tmp/cookies.txt ""https://docs.google.com/uc?export=download&amp;confirm=$CONFIRM&amp;id={{params.id1}}"" -O {{params.f1}} &amp;&amp; rm -rf /tmp/cookies.txt""""""
        #'wget -c {params.url} -O {output}'

rule checksum:
    input:
        i = 'data/{genome}/{afile}'
    output:
        o = temp('tmp/{genome}/{afile}.md5')
    shell:
        'md5sum {input} &gt; {output}'

rule file_size:
    input:
        i = 'data/{genome}/{afile}'
    output:
        o = temp('tmp/{genome}/{afile}.size')
    shell:
        'du -csh --apparent-size {input} &gt; {output}'

rule file_info:
    """"""md5 checksum and file size""""""
    input:
        md5 = 'tmp/{genome}/{afile}.md5',
        s = 'tmp/{genome}/{afile}.size'
    output:
        o = temp('tmp/{genome}/info/{afile}.csv')
    run:
        with open(input.md5) as f:
            md5, fp = f.readline().strip().split()
        with open(input.s) as f:
            size = f.readline().split()[0]
        with open(output.o, 'w') as fout:
            print('filepath,size,md5', file=fout)
            print(f""{fp},{size},{md5}"", file=fout)

rule manifest:
    input:
        expand('tmp/{genome}/info/{suffix}.csv', genome=('GRCh37','GRCh38'), suffix=('dbNSFP.txt.gz', 'dbNSFP.txt.gz.tbi'))
        #expand('tmp/{genome}/info/SnpSift{suffix}.csv', genome=('GRCh37','GRCh38'), suffix=('dbNSFP.txt.gz', 'dbNSFP.txt.gz.tbi'))
    output:
        o = 'MANIFEST.csv'
    run:
        pd.concat([pd.read_csv(afile) for afile in input]).to_csv(output.o, index=False)


There are four downloadable files for which I have ids (I only show one in params), however I don't know how to call the bash functions as written by ZPfeffer for all the ids I have with snakemake. Additionally, when I run this script, there are several errors, the most pressing being 

sed: -e expression #1, char 31: unterminated `s' command


I am far from a snakemake expert, any assistance on how to modify my script to a) call the functions with 4 different ids, b) remove the sed error, and c) verify whether this is the correct url format (currently url = 'https://docs.google.com/uc?export/{afile}) will be greatly appreciated. 
",-1,-1,-1.0
60030010,snakemake unpack with shell & conda,"I have the basic ""input can be single end or paired end reads"" problem for my snakemake pipeline. I'd like to use unpack if possible, since it seems designed for this situation (as illustrated in the answer for this issue), but I also want to use conda:, which requires shell:. I believe that shell: will die if I have {input.read2} but it's not provided by unpack(). Is there any good way of getting around this besides either 1) creating 2 nearly identical rules 2) making an empty read2 (if single-end) and then creating an if-else in shell to check for whether read2 is empty. Neither is ideal. 
",-1,-1,-1.0
59771487,Snakemake: HISAT2index buind and alignment using touch,"Following my previous question:Snakemake: HISAT2 alignment of many RNAseq reads against many genomes UPDATED.
I wanted to run the hisat2 alignment using touch in snakemake. 
I have several genome files with suffix .1.ht2l to .8.ht2l

bob.1.ht2l
...
bob.8.ht2l
steve.1.ht2l 
...
steve.8.ht2l


and sereval RNAseq samples

flower_kevin_1.fastq.gz
flower_kevin_2.fastq.gz
flower_daniel_1.fastq.gz
flower_daniel_2.fastq.gz 


I need to align all rnaseq reads against each genome.

workdir: ""/path/to/dir/""

(HISAT2_INDEX_PREFIX,)=glob_wildcards('/path/to/dir/{prefix}.fasta')
(SAMPLES,)=glob_wildcards(""/path/to/dir/{sample}_1.fastq.gz"")

rule all:
    input: 
        expand(""{prefix}.{sample}.bam"", zip, prefix=HISAT2_INDEX_PREFIX, sample=SAMPLES)

rule hisat2_build:
    input:
        database=""/path/to/dir/{prefix}.fasta""
    output:
        done = touch(""{prefix}"")
    threads: 2
    shell:
        (""/Tools/hisat2-2.1.0/hisat2-build -p {threads} {input.database} {wildcards.prefix}"")

rule hisat2:
    input:
        hisat2_prefix_done = ""{prefix}"",
        fastq1=""/path/to/dir/{sample}_1.fastq.gz"",
        fastq2=""/path/to/dir/{sample}_2.fastq.gz""
    output:
        bam = ""{prefix}.{sample}.bam"",
        txt = ""{prefix}.{sample}.txt"",
    log: ""{prefix}.{sample}.snakemake_log.txt""
    threads: 50
    shell:
        ""/Tools/hisat2-2.1.0/hisat2 -p {threads} -x {wildcards.prefix}""
        "" -1 {input.fastq1} -2 {input.fastq2}  --summary-file {output.txt} |""
        ""/Tools/samtools-1.9/samtools sort -@ {threads} -o {output.bam}""


The output gives me bob and steve aligned ONLY against ONE rna-seq sample (i.e. flower_kevin). I don't know how to solve. Any suggestions would be helpful.
",1,-1,-1.0
59766049,execute snakemake rule as last rule,"I tried to create a snakemake file to run sortmeRNA pipeline:

SAMPLES = ['test']
READS=[""R1"", ""R2""]

rule all:
    input: expand(""Clean/4.Unmerge/{exp}.non_rRNA_{read}.fastq"", exp = SAMPLES, read = READS)

rule unzip:
    input: 
        fq = ""trimmed/{exp}.{read}.trimd.fastq.gz""
    output: 
        ofq = ""Clean/1.Unzipped/{exp}.{read}.trimd.fastq""
    shell: ""gzip -dkc &lt; {input.fq} &gt; {output.ofq}""

rule merge_paired:
    input: 
        read1 = ""Clean/1.Unzipped/{exp}.R1.trimd.fastq"",
        read2 = ""Clean/1.Unzipped/{exp}.R2.trimd.fastq""
    output: 
        il = ""Clean/2.interleaved/{exp}.il.trimd.fastq""
    shell: ""merge-paired-reads.sh {input.read1} {input.read2} {output.il}""

rule sortmeRNA:
    input: 
        ilfq = ""Clean/2.interleaved/{exp}.il.trimd.fastq""
    output:
        reads_rRNA = ""Clean/3.sorted/{exp}_reads_rRNA"",
        non_rRNA = ""Clean/3.sorted/{exp}_reads_nonRNA""
    params:
        silvabac = ""rRNA_databases/silva-bac-16s-id90.fasta,index/silva-bac-16s-db:rRNA_databases/silva-bac-23s-id98.fasta,index/silva-bac-23s-db"",
        silvaarc = ""rRNA_databases/silva-arc-16s-id95.fasta,index/silva-arc-16s-db:rRNA_databases/silva-arc-23s-id98.fasta,index/silva-arc-23s-db"",
        silvaeuk = ""rRNA_databases/silva-euk-18s-id95.fasta,index/silva-euk-18s-db:rRNA_databases/silva-euk-28s-id98.fasta,index/silva-euk-28s-db"",
        rfam = ""rRNA_databases/rfam-5s-database-id98.fasta,index/rfam-5s-db:rRNA_databases/rfam-5.8s-database-id98.fasta,index/rfam-5.8s-db"",
        acc = ""--num_alignments 1 --fastx --log -a 20 -m 64000 --paired_in -v""
    log:
        ""Clean/sortmeRNAlogs/{exp}_sortmeRNA.log""
        shell:'''
        sortmerna --ref {params.silvabac}:{params.silvaarc}:{params.silvaeuk}:{params.rfam} --reads {input.ilfq} --aligned {output.reads_rRNA} --other {output.non_rRNA} {params.acc}
        '''
rule unmerge_paired:
    input:
        inun = ""Clean/3.sorted/{exp}_reads_nonRNA.fastq""
    output:
        R1 = ""Clean/4.Unmerge/{exp}.non_rRNA_R1.fastq"",
        R2 = ""Clean/4.Unmerge/{exp}.non_rRNA_R2.fastq""
    shell:""unmerge-paired-reads.sh {input.inun} {output.R1} {output.R2}""


This worked fine !. But for 1 sample it produced an output with size ~53 GB. I have 90 samples to run and cannot afford huge disk space. I tried to make output of rules unzip,merge_paired,sortmeRNA as temp(), but upon executing unmerge_paired raises ""Missing input files exception"" error. 
I also tried to add rule_remove to delete all those intermediate directories. But that is not executed as last rule, rather somewhere in the middle raising error again !. Is there any efficient way to do this ?

The error that occurs is:

MissingInputException in line 45 of sortmeRNA_pipeline_memv2.0.snakefile:
Missing input files for rule unmerge_paired:
Clean/3.sorted/test_reads_nonRNA.fastq


Also please note that, rule sortmeRNA requires a string for output and produces string.fastq file, which is then input into rule unmerge_paired !
Thanks.
",-1,-1,-1.0
59754604,Snakemake: HISAT2 alignment of many RNAseq reads against many genomes UPDATED,"I have several genome files with suffix .1.ht2l to .8.ht2l

bob.1.ht2l
...
bob.8.ht2l
steve.1.ht2l 
...
steve.8.ht2l


and sereval RNAseq samples

flower_kevin_1.fastq.gz
flower_kevin_2.fastq.gz
flower_daniel_1.fastq.gz
flower_daniel_2.fastq.gz 


I need to align all rnaseq reads against each genome.
UPDATED as dariober suggested:

workdir: ""/path/to/aligned""  
(HISAT2_INDEX_PREFIX,)=glob_wildcards(""/path/to/index/{prefix}.1.ht2l"")
(SAMPLES,)=glob_wildcards(""/path/to/{sample}_1.fastq.gz"") 
print(HISAT2_INDEX_PREFIX)  
print (SAMPLES)

rule all:
    input: 
        expand(""{prefix}.{sample}.bam"", zip, prefix=HISAT2_INDEX_PREFIX, sample=SAMPLES)

rule hisat2:
    input:
        hisat2_index=expand(""%s.{ix}.ht2l"" % ""/path/to/index/{prefix}"", ix=range(1, 9), prefix = HISAT2_INDEX_PREFIX),
        fastq1=""/path/to/{sample}_1.fastq.gz"",
        fastq2=""/path/to/{sample}_2.fastq.gz""
    output:
        bam = ""{prefix}.{sample}.bam"",
        txt = ""{prefix}.{sample}.txt"",
    log: ""{prefix}.{sample}.snakemake_log.txt""
    threads: 5
    shell:
      ""/Tools/hisat2-2.1.0/hisat2 -p {threads} -x {/path/to/index/{wildcards.prefix}""
      "" -1 {input.fastq1} -2 {input.fastq2}  --summary-file {output.txt} |""
      ""/Tools/samtools-1.9/samtools sort -@ {threads} -o {output.bam}""


The problem I get is when running HISAT2 is taking as -x input all bob.1.ht2l:bob.8.ht2l and steve.1.ht2l:steve.8.ht2l at once. While rna-seq should be mapped at each genome separately. Where is the error?
NB: my previous question: Snakemake: HISAT2 alignment of many RNAseq reads against many genomes
",-1,-1,-1.0
59642199,How can I run multiple runs of pipeline with different config files - issue with lock on .snakemake directory,"I am running a snakemake pipeline from the same working directory but with different config files and the input / output are in different directories too. The issue seems to be that although both runs are using data in different folders snakemake creates the lock on the pipeline folder due to the .snakemake folder and the lock folder within. Is there a way to force separate .snakemake folders? code example below: 

Both runs are ran from within /home/pipelines/qc_pipeline : 

run 1: 

/home/apps/miniconda3/bin/snakemake -p -k -j 999 --latency-wait 10 --restart-times 3 --use-singularity --singularity-args ""-B /pipelines_test/QC_pipeline/PE_trimming/,/clusterTMP/testingQC/,/home/www/codebase/references"" --configfile /clusterTMP/testingQC/config.yaml --cluster-config QC_slurm_roadsheet.json --cluster ""sbatch --job-name {cluster.name} --mem-per-cpu {cluster.mem-per-cpu} -t {cluster.time} --output {cluster.output}""   


run 2: 

/home/apps/miniconda3/bin/snakemake -p -k -j 999 --latency-wait 10 --restart-times 3 --use-singularity --singularity-args ""-B /pipelines_test/QC_pipeline/SE_trimming/,/clusterTMP/testingQC2/,/home/www/codebase/references"" --configfile /clusterTMP/testingQC2/config.yaml --cluster-config QC_slurm_roadsheet.json --cluster ""sbatch --job-name {cluster.name} --mem-per-cpu {cluster.mem-per-cpu} -t {cluster.time} --output {cluster.output}""   


error: 

Directory cannot be locked. Please make sure that no other Snakemake process is trying to create the same files in the following directory:
/home/pipelines/qc_pipeline
If you are sure that no other instances of snakemake are running on this directory, the remaining lock was likely caused by a kill signal or a power loss. It can be removed with the --unlock argument.

",-1,-1,-1.0
59342187,Iterating Over Strings in Shell Commands Using Snakemake,"To give a little background, I'm trying to put together a pipeline to analyze deep sequencing results of in-silico predicted CRISPR off targets. I amplify a known sequence from the genome in 50 different places and each amplicon contains a predicted off target site where my original CRISPR guide could potentially bind. 
   I input the two paired end NGS files into the program CRISPResso along with the amplicon sequence and off-target guide. The two paired end files, amplicon sequence, and gRNA are different for each of the 50 sites. I need to do this with multiple conditions, donors, and replicates so the numbers add up quick. 

I created the snakemake workflow below:

configfile: ""config.yaml""

singularity:
    ""docker://pinellolab/crispresso2""

SAMPLES, =glob_wildcards(""data/{sample}_L001_R1_001.fastq.gz"")

rule all:
    input:
        expand(""outfiles/{sample}"", sample=SAMPLES)

rule crispresso_run:
    input:
        R1=""data/{sample}_L001_R1_001.fastq.gz"",
        R2=""data/{sample}_L001_R2_001.fastq.gz""

    params:
        AMP=config['AMP'],
        gRNA=config['gRNA']

    output:
        directory(""outfiles/{sample}"")

    shell:
        ""docker run -v ${{PWD}}:/DATA -w /DATA -i pinellolab/crispresso2 \
            CRISPResso -r1 {input.R1} \
            -r2 {input.R2} \
            -a {params.AMP} \
            -g {params.gRNA} \
            -q 30 -s 30 --min_bp_quality_or_N 30 -w 3 -o {output}""


with the config file:

AMP:
  - ATAAAAACCATACACATTCAGTGGGAAACCTTCAGCCATAGAGAAGTATAGGCAGGGTGCAGCTGATTGCTCTGTCTTTGGGCAATTTAGCTTTTAGGCCAGAGGCCACAGATGGGTAGCCTGGTGTGTGCCTAGGGTGTTTTTGTTTGGCTGGCGCAATATTTTTTAAAACTGTAAGTTTATTGCCAGCATTTAA
  -GATGTGCTAGAGATGAGAAAGGATGTGGCAGAAGAAGTACCTATCTCTTGAGGGATGAAGTGGCCTCATTTCACCTACTGAGAGTCAGGAAGTGCCCCATCTGCAGCCTCTGGGCTGGTTGGGGTCAGTCTGCAGATTTTCCTTGCTTTCTCCCATGCCCTTGTCTTTCTCTCCCCTGTAGAGAAAGACACTGATGTTGCTGTTGTTCTAGGAACAGTGGAGACAACTG
gRNA:
  - TTAGGCCAGAGGCCACAGATGGG
  - CCAGCCCAGAGGCTGCAGATGGG


The -a and -g parameters in the shell command are where the amplicon and gRNA should be placed, respectively. 

When I have just one amplicon and gRNA in the config file everything works great. But when I include two in the config file, snakemake just concatenates the two amplicons or gRNAs which throws an error. I'm guessing that I'm using the params function incorrectly but a can't find anything that shows how to use it in this manner (namely iterating strings in the shell command). 

Also, just to clarify, the paired end files need to be run in the same order as each corresponding gRNA and amplicon pair or an error will be thrown by CRISPResso.  

I'm new to all of this and don't know enough to solve the problem on my own. Any help would be greatly appreciated. 

Thanks, 
Edward
",1,-1,-1.0
59083493,Snakemake wildscard unexpected changed,"I'm a novice of Snakemake and I come across a bug that struggled me a lot. 
I have a wildcards like this:

 rank = ['Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species']
 ordi = ['DCA', 'CCA', 'RDA', 'NMDS', 'MDS', 'NMDS', 'PCoA']


The previous version didn't have the wildcards problem and run successfully

the previous version rule all like this:

rule all:
  input:
    expand('common_taxonomic/abundance_table_{Rank}.biom', Rank = rank),
    directory('Gene/gene_Venn'),
    directory('Gene/gene_samples_heatmap'),
    directory('taxa_ternaryplot'),
    directory(expand('beta/PCA/{Rank}', Rank = rank))
  benchmark:
    ""Check_utility.tsv""


But when I exchange the wildcards position like 

directory(expand('beta/{Rank}/PCA/', Rank = rank)),
directory(expand('beta/{Rank}/{Ordi}', Rank = rank, Ordi = ordi))


I got this error

Building DAG of jobs...
MissingInputException in line 59 of /sysdata/Meta/pipeline/Snakefile:
Missing input files for rule biom_convert:
common_taxonomic/Table_taxa_NR_Kingdom/CCA.txt


As you can see, the rank wildcard is elongated with /PCA, or /{ordi}. I am quite confused about this, am I writing a wrong code?

my biom_convert rule is:

rule biom_convert:
  input: 'common_taxonomic/Table_taxa_NR_{rank}.txt'
  output:'common_taxonomic/abundance_table_{rank}.biom'
  shell:'biom convert -i {input} -o {output} --table-type=""OTU table"" --to-json'

",-1,-1,-1.0
58975374,Why is the shell command not executed by snakemake,"I've started working through a snakemake tutorial and the very first workflow from there does not work. Here is the rule I am using:

rule make_a_copy:
    input:
        ""a.txt""
    output:
        ""a_copy.txt""
    shell:
        """"""
        copy {input} {output}
        """"""


Then I run the workflow with

snakemake -p a_copy.txt

This results in the following output:

Building DAG of jobs...
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   convert_to_upper_case
    1

[Thu Nov 21 15:33:18 2019]
rule convert_to_upper_case:
    input: a.txt
    output: a_copy.txt
    jobid: 0


        copy a.txt a_copy.txt

Waiting at most 5 seconds for missing files.
MissingOutputException in line 1 of D:\OneDrive\projects\reproducible_research_course\snakemake\Snakefile:
Missing files after 5 seconds:
a_copy.txt
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: D:\OneDrive\projects\reproducible_research_course\snakemake\.snakemake\log\2019-11-21T153318.425144.snakemake.log


If I run copy a.txt a_copy.txt in cmd (I'm on windows) the command does produce an a.upper.txt file.

What am I missing?
",-1,-1,-1.0
58745493,Snakemake invalid syntax,"I am getting crazy with snakemake. I tried all types of ways, using config file etc, but nothing works and I cannot understand exactly how to detect the error checking line by line.

HISAT2_INDEX_PREFIX = ""/media/jim/Elements/Happy/index/genome_chromosomes""

SAMPLES, *_=glob_wildcards('/media/jim/Elements/Happy/test/tissue/trimmed/{sample}_1.fastq.gz')

rule all:
    input: expand(""{sample}.bam"", sample=SAMPLES)

rule hisat2:
    input:
        hisat2_index=expand(f""{HISAT2_INDEX_PREFIX}.{{ix}}.ht2l"", ix=range(1, 8)),
        fastq1=""/media/jim/Elements/Happy/test/tissue/trimmed/{sample}_1.fastq.gz"",
        fastq2=""/media/jim/Elements/Happy/test/tissue/trimmed/{sample}_2.fastq.gz""
    output:
        bam = ""{sample}.bam"",
        txt = ""{sample}.txt"",
    log: ""/snakemake_log.txt""
    threads: 8
    shell:
        ""hisat2 -p {threads} -x {HISAT2_INDEX_PREFIX}""
        "" -1 {input.fastq1} -2 {input.fastq2}  --summary-file {output.txt} |""
        ""samtools sort -@ {threads} -o {output.bam}""


Now the error I get is:

SyntaxError in line 11 of /media/jim/Elements/Happy/test/Snakefile:
invalid syntax


The error might be some small thing, but I cannot understand which......
The problem is that I have many tissues, so I have to make working at least this simple example...
",-1,-1,-1.0
58739674,How to properly use batch flag in snakemake to subset DAG,"I have a workflow written in snakemake that balloons at one point to a large number of output files. This happens because there are many combinations of my wildcards yielding on the order of 60,000 output files.

With this number of input files, DAG generation is very slow for subsequent rules to the point of being unusable. In the past, I've solved this issue by iterating with bash loops across subsetted configuration files where all but one of the wildcards is commented out. For example, in my case I had one wildcard (primer) that had 12 possible values. By running snakemake iteratively for each value of ""primer"", it divided up the workflow into digestible chunks (~5000 input files). With this strategy DAG generation is quick and the workflow proceeds well.

Now I'm interested in using the new --batch flag available in snakemake 5.7.4 since Johannes suggested to me on twitter that it should basically do the same thing I did with bash loops and subsetted config files.

However, I'm running into an error that's confusing me. I don't know if this is an issue I should be posting on the github issue tracker or I'm just missing something basic.

The rule my workflow is failing on is as follows:

rule get_fastq_for_subset:
    input:
        fasta=""compute-workflow-intermediate/06-subsetted/{sample}.SSU.{direction}.{group}_pyNAST_{primer}.fasta"",
        fastq=""compute-workflow-intermediate/04-sorted/{sample}.{direction}.SSU.{group}.fastq""
    output:
        fastq=temp(""compute-workflow-intermediate/07-subsetted-fastq/{sample}.SSU.{direction}.{group}_pyNAST_{primer}.full.fastq""),
        fastq_revcomp=""compute-workflow-intermediate/07-subsetted-fastq/{sample}.SSU.{direction}.{group}_pyNAST_{primer}.full.revcomped.fastq""
    conda:
        ""envs/bbmap.yaml""
    shell:
        ""filterbyname.sh names={input.fasta} include=t in={input.fastq} out={output.fastq} ; ""
        ""revcompfastq_according_to_pyNAST.py --inpynast {input.fasta} --infastq {output.fastq} --outfastq {output.fastq_revcomp}""



I set up a target in my all rule to generate the output specified in the rule, then tried running snakemake with the batch flag as follows:

snakemake --configfile config/config.yaml --batch get_fastq_for_subset=1/20 --snakefile Snakefile-compute.smk --cores 40 --use-conda 


It then fails with this message:

WorkflowError:
Batching rule get_fastq_for_subset has less input files than batches. Please choose a smaller number of batches.


Now the thing I'm confused about is that my input for this rule should actually be on the order of 60,000 files. Yet it appears snakemake getting less than 20 input files. Perhaps it's only counting 2, as in the number of input files that are specified by the rule? But this doesn't really make sense to me...

The source code for snakemake is here, showing how snakemake counts things but it's a bit beyond me:

https://snakemake.readthedocs.io/en/stable/_modules/snakemake/dag.html

If anyone has any idea what's going on, I'd be very grateful for your help!

Best,
Jesse
",-1,-1,-1.0
58597811,snakemake log files for failed jobs on slurm don't exist,"I am running a snakemake pipeline on a slurm HPC. Occasionally, jobs will fail due to exceeded wall time or memory. Such failed jobs do not create log files, or their log files are deleted as part of snakemakes automatic removal of files associated with failed jobs. It would be convenient to get the logging information for failed jobs so that I could more easily understand why the job failed. 

I currently have logs params sat for each job, and the cluster.json file then calls those logs for each job specifically. A general rule, it's cluster.json call and my snakemake call are shown below.

rule fastqScreen:
    input:
        Fast1=""{sample}/{sample}.R1.fq.gz"",
        Fast2=""{sample}/{sample}.R2.fq.gz""
    output:
        output1=""{sample}/{sample}.fq.gz"",
        output2=""{sample}/{sample}_screen.png"",
        output3=""{sample}/{sample}_screen.txt""
    log: ""logs/{sample}FastScreen.log""
    params: 
        outprefix = ""{sample}""
    threads: 4
    priority: 3
    shell:
        """"""
        cat {input.Fast1} {input.Fast2} &gt; {output.output1} &amp;&amp; /home/manninm/Programs/fastq_screen_v0.14.0/fastq_screen --aligner bowtie2 --quiet --force --threads {threads} {output.output1}
        """"""


""__default__"": {
        ""account"": ""kretzler"",
        ""job-name"": ""17_{rule}"",
        ""partition"": ""standard"",
        ""nodes"": ""1"",
        ""time"": ""10:00:00"",
        ""ntasks-per-node"": ""1"",
        ""cpus-per-task"": ""1"",
        ""mem"": ""4g"",
        ""output"": ""{log}.out.txt"",
        ""error"": ""{log}.err.txt"",
        ""mail-user"": ""$USER@umich.edu"",
        ""mail-type"": ""ALL""
    },
""HtSeq_Count"": {
        ""cpus-per-task"": ""{threads}"",
        ""--mem"": ""16g"",
        ""time"": ""8:00:00"",
        ""output"": ""{log}.out.txt"",
        ""error"": ""{log}.error.log""
    },


snakemake -j 1000 --restart-times 2 --max-jobs-per-second 5 --max-status-checks-per-second 5 --cluster-config cluster.json --cluster 'sbatch --job-name {cluster.job-name} --nodes {cluster.nodes} --ntasks-per-node {cluster.ntasks-per-node} --cpus-per-task {cluster.cpus-per-task} --mem {cluster.mem} --partition {cluster.partition} --time {cluster.time} --mail-user {cluster.mail-user} --mail-type {cluster.mail-type} --error {cluster.error} --output {cluster.output}'


I would like to get the error or reason a job failed printed to the error.log file associated with each job, if at all possible, I don't understand what I am doing wrong that causes log files for failed jobs to disappear.
",-1,1,-1.0
58540231,name input/output files in snakemake according to variable (not wildcard) in config.yaml,"I am trying to edit and run a snakemake pipeline. In a nutshell, the snakemake pipeline calls a default genome aligner (minimap) and produces output files with this name. I am trying to add a variable aligner to config.yaml to specify the aligner I want to call. Also (where I am actually stuck), the output files should have the name of the aligner specified in config.yaml.

My config.yaml looks like this:

# this config.yaml is passed to Snakefile in pipeline-structural-variation subfolder.
# Snakemake is run from this pipeline-structural-variation folder; it is necessary to
# pass an appropriate path to the input-files (the ../ prefix is sufficient for this demo)

aligner: ""ngmlr"" # THIS IS THE VARIABLE I AM ADDING TO THIS FILE. VALUES COULD BE minimap or ngmlr

# FASTQ file or folder containing FASTQ files
# check if this has to be gzipped
input_fastq: ""/nexusb/Gridion/20190917PGD2staal2/PD170815/PD170815_cat_all.fastq.gz"" # original is ../RawData/GM24385_nf7_chr20_af.fastq.gz

# FASTA file containing the reference genome
# note that the original reference sequence contains only the sequence of chr20
reference_fasta: ""/nexus/bhinckel/19/ONT_projects/PGD_breakpoint/ref_hg19_local/hg19_chr1-y.fasta"" # original is ../ReferenceData/human_g1k_v37_chr20_50M.fasta

# Minimum SV length
min_sv_length: 300000 # original value was 40

# Maximum SV length
max_sv_length: 1000000 # original value was 1000000. Note that the value I used to run the pipeline for the sample PD170677 was 100000000000, which will be coerced to NA in the R script (/home/bhinckel/ont_tutorial_sv/ont_tutorial_sv.R)

# Min read length. Shorter reads will be discarded
min_read_length: 1000

# Min mapping quality. Reads will lower mapping quality will be discarded
min_read_mapping_quality: 20

# Minimum read support required to call a SV (auto for auto-detect)
min_read_support: 'auto'

# Sample name
sample_name: ""PD170815"" # original value was GM24385.nf7.chr20_af. Note that this can be a list


I am posting below the sections of my snakefile which generate output files with the extension _minimap2.bam, which I would like to replace by either _minimap2.bam or _ngmlr.bam, depending on aligner on config.yaml

# INPUT BAM folder
bam = None
if ""bam"" in config:
    bam = os.path.join(CONFDIR, config[""bam""])

# INPUT FASTQ folder
FQ_INPUT_DIRECTORY = []
if not bam:
    if not ""input_fastq"" in config:
        print(""\""input_fastq\"" not specified in config file. Exiting..."")

    FQ_INPUT_DIRECTORY = os.path.join(CONFDIR, config[""input_fastq""])
    if not os.path.exists(FQ_INPUT_DIRECTORY):
        print(""Could not find {}"".format(FQ_INPUT_DIRECTORY))

    MAPPED_BAM = ""{sample}/alignment/{sample}_minimap2.bam"" # Original
    #MAPPED_BAM = ""{sample}/alignment/{sample}_{alignerName}.bam"" # this did not work
    #MAPPED_BAM = f""{sample}/alignment/{sample}_{config['aligner']}.bam"" # this did nor work either
else:
    MAPPED_BAM = find_file_in_folder(bam, ""*.bam"", single=True)

...

if config['aligner'] == 'minimap':
    rule index_minimap2:
        input:
            REF = FA_REF
        output:
            ""{sample}/index/minimap2.idx""
        threads: config['threads']
        conda: ""env.yml""
        shell:
            ""minimap2 -t {threads} -ax map-ont --MD -Y {input.REF} -d {output}""


    rule map_minimap2:
        input:
            FQ = FQ_INPUT_DIRECTORY,
            IDX = rules.index_minimap2.output,
            SETUP = ""init""
        output:
            BAM = ""{sample}/alignment/{sample}_minimap2.bam"",
            BAI = ""{sample}/alignment/{sample}_minimap2.bam.bai""

        conda: ""env.yml""
        threads: config[""threads""]
        shell:
            ""cat_fastq {input.FQ} | minimap2 -t {threads} -K 500M -ax map-ont --MD -Y {input.IDX} - | samtools sort -@ {threads} -O BAM -o {output.BAM} - &amp;&amp; samtools index -@ {threads} {output.BAM}""

else:
    print(f""Aligner is {config['aligner']} - skipping indexing step for minimap2"")

    rule map_ngmlr:
        input:
            REF = FA_REF,
            FQ = FQ_INPUT_DIRECTORY,
            SETUP = ""init""
        output:
            BAM = ""{sample}/alignment/{sample}_minimap2.bam"",
            BAI = ""{sample}/alignment/{sample}_minimap2.bam.bai""
        conda: ""env.yml""
        threads: config[""threads""]
        shell:
            ""cat_fastq {input.FQ} | ngmlr -r {input.REF} -t {threads} -x ont - | samtools sort -@ {threads} -O BAM -o {output.BAM} - &amp;&amp; samtools index -@ {threads} {output.BAM}""



I initially tried to create a alignerName parameter, similar to the sample parameter, as shown below:

# Parameter: sample_name
sample = ""sv_sample01""
if ""sample_name"" in config:
    sample = config['sample_name']

###############
#
# code below created by me
#
###############
# Parameter: aligner_name
alignerName = ""defaultAligner""
if ""aligner"" in config:
    alignerName = config['aligner']


Then I tried to input {alignerName} wherever I have minimap2 on my input/ output files (see commented MAPPED_BAM variable definition above), though this is throwing an error. I guess snakemake will interpret {alignerName} as a wildcard, though what I want is simply to pass the variable name defined in config['aligner'] to input/ output files. I also tried with f-string (MAPPED_BAM = f""{sample}/alignment/{sample}_{config['aligner']}.bam""), though I guess this it did not work either.
",-1,1,-1.0
58527369,sbatch: error: Batch job submission failed: Socket timed out on send/recv operation when running Snakemake,"I am running a snakemake pipeline on a HPC that uses slurm. The pipeline is rather long, consisting of ~22 steps. Periodically, snakemake will encounted a problem when attempting to submit a job. This reults in the error

sbatch: error: Batch job submission failed: Socket timed out on send/recv operation
Error submitting jobscript (exit code 1):


I run the pipeline via a sbatch file with the following snakemake call

snakemake -j 999 -p --cluster-config cluster.json --cluster 'sbatch --account {cluster.account} --job-name {cluster.job-name} --ntasks-per-node {cluster.ntasks-per-node} --cpus-per-task {threads} --mem {cluster.mem} --partition {cluster.partition} --time {cluster.time} --mail-user {cluster.mail-user} --mail-type {cluster.mail-type} --error {cluster.error} --output {cluster.output}' 


This results in not only an output for snakemake sbatch job, but also for the jobs that snakemake creates. The above error appears in the slurm.out for the sbatch file.

The specific job step the error indicates will run successfully, and give output, but the pipeline fails. The logs of the job step show that the job id ran without a problem. I have googled this error, and it appears to happen often with slurm, and especially when the scheduler is under high IO, which suggests it will be an inevitable and regular occurrence. I was hoping someone has encountered this problem, and could offer suggestions for a work around, so that the entire pipeline doesn't fail.
",-1,-1,-1.0
58486267,Snakemake report: Include html report in report,"I am trying to include in my snakemake report a (sub)report which is an html file in some directory and which includes various images in that same directory. Is there a nice way to do this kind of embedding with the standard snakemake reporting? 

With the directive:

output:
    report(""reports/somestats.html"", category = ""someCat"")


I manage to get the html file listed under ""Results"" in the snakemake report. 

However, if I click on it, my browser (firefox) asks me what to do with the file and if I tell it to open it with the browser, it opens:

file:///tmp/mozilla_xxx/yyy.html

but the links in the html file, which are relative, are all broken and no image is shown.

I could include all the images directly in the snakemake report but that seems counterproductive as the tool used to generate the html I want to embed has already formatted everything nicely.
",-1,-1,-1.0
58413114,Snakemake wrappers fails to open environment file: HTTP Error 404: Not Found,"When using snakemake wrappers with snakemake version 5.5.2 like this:

rule bcf_to_vcf:
    input:
        ""{prefix}.bcf""
    output:
        ""{prefix}.vcf""
    params:
        """"  # optional parameters for bcftools view (except -o)
    wrapper:
        ""0.38.1/bio/bcftools/view""


I get the following error:

Building DAG of jobs...
WorkflowError:
Failed to open environment file https://bitbucket.org/snakemake/snakemake-wrappers/raw/0.38.1/bio/bcftools/view/environment.yaml:
HTTPError: HTTP Error 404: Not Found


Apparently, the wrapper files are no longer available on bitbucket.
",-1,-1,-1.0
58194646,Snakemake: functions as input not working as expected (InputFunctionException with KeyError),"In my case, there is no pattern in the names of input and output files, so I used a dict to map the output to the input file, and then used this dict in the function as input. Please see the codes of Snakemake file below:

in_files = ['in_a', 'in_b']
out_files = ['out_c', 'out_d']

out_to_in_dict = dict()
for in_file, out_file in zip(in_files, out_files):
    out_to_in_dict[out_file] = in_file

rule all:
    input:
        out_files

rule copy_file:
    input:
        lambda wildcards: out_to_in_dict[wildcards.outfile]
    output:
        '{outfile}'
    shell:
        '''
        cp {input} {output}
        '''


After executing Snakemake, I got this error:

InputFunctionException in line 8 of /home/test.snake:
KeyError: 'in_a'
Wildcards:
outfile=in_a


The version of Snakemake I am using is 5.5.3. Could anyone please help? Thanks!
",-1,-1,-1.0
58186199,Snakemake Checkpoint Throws (exited with non-zero exit code) even after correct completion,"I'm currently running a snakemake checkpoint  that appears to be throwing a non-zero exit code even after correct completion of the command, and am unsure how to resolve the problem.

The purpose of the below script is to parse a file of coordinates, the bed_file, extract all regions from a bam file rna_file and eventually assemble these regions. The code is below, and my snakemake version is 5.6.0. 

#Pull coordinates from a BAM file, and use the command samtools view to extract the corresponding #data, naming the output as the coordinate file, here named ""6:25274434-25278245.bam"". There are #an unknown number of output files

checkpoint pull_reads_for_BAM:
    input:
    ¦   bed_file = get_lncRNA_file,
    ¦   rna_file = get_RNA_file
    conda:
    ¦   ""envs/pydev_1.yml""
    params:
    ¦   ""01.pulled_reads""
    output:
    ¦   directory(""01.pulled_reads/{tissue}"")
    shell:""""""

    mkdir 01.pulled_reads/{wildcards.tissue}

    store_regions=$(cat {input.bed_file} | awk -F'\t' '{{ print $1 "":"" $2 ""-"" $3 }}')

    for i in $store_regions ; do
    ¦   samtools view -b -h {input.rna_file} ${{i}} &gt; 01.pulled_reads/{wildcards.tissue}/${{i}}.bam ;
    done

    echo ""This completed fine""

    """"""

rule samtools_sort:
    input:
    ¦   ""01.pulled_reads/{tissue}/{i}.bam""
    params:
    ¦   ""{i}""
    output:
    ¦   ""01.pulled_reads/{tissue}/{i}.sorted.bam""
    shell:
    ¦   ""samtools sort -T sorted_reads/{params}.tmp {input} &gt; {output}""

rule samtools_index:
    input:
    ¦   ""01.pulled_reads/{tissue}/{i}.sorted.bam""
    output:
    ¦   ""01.pulled_reads/{tissue}/{i}.sorted.bam.bai""
    shell:
        ""samtools index {input}""

rule string_tie_assembly:
    input:
    ¦   ""01.pulled_reads/{tissue}/{i}.sorted.bam""
    output:
    ¦   ""02.string_tie_assembly/{tissue}/{i}_assembly.gtf""
    shell:
        ""stringtie {input} -f 0.0 -a 0 -m 50 -c 3.0 -f 0.0 -o {output}""


def trigger_aggregate(wildcards):
    checkpoint_output = checkpoints.pull_reads_for_BAM.get(**wildcards).output[0]

    x = expand(""02.string_tie_assembly/{tissue}/{i}_assembly.merged.gtf"",
    ¦   tissue = wildcards.tissue,
    ¦   i=glob_wildcards(os.path.join(checkpoint_output, ""{i}.bam"")).i)
    return x


#Aggregate function that triggers rule 
rule combine_all_gtf_things:
    input:
    ¦   trigger_aggregate
    output:
    ¦   ""03.final_stuff/{tissue}.merged.gtf""
    shell:""""""
    cat {input} &gt; {output}
    """"""



After the command has run to completion, snakemake returns (exited with non-zero exit code) for some mysterious reason. I can watch the output be generated in the file and it appears to be correct, so I'm unsure why it's throwing this error.

The checkpoint I have generated is modeled after this: 
https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html

Related Questions that have gone unanswered:
Snakemake checkpoint (exited with non-zero exit code)
",-1,-1,-1.0
57965649,"Snakemake + Docker Volumes - ""Missing Rule Exception""","I'm trying to use snakemake with a docker image, but am having trouble with the docker volume.  Unfortunately, there are no details on how to use 'singularity-args' to do this.

My snakemake file is:

rule all:
    input:
        'a/file3.txt'

rule step1:
    output:
        touch('a/file1.txt')

rule step2:
    input:
        rules.step1.output[0]
    output:
        'a/file2.txt'
    params:
        text = 'this is a test',
        path = '/data/file2.txt'
    singularity:
        ""docker://XXX/test""
    shell:
        ""python test.py {params.text} {params.path}""

rule step3:
    input:
        rules.step2.output[0]
    output:
        touch('a/file3.txt')


The docker image is basically a python file that writes a string to file (for testing purposes).  I'm trying to mount my home directory to the docker /data directory.  With docker, I'm able to mount a volume using '-v'.

What is the correct way of doing this with snakemake?  

I've tried the following commands (on MacOS and Ubuntu 18.04) and both have failed.

snakemake -s pipeline.py --use-singularity --singularity-args “-B /home/XXX/snakemake/a:/data”
snakemake -s pipeline.py --use-singularity --singularity-args “-B /home/XXX/snakemake/a”


The error message is:

No rule to produce /home/XXX/snakemake/a:/data” (if you use input functions make sure that they don't raise unexpected exceptions).


Am I missing a step?  

Thanks in advance!
",-1,-1,-1.0
57878152,"Snakemake always rebuilds targets, even when up to date","I'm new to snakemake and running into some behavior I don't understand. I have a set of fastq files with file names following the standard Illumina convention:

SAMPLENAME_SAMPLENUMBER_LANE_READ_001.fastq.gz

In a directory reads/raw_fastq. I'd like to create symbolic links to simplify the names to follow the pattern:

SAMPLENAME_READ.fastq.gz

In a directory reads/renamed_raw_fastq

My aim is that as I add new fastq files to the project, snakemake will create symlinks only for the newly-added files.

My snakefile is as follows:

# Get sample names from read file names in the ""raw"" directory

readRootDir = 'reads/'
readRawDir = readRootDir + 'raw_fastq/'

import os

samples = list(set([x.split('_', 1)[0] for x in os.listdir(readRawDir)]))
samples.sort()

# Generate simplified names

readRenamedRawDir = readRootDir + 'renamed_raw_fastq/'

newNames = expand(readRenamedRawDir + ""{sample}_{read}.fastq.gz"", sample = samples, read = [""R1"", ""R2""])

# Create symlinks

import glob

def getRawName(wildcards):
    rawName = glob.glob(readRawDir + wildcards.sample + ""_*_"" + wildcards.read + ""_001.fastq.gz"")[0]
    return rawName

rule all:
    input: newNames 

rule rename:
    input: getRawName
    output: ""reads/renamed_raw_fastq/{sample}_{read}.fastq.gz""
    shell: ""ln -sf {input} {output}""


When I run snakemake, it tries to generate the symlinks as expected but:


Always tries to create the target symlinks, even when they already exist and have later timestamps than the source fastq files.
Throws errors like:


MissingOutputException in line 68 of /work/nick/FAW-MIPs/renameRaw.snakefile:
Missing files after 5 seconds:
reads/renamed_raw_fastq/Ben21_R2.fastq.gz
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.


It's almost like snakemake isn't seeing the ouput files it creates. Can anyone suggest what I might be missing here?

Thanks!
",-1,-1,-1.0
57817515,Confused by `--default-remote-prefix` flag in snakemake for kubernetes on Google Cloud,"I'm trying to run a bioinformatics pipeline using Snakemake on GoogleCloud. The first two steps are:


Download the reads from ENA
Run bbmap's clumpify on the data.


The two rules look like this:

def get_fwd_url(wildcard):
    return samples.loc[wildcard, 'fwd'].values[0]

def get_rev_url(wildcard):
    return samples.loc[wildcard, 'rev'].values[0]

rule get_reads:
    output:
        fwd=temp(""samples/{sample}/fwd.gz""),
        rev=temp(""samples/{sample}/rev.gz"")
    threads: 1
    params:
        fwd_url=get_fwd_url,
        rev_url=get_rev_url
    log:
        ""logs/{sample}.get_reads.log""
    benchmark:
        ""benchmarks/{sample}.get_reads.tsv""
    shell:
        """"""
        wget -O {output.fwd} {params.fwd_url};
        wget -O {output.rev} {params.rev_url};
        """"""

rule run_bbmap_clumpify:
    input:
        raw_fwd=rules.get_reads.output.fwd,
        raw_rev=rules.get_reads.output.rev
    output:
        temp(""{sample}.clumped.fq.gz"")
    threads: 32
    resources:
        mem_mb=15000
    conda:
        ""../envs/conda_qc_reads.yml""
    log:
        ""logs/{sample}.run_bbmap_clumpify.log""
    benchmark:
        ""benchmarks/{sample}.run_bbmap_clumpify.tsv""
    group: ""bbtools""
    shell:
        """"""
            clumpify.sh -Xmx104g -eoom -da in1={input.raw_fwd} in2={input.raw_rev} out={output} dedupe optical 2&gt;&amp;1 | tee {log}
        """"""


When I run it locally using snakemake -p dryrun, it successfully builds the DAG.

rule get_reads:
    output: samples/196_SRF/fwd.gz, samples/196_SRF/rev.gz
    log: logs/196_SRF.get_reads.log
    jobid: 9
    benchmark: benchmarks/196_SRF.get_reads.tsv
    wildcards: sample=196_SRF


        wget -O samples/196_SRF/fwd.gz ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR276/ERR2762138/BNA_AAXOSW_4_1_C7T1BACXX.IND15_clean.fastq.gz;
        wget -O samples/196_SRF/rev.gz ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR276/ERR2762138/BNA_AAXOSW_4_2_C7T1BACXX.IND15_clean.fastq.gz;

rule run_bbmap_clumpify:
    input: samples/196_SRF/fwd.gz, samples/196_SRF/rev.gz
    output: 196_SRF.clumped.fq.gz
    log: logs/196_SRF.run_bbmap_clumpify.log
    jobid: 8
    benchmark: benchmarks/196_SRF.run_bbmap_clumpify.tsv
    wildcards: sample=196_SRF
    resources: mem_mb=15000


            clumpify.sh -Xmx104g -eoom -da in1=samples/196_SRF/fwd.gz in2=samples/196_SRF/rev.gz out=196_SRF.clumped.fq.gz dedupe optical 2&gt;&amp;1 | tee logs/196_SRF.run_bbmap_clumpify.log


I want to leverage Google Cloud to do this analysis, so I set up a GS bucket called temperton-lab-wec-store, then ran:

snakemake -p --kubernetes \
--use-conda -j 12 \
--default-remote-provider GS \
--default-remote-prefix temperton-lab-wec-store --dryrun


Building the DAG fails because the default remote prefix gets repeated over and over again in the path:

Building DAG of jobs...
MissingInputException in line 25 of snakemake/rules/qc_reads.smk:
Missing input files for rule run_bbmap_clumpify:
temperton-lab-wec-store/temperton-lab-wec-store/samples/temperton-lab-wec-store/temperton-lab-wec-store/temperton-lab-wec-store/temperton-lab-wec-store/temperton-lab-wec-store/temperton-lab-wec-store/196_SRF/rev.gz
temperton-lab-wec-store/temperton-lab-wec-store/samples/temperton-lab-wec-store/temperton-lab-wec-store/temperton-lab-wec-store/temperton-lab-wec-store/temperton-lab-wec-store/temperton-lab-wec-store/196_SRF/fwd.gz


I presume I am doing something wrong in either setting the name in the output in the rules, or setting the --default-remote-prefix flag. However, I can't find anything in the documentation that indicates how I might fix it. 

Any ideas?
",-1,-1,-1.0
57530914,Snakemake --forceall --dag results in mysterius Error: <stdin>: syntax error in line 1 near 'File' from Graphvis,"My attempts to construct DAG or rulegraph from RNA-seq pipeline using snakemake results in error message from graphviz. 'Error: : syntax error in line 1 near 'File'. 

The error can be corrected by commenting out two print commands with no visible syntax errors. I have tried converting the scripts from UTF-8 to Ascii in Notepad++. Graphviz seems to have issues with these two specific print statements because there are other print statements within the pipeline scripts. Even though the error is easily corrected, it's still annoying because I would like colleagues to be able to construct these diagrams for their publications without hassle, and the print statements inform them of what is happening in the workflow. My pipeline consists of a snakefile and multiple rule files, as well as a config file. If the offending line is commented out in the Snakefile, then graphviz takes issue with another line in a rule script.

#######Snakefile
!/usr/bin/env Python
import os
import glob
import re
from os.path import join
import argparse
from collections import defaultdict
import fastq2json
from itertools import chain, combinations
import shutil
from shutil import copyfile
#Testing for sequence file extension
directory = "".""
MainDir = os.path.abspath(directory) + ""/""
## build the dictionary with full path for each for sequence files
fastq=glob.glob(MainDir+'*/*'+'R[12]'+'**fastq.gz')
if len(fastq) &gt; 0 :
    print('Sequence file extensions have fastq')
    os.system('scripts/Move.sh')
    fastq2json.fastq_json(MainDir)
else :
    print('File extensions are good')
######Rule File
if not config[""GroupdFile""]:
    os.system('Rscript scripts/Table.R')
    print('No GroupdFile provided')


snakemake --forceall --rulegraph | dot -Tpdf > dag.pdf should result in an pdf output showing the snakemake workflow, but if the two lines aren't commented out it results in Error: : syntax error in line 1 near
",-1,-1,-1.0
57528586,Snakemake producing wildly incoherent error when dryrunning,"I've spent the past year working on a project executed on a SLURM-managed cluster. Now we want to make our results reproducible and to do so, we're porting it to snakemake. However, I am learning it from scratch and it's giving me a headache.

Below is my code:

# module load python/3.7
# python -m venv ./venv
# source ./venv/bin/activate
# pip install snakemake
# snakemake --version

configfile: ""config.yaml""
#localrules:
vcf=config[""vcf""],
ncbiFiles=config[""ncbiFiles""]
LC=config[""leafcutter""]


rule all:
    input: "".prepare_phen_table.chkpnt""

rule filter_vcf:
    input:
        expand(""{vcf}"", vcf=config[""vcf""]),
        expand(""{ncbiFiles}"", ncbiFiles=config[""ncbiFiles""])
    output:
        expand(""{ncbiFiles}/phg000830.v1.GTEx_WGS.genotype-calls-vcf.c1/GTExWGSGenotypeMatrixBiallelicOnly.HQ.vcf.gz"", ncbiFiles=config[""ncbiFiles""])
    shell:
        expand(""sbatch --wait --export=vcf={vcf},outdir=$PWD src/sqtl_mapping/primary/sh/00a_bcftools_filter.sh"", vcf=config[""vcf""])

rule index_vcf:
    input:
        expand(""{ncbiFiles}/phg000830.v1.GTEx_WGS.genotype-calls-vcf.c1/GTExWGSGenotypeMatrixBiallelicOnly.HQ.vcf.gz"", ncbiFiles=config[""ncbiFiles""])
    output:
        expand(""{ncbiFiles}/phg000830.v1.GTEx_WGS.genotype-calls-vcf.c1/GTExWGSGenotypeMatrixBiallelicOnly.vcf.gz.tbi"",ncbiFiles=config[""ncbiFiles""]),
        expand(""{ncbiFiles}/.index_vcf.chkpnt"",ncbiFiles=config[""ncbiFiles""])
    shell:
        expand(""sbatch --export=outdir=$PWD src/sqtl_mapping/primary/sh/00b_index_vcf.sh;""
        ""touch {ncbiFiles}/.index_vcf.chkpnt"",ncbiFiles=config[""ncbiFiles""])

rule junc_cluster:
    input:
        expand(""{ncbiFiles}/.index_vcf.chkpnt"", ncbiFiles=config[""ncbiFiles""])
    output:
        "".junc_cluster.chkpnt""
    shell:
        ""sbatch --wait src/sqtl_mapping/sh/01_junc_cluster.sh;""
        ""touch .junc_cluster.chkpnt""

rule intron_clustering:
    input:
        "".junc_cluster.chkpnt""
    output:
        "".intron_clustering.chkpnt""
    shell:
        expand(""sbatch --wait src/sqtl_mapping/sh/02_intronclustering.sh {LC};""
        ""touch .intron_clustering.chkpnt;""
        ""cd intronclustering/"", LC=config[""leafcutter""])

rule prepare_phen_table:
    input:
        LC,
        "".intron_clustering.chkpnt""
    output:
        "".prepare_phen_table.chkpnt""
    shell:
        expand(""sbatch --wait src/sqtl_mapping/sh/03_prepare_phen_table.sh {LC};""
        ""touch .prepare_phen_table.chkpnt"",LC=config[""leafcutter""])


Please assume the config.yaml is fine. When I call snakemake -n, I get the following error:

(venv) [aseyedi2@jhu.edu@rmccoy22-dev neand_sQTL]$ snakemake -n
Building DAG of jobs...
Job counts:
    count   jobs
    1   all
    1   index_vcf
    1   intron_clustering
    1   junc_cluster
    1   prepare_phen_table
    5

[Fri Aug 16 12:31:53 2019]
rule index_vcf:
    input: /scratch/groups/rmccoy22/Ne_sQTL/files/phg000830.v1.GTEx_WGS.genotype-calls-vcf.c1/GTExWGSGenotypeMatrixBiallelicOnly.HQ.vcf.gz
    output: /scratch/groups/rmccoy22/Ne_sQTL/files/phg000830.v1.GTEx_WGS.genotype-calls-vcf.c1/GTExWGSGenotypeMatrixBiallelicOnly.vcf.gz.tbi, /scratch/groups/rmccoy22/Ne_sQTL/files/.index_vcf.chkpnt
    jobid: 4

Traceback (most recent call last):
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/__init__.py"", line 547, in snakemake
    export_cwl=export_cwl)
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/workflow.py"", line 674, in execute
    success = scheduler.schedule()
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/scheduler.py"", line 278, in schedule
    self.run(job)
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/scheduler.py"", line 294, in run
    error_callback=self._error)
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/executors.py"", line 75, in run
    self._run(job)
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/executors.py"", line 86, in _run
    self.printjob(job)
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/executors.py"", line 92, in printjob
    job.log_info(skip_dynamic=True)
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/jobs.py"", line 825, in log_info
    logger.shellcmd(self.shellcmd, indent=indent)
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/jobs.py"", line 323, in shellcmd
    self.rule.shellcmd else None)
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/jobs.py"", line 732, in format_wildcards
    return format(string, **_variables)
  File ""/scratch/groups/rmccoy22/aseyedi2/neand_sQTL/venv/lib/python3.7/site-packages/snakemake/utils.py"", line 378, in format
    return fmt.format(_pattern, *args, **variables)
  File ""/software/apps/python/3.7/lib/python3.7/string.py"", line 186, in format
    return self.vformat(format_string, args, kwargs)
  File ""/software/apps/python/3.7/lib/python3.7/string.py"", line 190, in vformat
    result, _ = self._vformat(format_string, args, kwargs, used_args, 2)
  File ""/software/apps/python/3.7/lib/python3.7/string.py"", line 200, in _vformat
    self.parse(format_string):
  File ""/software/apps/python/3.7/lib/python3.7/string.py"", line 284, in parse
    return _string.formatter_parser(format_string)
TypeError: expected str, got list


I have no idea what to make of it other than possibly that there is some sort of bug or incompatibility with what I'm trying to do. 

Thank you for any help you can offer me.
",1,-1,-1.0
57493701,ImproperOutputException in snakemake 5.5.4,"I am new to snakemake and recently I encountered a new error which I get only from very recent version of snakemake. This my snakemake rule

rule fastqc_raw:
    input:
        expand(directory(""{fastqc_dir}/{samples}_fastqc/""),fastqc_dir = FASTQC_DIR, samples = SAMPLES_wo_extension)

rule do_fastqc_raw:
    input:
        expand(""{fastq_dir}/{{samples}}.fastq.gz"", fastq_dir =  inputDir)
    output:
        expand(directory(""{fastqc_dir}/{{samples}}_fastqc/""),fastqc_dir = FASTQC_DIR)
    log:
        expand(""{fastqc_dir}/{{samples}}.log"", fastqc_dir = FASTQC_DIR)
    threads:
        10
    message:
        ""performing fastQC of the sample : {wildcards.samples}""
    shell:
        """"""mkdir -p {output} &amp;&amp; fastqc -q -t {threads} --outdir {output} --contaminants /home/Contaminants/idot.txt {input} 2&gt; {log}""""""


I get the following error which i didn't receive when I use snakemake version lower than 5.2.0


  ImproperOutputException in line 10 of /home/FASTQC.snakefile:
  Outputs of incorrect type (directories when expecting files or vice versa). Output directories must be flagged with directory(). for rule do_fastqc_raw:

",-1,-1,-1.0
57401672,Using input function with remote files in snakemake,"I want to use a function to read inputs file paths from a dataframe and send them to my snakemake rule. I also have a helper function to select the remote from which to pull the files. 

from snakemake.remote.GS import RemoteProvider as GSRemoteProvider
from snakemake.remote.SFTP import RemoteProvider as SFTPRemoteProvider
from os.path import join
import pandas as pd

configfile: ""config.yaml""
units = pd.read_csv(config[""units""]).set_index([""library"", ""unit""], drop=False)
TMP= join('data', 'tmp')


def access_remote(local_path):
    """""" Connnects to remote as defined in config file""""""
    provider = config['provider']
    if provider == 'GS':
        GS = GSRemoteProvider()
        remote_path = GS.remote(join(""gs://"" + config['bucket'], local_path))
    elif provider == 'SFTP':
        SFTP = SFTPRemoteProvider(
            username=config['user'],
            private_key=config['ssh_key']
        )
        remote_path = SFTP.remote(
            config['host'] + "":22"" + join(base_path, local_path)
        )
    else: 
        remote_path = local_path
    return remote_path


def get_fastqs(wc):
    """"""
    Get fastq files (units) of a particular library - sample 
    combination from the unit sheet.
    """"""
    fqs = units.loc[
        (units.library == wc.library) &amp; 
        (units.libtype == wc.libtype), 
        ""fq1""
    ]
    return {
      ""r1"": list(map(access_remote, fqs.fq1.values)),
    }

# Combine all fastq files from the same sample / library type combination
rule combine_units:
  input: unpack(get_fastqs)
  output:
    r1 = join(TMP, ""reads"", ""{library}_{libtype}.end1.fq.gz"")
  threads: 12
  run:
    shell(""cat {i1} &gt; {o1}"".format(i1=input['r1'], o1=output['r1']))



My config file contains the bucket name and provider, which are passed to the function. This works as expected when running simply snakemake.

However, I would like to use the kubernetes integration, which requires passing the provider and bucket name in the command line. But when I run:

snakemake -n --kubernetes --default-remote-provider GS --default-remote-prefix bucket-name

I get this error:

ERROR :: MissingInputException in line 19 of Snakefile:
Missing input files for rule combine_units:
bucket-name/['bucket-name/lib1-unit1.end1.fastq.gz', 'bucket-name/lib1-unit2.end1.fastq.gz', 'bucket-name/lib1-unit3.end1.fastq.gz']


The bucket is applied twice (once mapped correctly to each element, and once before the whole list (which gets converted to a string). Did I miss something ? Is there a good way to work around this ?
",-1,1,-1.0
57272501,Snakemake: 'Missing input files' due to wrong wildcard expansion,"I am new to Snakemake and I want to write a very simple Snakefile with a rule that processes each input file separately to an output file, but somehow my wildcards aren't interpreted correctly.

I have set up a minimal, reproducible example environment in Ubuntu 18.04 with the input files ""test/test1.txt"", ""test/test2.txt"", and a Snakefile. (snakemake version 5.5.4)

Snakefile:

ins = glob_wildcards(""test/{f}.txt"")

rule all:
  input: expand(""out/{f}.txt"", f=ins)

rule test:
  input: ""test/{f}.txt""
  output: ""out/{f}.txt""
  shell: ""touch {output}""


This Snakefile throws the following error while building the DAG of jobs:

Missing input files for rule test:
test/['test1', 'test2'].txt


Any ideas how to fix this error?
",-1,-1,-1.0
60203852,Snakemake use wildcards in path of python function,"I have a simple function that read a file (one line) and get the first element after split.

def get_wc(wc):
    file = open(wc,""r"")
    normalization_value = file.readline().split(' ')[0]
    return(normalization_value)


I use this function in a rule snakemake. 

rule compute_fc:
input:
    ""data/annotated_clones/{cdna}_paste_{lib}.annotated.bed""
output:
    ""data/fold_change/{cdna}_paste_{lib}.fc.bed""
params:
    size_cdna=get_wc(""data/wc_bed/cdna/{cdna}.wc.txt""),
    size_lib=get_wc(""data/wc_bed/library/{lib}.wc.txt"")
shell:'''
    awk -v cdna1={params.size_cdna} -v inp={params.size_lib} -v addon=1 -v FS='\t' -v OFS='\t' '/^chr/{{ratio=(($7+addon)/($8+addon))*(inp/cdna1);print $0,ratio}}' {input} &gt; {output}
'''


I'm trying to get the value that get_wc function return and use it as params in snakemake rule.

But Snakemake don't get the path with the wildcards so it try to get the path with the wildcards and obviously it don't works.

[Errno 2] No such file or directory: 'data/wc_bed/cdna/{cdna}.wc.txt'

",-1,-1,-1.0
60310853,running metabat2 and checkM with snakemake,"Hi I have been trying to put metabat2 and checkm in my pipeline to see which bacteria in my sample but I keep running in to an error with snakema. My snakemake code is 

rule all:
    input:
        [f""nanoplot_out/"" for sample in samples],
    [f""zipped/zipped.gz"" for sample in samples],    
    [f""filtered/nanofilt_out.gz"" for sample in samples],
    [f""unzipped/read.fastq"" for sample in samples],
    [f""assembled/"" for sample in samples],
    [f""nanopolish/assembly.fasta"" for sample in samples],
[f""medaka_consensus/"" for sample in samples],
    [f""input/consensus.fasta"" for sample in samples],
    [f""output_antismash/"" for sample in samples],
    [f""metabat2/bin"" for sample in samples],
    [f""metabat2/CheckM.txt"" for sample in samples]
rule metabat2:
    input:
        ""medaka_consensus/consensus.fasta""
    output:
        directory(""metabat2/"")
    conda:
        ""envs/metabat2.yaml""
    shell:
        ""metabat2 -i {input} -o {output} -v""

rule checkM:
    input:
        ""metabat2/""
    output:
        ""metabat2/CheckM.txt""
    conda:
        ""envs/metabat2.yaml""
    shell:
        ""checkm lineage_wf -f {output} -t 30 -x fa {input}""


and my error message is MissingInputException in line 4 of /home/ec2-user/snakemake_JHB-2-14-20_uncor_polish_anti5_meta/Snakefile:
Missing input files for rule all:
metabat2/bin

Is there anyone that could help me to make it work?
",1,-1,-1.0
60334852,snakemake : rule's input with different pattern,"I am new to snakemake and would like to use the following rule :

input_path = config[""PATH""]
samples = pd.read_csv(config[""METAFILE""], sep = '\t', header = 0)['sample']

rule getPaired:
        output:
            fwd = temp(tmp_path + ""/reads/{sample}_fwd.fastq.gz""),
            rev = temp(tmp_path + ""/reads/{sample}_rev.fastq.gz"")
        params:
            input_path = input_path
        run:
            shell(""scp -i {params.input_path}/{wildcards.sample}_*1*.f*q.gz {output.fwd}""),
            shell(""scp -i {params.input_path}/{wildcards.sample}_*2*.f*q.gz {output.rev}"")


Input files have different patterns :


{sampleID}_R[1-2]_001.fq.gz (for example : 2160_J15_S480_R1_001.fastq.gz)
{sampleID}_[1-2].fq.gz (for example : SRX000001_1.fq.gz)


The getPaired rule works for input like {sample}_[1-2].fq.gz but not for the second pattern.

What am I doing wrong ?
",-1,-1,-1.0
60410225,"Snakemake pipeline to run one instance of rule 1 to generate multiple files, then one instance of rule 2 per file","I want to use snakemake to run one instance of a first rule which takes an input file and creates multiple output files. I then want to take each output file as inputs for a second rule. I only want to run one instance of the first rule to avoid unecessary duplicating of this rule as only one should be needed to create the outputs.

Here's an oversimplified example:

Say I have an input file, samplenames.txt containing the following:

sample1
sample2


I want to take the filenames from this file and make a file with the same name for each. I then want to make a copy of each with the following final output files:

sample1_copy
sample2_copy


My Snakefile contains the following:

SAMPLES = [1,2]

rule all:
    input:
        expand(
            ""sample{sample}_copy"",
            sample=SAMPLES
        )

rule fetch_filenames:
    input:
        ""samplenames.txt""
    output:
        ""sample{sample}""
    shell:
        ""while IFS= read -r line; do touch $line; done &lt; {input}""

rule copy_files:
    input:
        expand(
            ""sample{sample}"", 
            sample=SAMPLES
        )
    output:
        expand(
            ""sample{sample}_copy"", 
            sample=SAMPLES
        )
    shell:
        ""touch {output}""


This does the job, but two instances of the first rule are completed when only one is needed. When I apply this to many more files in a more complex workflow it results in many unnecessary instances. Is there a way of running of only running one instance of the first rule?

I have tried the following for the first rule:

rule fetch_filenames:
    input:
        ""samplenames.txt""
    output:
        ""sample1""
    shell:
        ""while IFS= read -r line; do touch $line; done &lt; {input}""


However this results in the following error:
""Missing input files for rule copy_files:
sample2""

I am sad. Any help make me many happys.
",-1,1,-1.0
60507650,Snakemake : SFTP on local machine,"I am connected with ssh on a remote server from my local machine.
I run my Snakemake on the remote server.
I would like to use as input of a rule, a file that is on my local machine.
Of course, since I run my Snakemake on the server, the server become the local machine and the local machine the remote one (for Snakemake).

from snakemake.remote.SFTP import RemoteProvider

# I am not sure about the private key, is it the one I have on the server ?
# I have the same result with or without private_key anyway

# SFTP = RemoteProvider(port=22, username=""myusername"", private_key=""/path/to/.ssh/id_rsa"")
SFTP = RemoteProvider(port=22, username=""myusername"")

configfile : ""config.json""

localrules: copyBclLocalToCluster

rule all:
    input:
        ""copycluster.txt""

rule copyBclLocalToCluster:
    input:
        SFTP.remote(""adress:path/to/filelocal.txt"")
    output:
        ""copycluster.txt""
    shell:
        ""scp {input} {output}""

-----------------------------------------
Building DAG of jobs...
MissingInputException in line 26 of /path/to/Snakefile:
Missing input files for rule copyBclLocalToCluster:
adress:path/to/filelocal.txt



  https://snakemake.readthedocs.io/en/stable/snakefiles/remote_files.html
  The remote file addresses used must be specified with the host (domain or IP address) and the absolute path to the file on the remote server. A port may be specified if the SSH daemon on the server is listening on a port other than 22, in either the RemoteProvider or in each instance of remote():


The doc says that the port shouldn't be port 22, but why ? I really would like to use it since I don't know how to configure another port and I'm not even sure to have the rights to do it.

Is it really a port issue ? Or I just don't understand how to use SFTP with Snakemake. 

What is the best way to use a file on my local machine as input of my snakemake ?



EDIT

It is not the port the problem, I don't even need to specify it because it is port 22.
I tried to specify the good ssh private key :

SFTP = RemoteProvider(port=22, username=""myusername"", private_key=""/path/to/.ssh/id_rsa"")
-----------------------------
Building DAG of jobs...
MissingInputException in line 26 of /path/to/Snakefile:
Missing input files for rule copyBclLocalToCluster:
adress:path/to/filelocal.txt


If I try sftp myusername@adress:path/to/filelocal.txt . on my console on the server it works fine.  

Why it doesn't work inside snakemake ?



EDIT

When I try to use my password instead of ssh-key in remoteProvider I have the same error.

SFTP = RemoteProvider(port=22, username=""myusername"", password=""mypassword"")
--------------------------------
Building DAG of jobs...
MissingInputException in line 26 of /path/to/Snakefile:
Missing input files for rule copyBclLocalToCluster:
adress:path/to/filelocal.txt


I am sure the adress, username, password, ssh-key are correct and file exist, I can do it outside snakemake it works fine.



EDIT

Since RemoteProvider uses pysftp, I tried to copy the same file with pysftp in a python script.

import pysftp
with pysftp.Connection(adress, 
                       username=""myusername"",
                       private_key_pass=""/path/to/.ssh/id_rsa"") as sftp:
    sftp.get(path/to/filelocal.txt, /path/on/cluster/fileCOPY.txt)


It works fine, so the problem come from my Snakefile for sure.



EDIT

RemoteProvider also need ftputil, I tried ftputil in a python script.

import ftputil
with ftputil.FTPHost(""adress"", ""myusername"", ""mypassword"") as ftp_host:
    print(getcwd())
    ftp_host.download(remote_path, local_path)
----------------------------------------------
Traceback (most recent call last):
  File ""/work/username/miniconda3/envs/RNAseq_snakemake/lib/python3.6/site-packages/ftputil/host.py"", line 129, in _make_session
    return factory(*args, **kwargs)
  File ""/work/username/miniconda3/envs/RNAseq_snakemake/lib/python3.6/ftplib.py"", line 117, in __init__
    self.connect(host)
  File ""/work/username/miniconda3/envs/RNAseq_snakemake/lib/python3.6/ftplib.py"", line 152, in connect
    source_address=self.source_address)
  File ""/work/username/miniconda3/envs/RNAseq_snakemake/lib/python3.6/socket.py"", line 724, in create_connection
    raise err
  File ""/work/username/miniconda3/envs/RNAseq_snakemake/lib/python3.6/socket.py"", line 713, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""sftptest.py"", line 16, in &lt;module&gt;
    with ftputil.FTPHost(""adress"", ""myusername"", ""mypassword"") as ftp_host:
  File ""/work/username/miniconda3/envs/RNAseq_snakemake/lib/python3.6/site-packages/ftputil/host.py"", line 69, in __init__
    self._session = self._make_session()
  File ""/work/username/miniconda3/envs/RNAseq_snakemake/lib/python3.6/site-packages/ftputil/host.py"", line 129, in _make_session
    return factory(*args, **kwargs)
  File ""/work/username/miniconda3/envs/RNAseq_snakemake/lib/python3.6/site-packages/ftputil/error.py"", line 146, in __exit__
    raise FTPOSError(*exc_value.args, original_exception=exc_value)
ftputil.error.FTPOSError: [Errno 111] Connection refused
Debugging info: ftputil 3.2, Python 3.6.7 (linux)


Could it be a problem ? But I don't have this kind of error in snakemake, just missing file error. I don't understand why ftputil is not working. 
",-1,-1,-1.0
60508384,MissingOutputException in snakemake,"I'm trying to run a peak calling tool within a conda environment using snakemake.

The script looks as such (I only added the rows connect to the problem):

rule all:
    input:
        expand('{project}/{organism}/{mapper}/seacr/{pattern}.auc.threshold.bed', pattern = PATTERN, sample = IDS, organism = config['org'], project = config['project'], mapper = config['mapper']) # SEACR - run the peak calling 

rule seacr_run:
    input:
        IP = '{project}/{organism}/{mapper}/seacr/IP_{PATTERN}.bedgraph',
        IgG = '{project}/{organism}/{mapper}/seacr/IgG_{PATTERN}.bedgraph',
    output:
        bed1 = '{project}/{organism}/{mapper}/seacr/{PATTERN}.auc.threshold.bed',
    shell:
        '''
        bash /fs/home/yeroslaviz/SEACR/SEACR_1.3.sh {input.IP} 0.01 non stringent {output.bed1}
        '''


When running the -nps dryrun of the snamemake command I get the correct command printed to STDOUT

&gt; snakemake -nps /fs/pool/pool-bcfngs/scripts/P193.ChipSeq.Snakemake -j 100
...
Building DAG of jobs...
Job counts:
        count   jobs
        1       all
        1       seacr_run
        2

[Tue Mar  3 13:56:19 2020]
rule seacr_run:
    input: P193/Mmu.GrCm38/bowtie2/seacr/IP_H3K4m3.bedgraph, P193/Mmu.GrCm38/bowtie2/seacr/IgG_H3K4m3.bedgraph
    output: P193/Mmu.GrCm38/bowtie2/seacr/H3K4m3.auc.threshold.bed
    jobid: 22
    wildcards: project=P193, organism=Mmu.GrCm38, mapper=bowtie2, PATTERN=H3K4m3

                bash /fs/home/yeroslaviz/SEACR/SEACR_1.3.sh P193/Mmu.GrCm38/bowtie2/seacr/IP_H3K4m3.bedgraph 0.01 non stringent P193/Mmu.GrCm38/bowtie2/seacr/H3K4m3.auc.threshold.bed

[Tue Mar  3 13:56:19 2020]
localrule all:
...

Job counts:
        count   jobs
        1       all
        1       seacr_run
        2
This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.



When running the command above in the command line the tool works without problems. But hwhen I try to run it within the snakemake workflow I get the following error:

Waiting at most 5 seconds for missing files.
MissingOutputException in line 67 of /fs/pool/pool-bcfngs/scripts/P193.ChipSeq.Snakemake:
Missing files after 5 seconds:
P193/Mmu.GrCm38/bowtie2/seacr/H3K4m3.auc.threshold.bed
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message


Can anyone explain what is happening? 

Thanks
",-1,-1,-1.0
60526646,"Snakemake - Problem trying to use global_wildcards (TypeError: expected str, got list)","I'm a newbie using Snakemake and not an expert in Python neither so the answer might be quite obvious. Everything in my workflow worked fine in my tests until I tried to use glob_wildcards in order to turn all of my fastq.gz files from one directory (FASTQDIR) into fastqc files.

The samples names in the SAMPLES list are okay but I have an error saying that a string is expected instead of a list (I assume this is my SAMPLES list) and I don't really know where to act in my Snakefile in order to correct it. I understand that it is surely linked to my use of glob_wildcards but I don't understand where is the problem. Do you have an idea of how I can fix it ?

Here is my Snakefile code :

FASTQDIR = ""/fastq/files/directory/""
WDIR = ""/my/working/directory/""
SAMPLES, = glob_wildcards(FASTQDIR + ""{sample}.fastq.gz"")

rule all:
    input:
        expand(WDIR + ""Fastqc/{sample}_fastqc.html"", sample=SAMPLES),
        expand(WDIR + ""Fastqc/{sample}_fastqc.zip"", sample=SAMPLES)

#Generates fastqc file for the sample fastq.gz file in the Fastqc directory
rule fastqc_generate_qc:
    input:
        expand(FASTQDIR + ""{sample}.fastq.gz"", sample=SAMPLES)
    output:
        expand(WDIR + ""Fastqc/{sample}_fastqc.html"", sample=SAMPLES),
        expand(WDIR + ""Fastqc/{sample}_fastqc.zip"", sample=SAMPLES)
    shell:
        ""fastqc --outdir Fastqc/ {input}""


Here is the entire Traceback :

Traceback (most recent call last):
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/__init__.py"", line 420, in snakemake
    force_use_threads=use_threads)
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/workflow.py"", line 480, in execute
    success = scheduler.schedule()
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/scheduler.py"", line 215, in schedule
    self.run(job)
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/scheduler.py"", line 229, in run
    error_callback=self._error)
  File ""/home/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/executors.py"", line 59, in run
    self._run(job)
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/executors.py"", line 120, in _run
    super()._run(job)
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/executors.py"", line 66, in _run
    self.printjob(job)
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/executors.py"", line 85, in printjob
    msg=job.message,
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/jobs.py"", line 175, in message
    self.rule.message else None)
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/jobs.py"", line 542, in format_wildcards
    return format(string, **_variables)
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/site-packages/snakemake/utils.py"", line 259, in format
    return fmt.format(_pattern, *args, **variables)
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/string.py"", line 187, in format
    return self.vformat(format_string, args, kwargs)
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/string.py"", line 191, in vformat
    result, _ = self._vformat(format_string, args, kwargs, used_args, 2)
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/string.py"", line 201, in _vformat
    self.parse(format_string):
  File ""/home/miniconda3/envs/snakemake-tutorial/lib/python3.5/string.py"", line 285, in parse
    return _string.formatter_parser(format_string)
TypeError: expected str, got list


Thank you in advance for your help
",1,-1,-1.0
60618609,Snakemake - Tibanna config support,"I trying to run snakemake --tibanna to deploy Snakemake on AWS using the ""Unicorn"" Step Functions Tibanna creates.

I can't seem to find a way to change the different arguments Tibanna accepts like which subnet, AZ or Security Group will be used for the actual EC2 instance deployed.

Argument example (when running Tibanna without Snakemake):
https://github.com/4dn-dcic/tibanna/blob/master/test_json/unicorn/shelltest4.json#L32

Thanks!
",-1,-1,-1.0
60740907,snakemake use wildcard input/output in python function,"I Have a simple python function that take an input and create an output 

def enlarge_overlapping_region(input,output):
    fi=open(input,""r"")
    fo=open(output,""w"")
    df = pd.read_table(fi, delimiter='\t',header=None,names=[""chr"",""start"",""end"",""point"",""score"",""strand"",""cdna_count"",""lib_count"",""region_type"",""region_id""])
    df1 = (df.groupby('region_id', as_index=False)
         .agg({'chr':'first', 'start':'min', 'end':'max','region_type':'first'})
         [['chr','start','end','region_type','region_id']])
    df1 = df1[df1.region_id != "".""]
    df1.to_csv(fo,index=False, sep='\t')

    return(df1)


I call this function in a rule snakemake. But I cannot access to the file I don't know why.

I tried something like that : 

rule get_enlarged_dhs:
    input:
        ""data/annotated_clones/{cdna}_paste_{lib}.annotated.bed""
    output:
        ""data/enlarged_coordinates/{cdna}/{cdna}_paste_{lib}.enlarged_dhs.bed""
    run:
        lambda wildcards: enlarge_overlapping_region(f""{wildcards.input}"",f""{wildcards.output}"")


I got this error :

Missing files after 5 seconds:
data/enlarged_coordinates/pPGK_rep1/pPGK_rep1_paste_pPGK_input.enlarged_dhs.bed
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wa
it.


If I put directly the python code into the rule like thath :

rule get_enlarged_dhs:
    input:
        ""data/annotated_clones/{cdna}_paste_{lib}.annotated.bed""
    output:
        ""data/enlarged_coordinates/{cdna}/{cdna}_paste_{lib}.enlarged_dhs.bed""
    run:
        fi=open(input,""r"")
        fo=open(output,""w"")
        df = pd.read_table(fi, delimiter='\t',header=None,names=[""chr"",""start"",""end"",""point"",""score"",""strand"",""cdna_count"",""lib_count"",""region_type"",""region_id""])
        df1 = (df.groupby('region_id', as_index=False)
             .agg({'chr':'first', 'start':'min', 'end':'max','region_type':'first'})
             [['chr','start','end','region_type','region_id']])
        df1 = df1[df1.region_id != "".""]
        df1.to_csv(fo,index=False, sep='\t')


I got this error :

expected str, bytes or os.PathLike object, not InputFiles

",-1,-1,-1.0
60792649,Snakemake variable number of files,"I'm in a situation, where I would like to scatter my workflow into a variable number of chunks, which I don't know beforehand. Maybe it is easiest to explain the problem by being concrete:

Someone has handed me FASTQ files demultiplexed using bcl2fastq with the no-lane-splitting option. I would like to split these files according to lane, map each lane individually, and then finally gather everything again. However, I don't know the number of lanes beforehand.

Ideally, I would like a solution like this,

rule split_fastq_file: (...)  # results in N FASTQ files
rule map_fastq_file: (...)  # do this N times
rule merge_bam_files: (...)  # merge the N BAM files


but I am not sure this is possbile. The expand function requires me to know the number of lanes, and can't see how it would be possible to use wildcards for this, either.

I should say that I am rather new to Snakemake, and that I may have complete misunderstood how Snakemake works. It has taken me some time to get used to think about things ""upside-down"" by focusing on output files and then working backwards.
",-1,-1,-1.0
60839282,Cannot execute bash script in Snakemake,"This is my first question on Snakemake as the existing resources and forums helped me immensely to solve most of my problems. 

The problem I currently had is that I cannot make the bash script execute in Snakemake, though I was able to execute the same bash script just fine on the command line

Here is how I run the bash script on the command line successfully

bash scripts/genome_coverage.sh -m results/mapped_reads -o final_out.txt


But when I run it in Snakemake, I get the error. 

Here is the rule in the Snakefile that corresponds to the executing bash script

rule genome_coverage:
    input:
        ""results/mapped_reads""
    output:
        ""genome_coverage.txt""
    script:
        ""scripts/genome_coverage.sh -m {input} -o {output}""


And this is the error I am getting. I don't know what I am doing wrong here

Error in rule genome_coverage:
    jobid: 16
    output: genome_coverage.txt
RuleException:
NameError in line 153 of /Illumina-mRNA/Snakefile:
The name 'input' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}

",1,-1,-1.0
60931328,snakemake - replacing wildcards in input directive by anonymous function,"I am writing a snakemake that will run a bioinformatics pipeline for several input samples. These input files (two for each analysis, one with the partial string match R1 and the second with the partial string match R2) start with a pattern and end with the extension .fastq.gz. Eventually I want to perform multiple operations, though, for this example I just want to align the fastq reads against a reference genome using bwa mem. So for this example my input file is NIPT-N2002394-LL_S19_R1_001.fastq.gz and I want to generate NIPT-N2002394-LL.bam (see code below specifying the directories where input and output are).

My config.yaml file looks like so:

# Run_ID
run: ""200311_A00154_0454_AHHHKMDRXX""

# Base directory: the analysis directory from which I will fetch the samples
bd: ""/nexusb/nipt/""


# Define the prefix
# will be used to subset the folders in bd
prefix: ""NIPT""

# Reference:
ref: ""/nexus/bhinckel/19/ONT_projects/PGD_breakpoint/ref_hg19_local/hg19_chr1-y.fasta""


And below is my snakefile

import os
import re
#############
# config file
#############
configfile: ""config.yaml""


#######################################
# Parsing variables from config.yaml
#######################################
RUN = config['run']

BD = config['bd']

PREFIX = config['prefix']

FQDIR = f'/nexusb/Novaseq/{RUN}/Unaligned/'

BASEDIR = BD + RUN
SAMPLES = [sample for sample in os.listdir(BASEDIR) if sample.startswith(PREFIX)]
# explanation: in BASEDIR I have multiple subdirectories. The names of the subdirectories starting with PREFIX will be the name of the elements I want to have in the list SAMPLES, which eventually shall be my {sample} wildcard

#############
# RULES
#############
rule all:
    input:
        expand(""aligned/{sample}.bam"", sample = SAMPLES)


rule bwa_map:
    input:
        REF = config['ref'],
        R1 = FQDIR + ""{sample}_S{s}_R1_001.fastq.gz"",
        R2 = FQDIR + ""{sample}_S{s}_R2_001.fastq.gz""
    output:
        ""aligned/{sample}.bam""
    shell:
        ""bwa mem {input.REF} {input.R1} {input.R2}| samtools view -Sb - &gt; {output}""



But I am getting:

Building DAG of jobs...
WildcardError in line 55 of /nexusb/nipt/200311_A00154_0454_AHHHKMDRXX/testMetrics/snakemake/Snakefile:
Wildcards in input files cannot be determined from output files:
's'


When calling snakemake -np

I believe my error lies in the definitions of R1 and R2 in the input directive. I find it puzzling because according to the official documentation snakemake should interpret any wildcard as the regex .+. But it is not doing that for sample NIPT-PearlPPlasma-05-PPx, whose R1 and R2 should be NIPT-PearlPPlasma-05-PPx_S5_R1_001.fastq.gz and NIPT-PearlPPlasma-05-PPx_S5_R2_001.fastq.gz, respectively.
",-1,-1,-1.0
60980819,Snakemake - Missing MissingInputException in line 20: Missing input files for rule stringt:,"I am unsuccessful in running this small snakemake program for StringTie; the dry-run gives an error of MissingInputException for the ""rule stringt"", I am unable to understand the issue here since the same directory structures works fine for a different snakemake program. 

I have already generated the bam files using ""hisat2"" and is located in the directory : ""/alternate_splice/bam_out/"" which is stored as ""bamdir"". 

The snakemake should be able to locate the input file since the names and location are appropriate, however it throws an error every time. 

I did look at the pervious snakemake related questions, however could not solve this issue. 
If anyone can help me out here, it would be great!


  There are 4 samples: for the wildcards, it takes the list from the
  directory which has the fastq files


~/alternate_splice/expdata$ ls -ltr
 -rw-r--r-- 1 shivani domain^users 1306438351 Jan  2 09:46 TL-19-DA4299_T_RSQ1_2.fastq.gz
 -rw-r--r-- 1 shivani domain^users 1185743896 Jan  2 09:46 TL-19-DA4299_T_RSQ1_1.fastq.gz
&gt; 
 -rw-r--r-- 1 shivani domain^users 1896352262 Jan  9 08:49 TL-20-24D57D_T_RSQ1_2.fastq.gz
 -rw-r--r-- 1 shivani domain^users 1730191383 Jan  9 08:49 TL-20-24D57D_T_RSQ1_1.fastq.gz
&gt; 
 -rwxr-xr-x 1 shivani domain^users 3215901253 Mar 25 10:28 BREAST_817_N1_RNA_REP1_1.fastq.gz
 -rwxr-xr-x 1 shivani domain^users 3396212102 Mar 25 10:36 BREAST_817_N1_RNA_REP1_2.fastq.gz
&gt; 
 -rwxr-xr-x 1 shivani domain^users 3633768287 Mar 25 10:45 BREAST_792_N1_RNA_REP1_1.fastq.gz
 -rwxr-xr-x 1 shivani domain^users 3932340643 Mar 25 10:54 BREAST_792_N1_RNA_REP1_2.fastq.gz


The snakefile here:
""bamdir"" = directory to bam output
""geneGTF"" = locating GFT file 

Two rule: 1. stringt 2. merge

 (SAMPLE,)=glob_wildcards(""/home/shivani/alternate_splice/expdata/{sample}_1.fastq.gz"")
#(SAMPLE,)=glob_wildcards(""/home/shivani/alternate_splice/bam_out/{sample}.bam"")
 bamdir = ""/home/shivani/alternate_splice/bam_out/""
 refere_genome = ""/home/shivani/ccb1_shivani/hisat2_trans_bam/hisat2_index/""
 geneGTF = ""/home/shivani/stringtie_run/Homo_sapiens.GRCh37.87.gtf""

rule all:
        input:
               expand(bamdir+""{sample}_transcript.gft"",sample=SAMPLE),
               expand(bamdir+""{sample}_abundance.tsv"",sample=SAMPLE),
               expand(bamdir+""{sample}_coverage.gtf"",sample=SAMPLE)
               expand(bamdir+""{sample}_stringtie_merge.gtf"",sample=SAMPLE)


rule stringt:
        input:
                bm=bamdir+""{sample}.bam"",
                gtf=geneGTF,
                tname=""{sample}""
        output:
                tscripts=bamdir+""{sample}_transcript.gft"",
                abund=bamdir+""{sample}_abundance.tsv"",
                cov=bamdir+""{sample}_coverage.gtf""
        shell:
                """"""stringtie -p 4 -e -c 3.5 -G {input.gtf} -o {output.tscripts} -A {output.abund} -C {output.cov} -l {sample}{input.bm}""""""


rule merge:
       input:
               trnsgtf=bamdir+""{sample}_transcript.gft"",
               ggtf=geneGTF
       output :bamdir+""{sample}_stringtie_merge.gtf""
       shell:""stringtie -p 4 --merge -G {input.ggtf} -o {output} {input.trnsgtf}""


The dry run for this snakemake: instead of using ""Snakefile"" as designated name, I have used ""stringtie_trans""


  snakemake -n -r -s stringtie_trans


The output is as follows:


  MissingInputException in line 20 of /home/shivani/alternate_splice/stringtie_trans ::::
  Missing input files :::: for rule stringt: BREAST_792_N1_RNA_REP1

",1,-1,-1.0
61084623,submitting a snakemake job to the cluster from within a ('correct') conda environment,"I am writing a snakemake file that shall perform multiple operations on multiple samples. After I validated the workflow running on my local computer, I am now working on running the workflow on a cluster.

My first two rules are independent from one another, the first uses fastqc and the other bwa mem

These two rules look like (at this point I am only calling the workflow on a single SAMPLE = 'NIPT-PearlPPlasma-03-PPx_S3downSample'):

rule fastQC:
    input:
        R1 = FQDIR + ""{sample}_R1_001.fastq.gz"",
        R2 = FQDIR + ""{sample}_R2_001.fastq.gz""
    output:
        directory(""fastQC/{sample}"")
    conda:
        ""envs/NIPTlibPrep.yaml""
    log:
        ""logs/fastQC/{sample}.log"" # log was giving an error when running at the command line
    shell:
        # 2&gt; {log} at the end of the command removed
        # See wrapper at https:/snakemake-wrappers.readthedocs.io/en/stable/wrappers/fastqc.html
        ""mkdir -p fastQC/{wildcards.sample} | fastqc --outdir fastQC/{wildcards.sample} -f fastq {input.R1} {input.R2}""


rule bwa_map: 
    input:
        R1 = FQDIR + ""{sample}_R1_001.fastq.gz"",
        R2 = FQDIR + ""{sample}_R2_001.fastq.gz"",
        REF = config['ref']    
    output:
        # wrap output in temp
        ""aligned/{sample}.bam""
    log:
        ""logs/bwa_mem/{sample}.log"" 
    conda:
        ""envs/NIPTlibPrep.yaml""
    shell:
        ""bwa mem {input.REF} {input.R1} {input.R2} ""
        ""| samtools view -Sb - &gt; {output} 2&gt; {log}""


But when I call:

snakemake -p -s Snakefile_v4_ngs_bngs05b --cluster qsub -j 5 --use-conda


I get:

Error in rule bwa_map:
    jobid: 10
    output: aligned/NIPT-PearlPPlasma-03-PPx_S3downSample.bam
    log: logs/bwa_mem/NIPT-PearlPPlasma-03-PPx_S3downSample.log (check log file(s) for error message)
    conda-env: /nexusb/nipt/200311_A00154_0454_AHHHKMDRXX/testMetrics/outSnakeMake_test/.snakemake/conda/38107c2c
    shell:
        bwa mem /home/ngs/data/genomes/b37/human_g1k_v37.fasta /nexusb/Novaseq/200311_A00154_0454_AHHHKMDRXX/Unaligned/NIPT-PearlPPlasma-03-PPx_S3downSample_R1_001.fastq.gz /nexusb/Novaseq/200311_A00154_0454_AHHHKMDRXX/Unaligned/NIPT-PearlPPlasma-03-PPx_S3downSample_R2_001.fastq.gz | samtools view -Sb - &gt; aligned/NIPT-PearlPPlasma-03-PPx_S3downSample.bam 2&gt; logs/bwa_mem/NIPT-PearlPPlasma-03-PPx_S3downSample.log
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)
    cluster_jobid: Your job 381368 (""snakejob.bwa_map.10.sh"") has been submitted

Error executing rule bwa_map on cluster (jobid: 10, external: Your job 381368 (""snakejob.bwa_map.10.sh"") has been submitted, jobscript: /nexusb/nipt/200311_A00154_0454_AHHHKMDRXX/testMetrics/outSnakeMake_test/.snakemake/tmp.bnhr7qck/snakejob.bwa_map.10.sh). For error details see the cluster log and the log files of the involved rule(s).
[Wed Apr  8 17:21:45 2020]
Error in rule fastQC:
    jobid: 1
    output: fastQC/NIPT-PearlPPlasma-03-PPx_S3downSample
    log: logs/fastQC/NIPT-PearlPPlasma-03-PPx_S3downSample.log (check log file(s) for error message)
    conda-env: /nexusb/nipt/200311_A00154_0454_AHHHKMDRXX/testMetrics/outSnakeMake_test/.snakemake/conda/38107c2c
    shell:
        mkdir -p fastQC/NIPT-PearlPPlasma-03-PPx_S3downSample | fastqc --outdir fastQC/NIPT-PearlPPlasma-03-PPx_S3downSample -f fastq /nexusb/Novaseq/200311_A00154_0454_AHHHKMDRXX/Unaligned/NIPT-PearlPPlasma-03-PPx_S3downSample_R1_001.fastq.gz /nexusb/Novaseq/200311_A00154_0454_AHHHKMDRXX/Unaligned/NIPT-PearlPPlasma-03-PPx_S3downSample_R2_001.fastq.gz
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)
    cluster_jobid: Your job 381369 (""snakejob.fastQC.1.sh"") has been submitted

Error executing rule fastQC on cluster (jobid: 1, external: Your job 381369 (""snakejob.fastQC.1.sh"") has been submitted, jobscript: /nexusb/nipt/200311_A00154_0454_AHHHKMDRXX/testMetrics/outSnakeMake_test/.snakemake/tmp.bnhr7qck/snakejob.fastQC.1.sh). For error details see the cluster log and the log files of the involved rule(s).
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message


Upon first execution of the workflow I noticed that the environment was created at .snakemake/conda (relative to the Snakefile). When I call the script for the second time, without changing the conda directives, snakemake uses the same conda-env.

The description of my environment looks like:

channels:
  - bioconda
  - conda-forge
dependencies:
  - bwa=0.7.17
  - samtools=1.9
  - picard=2.22.1
  - mosdepth=0.2.6
  - python=3.7.6
  - pandas=1.0.3
  - fastqc=0.11.9


and it is saved at envs/NIPTlibPrep.yaml (relative to the Snakefile)

The fact that the workflow finishes locally but cannot be run on the cluster I find really puzzling, especially considering the fact that the environment with the correct dependencies was successfully created.
",-1,-1,-1.0
61147109,snakemake job fails if --drmaa-log-dir specified,"I am using snakemake v. 5.7.0. The pipeline runs correctly when either launched locally or submitted to SLURM via snakemake --drmaa: jobs get submitted, everything works as expected. However, in the latter case, a number of slurm log files is produced in the current directory. 

Snakemake invoked with the --drmaa-log-dir option creates the directory specified in the option, but fails to execute the rules. No log files are produced.

Here is a minimal example. First, the Snakefile used:

rule all:
  shell: ""sleep 20 &amp; echo SUCCESS!""


Below is the output of snakemake --drmaa

Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       all
        1

[Fri Apr 10 21:03:50 2020]
rule all:
    jobid: 0

Submitted DRMAA job 0 with external jobid 13321.
[Fri Apr 10 21:04:00 2020]
Finished job 0.
1 of 1 steps (100%) done
Complete log: /XXXXX/snakemake_test/.snakemake/log/2020-04-10T210349.984931.snakemake.log


Here is the output of snakemake --drmaa --drmaa-log-dir foobar

Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       all
        1

[Fri Apr 10 21:06:19 2020]
rule all:
    jobid: 0

Submitted DRMAA job 0 with external jobid 13322.
[Fri Apr 10 21:06:29 2020]
Error in rule all:
    jobid: 0
    shell:
        sleep 20 &amp; echo SUCCESS!
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Error executing rule all on cluster (jobid: 0, external: 13322, jobscript: /XXXXXX/snakemake_test/.snakemake/tmp.9l7fqvgg/snakejob.all.0.sh). For error details see the cluster log and the log files of the involved rule(s).
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /XXXXX/snakemake_test/.snakemake/log/2020-04-10T210619.598354.snakemake.log


No log files are produced. The directory foobar has been created, but is empty. 

What am I doing wrong?
",-1,-1,-1.0
61229930,snakemake interpreting full path as relative path,"I am writing a snakemake to perform multiple operations. All the rules, except for the last one (mvQsubLogs) work for a test file. This last rule should move the .e and .o files produced by the qsub command (I am running snakemake on a cluster), from the directory specified with the -e and -o flag, to a directory specified in the output directive of my rule, as soon as some operations are completed (please see input directive in the rule below):

rule mvQsubLogs:
    input:
        # FastQC
        rules.fastQC.output,

        # Markduplicates
        rules.markDups.output.markDupBam,
        rules.markDups.output.markDupMetrics,

        # mosdepth
        rules.mosdepth.output.DIR,

        # editflagStat
        rules.edit_flagStat.output,

        # edit idxStats
        rules.edit_idxStats.output,

        # insertSizeMetrics
        rules.insertSizeMetrics.output.METRICS,
        rules.insertSizeMetrics.output.PDF

    output:
        directory(""{sample}/logs"")
    shell:
        ""mkdir -p {wildcards.sample}/logs "" 
        ""| mv {LOGDIR}{wildcards.sample}* {output}""


A DAG with all jobs that I want to perform can be found below:



The command that I am using to launch the jobs to the cluster is

snakemake -p -s Snakefile_v6_ngs_bngs05b --cluster ""qsub -q onlybngs05b -e {LOGD
IR} -o {LOGDIR}"" -j 5 --use-conda --jobname ""{wildcards.sample}.{rule}.{jobid}""


Whereas it is important to note where the .e and .o files should be produced, which for this example is LOGDIR. LOGDIR was actually retrieved from the config file (LOGDIR = config['logsOutDir'] - in the snakefile itself and logsOutDir: ""/home/ngs/jobout/"" - specified in the config file).

When I call the full snakemake the command that I get for the rule mvQsubLogs is :

rule mvQsubLogs:
    input: NIPT-PearlPPlasma-03-PPx_S3downSample/fastQC, NIPT-PearlPPlasma-03-PPx_S3downSample/aligned/NIPT-PearlPPlasma-03-PPx_S3downSample.sorted.markDup.bam, NIPT-PearlPPlasma-03-PPx_S3downSample/dups/NIPT-PearlPPlasma-03-PPx_S3downSample.markDups.metrics.txt, NIPT-PearlPPlasma-03-PPx_S3downSample/depth/, NIPT-PearlPPlasma-03-PPx_S3downSample/dups/NIPT-PearlPPlasma-03-PPx_S3downSample.sorted.markDup.flagstat.edited.csv, NIPT-PearlPPlasma-03-PPx_S3downSample/readsDist/NIPT-PearlPPlasma-03-PPx_S3downSample.sorted.markDup.idxstats.edited.csv, NIPT-PearlPPlasma-03-PPx_S3downSample/insertSizeDist/NIPT-PearlPPlasma-03-PPx_S3downSample_ISmetrics.txt, NIPT-PearlPPlasma-03-PPx_S3downSample/insertSizeDist/NIPT-PearlPPlasma-03-PPx_S3downSample_ISHist.pdf
    output: NIPT-PearlPPlasma-03-PPx_S3downSample/logs
    jobid: 7
    wildcards: sample=NIPT-PearlPPlasma-03-PPx_S3downSample

mkdir -p NIPT-PearlPPlasma-03-PPx_S3downSample/logs | mv /home/ngs/jobout/NIPT-PearlPPlasma-03-PPx_S3downSample* NIPT-PearlPPlasma-03-PPx_S3downSample/logs


Which does sound right to me: (after creating the directory to which the files should be moved, just to be on the safe side) I should move all files starting with NIPT-PearlPPlasma-03-PPx_S3downSample (i.e. wildcards.sample), located at /home/ngs/jobout/ to NIPT-PearlPPlasma-03-PPx_S3downSample/logs, whereas this last directory is relative to the working directory.

Having a look at the .e file generated by the mvQsubLogs rule I get:

mkdir -p NIPT-PearlPPlasma-03-PPx_S3downSample/logs | mv /home/ngs/jobout/NIPT-PearlPPlasma-03-PPx_S3downSample* NIPT-PearlPPlasma-03-PPx_S3downSample/logs
mv: target ‘NIPT-PearlPPlasma-03-PPx_S3downSample/logs’ is not a directory


Which does not make sense to me, as the output directory NIPT-PearlPPlasma-03-PPx_S3downSample/logs should have been created

I have already tried specifying the full path where the files should be moved to, though it did not work either, I got the same error.

Can anyone spot where the error in my code is?
",-1,-1,-1.0
61272066,How do I define a Snakemake workflow with an unknown number of output files?,"I have a workflow where it's possible that there is no output for some of my samples. Snakemake eats my Snakefile with dynamic(""{sample}..."") inputs and outputs. However, it also tries to produce a file literally called ""{sample}.out"", which no rule ever produces since the wildcards are substituted.

This is my example Snakefile to demonstrate the workflow. There is only one rule with unknown number of output files, the following one(s) should work on the files that were produced by the first.

rule all:
        input: dynamic(expand(""{sample}.out"", sample = [ ""a"", ""b"", ""c"", ""d"" ]))

rule first:
        output:
                dynamic(""{sample}.intermediate"")
        shell:
                ""(( $RANDOM % 2 )) &amp;&amp; touch {output} || true""

rule second:
        input:
                dynamic(""{sample}.intermediate"")
        output:
                ""{sample}.out""
        shell:
                ""echo 'processed' &gt; {output}""


After one round of Snakemake, not all output files are present because of the random factor in the first rule (it's possible that all are produced; just delete them and rerun the example). Snakemake fails with a MissingOutputException:

MissingOutputException in line 4 of Snakefile:
Job completed successfully, but some output files are missing. Missing files after 5 seconds:
{sample}.intermediate


In my real data, there is no randomness involved, but I cannot predict the final number of output files. I have a list of possible sample names that I expand to the list of the target files, but no way to know which (if any) will result in output.

I have seen the answer to this other question and it helped understand that for every dynamic() output there must be another rule that takes dynamic() input. I don't understand why Snakemake tries to produce the files ""{sample}.out"". How can I fix this and get a non-predictable set of output files?
",-1,-1,-1.0
61278391,"Using snakemake to copy a file to multiple directories, where a wildcard is used for part of the name of the target","I am trying to use snakemake to copy a file to multiple directories, and I need to be able to use a wildcard for part of the name of the target. Previously I had tried this with 'dirs' specified in the Snakefile (this is an example, the actual application has 15 directories).

dirs=['k_1','k2_10']
rule all:
        input:
                expand(""{f}/practice_phased_reversed.vcf"",f=dirs)
rule r1:
        input:
                ""practice_phased_reversed.vcf""
        output:
                ""{f}/{input}""
        shell:
               ""cp {input} {output}""


This copies the file as desired.  However the filename must be given in rule all.  How can I change this so that I can specify a target on the command line using a wildcard for part of the name? 

I then tried this (below), with the command ""snakemake practice_phased_reversed.vcf"", but it gave an error : ""MissingRuleException: No rule to produce practice_phased_reversed.vcf""

dirs=['k_1','k2_10']
rule all:
        input:
                expand(""{f}/{{base}}_phased_reversed.vcf"",f=dirs)
rule r1:
        input:
                ""{base}_phased_reversed.vcf""
        output:
                ""{f}/{input}""
        shell:
               ""cp {input} {output}""


Is there a way to fix this so I can use the command line and a wildcard.  Thanks for any help. 
",-1,1,-1.0
61278972,Snakemake can't identify the rule,"I'm writing a pipeline with Snakemake and the program can't identify the rule stringtie. I can't find what I'm doing wrong. I already runned the rule fastp and star, the problem is specific with the stringtie rule.

include:
'config.py'

rule all:
    input:
        expand(FASTP_DIR + ""{sample}R{read_no}.fastq"",sample=SAMPLES ,read_no=['1', '2']), #fastp       
        expand(STAR_DIR + STAR_DIR + ""output/{sample}/{sample}Aligned.sortedByCoord.out.bam"",sample=SAMPLES), #STAR
        expand(STRINGTIE_DIR + ""/{sample}/{sample}Aligned.sortedByCoord.out.gtf"", sample=SAMPLES),
        GTF_DIR + ""path_samplesGTF.txt""

rule fastp:
    input:
        R1= DATA_DIR + ""{sample}R1_001.fastq.gz"",
        R2= DATA_DIR + ""{sample}R2_001.fastq.gz""
    output:
        R1out= FASTP_DIR + ""{sample}R1.fastq"",
        R2out= FASTP_DIR + ""{sample}R2.fastq""
    params:
        data_dir = DATA_DIR,
        name_sample = ""{sample}""
    log: FASTP_LOG + ""{sample}.html""
    message: ""Executando o programa FASTP""
    run:
        shell('fastp -i {input.R1} -I {input.R2} -o {output.R1out} -O {output.R2out} \
    -h {log} -j {log}')
    shell(""find {params.data_dir} -type f -name '{params.name_sample}*' -delete "")

rule star:
    input:
        idx_star = IDX_DIR,
        R1 = FASTP_DIR + ""{sample}R1.fastq"",
        R2 = FASTP_DIR + ""{sample}R2.fastq"",
        parameters = ""parameters.txt"",

    params:
        outdir = STAR_DIR + ""output/{sample}/{sample}"",
        star_dir = STAR_DIR,
        star_sample = '{sample}'
    # threads: 18
    output:
        out = STAR_DIR + ""output/{sample}/{sample}Aligned.sortedByCoord.out.bam""
        #run_time = STAR + ""log/star_run.time""
    #  log: STAR_LOG
    # benchmark: BENCHMARK + ""star/{sample_star}""
    run:
        shell(""STAR --runThreadN 12 --genomeDir {input.idx_star} \
        --readFilesIn {input.R1} {input.R2} --outFileNamePrefix {params.outdir}\
        --parametersFiles {input.parameters} \
        --quantMode TranscriptomeSAM GeneCounts \
        --genomeChrBinNbits 12"")
        # shell(""find {params.star_dir} -type f ! -name 
'{params.star_sample}Aligned.sortedByCoord.out.bam' -delete"")

rule stringtie:
    input:
        star_output = STAR_DIR + ""output/{sample}/{sample}Aligned.sortedByCoord.out.bam""
    output:
        stringtie_output = STRINGTIE_DIR + ""/{sample}/{sample}Aligned.sortedByCoord.out.gtf""
    run:
        shell(""stringtie {input.star_output} -o {output.stringtie_output} \
        -v -p 12 "")

rule grep_gtf:
    input:
        list_gtf = STRINGTIE_DIR
    output:
        paths = GTF_DIR + ""path_samplesGTF.txt""
    shell:
        ""find {input.list_gtf} | grep .gtf &gt; {output.paths}""


This is the output I get with the option dry-run (flag -n)

Building DAG of jobs...
Job counts:
    count   jobs
    1   all
    1   grep_gtf
    2

[Fri Apr 17 15:59:24 2020]
rule grep_gtf:
    input: /homelocal/boralli/workdir/pipeline_v4/STRINGTIE/
    output: /homelocal/boralli/workdir/pipeline_v4/GTF/path_samplesGTF.txt
    jobid: 1

find /homelocal/boralli/workdir/pipeline_v4/STRINGTIE/ | grep .gtf &gt; 
/homelocal/boralli/workdir/pipeline_v4/GTF/path_samplesGTF.txt

[Fri Apr 17 15:59:24 2020]
localrule all:
    input: /homelocal/boralli/workdir/pipeline_v4/GTF/path_samplesGTF.txt
    jobid: 0

Job counts:
    count   jobs
    1   all
    1   grep_gtf
    2
This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.


I really don't know whats going on. The same pipeline worked before.
",-1,-1,-1.0
61350327,Nested re-evaluation of rules and checkpoints in snakemake,"I'm trying to set up a workflow where when given a file FIRST, the file is split to  $k$ separate files where k is unknown,SECOND_1, SECOND_2,...,SECOND_k for parallel processing.

Each file SECOND_i (i=1...k) is then processed to produce additional i_n files, that is, at the end of the process, we are going to have 1_n files for  SECOND_1, 2_n for SECOND_2 and so on, where, again, the numbers i_n are unknown and distinct. 

This step produces a set of files THIRD_ij where i=1...k and j=1...i_n. 

Now I wish to merge (cat) all files with the same index j, to get the files FOURTH_1, FOURTH_2,...,FOURTH_m
where FOURTH_2 for example is a concatenation of all files THIRD_i2 that exists.

I've tried to do so using snakemake, but I get some weird errors.

My current code looks like this 

rule split1:
    input: ""FIRST""
    output: ""first.done""
    shell: 
        ""[splitting_function1] {input} ;""
        ""touch {output}""

checkpoint split2:
    input: ""SECOND_{i}""
    output: ""{i}/second.done""
    shell: 
        ""[splitting_function2] {input} ; # makes a subdirectory {i} with the output""
        ""touch {output}""

def aggregate(wildcards):
    out = checkpoints.split2.get(**wildcards).output[0]
    out = out.replace(""second.out"","""")
    exp = expand(
            ""{i}/THIRD_{{j}}"",
            i= glob_wildcards(os.path.join(out, ""THIRD_{j}"")).i,
            j = glob_wildcards(os.path.join(out, ""THIRD_{j}"")).j
        )

rule merge:
    input: aggregate
    output: ""{j}/third.done""
    shell: 
        ""cat {input} &gt; {j}/FOURTH ;""
        ""touch {output}""



I keep getting WorkflowErrors and missing wildcard values for i. I'd love to figure this out!
",-1,-1,-1.0
61479321,Snakemake special symbol in wildcard,"I have Snakamake rule as follows, where my wildcard contains special characters, so I escape them using sub, see answer here. Output file: data/extract_AAV(1).csv.

import re
rule get_data:
    input:
    output: ""data/extract_{re.sub(r'([()])', r'\\\1', filename)}.csv""
    shell: ""python get_data.py --filename {re.sub(r'([()])', r'\\\1', wildcards.filename)}""


However, I get an error as follows:


  module 're' has no attribute 'sub(r'('


Running the re module works fine in Python:

filename = 'extract_AAV(1).csv'
print(re.sub(r'([()])', r'\\\1', filename)
# returns: extract_AAV\\(1\\).csv


A reproducible example of the error when passing wildcards with special characters from Snakemake to a python script is as follows:

Snakemake file:

rule get_data:
     output: ""extract_{sample}.csv""
     shell: ""python run.py --fn {wildcards.sample}""


run.py

import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--fn', type=str)
args = parser.parse_args()
import pandas as pd

df = pd.DataFrame({'a': [1,2,3]})
df.to_csv(""extract_""+args.fn+'.csv')


command to execute attempt 1:

$ snakemake extract_AAV(1).csv --cores 1
bash: syntax error near unexpected token `('


command to execute attempt 2:

$ snakemake extract_AAV\(1\).csv --cores 1
Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       get_data
        1

[Wed Apr 29 11:31:34 2020]
rule get_data:
    output: extract_AAV(1).csv
    jobid: 0
    wildcards: sample=AAV(1)
/bin/bash: -c: line 0: syntax error near unexpected token `('
/bin/bash: -c: line 0: `set -euo pipefail;  python run.py --fn AAV(1)'
[Wed Apr 29 11:31:34 2020]
Error in rule get_data:
    jobid: 0
    output: extract_AAV(1).csv
    shell:
        python run.py --fn AAV(1)
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/XXXXX/.snakemake/log/2020-04-29T113134.773987.snakemake.log

",-1,-1,-1.0
61726969,snakemake: AmbiguousRuleException,"I am trying to complete a simple pipeline in snakemake, but I can't figure out how to use the wildcards correctly.

I have a folder data with the following files:


data/sample1_P1.txt
data/sample1_P2.txt
data/sample2_P1.txt
data/sample2_P2.txt


The wildcars in this exapmle are sampleX and PX. What I want to achieve is to first move the files into the folders sample1 and sample2.

Desired output of this step:


data/sample1/sample1_P1.txt
data/sample1/sample1_P2.txt
data/sample2/sample2_P1.txt
data/sample2/sample2_P2.txt


In the next step, I want to concatenate the files inside the folders, producing the files:


data/sample1/sample1_concatenated.txt
data/sample2/sample2_concatenated.txt


This is what I tried:

pairs = {""P1"" : ""P1"", ""P2"" : ""P2""}

samples = {
    ""sample1"": ""sample1"",
    ""sample2"": ""sample2""
}

rule all:
    input: expand(""data/{sample}/{sample}_concatenated.txt"", sample = samples)

rule get_txt_files:
    output:
        ""data/{sample}_{pair}.txt""
    shell:
        """"""
        echo 1 &gt; {output}
        """"""

rule reorganise:
  input:
    expand(""data/{{sample}}_{pair}.txt"", \
        pair=pairs)
  output:
    ""data/{sample}/{sample}_{pair}.txt""
  shell:
    ""mv {input} data/{wildcards.sample}/.""

rule concat:
    input:
        expand(""data/{{sample}}/{{sample}}_{pair}.txt"", \
            pair=pairs)
    output:
        ""data/{sample}/{sample}_concatenated.txt""
    shell:
        ""cat {input} &gt; {output}""


I get an error message AmbiguousRuleException and I can't figure out how to solve this.
",-1,-1,-1.0
61905249,lambda function in snakemake output,"I currently have a snakemake workflow that requires the use of lambda wildcards, set up as follows:

Snakefile:

configfile: ""config.yaml""
workdir: config[""work""]

rule all:
    input:
        expand(""logs/bwa/{ref}.log"", ref=config[""refs""])

rule bwa_index:
    input:
        lambda wildcards: 'data/'+config[""refs""][wildcards.ref]+"".fna.gz""
    output:
        ""logs/bwa/{ref}.log""
    log:
        ""logs/bwa/{ref}.log""
    shell:
        ""bwa index {input} 2&amp;&gt;1 {log}""


Config file:

work: /datasets/work/AF_CROWN_RUST_WORK/2020-02-28_GWAS

refs:
    12NC29: GCA_002873275.1_ASM287327v1_genomic
    12SD80: GCA_002873125.1_ASM287312v1_genomic


This works, but I've had to use a hack to get the output of bwa_index to play with the input of all. My hack is to generate a log file as part of bwa_index, set the log to the output of bwa_index, and then set the input of all to these log files. As I said, it works, but I don't like it.
The problem is that the true outputs of bwa_index are of the format, for example, GCA_002873275.1_ASM287327v1_genomic.fna.sa. So, to specify these output files, I would need to use a lambda function for the output, something like:

rule bwa_index:
    input:
        lambda wildcards: 'data/'+config[""refs""][wildcards.ref]+"".fna.gz""
    output:
        lambda wildcards: 'data/'+config[""refs""][wildcards.ref]+"".fna.sa""
    log:
        ""logs/bwa/{ref}.log""
    shell:
        ""bwa index {input} 2&amp;&gt;1 {log}""


and then use a lambda function with expand for the input of rule all. However, snakemake will not accept functions as output, so I'm at a complete loss how to do this (other than my hack). Does anyone have suggestions of a sensible solution? TIA!
",-1,-1,-1.0
62158367,How to access snakemake.input[0]?,"This should be a simple problem, but for the life of me I cannot figure it out. I have a very, very simple Snakefile (shown below):

rule test:
    input:
        ""/home/username/input_data""
    script:
        ""scripts/test.py""


And in my test.py file, I am doing the following:

import snakemake
print(snakemake.input[0])


But I cannot access snakemake.input[0]. The docs have an example as simple as mine, but I can't get it to work. Additionally, in a Python interactive shell I am able to import snakemake, but I get an error:

ImportError: No module named snakemake


when importing it from my test.py script.

I am calling snakemake test from the command line in the same folder as the Snakefile to run this.

I appreciate any help on this.
",-1,-1,-1.0
62164904,Snakemake copy from several directories,"Snakemake is super-confusing to me. I have files of the form:

indir/type/name_1/run_1/name_1_processed.out
indir/type/name_1/run_2/name_1_processed.out
indir/type/name_2/run_1/name_2_processed.out
indir/type/name_2/run_2/name_2_processed.out


where type, name, and the numbers are variable. I would like to aggregate files such that all files with the same ""name"" end up in a single dir:

outdir/type/name/name_1-1.out
outdir/type/name/name_1-2.out
outdir/type/name/name_2-1.out
outdir/type/name/name_2-2.out


How do I write a snakemake rule to do this? I first tried the following

rule rename:
    input:
        ""indir/{type}/{name}_{nameno}/run_{runno}/{name}_{nameno}_processed.out""
    output:
        ""outdir/{type}/{name}/{name}_{nameno}-{runno}.out""
    shell:
        ""cp {input} {output}""

# example command: snakemake --cores 1 outdir/type/name/name_1-1.out


This worked, but doing it this way doesn't save me any effort because I have to know what the output files are ahead of time, so basically I'd have to pass all the output files as a list of arguments to snakemake, requiring a bit of shell trickery to get the variables.

So then I tried to use directory (as well as give up on preserving runno).

rule rename2:
    input:
        ""indir/{type}/{name}_{nameno}""
    output:
        directory(""outdir/{type}/{name}"")
    shell:
        """"""
        for d in {input}/run_*; do
          i=0
          for f in ${{d}}/*processed.out; do
            cp ${{f}} {output}/{wildcards.name}_{wildcards.nameno}-${{i}}.out
          done
          let ++i
        done
        """"""


This gave me the error, Wildcards in input files cannot be determined from output files: 'nameno'. I get it; {nameno} doesn't exist in output. But I don't want it there in the directory name, only in the filename that gets copied.

Also, if I delete {nameno}, then it complains because it can't find the right input file.

What are the best practices here for what I'm trying to do? Also, how does one wrap their head around the fact that in snakemake, you specify outputs, not inputs? I think this latter fact is what is so confusing.
",-1,-1,-1.0
62641766,Snakemake: --use-conda with --cluster,"When I run with --cluster and --use-conda, Snakemake does not appear to set the conda environment before submitting to the cluster, and my jobs fail accordingly. Is there a trick I am missing to set the conda environment before cluster submission?
EDIT:
I get snakemake in a conda environment like:
channels:
  - bioconda
  - conda-forge
dependencies:
  - snakemake-minimal=5.19.3
  - xrootd=4.12.2

Reproducer:
I create a directory with Snakefile, dothing.py, and environment.yml:
Snakefile:
shell.prefix('unset PYTHONPATH; unset LD_LIBRARY_PATH; unset PYTHONHOME; ')

rule dothing:
    conda: 'environment.yml'
    output: 'completed.out'
    log: 'thing.log'
    shell: 'python dothing.py &amp;&gt; {log} &amp;&amp; touch {output}'

dothing.py:
import uncertainties

print('it worked!')

environment.yml:
name: testsnakeconda
channels:
  - conda-forge
dependencies:
  - uncertainties=3.1.4

If I run locally like
snakemake --cores all --use-conda

It runs with no problems:
Building DAG of jobs...
Creating conda environment environment.yml...
Downloading and installing remote packages.
Environment for environment.yml created (location: .snakemake/conda/e0fff47f)
Using shell: /usr/bin/bash
Provided cores: 10
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       dothing
        1

[Tue Jun 30 16:19:38 2020]
rule dothing:
    output: completed.out
    log: thing.log
    jobid: 0

Activating conda environment: /path/to/environment.yml
[Tue Jun 30 16:19:39 2020]
Finished job 0.
1 of 1 steps (100%) done
Complete log: /path/to/.snakemake/log/2020-06-30T161824.906217.snakemake.log

If I try to submit using --cluster like
snakemake --cores all --use-conda --cluster 'condor_qsub -V -l procs={threads}' --latency-wait 30 --max-jobs-per-second 100 --jobs 50

there is no message about setting up a conda environment and the job fails with an error:
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cluster nodes: 50
Job counts:
        count   jobs
        1       dothing
        1

[Tue Jun 30 16:20:49 2020]
rule dothing:
    output: completed.out
    log: thing.log
    jobid: 0

Submitted job 0 with external jobid 'Your job 9246856 (&quot;snakejob.dothing.0.sh&quot;) has been submitted'.
[Tue Jun 30 16:26:00 2020]
Error in rule dothing:
    jobid: 0
    output: completed.out
    log: thing.log (check log file(s) for error message)
    conda-env: /path/to/.snakemake/conda/e0fff47f
    shell:
        python dothing.py &amp;&gt; thing.log &amp;&amp; touch completed.out
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)
    cluster_jobid: Your job 9246856 (&quot;snakejob.dothing.0.sh&quot;) has been submitted

Error executing rule dothing on cluster (jobid: 0, external: Your job 9246856 (&quot;snakejob.dothing.0.sh&quot;) has been submitted, jobscript: /path/to/.snakemake/tmp.a7fpixla/snakejob.dothing.0.sh). For error details see the cluster log and the log files of the involved rule(s).
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /path/to/.snakemake/log/2020-06-30T162049.793041.snakemake.log

and I can see that the problem is that the uncertainties package is not available:
$ cat thing.log
Traceback (most recent call last):
  File &quot;dothing.py&quot;, line 1, in &lt;module&gt;
    import uncertainties
ImportError: No module named uncertainties

EDIT:
verbose output without --cluster:
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 10
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       dothing
        1
Resources before job selection: {'_cores': 10, '_nodes': 9223372036854775807}
Ready jobs (1):
        dothing
Selected jobs (1):
        dothing
Resources after job selection: {'_cores': 9, '_nodes': 9223372036854775806}

[Thu Jul  2 21:51:18 2020]
rule dothing:
    output: completed.out
    log: thing.log
    jobid: 0

Activating conda environment: /path/to/workingdir/.snakemake/conda/e0fff47f
[Thu Jul  2 21:51:33 2020]
Finished job 0.
1 of 1 steps (100%) done
Complete log: /path/to/workingdir/.snakemake/log/2020-07-02T215117.964474.snakemake.log
unlocking
removing lock
removing lock
removed all locks

verbose output with --cluster:
Building DAG of jobs...
Checking status of 0 jobs.
Using shell: /usr/bin/bash
Provided cluster nodes: 50
Job counts:
    count   jobs
    1   dothing
    1
Resources before job selection: {'_cores': 9223372036854775807, '_nodes': 50}
Ready jobs (1):
    dothing
Selected jobs (1):
    dothing
Resources after job selection: {'_cores': 9223372036854775806, '_nodes': 49}

[Thu Jul  2 21:40:23 2020]
rule dothing:
    output: completed.out
    log: thing.log
    jobid: 0

Jobscript:
#!/bin/sh
# properties = {&quot;type&quot;: &quot;single&quot;, &quot;rule&quot;: &quot;dothing&quot;, &quot;local&quot;: false, &quot;input&quot;: [], &quot;output&quot;: [&quot;completed.out&quot;], &quot;wildcards&quot;: {}, &quot;params&quot;: {}, &quot;log&quot;: [&quot;thing.log&quot;], &quot;threads&quot;: 1, &quot;resources&quot;: {}, &quot;jobid&quot;: 0, &quot;cluster&quot;: {}}
 cd /path/to/workingdir &amp;&amp; \
/path/to/miniconda/envs/envname/bin/python3.8 \
-m snakemake dothing --snakefile /path/to/workingdir/Snakefile \
--force -j --keep-target-files --keep-remote \
--wait-for-files /path/to/workingdir/.snakemake/tmp.5n32749i /path/to/workingdir/.snakemake/conda/e0fff47f --latency-wait 30 \
 --attempt 1 --force-use-threads \
--wrapper-prefix https://github.com/snakemake/snakemake-wrappers/raw/ \
   --allowed-rules dothing --nocolor --notemp --no-hooks --nolock \
--mode 2  --use-conda  &amp;&amp; touch /path/to/workingdir/.snakemake/tmp.5n32749i/0.jobfinished || (touch /path/to/workingdir/.snakemake/tmp.5n32749i/0.jobfailed; exit 1)

Submitted job 0 with external jobid 'Your job 9253728 (&quot;snakejob.dothing.0.sh&quot;) has been submitted'.
Checking status of 1 jobs.
...
Checking status of 1 jobs.
[Thu Jul  2 21:46:23 2020]
Error in rule dothing:
    jobid: 0
    output: completed.out
    log: thing.log (check log file(s) for error message)
    conda-env: /path/to/workingdir/.snakemake/conda/e0fff47f
    shell:
        python dothing.py &amp;&gt; thing.log &amp;&amp; touch completed.out
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)
    cluster_jobid: Your job 9253728 (&quot;snakejob.dothing.0.sh&quot;) has been submitted

Error executing rule dothing on cluster (jobid: 0, external: Your job 9253728 (&quot;snakejob.dothing.0.sh&quot;) has been submitted, jobscript: /path/to/workingdir/.snakemake/tmp.5n32749i/snakejob.dothing.0.sh). For error details see the cluster log and the log files of the involved rule(s).
Cleanup job metadata.
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /path/to/workingdir/.snakemake/log/2020-07-02T214022.614691.snakemake.log
unlocking
removing lock
removing lock
removed all locks

",-1,-1,-1.0
62713371,Referring to input function of another Snakemake rule,"I'm having problems referring top the input function of another Snakemake rule.
I illustrated this in the code below:
rule bla:
    output: '{x}.txt'
    shell: 'touch {output}'


rule all_dogs:
    input: expand(rules.bla.output, x=['goofy', 'buster', 'fifi'])

rule all_cats:
    input: expand(rules.bla.output, x=['tom', 'felix', 'sylvester'])

def all_birds_infun(wildcards):
    return expand(rules.bla.output, x=['tweety', 'donald'])

rule all_birds:
    input: all_birds_infun

rule all_mammals:
    input: rules.all_dogs.input,
           rules.all_cats.input

rule all_animals:
    input: rules.all_dogs.input,
           rules.all_cats.input,
           rules.all_birds.input

localrules: bla

The rule all_mammals refers to the inputs of two other rules and works fine. The rule all_animals, however, does not work because it refers to the input of all_birds, which uses an input function as a rule. I realize that as a workaround, I could simply refer to the input function all_birds_infun directly, but in my real world example that would mean copy&amp;pasting some parameterized functions, which would be very ugly.
The error I get from referring to rules.all_birds.input is
TypeError in line 25 of Snakefile:
expected string or bytes-like object
  File &quot;Snakefile&quot;, line 25, in &lt;module&gt;

",-1,-1,-1.0
62713676,Is it possible to use wildcards in config files for a Snakemake pipeline?,"I'm new to building Snakefiles and for my bioinformatics research, I'm trying to loop my rules over multiple samples. I looked for similar questions and answers, but I can't seem to fix this problem. It may be because I still don't really understand how Snakemake works exactly. If you guys can help me out that would be great.
At the moment I have multiple rules, which currently works for one sample:
# variables for every species
SAMPLE = &quot;SRR8528338&quot;
SAMPLES = &quot;SRR8528338 SRR8528339 SRR8528340&quot;.split()

configfile: &quot;./envs/contigs/&quot; + SAMPLE + &quot;.yaml&quot;

var_variables = expand(&quot;results/4_mapped_contigs/&quot; + SAMPLE + &quot;/var/Contig{nr}_AT_sort.var&quot;, nr = config[&quot;contig_nrs&quot;])
#make_contig_consensus = expand(&quot;results/5_consensus_contigs/{sample}&quot;, sample = SAMPLES)

rule all:
    input:
         var_variables
#        make_contig_consensus

rule convert_to_fBAM:
    input:
        &quot;results/4_mapped_contigs/&quot; + SAMPLE + &quot;/sam/Contig{nr}_AT.sam&quot;
    output:
        &quot;results/4_mapped_contigs/&quot; + SAMPLE + &quot;/bam/Contig{nr}_AT.bam&quot;
    shell:
        &quot;samtools view -bS {input} &gt; {output}&quot;

rule sort_fBAM:
    input:
        &quot;results/4_mapped_contigs/&quot; + SAMPLE + &quot;/bam/Contig{nr}_AT.bam&quot;
    output:
        &quot;results/4_mapped_contigs/&quot; + SAMPLE + &quot;/sorted_bam/Contig{nr}_AT_sort.bam&quot;
    shell:
        &quot;samtools sort -m5G {input} -o {output}&quot;

rule convert_to_fpileup:
    input:
        &quot;results/4_mapped_contigs/&quot; + SAMPLE + &quot;/sorted_bam/Contig{nr}_AT_sort.bam&quot;
    output:
        &quot;results/4_mapped_contigs/&quot; + SAMPLE + &quot;/pileup/Contig{nr}_AT_sort.pileup&quot;
    shell:
        &quot;samtools mpileup -B {input} &gt; {output}&quot;

rule SNP_calling:
    input:
        &quot;results/4_mapped_contigs/&quot; + SAMPLE + &quot;/pileup/Contig{nr}_AT_sort.pileup&quot;
    output:
        &quot;results/4_mapped_contigs/&quot; + SAMPLE + &quot;/var/Contig{nr}_AT_sort.var&quot;
    shell:
        &quot;varscan pileup2cns {input} &quot;
        &quot;--min-freq-for-hom 0.6 &quot;
        &quot;--min-coverage 5 &quot;
        &quot;--min-var-freq 0.6 &quot;
        &quot;--p-value 0.1 &quot;
        &quot;--min-reads2 5 &quot;
        &quot;&gt; {output}&quot;

rule make_contig_consensus:
    input:
        &quot;src/read_var.py&quot;
    output:
        &quot;results/5_consensus_contigs/{sample}&quot;
    params:
        &quot;{sample}&quot;
    shell:
        &quot;python3 {input} {params}&quot;


The config file differs for every sample (the numbers of contigs). For SRR8528338, it looks like this:
contig_nrs:
    1: ./results/4_mapped_contigs/SRR8528338/var/Contig1_AT_sort.var
    2: ./results/4_mapped_contigs/SRR8528338/var/Contig2_AT_sort.var
    3: ./results/4_mapped_contigs/SRR8528338/var/Contig3_AT_sort.var
    ...
    2146: ./results/4_mapped_contigs/SRR8528338/var/Contig2146_AT_sort.var 

However, I want to loop all these rules over multiple samples as referred to in the &quot;SAMPLES&quot; variable.
Now I tried using double braces before, which worked for multiple samples. (Changing all 'SAMPLES' to {{sample}} and adding: , sample = SAMPLES). Then my code should be looking like this:
# variables for every species
SAMPLES = &quot;SRR8528338 SRR8528339 SRR8528340&quot;.split()

for sample in SAMPLES:
    configfile: &quot;./envs/contigs/&quot; + sample + &quot;.yaml&quot;

var_variables = expand(&quot;results/4_mapped_contigs/{sample}/var/Contig{nr}_AT_sort.var&quot;, sample = SAMPLES, nr = config[&quot;contig_nrs&quot;])
make_contig_consensus = expand(&quot;results/5_consensus_contigs/{sample}&quot;, sample = SAMPLES)

rule all:
    input:
         var_variables
#        make_contig_consensus

rule convert_to_fBAM:
    input:
        &quot;results/4_mapped_contigs/{{sample}}/sam/Contig{nr}_AT.sam&quot;
    output:
        &quot;results/4_mapped_contigs/{{sample}}/bam/Contig{nr}_AT.bam&quot;
    shell:
        &quot;samtools view -bS {input} &gt; {output}&quot;

rule sort_fBAM:
    input:
        &quot;results/4_mapped_contigs/{{sample}}/bam/Contig{nr}_AT.bam&quot;
    output:
        &quot;results/4_mapped_contigs/{{sample}}/sorted_bam/Contig{nr}_AT_sort.bam&quot;
    shell:
        &quot;samtools sort -m5G {input} -o {output}&quot;

rule convert_to_fpileup:
    input:
        &quot;results/4_mapped_contigs/{{sample}}/sorted_bam/Contig{nr}_AT_sort.bam&quot;
    output:
        &quot;results/4_mapped_contigs/{{sample}}/pileup/Contig{nr}_AT_sort.pileup&quot;
    shell:
        &quot;samtools mpileup -B {input} &gt; {output}&quot;

rule SNP_calling:
    input:
        &quot;results/4_mapped_contigs/{{sample}}/pileup/Contig{nr}_AT_sort.pileup&quot;
    output:
        &quot;results/4_mapped_contigs/{{sample}}/var/Contig{nr}_AT_sort.var&quot;
    shell:
        &quot;varscan pileup2cns {input} &quot;
        &quot;--min-freq-for-hom 0.6 &quot;
        &quot;--min-coverage 5 &quot;
        &quot;--min-var-freq 0.6 &quot;
        &quot;--p-value 0.1 &quot;
        &quot;--min-reads2 5 &quot;
        &quot;&gt; {output}&quot;

rule make_contig_consensus:
    input:
        &quot;src/read_var.py&quot;
    output:
        &quot;results/5_consensus_contigs/{sample}&quot;
    params:
        &quot;{sample}&quot;
    shell:
        &quot;python3 {input} {params}&quot;


However, when I run this I get an error. I'm not exactly sure, but I think it is because of the for loop (sample in SAMPLES):
Missing input files for rule all:
results/4_mapped_contigs/SRR8528338/var/Contig1266_AT_sort.var
results/4_mapped_contigs/SRR8528338/var/Contig1299_AT_sort.var
...

Now I was wondering: is there a way to expand the config file by using wildcards? Something like:
configfile: expand(&quot;./envs/contigs/{sample}.yaml&quot;, sample = SAMPLES)

Doing this will give me the error:
TypeError in line 4
expected str, bytes or os.PathLike object, not list

or do you have other solutions for this problem?
Thank you!

Update:
I've been trying some things out and I think it would be useful to change the config file into a nested dictionary instead of separate ones. It should look something like this:
    contigs:
         SRR8528336: - 1
                     - 2
                     - ...
                     - 2113
         SRR8528337: - 1
                      ...
          ...
    exons:
         SRR8528336: - 1
                      ...
                     - 1827
         SRR8528337: - 1
                       ...
                     - 1826
          ...

So for example, if I want to run for the samples: SRR8528338 until SRR8528340 I give this as input:
SAMPLES = &quot;SRR8528338 SRR8528339 SRR8528340&quot;.split()

and call the contigs by sample name:
var_variables = expand(&quot;results/4_mapped_contigs/{{sample}}/var/Contig{nr}_AT_sort.var&quot;, nr = config[&quot;contigs&quot;][wildcards.sample])

or exons by:
expand(&quot;results/7_exons/{{sample}}/var/exon{nr}_AT_sort.var&quot;, nr = config[&quot;exons&quot;][wildcards.sample])

How does the 'wildcards.sample' works exactly if I only want to obtain the value?

Solution (and next problem) 31/7/2020
I made my changes according to bli, which is now working now:
# variables for every species
SAMPLES = &quot;SRR8528347 SRR8528355 SRR8528356&quot;.split()

configfile: &quot;./envs/config_contigs.yaml&quot;

# bam = []
# sort_bam = []
# fpileup = []
var_variables = []
make_contig_consensus = []
blat_variables = []
extract_hits_psl = []
for sample in SAMPLES:
    contig_nrs = config[sample]
    for nr in contig_nrs:
        # bam.append(&quot;results/A04_mapped_contigs/{sample}/bam/Contig{nr}_AT.bam&quot;.format(sample=sample, nr=nr))
        # sort_bam.append(&quot;results/A04_mapped_contigs/{sample}/sorted_bam/Contig{nr}_AT_sort.bam&quot;.format(sample=sample, nr=nr))
        # fpileup.append(&quot;results/A04_mapped_contigs/{sample}/pileup/Contig{nr}_AT_sort.pileup&quot;.format(sample=sample, nr=nr))
        var_variables.append(&quot;results/A04_mapped_contigs/{sample}/var/Contig{nr}_AT_sort.var&quot;.format(sample=sample, nr=nr))
        make_contig_consensus.append(&quot;results/A05_consensus_contigs/{sample}/Contig{nr}.fasta&quot;.format(sample=sample, nr=nr))
        blat_variables.append(&quot;results/A06_identified_contigs_blat/{sample}/contig{nr}_AT.psl&quot;.format(sample=sample, nr=nr))
        extract_hits_psl.append(&quot;results/A07_mapped_exons/{sample}/&quot;.format(sample=sample, nr=nr))

rule all:
    input:
        # bam,
        # sort_bam,
        # fpileup,
        var_variables,
        make_contig_consensus,
        blat_variables,
        extract_hits_psl

rule convert_to_fBAM:
    input:
        &quot;results/A04_mapped_contigs/{sample}/sam/Contig{nr}_AT.sam&quot;
    output:
        &quot;results/A04_mapped_contigs/{sample}/bam/Contig{nr}_AT.bam&quot;
    shell:
        &quot;samtools view -bS {input} &gt; {output}&quot;

rule sort_fBAM:
    input:
        &quot;results/A04_mapped_contigs/{sample}/bam/Contig{nr}_AT.bam&quot;
    output:
        &quot;results/A04_mapped_contigs/{sample}/sorted_bam/Contig{nr}_AT_sort.bam&quot;
    shell:
        &quot;samtools sort -m5G {input} -o {output}&quot;

rule convert_to_fpileup:
    input:
        &quot;results/A04_mapped_contigs/{sample}/sorted_bam/Contig{nr}_AT_sort.bam&quot;
    output:
        &quot;results/A04_mapped_contigs/{sample}/pileup/Contig{nr}_AT_sort.pileup&quot;
    shell:
        &quot;samtools mpileup -B {input} &gt; {output}&quot;

rule SNP_calling:
    input:
        &quot;results/A04_mapped_contigs/{sample}/pileup/Contig{nr}_AT_sort.pileup&quot;
    output:
        &quot;results/A04_mapped_contigs/{sample}/var/Contig{nr}_AT_sort.var&quot;
    shell:
        &quot;varscan pileup2cns {input} &quot;
        &quot;--min-freq-for-hom 0.6 &quot;
        &quot;--min-coverage 5 &quot;
        &quot;--min-var-freq 0.6 &quot;
        &quot;--p-value 0.1 &quot;
        &quot;--min-reads2 5 &quot;
        &quot;&gt; {output}&quot;

rule make_contig_consensus:
    input:
        script = &quot;src/read_var.py&quot;,
        file = &quot;results/A04_mapped_contigs/{sample}/var/Contig{nr}_AT_sort.var&quot;
    output:
        &quot;results/A05_consensus_contigs/{sample}/Contig{nr}.fasta&quot;
    params:
        &quot;{sample}&quot;
    shell:
        &quot;python3 {input.script} {params}&quot;

rule BLAT_assembled:
    input:
        &quot;data/exons/exons_AT.fasta&quot;,
        &quot;results/A05_consensus_contigs/{sample}/Contig{nr}.fasta&quot;
    output:
        &quot;results/A06_identified_contigs_blat/{sample}/contig{nr}_AT.psl&quot;
    shell:
        &quot;blat &quot;
        &quot;-t=dnax &quot;
        &quot;-q=dnax &quot;
        &quot;-stepSize=5 &quot;
        &quot;-repMatch=2253 &quot;
        &quot;-minScore=0 &quot;
        &quot;-minIdentity=0 &quot;
        &quot;{input} {output}&quot;

rule extract_hits_psl:
    input:
        script = &quot;src/extract_hits_psl.py&quot;
        # file = &quot;results/A06_identified_contigs_blat/{sample}/contig{nr}_AT.psl&quot;
    output:
        &quot;results/A07_mapped_exons/{sample}/&quot;
    params:
        &quot;{sample}&quot;
    shell:
        &quot;python {input.script} {params}&quot;


config_contigs.yaml:
SRR8528347:
    - 1
    - ...
    - 5
SRR8528348:
    - 1
    - ...
    - 5
...


Now calling them from the .yaml is working, but the rules should be run in the same order as written (from top to bottom). When running this, the rules are run in a different order and therefore gives an error because the files don't exist yet. I read that the output of the order before should be the same as the input after, but it is not working.
",1,-1,-1.0
62720926,Executing snakemake rules in conda environments on Windows,"I used the latest version of Miniconda on Windows (Miniconda3-py37_4.8.3-Windows-x86_64) to install snakemake in a separate conda environment (snakemake-minimal=5.19.2, python=3.8.3).
With this setup, I tried to run a snakemake workflow that uses individual conda environments for each rule.
The workflow itself was tested on Ubuntu and is working fine there.
However, when running on the Windows setup described above, I ran into several problems.
Looking into these issues I stumbled upon a few things in snakemake/deployment/conda.py that seem to be incompatible with Windows.
I found 2 examples, but I am quite sure there are a few more:

Lines 301 to 305: Here conda env create is called with the optional the --file and --prefix tags. On Windows, their arguments need to be surrounded by double quotes instead of single quotes. For instance, line 304 should read &quot;--file \&quot;{}\&quot;&quot;.format(target_env_file).
Line 460 uses the source command to activate a conda environment, obviously not compatible with   a default Windows setup.

Based on these observations, I came to the conclusion that running snakemake rules in separate conda environments is currently not supported for Windows.
However, I was quite surprised that I found no mention about this in the docs or anywhere else.
Did I miss something here? I know I could use for instance WSL to make it run on Windows, but is there also a way to run it in a &quot;native&quot; Windows setup?
",-1,-1,-1.0
63019085,Using checkpoints with snakemake gives each instance of a rule all input files,"I've recently come across checkpoints in snakemake and realized they will work perfectly with what I am trying to do. I've been able to implement the workflow listed here. I also found this stackoverflow question, but can't quite make sense of it or how I might make it work for what I am doing
The rules I am working with are as follows:
def ReturnBarcodeFolderNames():
    path = config['results_folder'] + &quot;Barcode/&quot;
    return_direc = []
    for root, directory, files in os.walk(path):
        for direc in directory:
            return_direc.append(direc)
    return return_direc


rule all:
    input:
        expand(config['results_folder'] + &quot;Barcode/{folder}.merged.fastq&quot;, folder=ReturnBarcodeFolderNames())


checkpoint barcode:
    input:
        expand(config['results_folder'] + &quot;Basecall/{fast5_files}&quot;, fast5_files=FAST5_FILES)
    output:
        temp(directory(config['results_folder'] + &quot;Barcode/.tempOutput/&quot;))
    shell:
        &quot;guppy_barcoder &quot;
        &quot;--input_path {input} &quot;
        &quot;--save_path {output} &quot;
        &quot;--barcode_kits EXP-PBC096 &quot;
        &quot;--recursive&quot;

def aggregate_barcode_folders(wildcards):
    checkpoint_output = checkpoints.barcode.get(**wildcards).output[0]
    folder_names = []
    for root, directories, files in os.walk(checkpoint_output):
        for direc in directories:
            folder_names.append(direc)

    return expand(config['results_folder'] + &quot;Barcode/.tempOutput/{folder}&quot;, folder=folder_names)

rule merge:
    input:
        aggregate_barcode_folders
    output:
        config['results_folder'] + &quot;Barcode/{folder}.merged.fastq&quot;
    shell:
         &quot;echo {input}&quot;


The rule barcode and def aggregate_barcode_folders work as expected, but when rule merge is reached, every input folder is being passed to each instance of the rule. This results in something like the following:
rule merge:
    input: /Results/Barcode/.tempOutput/barcode81, 
/Results/Barcode/.tempOutput/barcode28, 
/Results/Barcode/.tempOutput/barcode17, 
/Results/Barcode/.tempOutput/barcode10, 
/Results/Barcode/.tempOutput/barcode26, 
/Results/Barcode/.tempOutput/barcode21, 
/Results/Barcode/.tempOutput/barcode42, 
/Results/Barcode/.tempOutput/barcode89, 
/Results/Barcode/.tempOutput/barcode45, 
/Results/Barcode/.tempOutput/barcode20, 
/Results/Barcode/.tempOutput/barcode18, 
/Results/Barcode/.tempOutput/barcode27, 
/Results/Barcode/.tempOutput/barcode11, 
.
.
.
.
.
    output: /Results/Barcode/barcode75.merged.fastq
    jobid: 82
    wildcards: folder=barcode75


The same exact input is needed for each job of rule merge, which amounts to about 80 instances. But, the wildcards portion in each job is different for each folder. How can I use this as input for each instance of my rule merge, instead of passing the entire list received from def aggregate_barcode_folders?
I feel there may be something amiss with the input from rule all, but I'm not 100% sure what the problem may be.
As a note, I know snakemake will throw an error stating that it is waiting for output files from rule merge, as I am not doing anything with the output other than printing it to the screen.
EDIT
I've decided to go against checkpoints for now, and instead opt for the following. To make things more clear, the goal for this pipeline is as follows: I am attempting to merge fastq files from an output folder into one file, with the input files having a variable number of files (1 to about 3 per folder, but I won't know how many). The structure of the input is as follows
INPUT
|-- Results
    |-- FolderA
        |-- barcode01
            |-- file1.fastq
        |-- barcode02
            |-- file1.fastq
            |-- file2.fastq
        |-- barcode03
            |-- file1.fastq
    |-- FolderB
        |-- barcode01
            |-- file1.fastq
        |-- barcode02
            |-- file1.fastq
            |-- file2.fastq
        |-- barcode03
            |-- file1.fastq
    |-- FolderC
        |-- barcode01
            |-- file1.fastq
            |-- file2.fastq
        |-- barcode02
            |-- file1.fastq
        |-- barcode03
            |-- file1.fastq
            |-- file2.fastq


OUTPUT
I would like to turn that output resembling something such as:
|-- Results
    |-- barcode01.merged.fastq
    |-- barcode02.merged.fastq
    |-- barcode03.merged.fastq

The output files would contain data from all file#.fastq from its respective barcode folder, from folder A, B, and C.
I've been able to get (I think) further than I was before, but snakemake is throwing an error that says Missing input files for rule basecall: /Users/joshl/PycharmProjects/ARS/Results/DataFiles/fast5/FAL03879_67a0761e_1055/ barcode72.fast5. My code relevant code is here:
CODE

configfile: &quot;config.yaml&quot;
FAST5_FILES = glob_wildcards(config['results_folder'] + &quot;DataFiles/fast5/{fast5_files}.fast5&quot;).fast5_files

def return_fast5_folder_names():
    path = config['results_folder'] + &quot;Basecall/&quot;
    fast5_folder_names = []
    for item in os.scandir(path):
        if Path(item).is_dir():
            fast5_folder_names.append(item.name)

    return fast5_folder_names

def return_barcode_folder_names():
    path = config['results_folder'] + &quot;.barcodeTempOutput&quot;
    fast5_folder_names = []
    collated_barcode_folder_names = []

    for item in os.scandir(path):
        if Path(item).is_dir():
            full_item_path = os.path.join(path, item.name)
            fast5_folder_names.append(full_item_path)

    index = 0
    for item in fast5_folder_names:
        collated_barcode_folder_names.append([])
        for folder in os.scandir(item):
            if Path(folder).is_dir():
                collated_barcode_folder_names[index].append(folder.name)
        index += 1

    return collated_barcode_folder_names


rule all:
    input:
        # basecall
        expand(config['results_folder'] + &quot;Basecall/{fast5_file}&quot;, fast5_file=FAST5_FILES),

         # barcode
        expand(config['results_folder'] + &quot;.barcodeTempOutput/{fast5_folders}&quot;, fast5_folders=return_fast5_folder_names()),

        # merge files
        expand(config['results_folder'] + &quot;Barcode/{barcode_numbers}.merged.fastq&quot;, barcode_numbers=return_barcode_folder_names())

rule basecall:
    input:
         config['results_folder'] + &quot;DataFiles/fast5/{fast5_file}.fast5&quot;
    output:
        directory(config['results_folder'] + &quot;Basecall/{fast5_file}&quot;)
    shell:
         r&quot;&quot;&quot;
         guppy_basecaller \
         --input_path {input} \
         --save_path {output} \
         --quiet \
         --config dna_r9.4.1_450bps_fast.cfg \
         --num_callers 2 \
         --cpu_threads_per_caller 6
         &quot;&quot;&quot;

rule barcode:
    input:
        config['results_folder'] + &quot;Basecall/{fast5_folders}&quot;
    output:
        directory(config['results_folder'] + &quot;.barcodeTempOutput/{fast5_folders}&quot;)
    threads: 12
    shell:
         r&quot;&quot;&quot;
         for item in {input}; do
                guppy_barcoder \
                --input_path $item \
                --save_path {output} \
                --barcode_kits EXP-PBC096 \
                --recursive
         done         
         &quot;&quot;&quot;

rule merge_files:
    input:
        expand(config['results_folder'] + &quot;.barcodeTempOutput/&quot; + &quot;{fast5_folder}/{barcode_numbers}&quot;,
               fast5_folder=glob_wildcards(config['results_folder'] + &quot;.barcodeTempOutput/{fast5_folders}/{barcode_numbers}/{fastq_files}.fastq&quot;).fast5_folders,
               barcode_numbers=glob_wildcards(config['results_folder'] +&quot;.barcodeTempOutput/{fast5_folders}/{barcode_numbers}/{fastq_files}.fastq&quot;).barcode_numbers)
    output:
        config['results_folder'] + &quot;Barcode/{barcode_numbers}.merged.fastq&quot;
    shell:
        r&quot;&quot;&quot;
        echo &quot;Hello world&quot;
        echo {input}
        &quot;&quot;&quot;


Under rule all, if I comment out the line that corresponds to merge files, there is no error
",-1,-1,-1.0
63198867,Errors when trying to set command-line values via snakemake profile file,"I'm trying to get the --profile argument to snakemake (version 5.20.0 running on Ubuntu 20.04) to work. I have a profile directory set up with a config.yaml file in it. If I put this into config.yaml:
verbose: 1

and run snakemake --profile xxx target, all goes well. However, if the config file contains
set-threads: &quot;trim=7 diamond_dna=5&quot;

snakemake complains:
MissingRuleException:
No rule to produce --set-threads=trim=7 diamond_dna=5 (if you use input functions make sure that they don't raise unexpected exceptions).

So it looks like an = is being put into the --set-threads argument, which snakemake then interprets as a target I want to make.  (I get the same thing if I use set-threads: &quot;'trim=7 diamond_dna=5'&quot; in case anyone is wondering - even though I don't think Python's argparse would handle that correctly, if it got that far).
If I put this into the config file:
verbose: 1
set-threads: &quot;trim=7 diamond_dna=5&quot;

I sometimes get
MissingRuleException:
No rule to produce --verbose (if you use input functions make sure that they don't raise unexpected exceptions).

but other times get
MissingRuleException:
No rule to produce --set-threads=trim=7 diamond_dna=5 (if you use input functions make sure that they don't raise unexpected exceptions).

Yes, the error changes for the exact same input config file. I guess snakemake is considering both --verbose and --set-threads=trim=7 diamond_dna=5 to be targets in both cases and (in some way) randomizing which one it decides to try to make first.
In any case, I am clearly doing something wrong or not understanding how --profile is supposed to work. Any help would be much appreciated!  I have seen https://github.com/snakemake-profiles/doc but it doesn't shed any light on this.
",1,-1,-1.0
63469189,Snakemake - can you access job properties from within the jobscript?,"The default snakemake jobscript looks like this:
#!/bin/sh
# properties = {properties}
{exec_job}

I have modified it a bit to print some useful information:
#!/bin/sh
# properties = {properties}
hostname=`hostname`
echo &quot;Running on $hostname&quot;
startTime=`date`
echo &quot;Start time: $startTime&quot;
{exec_job}
endTime=`date`
echo &quot;End time: $endTime&quot;

and I pass it through the --jobscript option when executing a workflow.
I'd like to add another piece of information - how many CPUs are used. How can I access this (and other) properties of the job? Maybe it has to do with the cryptic # properties = {properties} line? but how exactly are the properties used?
Would appreciate any hints, as the documentation is pretty vague on this.
Thanks!
",-1,1,-1.0
63591656,WildcardError - No values given for wildcard - Snakemake,"I am really lost what exactly I can do to fix this error.
I am running snakemake to perform some post alignment quality checks.
My code looks like this:
SAMPLES = [&quot;Exome_Tumor_sorted_mrkdup_bqsr&quot;, &quot;Exome_Norm_sorted_mrkdup_bqsr&quot;,
           &quot;WGS_Tumor_merged_sorted_mrkdup_bqsr&quot;, &quot;WGS_Norm_merged_sorted_mrkdup_bqsr&quot;]

rule all:
    input:
        expand(&quot;post-alignment-qc/flagstat/{sample}.txt&quot;, sample=SAMPLES),
        expand(&quot;post-alignment-qc/CollectInsertSizeMetics/{sample}.txt&quot;, sample=SAMPLES),
        expand(&quot;post-alignment-qc/CollectAlignmentSummaryMetrics/{sample}.txt&quot;, sample=SAMPLES),
        expand(&quot;post-alignment-qc/CollectGcBiasMetrics/{sample}_summary.txt&quot;, samples=SAMPLES) # this is the problem causing line

rule flagstat:
    input:
         bam = &quot;align/{sample}.bam&quot;
    output:
          &quot;post-alignment-qc/flagstat/{sample}.txt&quot;
    log:
       err='post-alignment-qc/logs/flagstat/{sample}_stderr.err'
    shell:
         &quot;samtools flagstat {input} &gt; {output} 2&gt; {log.err}&quot;


rule CollectInsertSizeMetics:
    input:
         bam = &quot;align/{sample}.bam&quot;
    output:
          txt=&quot;post-alignment-qc/CollectInsertSizeMetics/{sample}.txt&quot;,
          pdf=&quot;post-alignment-qc/CollectInsertSizeMetics/{sample}.pdf&quot;
    log:
       err='post-alignment-qc/logs/CollectInsertSizeMetics/{sample}_stderr.err',
       out='post-alignment-qc/logs/CollectInsertSizeMetics/{sample}_stdout.txt'

    shell:
         &quot;gatk CollectInsertSizeMetrics -I {input} -O {output.txt} -H {output.pdf} 2&gt; {log.err}&quot;

rule CollectAlignmentSummaryMetrics:
    input:
         bam = &quot;align/{sample}.bam&quot;,
         genome= &quot;references/genome/ref_genome.fa&quot;
    output:
          txt=&quot;post-alignment-qc/CollectAlignmentSummaryMetrics/{sample}.txt&quot;,
    log:
       err='post-alignment-qc/logs/CollectAlignmentSummaryMetrics/{sample}_stderr.err',
       out='post-alignment-qc/logs/CollectAlignmentSummaryMetrics/{sample}_stdout.txt'

    shell:
         &quot;gatk CollectAlignmentSummaryMetrics -I {input.bam} -O {output.txt} -R {input.genome} 2&gt; {log.err}&quot;

rule CollectGcBiasMetrics:
    input:
         bam = &quot;align/{sample}.bam&quot;,
         genome= &quot;references/genome/ref_genome.fa&quot;

    output:
          txt=&quot;post-alignment-qc/CollectGcBiasMetrics/{sample}_metrics.txt&quot;,
          CHART=&quot;post-alignment-qc/CollectGcBiasMetrics/{sample}_metrics.pdf&quot;,
          S=&quot;post-alignment-qc/CollectGcBiasMetrics/{sample}_summary.txt&quot;

    log:
       err='post-alignment-qc/logs/CollectGcBiasMetrics/{sample}_stderr.err',
       out='post-alignment-qc/logs/CollectGcBiasMetrics/{sample}_stdout.txt'

    shell:
         &quot;gatk CollectGcBiasMetrics -I {input.bam} -O {output.txt} -R {input.genome} -CHART = {output.CHART} &quot;
         &quot;-S {output.S} 2&gt; {log.err}&quot;


The error message says the following:
WildcardError in line 9 of Snakefile:
No values given for wildcard 'sample'.
  File &quot;Snakefile&quot;, line 9, in &lt;module&gt;

In my code above I have indicated the problem causing line. When I simply remove this line everything runs perfekt. I am really confused, because I pretty much copy and pasted each rule, and this is the only rule that causes any problems.
If someone could point out what I did wrong, I would be very thankful!
Cheers!
",-1,-1,-1.0
63708047,lambda wildcards not working in BWA MEM (or BWA-MEM2) Snakemake wrapper(s),"I'm in the process of porting Snakemake Shell: into Snakemake Wrappers and have noticed that the lambda wildcards:  I've successfully used for other wrappers is failing for the BWA MEM wrapper.
I've only managed to get the wrapper to work if it's hard-coded like:
input:
        reads=[&quot;trimming/trimmomatic/{sample}.1.fastq&quot;, &quot;trimming/trimmomatic/{sample}.2.fastq&quot;]

However, I would prefer the use of lambda wildcards: getTrims(wildcards.sample)[0] and lambda wildcards: getTrims(wildcards.sample)[1]; similar to the input for the trimming rule (below).
Minimal example
Snakefile
# Directories------------------------------------------------------------------
configfile: &quot;config.yaml&quot;

# Setting the names of all directories
dir_list = [&quot;REF_DIR&quot;, &quot;LOG_DIR&quot;, &quot;BENCHMARK_DIR&quot;, &quot;QC_DIR&quot;, &quot;TRIM_DIR&quot;, &quot;ALIGN_DIR&quot;, &quot;MARKDUP_DIR&quot;, &quot;CALLING_DIR&quot;, &quot;ANNOT_DIR&quot;]
dir_names = [&quot;refs&quot;, &quot;logs&quot;, &quot;benchmarks&quot;, &quot;qc&quot;, &quot;trimming&quot;, &quot;alignment&quot;, &quot;mark_duplicates&quot;, &quot;variant_calling&quot;, &quot;annotation&quot;]
dirs_dict = dict(zip(dir_list, dir_names))

import os
import pandas as pd
# getting the samples information (names, path to r1 &amp; r2) from samples.txt
samples_information = pd.read_csv(&quot;samples.txt&quot;, sep='\t', index_col=False)
# get a list of the sample names
sample_names = list(samples_information['sample'])
sample_locations = list(samples_information['location'])
samples_dict = dict(zip(sample_names, sample_locations))
# get number of samples
len_samples = len(sample_names)


# Rules -----------------------------------------------------------------------

rule all:
    input:
        expand('{TRIM_DIR}/{TRIM_TOOL}/{sample}_{pair}_trim_{paired}.fq.gz', TRIM_DIR=dirs_dict[&quot;TRIM_DIR&quot;], TRIM_TOOL=config[&quot;TRIM_TOOL&quot;], sample=sample_names, pair=['R1', 'R2'], paired=['paired', 'unpaired']),
        expand('{ALIGN_DIR}/{ALIGN_TOOL}/{sample}.bam', ALIGN_DIR=dirs_dict['ALIGN_DIR'], ALIGN_TOOL=config['ALIGN_TOOL'], sample=sample_names),

def getHome(sample):
  return(list(os.path.join(samples_dict[sample],&quot;{0}_{1}.fastq.gz&quot;.format(sample,pair)) for pair in ['R1','R2']))

rule trimming:
    input:
        r1 = lambda wildcards: getHome(wildcards.sample)[0],
        r2 = lambda wildcards: getHome(wildcards.sample)[1]
    output:
        r1 = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R1_trim_paired.fq.gz&quot;),
        r1_unpaired = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R1_trim_unpaired.fq.gz&quot;),        
        r2 = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R2_trim_paired.fq.gz&quot;),
        r2_unpaired = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R2_trim_unpaired.fq.gz&quot;)
    log: os.path.join(dirs_dict[&quot;LOG_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}.log&quot;)
    threads: 32
    params:
        # list of trimmers (see manual)
        trimmer=[&quot;MINLEN:36&quot;],
        # optional parameters
        extra=&quot;&quot;,
        compression_level=&quot;-9&quot;
    resources:
        mem = 1000,
        time = 120
    message: &quot;&quot;&quot;--- Trimming FASTQ files with Trimmomatic.&quot;&quot;&quot;
    wrapper:
        &quot;0.64.0/bio/trimmomatic/pe&quot;

trim_dir = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;])
trims_locations = [trim_dir] * len_samples
trims_dict = dict(zip(sample_names, trims_locations))

def getTrims(sample):
  return(list(os.path.join(trims_dict[sample],&quot;{0}_{1}_trim_paired.fq.gz&quot;.format(sample,pair)) for pair in ['R1','R2']))

rule alignment:
    input:
        reads=[&quot;trimming/trimmomatic/{sample}_R1_trim_paired.fq.gz&quot;, &quot;trimming/trimmomatic/{sample}_R2_trim_paired.fq.gz&quot;]
    output:
        os.path.join(dirs_dict[&quot;ALIGN_DIR&quot;],config[&quot;ALIGN_TOOL&quot;],&quot;{sample}.bam&quot;)
    log: os.path.join(dirs_dict[&quot;LOG_DIR&quot;],config[&quot;ALIGN_TOOL&quot;],&quot;{sample}.log&quot;)
    message: &quot;&quot;&quot;--- Alignment with BWA.&quot;&quot;&quot;
    threads: 8
    resources:
        mem = 2500,
        time = 100
    params:
        index=os.path.join(dirs_dict[&quot;REF_DIR&quot;], config[&quot;REF_GENOME&quot;]),
        extra=r&quot;-R '@RG\tID:{sample}\tPL:ILLUMINA\tSM:{sample}'&quot;,
        sort=&quot;none&quot;,             
        sort_order=&quot;queryname&quot;,  
        sort_extra=&quot;&quot;            
    wrapper:
        &quot;0.64.0/bio/bwa/mem&quot;

Config.yaml
# Files
REF_GENOME: &quot;c_elegans.PRJNA13758.WS265.genomic.fa&quot;
GENOME_ANNOTATION: &quot;c_elegans.PRJNA13758.WS265.annotations.gff3&quot;

# Tools
QC_TOOL: &quot;fastQC&quot;
TRIM_TOOL: &quot;trimmomatic&quot;
ALIGN_TOOL: &quot;bwa&quot;
MARKDUP_TOOL: &quot;picard&quot;
CALLING_TOOL: &quot;varscan&quot;
ANNOT_TOOL: &quot;vep&quot;

samples.txt
MTG325

This works:
(snakemake)$ snakemake -n -r 
Building DAG of jobs...
Job counts:
    count   jobs
    1   alignment
    1   all
    2

[Wed Sep  2 08:17:16 2020]
Job 2: --- Alignment with BWA.
Reason: Missing output files: alignment/bwa/MTG325.bam

[Wed Sep  2 08:17:16 2020]
localrule all:
    input: trimming/trimmomatic/MTG325_R1_trim_paired.fq.gz, trimming/trimmomatic/MTG325_R1_trim_unpaired.fq.gz, trimming/trimmomatic/MTG325_R2_trim_paired.fq.gz, trimming/trimmomatic/MTG325_R2_trim_unpaired.fq.gz, alignment/bwa/MTG325.bam
    jobid: 0
    reason: Input files updated by another job: alignment/bwa/MTG325.bam

Job counts:
    count   jobs
    1   alignment
    1   all
    2
This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.

This does not:
rule alignment:
    input:
        reads=[&quot;lambda wildcards: getTrims(wildcards.sample)[0]&quot;, &quot;lambda wildcards: getTrims(wildcards.sample)[1]&quot;]
    output:
        os.path.join(dirs_dict[&quot;ALIGN_DIR&quot;],config[&quot;ALIGN_TOOL&quot;],&quot;{sample}.bam&quot;)
    log: os.path.join(dirs_dict[&quot;LOG_DIR&quot;],config[&quot;ALIGN_TOOL&quot;],&quot;{sample}.log&quot;)
    message: &quot;&quot;&quot;--- Alignment with BWA.&quot;&quot;&quot;
    threads: 8
    resources:
        mem = 2500,
        time = 100
    params:
        index=os.path.join(dirs_dict[&quot;REF_DIR&quot;], config[&quot;REF_GENOME&quot;]),
        extra=r&quot;-R '@RG\tID:{sample}\tPL:ILLUMINA\tSM:{sample}'&quot;,
        sort=&quot;none&quot;,
        sort_order=&quot;queryname&quot;,
        sort_extra=&quot;&quot;
    wrapper:
        &quot;0.64.0/bio/bwa/mem&quot;

Results in:
(snakemake) [moldach@arc wrappers]$ snakemake -n -r
Building DAG of jobs...
MissingInputException in line 65 of /home/moldach/wrappers/Snakefile:
Missing input files for rule alignment:
lambda wildcards: getTrims(wildcards.sample)[1]
lambda wildcards: getTrims(wildcards.sample)[0]

",-1,-1,-1.0
63723343,"Using ""snakemake --config"" to pass a list of strings","I'm having issues trying to pass a list of two strings to my Snakemake workflow via the --config parameter.
My config.yaml contains a list variable with two strings.
illumina_input: [&quot;sample1/forward.fastq&quot;, &quot;sample1/reverse.fastq&quot;]
This works properly when I use Snakemake with changing the values in this YAML file.
Now I would like to run a batch of multiple Snakemake runs using a shell script, each providing different Illumina paired-end reads to use. I'm currently testing on one rule only.
Example:
snakemake --config \
outdir=testoutputfolder/ \
illumina_input=[&quot;sample1/forward.fastq&quot;,&quot;sample1/reverse.fastq&quot;] -r short_read_trimming

snakemake --config \
outdir=testoutputfolder/ \
illumina_input=[&quot;sample2/forward.fastq&quot;,&quot;sample2/reverse.fastq&quot;] -r short_read_trimming

Part of Snakefile
rule all:
    input:
        config[&quot;outdir&quot;] + &quot;reference-data-shortreads/trimmed/illumina1_trimmed_paired.fq&quot;, config[&quot;outdir&quot;] + &quot;reference-data-shortreads/trimmed/illumina2_trimmed_paired.fq&quot;

rule short_read_trimming:
    input:
        config[&quot;illumina_input&quot;]
    output:
        [config[&quot;outdir&quot;] + &quot;reference-data-shortreads/trimmed/illumina1_trimmed_paired.fq&quot;,
        config[&quot;outdir&quot;] + &quot;reference-data-shortreads/trimmed/illumina2_trimmed_paired.fq&quot;]
    shell:
        &quot;java -jar {config[trimmomatic_loc]} PE {input[0]} {input[1]} \
        {config[outdir]}reference-data-shortreads/trimmed/illumina1_trimmed_paired.fq {config[outdir]}reference-data-shortreads/trimmed/illumina1_trimmed_unpaired.fq \
        {config[outdir]}reference-data-shortreads/trimmed/illumina2_trimmed_paired.fq {config[outdir]}reference-data-shortreads/trimmed/illumina2_trimmed_unpaired.fq \
        {config[trimmomatic_params]}&quot; 

When I try to run this code I receive the following error
Invalid config definition: Config entries have to be defined as name=value pairs.

I can't seem to figure out how to provide lists to the Snakemake configuration via the command line. I've tried to remove the brackets, comma's and replace them with whitespace but nothing seems to work.
Anyone got any ideas?
",-1,-1,-1.0
63724513,Snakemake ancient tag with wildcards,"I have few SRA files which I download from NCBI website. Now I want to add them to my snakemake workflow. However, I want to retain ability to download them with prefetch if they are not available. I had following simple rule,
BASE = &quot;/path/to/working/folder&quot;

rule all:
    input: [f&quot;{BASE}/fastq/SRR000001.sra_1.fastq&quot;, f&quot;{BASE}/fastq/SRR000001.sra_2.fastq&quot;]
    shell:
        &quot;echo Finished&quot;

rule get_sra:
    input: ancient(&quot;config/config.yaml&quot;)
    output:&quot;{BASE_FOLDER}/sra/{SSR_ID}.sra&quot;
    shell:
         &quot;prefetch -p {wildcards.SSR_ID} --output-file {output} &quot;

rule get_fastq:
    input: expand(&quot;{folder}/sra/{srr}.sra&quot;, folder=BASE, srr=&quot;{SRR_ID}&quot;)
    output:
          expand(&quot;{folder}/fastq/{srr}.sra_{i}.fastq&quot;, folder=BASE,
                 srr=&quot;{SRR_ID}&quot;, i=[1, 2])
    shell:
         &quot;fasterq-dump {input} --outdir {BASE}/fastq&quot;

         

If I use above rule, my workflow will recreate my SRA files as their timestamp will be older. However, I do not want to download full SRA file again from the server and use the already downloaded one.
For this purpose I am trying to use the ancient tag. But I am not able to use this tag with any of the wildcards.
input: ancient(&quot;{BASE_FOLDER}/sra/{SSR_ID}.sra&quot;)

Above rule gives error as

Wildcards in input files cannot be determined from output files:

Any solution to this problem? This also does not work when I use expand.
",-1,-1,-1.0
63766652,snakemake wildcard in input files,"I am very new to snakemake and I am trying to create a merged.fastq for each sample. Following is my Snakefile.
configfile: &quot;config.yaml&quot;
print(config['samples'])
print(config['ss_files'])
print(config['pass_files'])

rule all:
    input:
        expand(&quot;{sample}/data/genome_assembly/medaka/medaka.fasta&quot;, sample=config[&quot;samples&quot;]),
        expand(&quot;{pass_file}&quot;, pass_file=config[&quot;pass_files&quot;]),
        expand(&quot;{ss_file}&quot;, ss_file=config[&quot;ss_files&quot;]) 

rule merge_fastq:
    input: 
        directory(&quot;{pass_file}&quot;)
    output: 
        &quot;{sample}/data/merged.fastq.gz&quot;
    wildcard_constraints:
        id=&quot;*.fastq.gz&quot;
    shell:
        &quot;cat {input}/{id} &gt; {output}&quot;   

where, 'samples' is a list of sample names,
'pass_files' is a list of directory path to fastq_pass folder which contains small fastq files
I am trying to merge small fastq files to a large merged.fastq for each sample.
I am getting the following,

Wildcards in input files cannot be determined from output files:
'pass_file'

as the error.
",-1,-1,-1.0
63778054,Execute a Snakemake workflow via Tibanna on AWS,"I am trying to execute the workflow of the Snakemake's official tutorial via Tibanna on AWS.
As instructed here,

I have installed Tibanna and set up environment variables.
Then I deployed Tibanna Unicorn to a folder snakemake-tutorial in a specific S3 bucket specific-bucket.
I set up the default unicorn.
As a last step, I run the following command:

$ snakemake --tibanna --default-remote-prefix=specific-bucket/snakemake-tutorial

However, I get an error at bwa_map rule. And the log says that:
/bin/bash: bwa: command not found
/bin/bash: samtools: command not found

For some reason, I am not able to use conda and/or packages.
",-1,-1,-1.0
63799148,Snakemake gives InputFunctionException when using --profile slurm,"I'm creating a pipeline using snakemake to call methylation in nanopore sequencing data. I've run snakenake using the --dryrun option and the dag is constructed successfully. But when I add the option --profile slurm, I get the following error:
(nanopolish) [danielle.perley@talonhead2 nanopolish-CpG-calling]$ snakemake -np --use-conda --profile slurm test_data/20-001-002/20-001-002_fastq_pass.gz

Building DAG of jobs...
Job counts:
    count   jobs
    1   combine_tech_reps
    1
InputFunctionException in line 32 of /home/danielle.perley/nanopolish-CpG-calling/Snakefile:
Error:
  SyntaxError: invalid syntax (&lt;string&gt;, line 1)
Wildcards:
  sample=20-001-002
Traceback:

  File &quot;/home/danielle.perley/miniconda3/envs/nanopolish/lib/python3.7/site-packages/snakemake/executors/__init__.py&quot;, line 115, in run_jobs
  File &quot;/home/danielle.perley/miniconda3/envs/nanopolish/lib/python3.7/site-packages/snakemake/executors/__init__.py&quot;, line 120, in run
  File &quot;/home/danielle.perley/miniconda3/envs/nanopolish/lib/python3.7/site-packages/snakemake/executors/__init__.py&quot;, line 131, in _run
  File &quot;/home/danielle.perley/miniconda3/envs/nanopolish/lib/python3.7/site-packages/snakemake/executors/__init__.py&quot;, line 151, in printjob
  File &quot;/home/danielle.perley/miniconda3/envs/nanopolish/lib/python3.7/site-packages/snakemake/executors/__init__.py&quot;, line 137, in printjob

Line 33 is rule combine_tech_reps in my snakefile. (I'm only showing the first part of my snakefile here)
from snakemake.utils import validate
import pandas as pd
import os.path
import glob

configfile: &quot;config.yaml&quot;

samples_df = pd.read_table(config[&quot;samples&quot;],sep = '\t')
samples_df = samples_df.set_index(&quot;Sample&quot;)
samples = list(samples_df.index.unique())


wildcard_constraints:
    sample = &quot;|&quot;.join(samples)
         
def get_fast5(wildcards):
      
    f5 = glob.glob(os.path.join(config[&quot;raw_data&quot;],wildcards.sample,&quot;2*&quot;,&quot;fast5_pass&quot;))
    return(f5)

localrules: all,build_index

rule all:
    input: 
        expand(&quot;results/Methylation/{sample}_frequency.tsv&quot;,sample=samples),
        expand(&quot;results/alignments/{sample}_flagstat.txt&quot;,sample=samples),
        expand(&quot;resources/QC/{sample}_pycoQC.json&quot;,sample=samples),
        expand(&quot;results/QC/{sample}_pycoQC.html&quot;,sample=samples),
        &quot;report/multiQC.html&quot;


rule combine_tech_reps:
    input:
        fqs = lambda wildcards: glob.glob(os.path.join(config[&quot;raw_data&quot;],&quot;{sample}&quot;,&quot;2*&quot;,&quot;{sample}_fastq_pass.gz&quot;).format(sample=wildcards.sample))

    output:
        fq = os.path.join(config[&quot;raw_data&quot;],&quot;{sample}&quot;,&quot;{sample}_fastq_pass.gz&quot;)

    shell: &quot;&quot;&quot;
        zcat {input} &gt; {output}
    &quot;&quot;&quot;

I have a slurm profile file in the directory:
~/.config/snakemake/slurm/config.yaml
jobs: 10
cluster: &quot;sbatch -p talon -t {resources.time} --mem={resources.mem} -c {resources.cpus} -o logs_slurm/{rule}_{wildcards} -e logs_slurm/{rule}_{wildcards}&quot;
default-resources: [cpus=1, mem=2000, time=10:00]
use-conda: true


I'd really like to use this pipeline on our HPC, but I'm not sure what's causing this error.
",-1,-1,-1.0
63886816,A number of Trimmomatic trimming parameters not working in Snakemake wrapper,"Previously my Trimmomatic shell command included the following trimmers:

ILLUMINACLIP:adapters.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36

For the Snakemake wrapper for Trimmomatic only LEADING:3 and MINLEN:36 work in the params: trimmer:
rule trimming:
    input:
        r1 = lambda wildcards: getHome(wildcards.sample)[0],
        r2 = lambda wildcards: getHome(wildcards.sample)[1]
    output:
        r1 = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R1_trim_paired.fastq.gz&quot;),
        r1_unpaired = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R1_trim_unpaired.fastq.gz&quot;),
        r2 = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R2_trim_paired.fastq.gz&quot;),
        r2_unpaired = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R2_trim_unpaired.fastq.gz&quot;)
    log: os.path.join(dirs_dict[&quot;LOG_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}.log&quot;)
    threads: 32
    params:
    # list of trimmers (see manual)
        trimmer=[&quot;LEADING:3&quot;, &quot;MINLEN:36&quot;],
        # optional parameters
        extra=&quot;&quot;,
        compression_level=&quot;-9&quot;
    resources:
    mem = 1000,
        time = 120
    message: &quot;&quot;&quot;--- Trimming FASTQ files with Trimmomatic.&quot;&quot;&quot;
    wrapper:
    &quot;0.64.0/bio/trimmomatic/pe&quot;

When trying to use any of the other parameters (ILLUMINACLIP:adapters.fa:2:30:10 TRAILING:3 SLIDINGWINDOW:4:15) it fails.
For example, trying only TRAILING:3:
rule trimming:
    input:
        r1 = lambda wildcards: getHome(wildcards.sample)[0],
        r2 = lambda wildcards: getHome(wildcards.sample)[1]
    output:
        r1 = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R1_trim_paired.fastq.gz&quot;),
        r1_unpaired = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R1_trim_unpaired.fastq.gz&quot;),
        r2 = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R2_trim_paired.fastq.gz&quot;),
        r2_unpaired = os.path.join(dirs_dict[&quot;TRIM_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}_R2_trim_unpaired.fastq.gz&quot;)
    log: os.path.join(dirs_dict[&quot;LOG_DIR&quot;],config[&quot;TRIM_TOOL&quot;],&quot;{sample}.log&quot;)
    threads: 32
    params:
    # list of trimmers (see manual)
        trimmer=[&quot;TRAILING:3&quot;],
        # optional parameters
        extra=&quot;&quot;,
        compression_level=&quot;-9&quot;
    resources:
    mem = 1000,
        time = 120
    message: &quot;&quot;&quot;--- Trimming FASTQ files with Trimmomatic.&quot;&quot;&quot;
    wrapper:
    &quot;0.64.0/bio/trimmomatic/pe&quot;

Results in the following error:
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
        1   qc_before_align_r1
        1

[Mon Sep 14 13:42:08 2020]
Job 0: --- Quality check of raw data with FastQC before alignment.

Activating conda environment: /home/moldach/wrappers/.snakemake/conda/975fb1fd
Activating conda environment: /home/moldach/wrappers/.snakemake/conda/975fb1fd
Skipping ' 2&gt; logs/fastqc/before_align/MTG324_R1.log' which didn't exist, or couldn't be read
Failed to process file MTG324_R1_trim_paired.fastq.gz
uk.ac.babraham.FastQC.Sequence.SequenceFormatException: Ran out of data in the middle of a fastq entry.  Your file is probably truncated
        at uk.ac.babraham.FastQC.Sequence.FastQFile.readNext(FastQFile.java:179)
        at uk.ac.babraham.FastQC.Sequence.FastQFile.next(FastQFile.java:125)
        at uk.ac.babraham.FastQC.Analysis.AnalysisRunner.run(AnalysisRunner.java:77)
        at java.base/java.lang.Thread.run(Thread.java:834)
mv: cannot stat ‘/tmp/tmpsnncjthh/MTG324_R1_trim_paired_fastqc.html’: No such file or directory
Traceback (most recent call last):
  File &quot;/home/moldach/wrappers/.snakemake/scripts/tmpp34b98yj.wrapper.py&quot;, line 47, in &lt;module&gt;
    shell(&quot;mv {html_path:q} {snakemake.output.html:q}&quot;)
  File &quot;/home/moldach/anaconda3/envs/snakemake/lib/python3.7/site-packages/snakemake/shell.py&quot;, line 205, in __new__
    raise sp.CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command 'set -euo pipefail;  mv /tmp/tmpsnncjthh/MTG324_R1_trim_paired_fastqc.html qc/fastQC/before_align/MTG324_R1_trim_paired_fastqc.html' returned non-zero exit status $
[Mon Sep 14 13:45:16 2020]
Error in rule qc_before_align_r1:
    jobid: 0
    output: qc/fastQC/before_align/MTG324_R1_trim_paired_fastqc.html, qc/fastQC/before_align/MTG324_R1_trim_paired_fastqc.zip
    log: logs/fastqc/before_align/MTG324_R1.log (check log file(s) for error message)
    conda-env: /home/moldach/wrappers/.snakemake/conda/975fb1fd

RuleException:
CalledProcessError in line 181 of /home/moldach/wrappers/Trim:
Command 'source /home/moldach/anaconda3/bin/activate '/home/moldach/wrappers/.snakemake/conda/975fb1fd'; set -euo pipefail;  python /home/moldach/wrappers/.snakemake/scripts/tmpp34b98yj.wrapper.py' retu$
  File &quot;/home/moldach/anaconda3/envs/snakemake/lib/python3.7/site-packages/snakemake/executors/__init__.py&quot;, line 2189, in run_wrapper
  File &quot;/home/moldach/wrappers/Trim&quot;, line 181, in __rule_qc_before_align_r1
  File &quot;/home/moldach/anaconda3/envs/snakemake/lib/python3.7/site-packages/snakemake/executors/__init__.py&quot;, line 529, in _callback
  File &quot;/home/moldach/anaconda3/envs/snakemake/lib/python3.7/concurrent/futures/thread.py&quot;, line 57, in run
  File &quot;/home/moldach/anaconda3/envs/snakemake/lib/python3.7/site-packages/snakemake/executors/__init__.py&quot;, line 515, in cached_or_run
  File &quot;/home/moldach/anaconda3/envs/snakemake/lib/python3.7/site-packages/snakemake/executors/__init__.py&quot;, line 2201, in run_wrapper
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

",-1,-1,-1.0
64284105,How to create a Docker container for Snakemake,"This is my first attempt at creating a Docker container.
I need to make a container for a legacy version of VarScan2 so I based much of it off a Dockerfile found for a newer version of VarScan2
I got an error trying to run the first container I built hinting that it may not be running because Snakemake.utils was not available. Therefore I think I need to install Snakemake in my container for which I need &gt;= Python3.5.
I'm having trouble trying to get Python3.8 as Snakemake is failing to install on docker build:
Error:
Downloading snakemake-5.26.1.tar.gz (237 kB)
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'/tmp/pip-install-_50ix5/snakemake/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'/tmp/pip-install-_50ix5/snakemake/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\r\n'&quot;'&quot;', '&quot;'&quot;'\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base /tmp/pip-pip-egg-info-AMGZ1t
         cwd: /tmp/pip-install-_50ix5/snakemake/
    Complete output (2 lines):
    At least Python 3.5 is required.
    
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.

Dockerfile
FROM ubuntu:14.04

MAINTAINER Matthew Jordan Oldach, moldach686@gmail.com

# create a working directory and work from there
RUN mkdir /tmp/install
WORKDIR /tmp/install

RUN apt-get update &amp;&amp; apt-get install -y \
        build-essential \
        libncurses5-dev \
        libgdbm-dev \
        libnss3-dev \
        libssl-dev \
        libreadline-dev \
        libffi-dev \
        gcc \
        make \
        zlib1g-dev \
        git \
        wget \
        python3-pip \
        default-jre \
        r-base \
        bc

# [Download Python3.7.5 following these instructions][2]
RUN wget https://www.python.org/ftp/python/3.7.5/Python-3.7.5.tgz
RUN tar -xf Python-3.7.5.tgz
RUN cd python-3.7.5; ./configure --enable-optimizations; cd ..

# Download Snakemake
RUN wget https://bootstrap.pypa.io/get-pip.py
RUN python get-pip.py
RUN pip install snakemake

# Samtools 0.1.18 - note: 0.1.19 and 1.1 do NOT work, VarScan copynumber dies on the mpileup
RUN wget http://downloads.sourceforge.net/project/samtools/samtools/0.1.18/samtools-0.1.18.tar.bz2
RUN tar -xvf samtools-0.1.18.tar.bz2
# the make command generates a lot of warnings, none of them relevant to the final samtools code, hence 2&gt;/dev/null
#RUN (cd samtools-0.1.18/ &amp;&amp; make DFLAGS='-D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE -D_USE_KNETFILE -D_CURSES_LIB=0' LIBCURSES='' 2&gt;/dev/null &amp;&amp; mv samtools /usr/local/bin)

# get varscan 
RUN wget  -O /usr/local/bin/VarScan.jar https://netactuate.dl.sourceforge.net/project/varscan/VarScan.v2.3.9.jar
# Set WORKDIR to /data -- predefined mount location.
RUN mkdir /data
WORKDIR /data

# And clean up
RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/install

ENTRYPOINT [&quot;bash&quot;, &quot;java -jar /usr/local/bin/VarScan.jar&quot;]

I'd appreciate if anyone could help me figure out what's going wrong here, thanks.
Update
The solution by OneCricketeer solved this solution.
Dockerfile Solution
FROM python:3.7.5

MAINTAINER Matthew Jordan Oldach, moldach686@gmail.com

# create a working directory and work from there
RUN mkdir /tmp/install
WORKDIR /tmp/install

RUN apt-get update &amp;&amp; apt-get install -y \
        gcc \
        make \
        zlib1g-dev \
        git \
        wget \
        python3-pip \
        default-jre \
        r-base \
        bc

# Download Snakemake
RUN wget https://bootstrap.pypa.io/get-pip.py
RUN python get-pip.py
RUN pip install snakemake

# Samtools 0.1.18 - note: 0.1.19 and 1.1 do NOT work, VarScan copynumber dies on the mpileup
RUN wget http://downloads.sourceforge.net/project/samtools/samtools/0.1.18/samtools-0.1.18.tar.bz2
RUN tar -xvf samtools-0.1.18.tar.bz2
# the make command generates a lot of warnings, none of them relevant to the final samtools code, hence 2&gt;/dev/null
#RUN (cd samtools-0.1.18/ &amp;&amp; make DFLAGS='-D_FILE_OFFSET_BITS=64 -D_LARGEFILE64_SOURCE -D_USE_KNETFILE -D_CURSES_LIB=0' LIBCURSES='' 2&gt;/dev/null &amp;&amp; mv samtools /usr/local/bin)

# get varscan 
RUN wget  -O /usr/local/bin/VarScan.jar https://netactuate.dl.sourceforge.net/project/varscan/VarScan.v2.3.9.jar
# Set WORKDIR to /data -- predefined mount location.
RUN mkdir /data
WORKDIR /data

# And clean up
RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/install

ENTRYPOINT [&quot;bash&quot;, &quot;java -jar /usr/local/bin/VarScan.jar&quot;]

",1,-1,-1.0
64340217,Converting a large pipeline to snakemake,"I have developed MOSCA, a pipeline for meta-omics analysis (MG with MT), which is available through Bioconda. I want to convert it to snakemake, since it would easily allow MOSCA to run simultaneously some accesses through APIs and some computationally demanding tasks. Also, I think it would help better shape the tool to a standard format.
My question is, MOSCA has a lot of parameters, which will have to be transfered to a configuration file. While this is trivial for most parameters, inputting MG and MT files together is trickier. Also, MOSCA considers samples together. So I created a samples' file, samples.tsv
MG files    MT files    Sample
path/to/mg_R1.fastq,path/to/mg_R2.fastq path/to/mt_R1.fastq,path/to/mt_R2.fastq Sample

and I want the Snakefile to read it and retain the &quot;Sample&quot; information. Follows the example for including the MG preprocessing and the assembly of the pipeline, where the preprocess.py and assembly.py scripts contain the corresponding functionalities.
This is the Snakefile
from pandas import read_table

configfile: &quot;config.json&quot;

rule all:
  input:
    expand(&quot;{output}/Assembly/{experiment[1][Sample]}/contigs.fasta&quot;, 
            output = config[&quot;output&quot;], experiment = read_table(config[&quot;experiments&quot;], &quot;\t&quot;).iterrows())

rule mg_preprocess:             # mt_preprocess make same, but data = mrna?
  input:
    list(expand({experiment}[1][&quot;MG files&quot;], experiment = read_table(config[&quot;experiments&quot;], &quot;\t&quot;).iterrows()))[0]
  output:
    expand(&quot;{output}/Preprocess/Trimmomatic/quality_trimmed_{name}{fr}_paired.fq&quot;, 
            fr = ['forward', 'reverse'], output = config[&quot;output&quot;], name = 'pretty_commune')
  threads: 
    config[&quot;threads&quot;]
  run:
    shell(&quot;&quot;&quot;python preprocess.py -i {reads} -t {threads} -o {output} 
            -adaptdir MOSCA/Databases/illumina_adapters -rrnadbs MOSCA/Databases/rRNA_databases&quot;&quot;&quot;, 
            output = config[&quot;output&quot;])

rule assembly:
  input:
    expand(&quot;{output}/Preprocess/Trimmomatic/quality_trimmed_{name}{fr}_paired.fq&quot;, 
            fr = ['forward', 'reverse'], output = config[&quot;output&quot;], name = 'pretty_commune')
  output:
    expand(&quot;{output}/Assembly/{experiment[1][Sample]}/contigs.fasta&quot;, 
            output = config[&quot;output&quot;], experiment = read_table(config[&quot;experiments&quot;], &quot;\t&quot;).iterrows())
  threads:
    config[&quot;threads&quot;]
  run:
    reads = &quot;,&quot;.join(map(str, input))
    shell(&quot;python assembly.py -r {input} -t {threads} -o {output}/Assembly/{sample} -a {assembler}&quot;, 
    output = config[&quot;output&quot;], sample = config[&quot;experiments&quot;][1][&quot;sample&quot;], 
    assembler = config[&quot;assembler&quot;])

and this is the config.json
{
&quot;output&quot;: 
    &quot;snakemake_learn&quot;,
&quot;threads&quot;:
    14,
&quot;experiments&quot;:
    &quot;samples.tsv&quot;,
&quot;assembler&quot;:
    &quot;metaspades&quot;
    }

and I get the error
NameError in line 12 of /path/to/Snakefile:
name 'experiment' is not defined
  File &quot;/path/to/Snakefile&quot;, line 12, in &lt;module&gt;

How is experiment not defined?
",1,-1,-1.0
64043503,How to define key=var arguments in a snakemake profile yaml file,"Some of the snakemake arguments follow the key=value pattern. I would like to add them to the snakemake profile yaml file.
for example, the command line arguments should be stored in the snakemake profile.
 --default-resources mem=50 time=5
I tried the two following options but it didn't work:

default-resources: &quot;mem=50 time=5&quot;


default-resources: 
  mem: 50
  time: 5

Problem continues for snakemake &gt; 6.3 https://github.com/snakemake/snakemake/issues/1186
",-1,-1,-1.0
63400957,combine to outputs of diffrent rules in Snakemake,"I would like to use snakemake to first merge some files and than later process other files based on that merge. (Less abstract: I want to combine control IGG bam files of two different sets and than use those to perform peakcalling on other files.
In a minimal example, the folder structure would look like this.
├── data
│   ├── toBeMerged
│   │   ├── singleA
│   │   ├── singleB
│   │   ├── singleC
│   │   └── singleD
│   └── toBeProcessed
│       ├── NotProcess1
│       ├── NotProcess2
│       ├── NotProcess3
│       ├── NotProcess4
│       └── NotProcess5
├── merge.cfg
├── output
│   ├── mergeAB_merge
│   ├── mergeCD_merge
│   ├── NotProcess1_processed
│   ├── NotProcess2_processed
│   ├── NotProcess3_processed
│   ├── NotProcess4_processed
│   └── NotProcess5_processed
├── process.cfg
└── Snakefile

Which files are combined and which are processed are defined in two config files.
merge.cfg
singlePath  controlName
data/toBeMerged/singleA output/controlAB
data/toBeMerged/singleB output/controlAB
data/toBeMerged/singleC output/controlCD
data/toBeMerged/singleD output/controlCD

and process.cfg
controlName inName
output/controlAB    data/toBeProcessed/NotProcess1
output/controlAB    data/toBeProcessed/NotProcess2
output/controlCD    data/toBeProcessed/NotProcess3
output/controlCD    data/toBeProcessed/NotProcess4
output/controlAB    data/toBeProcessed/NotProcess5

I am currently stuck with a snakefile like this, which itself does not work and gives me the error that both rules are ambiguous. And even if I would get it to work, I suspect, that this not the &quot;correct&quot; way, since the  process rule, should have {mergeName} as input to build its dag. But this does not work, since then I would need two wildcarts in one rule.
import pandas as pd
cfgMerge = pd.read_table(&quot;merge.cfg&quot;).set_index(&quot;controlName&quot;, drop=False)
cfgProc= pd.read_table(&quot;process.cfg&quot;).set_index(&quot;inName&quot;, drop=False)


rule all:
    input:
        expand('{mergeName}', mergeName= cfgMerge.controlName),
        expand('{rawName}_processed', rawName= cfgProc.inName)

rule merge:
    input:
        lambda wc: cfgMerge[cfgMerge.controlName == wc.mergeName].singlePath
    output:
        &quot;{mergeName}&quot;
    shell:
        &quot;cat {input} &gt; {output}&quot;

rule process:
    input:
        inMerge=lambda wc: cfgProc[cfgProc.inName == wc.rawName].controlName.iloc[0],
        Name=lambda wc: cfgProc[cfgProc.inName == wc.rawName].inName.iloc[0]
    output:
        '{rawName}_processed'
    shell:
    &quot;cat {input.inMerge} {input.Name} &gt; {output}&quot;

I guess the key problem is how to use the output of a rule as the input for another one, when it does not depend on the same wildcard, or includes other another wildcard.
",-1,1,-1.0
63364143,Problems with the VEP snakemake wrapper,"I'm experiencing two issues trying to run the VEP wrapper for snakemake.
The first is that I would like to use lambda wildcards in calls like so:
calling_dir = os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;])
callings_locations = [calling_dir] * len_samples
callings_dict = dict(zip(sample_names, callings_locations))

def getVCFs(sample):
  return(list(os.path.join(callings_dict[sample],&quot;{0}_sorted_dedupped_snp_varscan.vcf&quot;.format(sample,pair)) for pair in ['']))

rule variant_annotation:
    input:
        calls= lambda wildcards: getVCFs(wildcards.sample),
        cache=&quot;resources/vep/cache&quot;,
        plugins=&quot;resources/vep/plugins&quot;,
    output:
        calls=&quot;variants.annotated.vcf&quot;,
        stats=&quot;variants.html&quot;
    params:
        plugins=[&quot;LoFtool&quot;],
        extra=&quot;--everything&quot;
    message: &quot;&quot;&quot;--- Annotating Variants.&quot;&quot;&quot;
    resources:
        mem = 30000,
        time = 120
    threads: 4
    wrapper:
        &quot;0.64.0/bio/vep/annotate&quot;

However, I get an error:
When I replace lambda wildcards with a calls= expand('{CALLING_DIR}/{CALLING_TOOL}/{sample}_sorted_dedupped_snp_varscan.vcf', CALLING_DIR=dirs_dict[&quot;CALLING_DIR&quot;], CALLING_TOOL=config[&quot;CALLING_TOOL&quot;], sample=sample_names) ([which is not ideal - see this post for reason][1]) it give me errors about resources folder?
(snakemake) [moldach@cedar1 MTG353]$ snakemake -n -r
Building DAG of jobs...
MissingInputException in line 333 of /scratch/moldach/MADDOG/VCF-FILES/biostars439754/MTG353/Snakefile:
Missing input files for rule variant_annotation:
resources/vep/cache
resources/vep/plugins

I'm also [confused from the documentation as to how it knows which reference genome (version, _etc.) should be specified][2].
UPDATE:
Because of the character limit I cannot even respond to the two respondents so I will continue the issue here:
As @jafors mentioned the two wrappers solved the issue for cache and plugins - thanks!
Now I get an error from trying to run VEP though from the following rule:
rule variant_annotation:
    input:
        calls= expand('{CALLING_DIR}/{CALLING_TOOL}/{sample}_sorted_dedupped_snp_varscan.vcf', CALLING_DIR=dirs_dict[&quot;CALLING_DIR&quot;], CALLING_TOOL=config[&quot;CALLING_TOOL&quot;], sample=sample_names),
        cache=&quot;resources/vep/cache&quot;,
        plugins=&quot;resources/vep/plugins&quot;,
    output:
        calls=expand('{ANNOT_DIR}/{ANNOT_TOOL}/{sample}.annotated.vcf', ANNOT_DIR=dirs_dict[&quot;ANNOT_DIR&quot;], ANNOT_TOOL=config[&quot;ANNOT_TOOL&quot;], sample=sample_names),
        stats=expand('{ANNOT_DIR}/{ANNOT_TOOL}/{sample}.html', ANNOT_DIR=dirs_dict[&quot;ANNOT_DIR&quot;], ANNOT_TOOL=config[&quot;ANNOT_TOOL&quot;], sample=sample_names)
    params:
        plugins=[&quot;LoFtool&quot;],
        extra=&quot;--everything&quot;
    message: &quot;&quot;&quot;--- Annotating Variants.&quot;&quot;&quot;
    resources:
        mem = 30000,
        time = 120
    threads: 4
    wrapper:
        &quot;0.64.0/bio/vep/annotate&quot;

this is the error I get from the log:
Building DAG of jobs...
Using shell: /cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       variant_annotation
        1

[Wed Aug 12 20:22:49 2020]
Job 0: --- Annotating Variants.

Activating conda environment: /scratch/moldach/MADDOG/VCF-FILES/biostars439754/.snakemake/conda/f16fdb5f
Traceback (most recent call last):
  File &quot;/scratch/moldach/MADDOG/VCF-FILES/biostars439754/.snakemake/scripts/tmpwx1u_776.wrapper.py&quot;, line 36, in &lt;module&gt;
    if snakemake.output.calls.endswith(&quot;.vcf.gz&quot;):
AttributeError: 'Namedlist' object has no attribute 'endswith'
[Wed Aug 12 20:22:53 2020]
Error in rule variant_annotation:
    jobid: 0
    output: ANNOTATION/VEP/BC1217.annotated.vcf, ANNOTATION/VEP/470.annotated.vcf, ANNOTATION/VEP/MTG109.annotated.vcf, ANNOTATION/VEP/BC1217.html, ANNOTATION/VEP/470.html, ANNOTATION/VEP/MTG$
    conda-env: /scratch/moldach/MADDOG/VCF-FILES/biostars439754/.snakemake/conda/f16fdb5f

RuleException:
CalledProcessError in line 393 of /scratch/moldach/MADDOG/VCF-FILES/biostars439754/Snakefile:
Command 'source /home/moldach/miniconda3/bin/activate '/scratch/moldach/MADDOG/VCF-FILES/biostars439754/.snakemake/conda/f16fdb5f'; set -euo pipefail;  python /scratch/moldach/MADDOG/VCF-FILE$
  File &quot;/scratch/moldach/MADDOG/VCF-FILES/biostars439754/Snakefile&quot;, line 393, in __rule_variant_annotation
  File &quot;/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.8.0/lib/python3.8/concurrent/futures/thread.py&quot;, line 57, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

TO BE CLEAR:
This is the code I had running VEP prior to trying out the wrapper so I would like to preserve similar options (e.g. offline, etc.):
vep \
        -i {input.sample} \
        --species &quot;caenorhabditis_elegans&quot; \
        --format &quot;vcf&quot; \
        --everything \
        --cache_version 100 \
        --offline \
        --force_overwrite \
        --fasta {input.ref} \
        --gff {input.annot} \
        --tab \
        --variant_class \
        --regulatory \
        --show_ref_allele \
        --numbers \
        --symbol \
        --protein \
        -o {params.sample}

UPDATE 2:
Yes the use of expand() was the issue. I remember this is why I like to use lambda or os.path.join() as rule input/output except for as you mentioned in rule all:
The following seems to get rid of that problem although I'm met with a new one:
rule variant_annotation:
    input:
        calls= lambda wildcards: getVCFs(wildcards.sample),
        cache=&quot;resources/vep/cache&quot;,
        plugins=&quot;resources/vep/plugins&quot;,
    output:
        calls=os.path.join(dirs_dict[&quot;ANNOT_DIR&quot;],config[&quot;ANNOT_TOOL&quot;],&quot;{sample}.annotated.vcf&quot;),
        stats=os.path.join(dirs_dict[&quot;ANNOT_DIR&quot;],config[&quot;ANNOT_TOOL&quot;],&quot;{sample}.html&quot;)

Not sure why I get the unknown file type error - as I mentioned this was first tested out with the full command with the same input data?
Activating conda environment: /scratch/moldach/MADDOG/VCF-FILES/biostars439754/.snakemake/conda/f16fdb5f
Failed to open VARIANT_CALLING/varscan/MTG109_sorted_dedupped_snp_varscan.vcf: unknown file type
Possible precedence issue with control flow operator at /scratch/moldach/MADDOG/VCF-FILES/biostars439754/.snakemake/conda/f16fdb5f/lib/site_perl/5.26.2/Bio/DB/IndexedBase.pm line 805.
Traceback (most recent call last):
  File &quot;/scratch/moldach/MADDOG/VCF-FILES/biostars439754/.snakemake/scripts/tmpsh388k23.wrapper.py&quot;, line 44, in &lt;module&gt;
    &quot;(bcftools view {snakemake.input.calls} | &quot;
  File &quot;/home/moldach/bin/snakemake/lib/python3.8/site-packages/snakemake/shell.py&quot;, line 156, in __new__
    raise sp.CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command 'set -euo pipefail;  (bcftools view VARIANT_CALLING/varscan/MTG109_sorted_dedupped_snp_varscan.vcf | vep --everything --fork 4 --format vcf --vcf --cach$
[Thu Aug 13 09:02:22 2020]

Update 3:
bcftools view is giving the warning from the output of samtools mpileup/varscan pileup2snp:
def getDeduppedBamsIndex(sample):
  return(list(os.path.join(aligns_dict[sample],&quot;{0}.sorted.dedupped.bam.bai&quot;.format(sample,pair)) for pair in ['']))

rule mpilup:
    input:
    bam=lambda wildcards: getDeduppedBams(wildcards.sample),
        reference_genome=os.path.join(dirs_dict[&quot;REF_DIR&quot;],config[&quot;REF_GENOME&quot;])
    output:
    os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_{contig}.mpileup.gz&quot;),
    log:
        os.path.join(dirs_dict[&quot;LOG_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_{contig}_samtools_mpileup.log&quot;)
    params:
        extra=lambda wc: &quot;-r {}&quot;.format(wc.contig)
    resources:
    mem = 1000,
        time = 30
    wrapper:
    &quot;0.65.0/bio/samtools/mpileup&quot;

rule mpileup_to_vcf:
    input:
    os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_{contig}.mpileup.gz&quot;),
    output:
    os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_{contig}.vcf&quot;)
    message:
    &quot;Calling SNP with Varscan2&quot;
    threads:
    2 # Keep threading value to one for unzipped mpileup input
          # Set it to two for zipped mipileup files
    log:
        os.path.join(dirs_dict[&quot;LOG_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;varscan_{sample}_{contig}.log&quot;)
    resources:
    mem = 1000,
        time = 30
    wrapper:
    &quot;0.65.0/bio/varscan/mpileup2snp&quot;

rule vcf_merge:
    input:
    os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_I.vcf&quot;),
        os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_II.vcf&quot;),
        os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_III.vcf&quot;),
        os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_IV.vcf&quot;),
        os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_V.vcf&quot;),
        os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_X.vcf&quot;),
        os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_MtDNA.vcf&quot;)
    output:
    os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}.vcf&quot;)
    log: os.path.join(dirs_dict[&quot;LOG_DIR&quot;],config[&quot;CALLING_TOOL&quot;],&quot;{sample}_vcf-merge.log&quot;)
    resources:
    mem = 1000,
        time = 10
    threads: 1
    message: &quot;&quot;&quot;--- Merge VarScan by Chromosome.&quot;&quot;&quot;
    shell: &quot;&quot;&quot;
    awk 'FNR==1 &amp;&amp; NR!=1 {{ while (/^&lt;header&gt;/) getline; }} 1 {{print}} ' {input} &gt; {output}
        &quot;&quot;&quot;

calling_dir = os.path.join(dirs_dict[&quot;CALLING_DIR&quot;],config[&quot;CALLING_TOOL&quot;])
callings_locations = [calling_dir] * len_samples
callings_dict = dict(zip(sample_names, callings_locations))

def getVCFs(sample):
  return(list(os.path.join(callings_dict[sample],&quot;{0}.vcf&quot;.format(sample,pair)) for pair in ['']))

rule annotate_variants:
    input:
    calls=lambda wildcards: getVCFs(wildcards.sample),
        cache=&quot;resources/vep/cache&quot;,
        plugins=&quot;resources/vep/plugins&quot;,
    output:
    calls=&quot;{sample}.annotated.vcf&quot;,
        stats=&quot;{sample}.html&quot;
    params:
    # Pass a list of plugins to use, see https://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html
        # Plugin args can be added as well, e.g. via an entry &quot;MyPlugin,1,FOO&quot;, see docs.
        plugins=[&quot;LoFtool&quot;],
        extra=&quot;--everything&quot;  # optional: extra arguments
    log:
        &quot;logs/vep/{sample}.log&quot;
    threads: 4
    resources:
    time=30,
        mem=5000
    wrapper:
    &quot;0.65.0/bio/vep/annotate&quot;

If I run bcftools view on the output I get the error:
$ bcftools view variant_calling/varscan/MTG324.vcf 
Failed to read from variant_calling/varscan/MTG324.vcf: unknown file type

",-1,-1,-1.0
63173061,problem with snakemake submitting jobs with multiple wildcard on SGE,"I used snakemake on LSF cluster before and everything worked just fine. However, recently I migrated to SGE cluster and I am getting a very strange error when I try to run a job with more than one wildcard.
When I try to submit a job based on this rule
rule download_reads :
    threads : 1
    output : &quot;data/{sp}/raw_reads/{accesion}_1.fastq.gz&quot;
    shell : &quot;scripts/download_reads.sh {wildcards.sp} {wildcards.accesion} data/{wildcards.sp}/raw_reads/{wildcards.accesion}&quot;

I get a following error (snakemake_clust.sh details bellow)
./snakemake_clust.sh data/Ecol1/raw_reads/SRA123456_1.fastq.gz                                          
Building DAG of jobs...
Using shell: /bin/bash
Provided cluster nodes: 10
Job counts:
        count   jobs
        1       download_reads
        1

[Thu Jul 30 12:08:57 2020]
rule download_reads:
    output: data/Ecol1/raw_reads/SRA123456_1.fastq.gz
    jobid: 0
    wildcards: sp=Ecol1, accesion=SRA123456

scripts/download_reads.sh Ecol1 SRA123456 data/Ecol1/raw_reads/SRA123456
Unable to run job: ERROR! two files are specified for the same host
ERROR! two files are specified for the same host
Exiting.
Error submitting jobscript (exit code 1):

Shutting down, this might take some time.

When I replace the sp wildcard with a constant, it works as expected:
rule download_reads :
        threads : 1
        output : &quot;data/Ecol1/raw_reads/{accesion}_1.fastq.gz&quot;
        shell : &quot;scripts/download_reads.sh Ecol1 {wildcards.accesion} data/Ecol1/raw_reads/{wildcards.accesion}&quot;

I.e. I get
Submitted job 1 with external jobid 'Your job 50731 (&quot;download_reads&quot;) has been submitted'.

I wonder why I might have this problem, I am sure I used exactly the same rule on the LSF-based cluster before without any problem.
some details
The snakemake submitting script looks like this
#!/usr/bin/env bash                                                                                                                                                                
                                                                                                                                                                                   
mkdir -p logs                                                                                                                                                                      
                                                                                                                                                                                   
snakemake $@ -p --jobs 10 --latency-wait 120 --cluster &quot;qsub \                                                                                                                     
    -N {rule} \                                                                                                                                                                    
    -pe smp64 \                                                                                                                                                                    
    {threads} \                                                                                                                                                                    
    -cwd \                                                                                                                                                                         
    -b y \                                                                                                                                                                         
    -o \&quot;logs/{rule}.{wildcards}.out\&quot; \                                                                                                                                           
    -e \&quot;logs/{rule}.{wildcards}.err\&quot;&quot;   

-b y makes the command executed as it is, -cwd changes the working directory on the computing node the the working directory from where the job was submitted. Other flags / specifications are clear I hope.
Also, I am aware of --drmaa flag, but I think out cluster is not well configured for that. --cluster was till now a more robust solution.
-- edit 1 --
When I execute exactly the same snakefile locally (on the fronend, without the --cluster flag), the script gets executed as expected. It seems to be a problem of interaction of snakemake and the scheduler.
",-1,-1,-1.0
63124297,Snakemake collecting output of previous rule for input of a second rule,"I am trying to run a workflow that will run on a series of .fastq files, merge any files in the same folder, then perform downstream analysis on the merged files. So far, I am able to merge the files, but I cannot get the downstream analysis to run without running snakemake a second time through. I am 99% positive this is because the outputs of the merging rule are not &quot;picked up&quot; by snakemake (and the downstream rules) until snakemake is ran a second time. The relevant rules are as follows:
def get_barcode_folders(wildcards):
    checkpoint_output = checkpoints.basecall_barcode.get(**wildcards).output[0]
    barcodes = set()
    for folder in os.listdir(checkpoint_output):
        full_path = os.path.join(checkpoint_output, folder)
        if Path(full_path).is_dir():
            barcodes.add(folder)

    merge_files = [config['results_folder'] + &quot;Barcode/&quot; + barcode + &quot;.merged.fastq&quot; for barcode in barcodes]
    return merge_files

FAST5_FILES = glob_wildcards(config['results_folder'] + &quot;DataFiles/fast5/{fast5_file}.fast5&quot;).fast5_file

rule all:
    input:
        # basecall, barcode, and merge files
        get_barcode_folders,

        # nanoQC pre trim
        expand(config['results_folder'] + &quot;NanoQC/Pre-Trim/{barcode}&quot;,
               barcode=set(glob_wildcards(config['results_folder'] + &quot;Barcode/{barcode}.merged.fastq&quot;).barcode))


checkpoint basecall_barcode:
    input:
        config['results_folder'] + &quot;DataFiles/fast5/&quot;
    output:
        directory(config['results_folder'] + &quot;.barcodeTempOutput/&quot;)
    conda:
        &quot;envs/pythonEnv.yaml&quot;
    shell:
        r&quot;&quot;&quot;
        echo {input}
        &quot;&quot;&quot;

def merge_files_input(wildcards):
    return glob.glob(config['results_folder'] + f&quot;.barcodeTempOutput/{wildcards.barcode}/*.fastq&quot;)
rule merge_files:
    input:
        merge_files_input          
    output:
        config['results_folder'] + &quot;Barcode/{barcode}.merged.fastq&quot;
    shell:
        r&quot;&quot;&quot;
            # merging files in here, works as expected. Touch used as placeholder
            touch {output}
        &quot;&quot;&quot;

# This rule (and all subsequent rules) I am having problems with
rule NanoQCPreTrim:
    input:
        rules.merge_files.output[0]
    output:
        directory(config['results_folder'] + &quot;NanoQC/Pre-Trim/{barcode}&quot;)
    shell:
        r&quot;&quot;&quot;
        mkdir {output}
        # Perform NanoQC analysis. mkdir used as placeholder again
        &quot;&quot;&quot;


When I run this on a &quot;clean&quot; run-through (i.e. only some initial .fast5 files are present), the only rules that execute are basecall_barcode and merge_files. On the first run, merge_files is not mentioned by snakemake as it must re-evaluate the output to determine what must be ran, which makes sense to me. However, I do not get NanoQCPreTrim to work until I run the workflow a second time, which is what is confusing me very much.
I think this is because there is no data located at config['results_folder'] + &quot;Barcode/{barcode}.merged.fastq&quot; under rule all when the workflow is ran the first time, and as a result, none of the downstream rules (past merge_files) have a need to be ran, because no input is present.
However, I was under the impression that when using snakemake, it will attempt to find necessary input for the output of each rule, and as a result, even though the input files for rule NanoQCPreTrim are not currently present, it will run the rule because its outputs are not present, and then work backwards attempting to determine what rules can make its input?
In addition, I thought using checkpoints would also alleviate this problem as the DAG is re-evaluated after checkpoint basecall_barcode is complete. As a result, downstream rules would be able to create the necessary input/output based on the now-present files.
I am missing something, and it may be simple, but I have been working nearly all day on this and cannot figure out what seems to be a naive problem. I appreciate any help!
EDIT
When I run the workflow with -F to force all rules, everything works as expected; i.e. each rule is stated in the Job Counts section of snakemake
",1,-1,-1.0
63048722,Snakemake change conda activation command to 'conda activate',"I want to use snakemake with fish shell and conda environments in my managed environment (basically I have no root rights and the default shell cannot be changed).
I set up fish as the 'default' shell using this hack inside the .bashrc:
if [ &quot;$REALBASH&quot; != &quot;1&quot; ]; then
  case &quot;$-&quot; in
  *i*)
    export SHELL=/usr/bin/fish
    tty &gt; /dev/null &amp;&amp; echo &quot;Your ~/.bashrc is switching interactive SHELL to $SHELL&quot;
    [ -x $SHELL ] &amp;&amp; exec $SHELL &quot;$@&quot;
    echo &quot;Apparently $SHELL is not available here. Continuing with bash.&quot;
    export SHELL=/bin/bash
  ;;
  esac
fi

There is also a command realbash that sets the environment variable REALBASH=1 to bypass this hack.
I managed to get conda to work with fish using this, but it has the disadvantage that within fish the command to activate conda environments is different from bash. In bash, the command is source activate ... and in fish it is conda activate ....
Activating environments works both from bash using source activate ... and from fish using conda activate ....
When I now execute snakemake from fish, I get the following error:
Activating conda environment ...
source: Error encountered while sourcing file “activate”:
source: No such file or directory

If I execute snakemake from bash, the same error occurs.
If I execute snakemake from bash via snakemake --overwrite-shellcmd realbash, I get the same error and end up in the bash shell that was opened by snakemake. Only after typing exit, snakemake completes (but unsuccessfully, of course).
If I execute snakemake from fish via snakemake --overwrite-shellcmd realbash, the same behaviour occurs.
I am confused by the behaviour of --overwrite-shellcmd, is there a way to make this work with my hack?
Otherwise, can I configure snakemake to call conda activate instead of source activate?
Or is there any other solution to this?
",-1,-1,-1.0
61864691,Snakemake slurm ouput file redirect to new directory,"I'm putting together a snakemake slurm workflow and am having trouble with my working directory becoming cluttered with slurm output files. I would like my workflow to, at a minimum, direct these files to a 'slurm' directory inside my working directory. I currently have my workflow set up as follows:

config.yaml:

reads:
    1:
    2:
samples:
    15FL1-2: /datasets/work/AF_CROWN_RUST_WORK/2020-02-28_GWAS/data/15FL1-2
    15Fl1-4: /datasets/work/AF_CROWN_RUST_WORK/2020-02-28_GWAS/data/15Fl1-4


cluster.yaml:

localrules: all

__default__:
    time: 0:5:0
    mem: 1G
    output: _{rule}_{wildcards.sample}_%A.slurm

fastqc_raw:
    job_name: sm_fastqc_raw
    time: 0:10:0
    mem: 1G
    output: slurm/_{rule}_{wildcards.sample}_{wildcards.read}_%A.slurm


Snakefile:

configfile: ""config.yaml""
workdir: config[""work""]

rule all:
    input:
        expand(""analysis/fastqc_raw/{sample}_R{read}_fastqc.html"", sample=config[""samples""],read=config[""reads""])

rule clean:
    shell:
        ""rm -rf analysis logs""

rule fastqc_raw:
    input:
        'data/{sample}_R{read}.fastq.gz'
    output:
        'analysis/fastqc_raw/{sample}_R{read}_fastqc.html'
    log:
        err = 'logs/fastqc_raw/{sample}_R{read}.out',
        out = 'logs/fastqc_raw/{sample}_R{read}.err'
    shell:
        """"""
        fastqc {input} --noextract --outdir 'analysis/fastqc_raw' 2&gt; {log.err} &gt; {log.out}
        """"""


I then call with:

snakemake --jobs 4  --cluster-config cluster.yaml --cluster ""sbatch --mem={cluster.mem} --time={cluster.time} --job-name={cluster.job_name} --output={cluster.output}""


This does not work, as the slurm directory does not already exist. I don't want to manually make this before running my snakemake command, that will not work for scalability. Things I've tried, after reading every related question, are:

1) simply trying to capture all the output via the log within the rule, and setting cluster.output='/dev/null'. Doesn't work, the info in the slurm output isn't captured as it's not output of the rule exactly, its info on the job

2) forcing the directory to be created by adding a dummy log:

    log:
        err = 'logs/fastqc_raw/{sample}_R{read}.out',
        out = 'logs/fastqc_raw/{sample}_R{read}.err'
        jobOut = 'slurm/out.err'


I think this doesn't work because sbatch tries to find the slurm folder before implementing the rule

3) allowing the files to be made in the working directory, and adding bash code to the end of the rule to move the files into a slurm directory. I believe this doesn't work because it tries to move the files before the job has finished writing to the slurm output.

Any further ideas or tricks?
",1,-1,-1.0
64364299,Accessing file path from a config.yaml in Snakemake,"I'm working with Snakemake for NGS analysis. I have a list of input files, stored in a YAML file as follows:
DATASETS:
    sample1: /path/to/input/bam
    .
    .

A very simplified skeleton of my Snakemake file, as described earlier in Snakemake: How to use config file efficiently and https://www.biostars.org/p/406452/, is as follows:
rule all:
    input:
        expand(&quot;report/{sample}.xlsx&quot;, sample = config[&quot;DATASETS&quot;])

rule call:
    input:
        lambda wildcards: config[&quot;DATASETS&quot;][wildcards.sample]
    output:
        &quot;tmp/{sample}.vcf&quot;
    shell: 
        &quot;some mutect2 script&quot;

rule summarize:
    input:
        &quot;tmp/{sample}.vcf&quot;
    output:
        &quot;report/{sample}.xlsx&quot;
    shell:
        &quot;processVCF.py&quot;  

This complains about missing input files for rule all. I'm really not too sure what I am missing out here: Could someone perhaps point out where I can start looking to try to solve my problem?
This problem persists even when I execute snakemake -n tmp/sample1.vcf, so it seems the problem is related to the inability to pass the input file to the rule call. I have a nagging feeling that I'm really missing something trivial here.
",1,-1,-1.0
64447585,Running multiple snakemake rules,"I would like to run multiple rules one after another using snakemake. However, when I run this script, the bam_list rule appears before samtools_markdup rule, and gives me an error that it cannot find input files, which are obviously have not been generated yet.
How to solve this problem?
rule all:
    input: 
        expand(&quot;dup/{sample}.dup.bam&quot;, sample=SAMPLES)
        &quot;dup/bam_list&quot;

rule samtools_markdup:
    input:
        sortbam =&quot;rg/{sample}.rg.bam&quot;
    output:
        dupbam = &quot;dup/{sample}.dup.bam&quot;
    threads: 5
    shell:
        &quot;&quot;&quot;
        samtools markdup -@ {threads} {input.sortbam} {output.dupbam}
        &quot;&quot;&quot;

rule bam_list:
    output:
         outlist = &quot;dup/bam_list&quot;
    shell:
         &quot;&quot;&quot;
         ls dup/*.bam &gt; {output.outlist}
         &quot;&quot;&quot;

",-1,-1,-1.0
64468895,Snakemake exit a rule during execution,"Is there a way to print a helpful message and allow Snakemake to exit the workflow without giving an error? I have this example workflow:
def readFile(file):
    with open(file) as f:
        line = f.readline()
        return(line.strip())

def isFileEmpty(file):
    with open(file) as f:
        line = f.readline()
        if line.strip() != '':
            return True
        else:
            return False

rule all:
    input: &quot;output/final.txt&quot;

rule step1:
    input: &quot;input.txt&quot;
    output: &quot;out.txt&quot;
    run:
        if readFile(input[0]) == 'a':
            shell(&quot;echo 'a' &gt; out.txt&quot;)
        else:
            shell(&quot;echo '' &gt; out.txt&quot;)
rule step2:
    input: &quot;out.txt&quot;
    output: dynamic(&quot;output/{files}&quot;)
    run:
        i = isFileEmpty(input[0])
        if i:
            shell(&quot;echo 'out2' &gt; output/out2.txt&quot;)
        else:
            print(&quot;Out.txt is empty, workflow ended&quot;)
            

rule step3:
    input: &quot;output/out2.txt&quot;
    output: &quot;output/final.txt&quot;
    run: shell(&quot;echo 'final' &gt; output/final.txt&quot;)

In step 1, I'm reading the file contents of input.txt and if doesn't contain the letter 'a' then an empty out.txt will be produced. In step 2, whether out.txt is empty is checked. If it's not empty, step2 and 3 will be performed to give final.txt at the end. If it's empty, I want Snakemake to print the message &quot;Out.txt is empty, workflow ended&quot; and exit immediately without performing step 3 and giving an error message. Right now the code I have will print the message at step 2 if input.txt is empty, but it'll still try to run step 3 and will give MissingOutputException because final.txt is not generated. I understand the reason is because final.txt is one of the input files in the rule all, but I'm having trouble writing up this workflow because final.txt may or may not be produced.
",-1,1,-1.0
64675000,Permission denied error from Docker container in Snakemake,"I had built a Docker container from this Dockerfile previously and it worked fine:
FROM perl:5.32

MAINTAINER Matthew Jordan Oldach, moldach686@gmail.com

WORKDIR /usr/local/bin

# Install cpan modules
RUN cpanm install --force Cwd Getopt::Long POSIX File::Basename List::Util Bio::DB::Fasta Bio::Seq Bio::SeqUtils Bio::SeqIO Set::IntervalTree Set::IntSpan

RUN apt-get install tar

# Download CooVar-v0.07
RUN wget http://genome.sfu.ca/projects/coovar/CooVar-0.07.tar.gz
RUN tar xvf CooVar-0.07.tar.gz
RUN cd coovar-0.07; chmod +x scripts/* coovar.pl

# Set WORKDIR to /data -- predefined mount location.
RUN mkdir /data
WORKDIR /data

# Set Entrypoint
ENTRYPOINT [&quot;perl&quot;, &quot;/usr/local/bin/coovar-0.07/coovar.pl&quot;]

The only issue was that I found there was a slight difference between what is on the repo and the coovar-0.07 which is on our server (there was slight difference in the extract-cdna.pl script).
In order to reproduce our pipeline I'll need to COPY CooVar locally into the container (rather than wget).
I've therefore tried the following Dockerfile:
FROM perl:5.32

MAINTAINER Matthew Jordan Oldach, moldach686@gmail.com

WORKDIR /usr/local/bin

# Install cpan modules
RUN cpanm install --force Cwd Getopt::Long POSIX File::Basename List::Util Bio::DB::Fasta Bio::Seq Bio::SeqUtils Bio::SeqIO Set::IntervalTree Set::IntSpan

# Download CooVar-v0.07
COPY coovar-0.07 /usr/local/bin/coovar-0.07
RUN cd coovar-0.07; chmod +x scripts/* coovar.pl

# Set WORKDIR to /data -- predefined mount location.
RUN mkdir /data
WORKDIR /data

# Set Entrypoint
ENTRYPOINT [&quot;perl&quot;, &quot;/usr/local/bin/coovar-0.07/coovar.pl&quot;]

It appears I could run the main script (coovar.pl) from Docker (no Permission Denied error):
# pull the container
$ sudo docker pull moldach686/coovar-v0.07:latest

# force entry point of `moldach686/coovar-v0.07` to /bin/bash
## in order to investigate file system
$ sudo docker run -it --entrypoint /bin/bash moldach686/coovar-v0.07

root@c7459dbe216a:/data# perl /usr/local/bin/coovar-0.07/coovar.pl 
USAGE: ./coovar.pl -e EXONS_GFF -r REFERENCE_FASTA (-t GVS_TAB_FORMAT | -v GVS_VCF_FORMAT) [-o OUTPUT_DIRECTORY] [--circos] [--feature_source] [--feature_type]
Program parameter details provided in file README.

However, when I tried to incorporate this into my Snakemake workflow I get the following Permission Denied error:
Workflow defines that rule get_vep_cache is eligible for caching between workflows (use the --cache argument to enable this).
Building DAG of jobs...
Using shell: /cvmfs/soft.computecanada.ca/nix/var/nix/profiles/16.09/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       coovar
        1
[Tue Nov  3 21:56:51 2020]
rule coovar:
    input: variant_calling/varscan/MTG470.vcf, refs/c_elegans.PRJNA13758.WS265.genomic.fa
    output: annotation/coovar/varscan/MTG470/categorized-gvs.gvf, annotation/coovar/varscan/MTG470.annotated.vcf, annotation/coovar/varscan/filtration/MTG470_keep.tsv, annotation/coovar/varscan/filtration/MTG470_exclude.tsv
    jobid: 0
    wildcards: sample=MTG470
    resources: mem=4000, time=10
Activating singularity image /scratch/moldach/COOVAR/cbc22e3a26af1c31fb0e4fcae240baf8.simg
Can't open perl script &quot;/usr/local/bin/coovar-0.07/coovar.pl&quot;: Permission denied

",-1,-1,-1.0
64716693,How does snakemake --show-failed-logs works,"I want to use snakemake with the --show-failed-logs parameter but not sure how it works or what to expect.
For example (not a working example I just typed and copied some code parts, if necessary I can create a working example)
My command is:
snakemake -s Snakefile_test.smk --configfile test.yml -j 4 --restart-times 2 --show-failed-logs

In Snakefile_test.smk I have a rule:
rule testrule:
    input:
        R1_trimmed = rules.trim.output.R1_trimmed,
        R2_trimmed = rules.trim.output.R2_trimmed
    output:
        plot_R1 = output+&quot;/{sample}/figures/{sample}_R1_plot.png&quot;,
        plot_R2 = output+&quot;/{sample}/figures/{sample}_R2_plot.png&quot;,
    log:
        testrule_log = output+&quot;/{sample}/logs/plot_log.txt&quot;
    run:
        shell(&quot;python scripts/createplot.py -r1_trimmed {input.R1_trimmed} -r2_trimmed {input.R2_trimmed} -plot_r1 {output.plot_R1} -plot_r2 {output.plot_R2} -log {log.testrule_log}&quot;)


In createplot.py to test I have something like:
if __name__ == &quot;__main__&quot;:
    logging.basicConfig(filename=args.log, level=logging.DEBUG, format='%(asctime)s %(levelname)s %(name)s %(message)s')
    logger=logging.getLogger(__name__)
    #to add some content to the log file
    try:
        1/0
    except ZeroDivisionError as err:
        logger.error(err)
    main()
    #to let the script crash
    a = 1/0

Because obviously createplot.py will crash the pipeline I expected that snakemake would print the contents of testrule_log to the screen. But that is not the case. I am using version 5.25.0
EDIT:
I am familiar with
    script:
        &quot;scripts/createplot.py&quot;

But need to use shell in this case. If that causes the problem let me know.
",-1,-1,-1.0
64898553,"In snakemake, how do you use wildcards with scatter-gather processes?","I am trying to use snakemake's scatter-gather functionality to parallelize a slow step in my workflow. However, I cannot figure out how to apply it in situations where I am using wildcards. For example, I have defined the wildcard library in rule all, however, this does not seem to apply to the scatter function in ScatterIntervals:
import re

SCATTER_COUNT = 100
scattergather: 
    split=SCATTER_COUNT

rule all:
    input:
        expand(&quot;{library}_output.txt&quot;, library=[&quot;FC19271512&quot;, &quot;FC19271513&quot;])


rule ScatterIntervals:
    input:
        &quot;{library}_baits.interval_list&quot;
    output:
        temp(scatter.split(&quot;tmp/{library}_baits.scatter_{scatteritem}.interval_list&quot;))
    params:
        output_prefix = (
            lambda wildcards, output: 
            re.sub(&quot;\.scatter_\d+\.interval_list&quot;, &quot;&quot;, output[0])
        ),
        scatter_count = SCATTER_COUNT
    shell:
        &quot;&quot;&quot;
        python ScatterIntervals.py \
            -i {input} \
            -o {params.output_prefix} \
            -s {params.scatter_count}
        &quot;&quot;&quot;


rule ProcessIntervals:
    input:
        bam = &quot;{library}.bam&quot;,
        baits = &quot;tmp/{library}_baits.scatter_{scatteritem}.interval_list&quot;
    output:
        temp(&quot;tmp/{library}_output.scatter_{scatteritem}.txt&quot;)
    shell:
        &quot;&quot;&quot;
        python ProcessIntervals.py \
            -b {input.bam} \
            -l {input.baits} \
            -o {output}
        &quot;&quot;&quot;


rule GatherIntervals:
    input:
        gather.split(&quot;tmp/{library}_output.scatter_{scatteritem}.txt&quot;)
    output:
        &quot;{library}_output.txt&quot;
    run:
        inputs = &quot;-i &quot;.join(input)
        command = f&quot;python GatherOutputs.py {inputs} -o {output[0]}&quot;
        shell(command)
    

WildcardError in line 16 of Snakefile: 
No values given for wildcard 'library'.

",-1,-1,-1.0
64935325,Snakemake - Jupyter lab notebook does not find kernel,"When writing a program in a jupyter lab notebook using R or python I install specific conda environments as kernels to access that environment-specific packages from a single jupyter lab installation in the base conda environment
After finishing developing the notebook I want to plug it into my snakemake file to ensure later reproducibility and of course, I do that with the respective conda environment .yaml file so that all the needed packages/libraries are provided.
Now comes the problem: the rule which references the notebook, is not reproducible on another machine/environment, as it tries to access/run a kernel which is specific to my development environment
Does anyone have a workaround or solution to this particular problem?
EDIT:
More detailed steps leading to my problem

Setup a conda environment (matplotlib_env) for a specific tool or task (here: matplotlib)

conda create -n matplotlib_env python=3.8 ipykernel matplotlib nbconvert


I created a ipython kernel so my jupyter-lab instance from the base environment can access the conda environment and the respective packages (matplotlib)

python -m ipykernel install --user --name matplotlib_env --display-name &quot;Python_maplotlib&quot;


I created a short notebook (1) that uses the kernel configured in step 2 (Pyhton_matplotlib 2)

import matplotlib.pyplot as plt
plt.plot([1,2,3],[1,4,9])
plt.savefig('exp.png')


Having finished the task I want to plug it into my snakemake workflow for management and reproducibility reasons. For that, I define a rule, with the respective notebook and conda environment .yaml file (automatically generated via: conda env export &gt; matplotlib_env.yaml).

rule make_plot:
    output:
        &quot;exp.png&quot;
    conda:
        &quot;matplotlib_env.yaml&quot;
    notebook:
        &quot;make_plot.ipynb&quot;


Executing the rule works fine as long as the original conda environment (with the configured kernel) still exists. As soon as I remove the original conda environment (optionally also the kernel from the kernelspec list) I get an error message, as the kernel can not be found anymore and the snakemake generated conda environment does not possess the kernel, the notebook is looking for.

snakemake -p --cores 1 --use-conda make_plot

ERROR message after removing the original environment
[NbConvertApp] ERROR | Failed to run command:
...
FileNotFoundError: [Errno 2] No such file or directory: '/home/miniconda3/envs/matplotlib_env/bin/python'

ERROR message after removing kernel from kernel list
...
raise NoSuchKernel(kernel_name)
jupyter_client.kernelspec.NoSuchKernel: No such kernel named matplotlib_env

",-1,-1,-1.0
64986357,"""Error: No such file or directory"" for input that has to be generated first","I have two rules for STAR, STAR_genome does some indexing for the STAR rule, therefore the input of STAR is the direct output from STAR_genome - so far so simple. But when I try to run this, the STAR_genome rule is ignored (not listed in job count) and I get the following exception:

FileNotFoundError: [Errno 2] No such file or directory: '[...]STAR/cauliflower/genome/genome.ok'

I don't understand why snakemake would ignore the generating rule and just complain about a missing file instead, as it even takes the path from the very rule that should generate it...
rule STAR_genome:
    input: genome=lambda wildcards: config[wildcards.species][&quot;genomefile&quot;]
    output: ok=path.join(STAR_DIR, &quot;{species}&quot;, &quot;genome&quot;, &quot;genome.ok&quot;)
    threads: 32
    envmodules:
        config[&quot;STAR&quot;][0],
        config[&quot;STAR&quot;][1]
    script:
        &quot;scripts/Trinity_GG/STAR_genome.py&quot;

############################################################################

rule STAR:
    input:
         genome=rules.STAR_genome.output.ok,
         r1=rules.trim_galore.output.r1,
         r2=rules.trim_galore.output.r2
    output:
        bam=path.join(STAR_DIR, &quot;{species}_{rep}_Aligned.sortedByCoord.out.bam&quot;)
    threads: 32
    envmodules:
        config[&quot;STAR&quot;][0],
        config[&quot;STAR&quot;][1]
    script:
        &quot;scripts/Trinity_GG/STAR.py&quot;

And here is the full traceback, just in case it could help.
    Traceback (most recent call last):
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/__init__.py&quot;, line 751, in snakemake
    keepmetadata=keep_metadata,
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/workflow.py&quot;, line 1000, in execute
    success = scheduler.schedule()
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/scheduler.py&quot;, line 444, in schedule
    run = self.job_selector(needrun)
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/scheduler.py&quot;, line 731, in job_selector_greedy
    c = list(map(self.job_reward, jobs))  # job rewards
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/scheduler.py&quot;, line 814, in job_reward
    input_size = job.inputsize
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/jobs.py&quot;, line 378, in inputsize
    self._inputsize = sum(f.size for f in self.input)
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/jobs.py&quot;, line 378, in &lt;genexpr&gt;
    self._inputsize = sum(f.size for f in self.input)
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/io.py&quot;, line 239, in wrapper
    return func(self, *args, **kwargs)
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/io.py&quot;, line 254, in wrapper
    return func(self, *args, **kwargs)
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/io.py&quot;, line 553, in size
    return self.size_local
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/io.py&quot;, line 558, in size_local
    self.check_broken_symlink()
  File &quot;/cluster/easybuild/broadwell/software/mflow/0.0-foss-2019b-Python-3.7.4/lib/python3.7/site-packages/snakemake-5.27.4-py3.7.egg/snakemake/io.py&quot;, line 563, in check_broken_symlink
    if not self.exists_local and os.lstat(self.file):
FileNotFoundError: [Errno 2] No such file or directory: '[...]/Cauliflower_Test/STAR/cauliflower/genome/genome.ok'

",-1,-1,-1.0
65050169,(one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!),"I have been trying to run this script and I am not sure why I am facing this error, can somebody help?
I have tried using the suggested methods in StackOverflow but it does not work, I have used &quot;set +e, set -e&quot;  as well but then the terminal is taking forever to complete the process.
shell.executable(&quot;/bin/bash&quot;)

import pprint
import os
import yaml
yaml.warnings({'YAMLLoadWarning': False}) # Suppress yaml &quot;unsafe&quot; warnings.

from globals import *

configfile: f&quot;{cnf}pipeline_parameters.yaml&quot;
configfile: f&quot;{cnf}variables.yaml&quot;

minlensize  =   config[&quot;Illumina_meta&quot;][&quot;minlen&quot;]

SAMPLES     =   {}
with open(config[&quot;sample_sheet&quot;]) as sample_sheet_file:
    SAMPLES =   yaml.load(sample_sheet_file) # SAMPLES is a dict with sample in the form sample &gt; read number &gt; file. E.g.: SAMPLES[&quot;sample_1&quot;][&quot;R1&quot;] = &quot;x_R1.gz&quot;
rule all:
    input:
        expand(&quot;{p}{sample}/scaffolds.fasta&quot;,
                p= f&quot;{datadir + scf_raw}&quot;,
                sample  =   SAMPLES),
        expand(&quot;{p}{sample}_scaffolds_ge{minlensize}nt.fasta&quot;,
                        p= f&quot;{datadir + scf_filt}&quot;,
                        sample  =   SAMPLES,
                        minlensize  =   config[&quot;Illumina_meta&quot;][&quot;minlen&quot;] )
rule De_novo_assembly:
    input:
        fastq_pR1       =   f&quot;{datadir + cln}&quot; + &quot;{sample}_pR1.fq&quot;,
        fastq_pR2       =   f&quot;{datadir + cln}&quot; + &quot;{sample}_pR2.fq&quot;,
        fastq_unpaired  =   f&quot;{datadir + cln}&quot; + &quot;{sample}_unpaired.fq&quot;
    output:
        all_scaffolds   =   f&quot;{datadir + scf_raw}&quot; + &quot;{sample}/scaffolds.fasta&quot;,
        filt_scaffolds  =   f&quot;{datadir + scf_filt}&quot; + &quot;{sample}&quot; + f&quot;_scaffolds_ge{minlensize}nt.fasta&quot;
    conda:
        f&quot;{conda_envs}de_novo_assembly.yaml&quot;
    log:
        f&quot;{logdir}&quot; + &quot;De_novo_assembly_{sample}.log&quot;
    benchmark:
        f&quot;{logdir + bench}&quot; + &quot;De_novo_assembly_{sample}.txt&quot;
    threads: config[&quot;threads&quot;][&quot;De_novo_assembly&quot;]
    params:
        max_GB_RAM          =   config[&quot;Illumina_meta&quot;][&quot;Spades&quot;][&quot;Max_gb_ram&quot;],
        kmersizes           =   config[&quot;Illumina_meta&quot;][&quot;Spades&quot;][&quot;kmersizes&quot;],
        minlength           =   config[&quot;Illumina_meta&quot;][&quot;minlen&quot;],
        outputfoldername    =   f&quot;{datadir + scf_raw}&quot; + &quot;{sample}/&quot;
    shell:
        &quot;&quot;&quot;
spades.py --only-assembler --meta \
-1 {input.fastq_pR1} \
-2 {input.fastq_pR2} \
-s {input.fastq_unpaired} \
-t {threads} \
-m {params.max_GB_RAM} \
-k {params.kmersizes} \
-o {params.outputfoldername} &gt; {log} 2&gt;&amp;1
seqtk seq {output.all_scaffolds} 2&gt;&gt; {log} |\
gawk -F &quot;_&quot; '/^&gt;/ {{if ($4 &gt;= {params.minlength}) {{print $0; getline; print $0}};}}' 2&gt;&gt; {log} 1&gt; {output.filt_scaffolds}
 &quot;&quot;&quot;

The error is
Error in rule De_novo_assembly:
    jobid: 1
    output: data/scaffolds_raw/sample/scaffolds.fasta, data/scaffolds_filtered/sample_scaffolds_ge250nt.fasta
    log: logs/De_novo_assembly_sample.log (check log file(s) for error message)
    conda-env: /Users/dishaaa/Documents/Documents/internship/Jovian/.snakemake/conda/1057b0be
    shell:
        set +u
spades.py --only-assembler --meta -1 data/cleaned_fastq/sample_pR1.fq -2 data/cleaned_fastq/sample_pR2.fq -s data/cleaned_fastq/sample_unpaired.fq -t 4 -m 100 -k 21,33,55,77 -o data/scaffolds_raw/sample/ &gt; logs/De_novo_assembly_sample.log 2&gt;&amp;1
seqtk seq data/scaffolds_raw/sample/scaffolds.fasta 2&gt;&gt; logs/De_novo_assembly_sample.log |gawk -F &quot;_&quot; '/^&gt;/ {if ($4 &gt;= 250) {print $0; getline; print $0};}' 2&gt;&gt; logs/De_novo_assembly_sample.log 1&gt; data/scaffolds_filtered/sample_scaffolds_ge250nt.fasta
 set -u
 
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

",-1,-1,-1.0
65071531,"Rule fails in snakemake, but script runs fine outside of snakemake","I am testing my snakemake pipeline, and it fails on the final step, where it calls a tool I developed - KEGGCharter. The command for this tool runs fine without snakemake, but it fails with RuleException: FileNotFoundError, because a file it creates, and is available throughout the execution of the pipeline, it removed at the end, by snakemake. I see that snakemake does this everytime a rule fails, it removes the expected outputs. However, KEGGCharter runs fine outside of snakemake, with the same command!
KEGGCharter loads KEGG maps using Biopython, and some of those loadings will fail with connection time out. This is catch inside KEGGCharter with a try except, but does snakemake see this errors and considers an exit code != 0? I have also tried putting the exit(0) at the end of the KEGGCharter script, but the result is the same.
",-1,-1,-1.0
65179800,How to start a Snakemake workflow on AWS and detach?,"I am trying to execute a Snakemake workflow on AWS, and have succeeded in executing my workflow using the command:
snakemake --tibanna --use-conda --default-remote-prefix=mybucket/myproject

and it works successfully. So far, so good. Unfortunately snakemake keeps running in the foreground in the terminal until the workflow ends. Using Ctrl-C on it ends the run. This is problematic for me when I want to run a pipeline that takes a few days.
Is there a way to run pipelines using snakemake --tibanna and detach and poll the results later?
I believe tibanna has the capability: tibanna run_workflow runs the workflow and detatches, and you can check the status later using tibanna stat. I just can't get snakemake to finish leaving the processes scheduled in the cloud.
",1,-1,-1.0
65216515,TypeError when creating Snakemake report (v5.30.1),"I have an error while creating a report with snakemake (5.30.1). The pipeline works well, the error is raised with the --report argument:
Command line used while running the pipeline:
snakemake -s /mnt/beegfs/pipelines/rna-count-salmon/Snakefile --profile /mnt/beegfs/pipelines/rna-count-salmon/.igr/profile/slurm  --cache salmon_index tr2gene

Command line used while creating the report:
snakemake -s /mnt/beegfs/pipelines/rna-count-salmon/Snakefile --profile /mnt/beegfs/pipelines/rna-count-salmon/.igr/profile/slurm --report Quantification_Report.html --cache salmon_index tr2gene

The error I have only while reporting:
Traceback (most recent call last):
  File &quot;/mnt/beegfs/userdata/t_dayris/anaconda3/envs/rna-count-salmon/lib/python3.8/site-packages/snakemake/__init__.py&quot;, line 687, in snakemake
    success = workflow.execute(
  File &quot;/mnt/beegfs/userdata/t_dayris/anaconda3/envs/rna-count-salmon/lib/python3.8/site-packages/snakemake/workflow.py&quot;, line 820, in execute
    auto_report(dag, report, stylesheet=report_stylesheet)
  File &quot;/mnt/beegfs/userdata/t_dayris/anaconda3/envs/rna-count-salmon/lib/python3.8/site-packages/snakemake/report/__init__.py&quot;, line 722, in auto_report
    rec.starttime = min(rec.starttime, meta[&quot;starttime&quot;])
TypeError: '&lt;' not supported between instances of 'NoneType' and 'int'

The error is not raised if I run the pipeline within the snakemake-pipeline's repository. I have used this profile in the past (without any error) and created reports without any error with snakemake version 5.27.0 and previous ones.
If I delete the .snakemake repository within the working directory, then the error is solved. If anyone could point out what I am doing wrong, or giving me advice to help me investigate on the error, it would be great.
Thanks in advance
Edit: Disabling the --cache option did not fix the issue, cf. comment of Dmitry Kuzminov
Edit2: I am 100% positive, the error comes from the repository .snakemake/metadata. I am now trying to identify the issue within this file.
",1,-1,-1.0
65227729,How to make Snakemake input optional but not empty?,"I'm building an SQL script out of text data. The (part of) script shall consist of a CREATE TABLE statement and an optional INSERT INTO statement. The values for INSERT INTO statement are taken from the list of files, each one may exist or may not; all values of existing files are merged. The crucial part is that the INSERT INTO statement shall be skipped whenever no one data file exists.
I've created a script in Snakemake that does that. There are two ambiguous rules that create a script: the one that creates a script for empty data, and the one that creates table but inserts data (the ambiguity is resolved with ruleorder statement).
The interesting part is the rule that merges values from data files. It shall create the output whenever at least one input is present, and this rule shall not be considered otherwise. There are two difficulties: to make each input optional, and to prevent Snakemake using this rule whenever no files exist. I've done that with a trick:
def require_at_least_one(filelist):
    existing = [file for file in filelist if os.path.isfile(file)]
    return existing if len(existing) else &quot;non_existing_file&quot;

rule merge_values:
    input: require_at_least_one(expand(&quot;path_to_data/{dataset}/values&quot;, dataset=[&quot;A&quot;, &quot;B&quot;, &quot;C&quot;]))
    output: ...
    shell: ...

The require_at_least_one function takes a list of filenames, and filters out those filenames that don't represent a file. This allows to make each input optional. For the corner case when no one file exists, this function returns a special value that represents a non-existing file. This allows to prune this branch and prefer the one that creates a script without INSERT statement.
I feel like reinventing the wheel, moreover the &quot;non_existing_file&quot; trick looks a little dirty. Are there better and idiomatic ways to do that in Snakemake?
",-1,1,-1.0
65409859,snakemake - do not delete output of failed rules,"I have a snakemake workflow containing a rule that runs another &quot;inner&quot; snakemake workflow.
Sometimes a certain rule of the inner workflow fails, which means the inner workflow fails. As a result, all files listed under the output of the inner workflow are deleted by the outer workflow, even if the rules of the inner workflow that created them completed successfully.
Is there a way to prevent snakemake from deleting the outputs of failed rules? Or maybe you can suggest another workaround?
A few notes:

The outputs of the inner workflow must be listed, b/c they are used as input for other rules in the outer workflow.
I tried setting the outputs of the inner workflow as protected, but this didn't help.
I've also tried adding exit 0 to the end of the call to the inner workflow to make snakemake think it completed successfully,

like this:
rule run_inner:
    input:
        inputs...
    output:
        outputs...
    shell:
        &quot;&quot;&quot;
        snakemake -s inner.snakefile
        exit 0
        &quot;&quot;&quot;

but outputs are still deleted.
Would appreciate any help. Thanks!
",-1,-1,-1.0
65420026,strange Gurobi error when running Snakemake,"I just installed Snakemake (with sudo) on a shared system, and when I run it, even snakemake --version, I get the strange error:

murray@alina:~$ snakemake --version
/usr/local/lib/python3.7/site-packages/pulp/apis/gurobi_api.py:297: UserWarning: GUROBI error:
ERROR 10009: No Gurobi license found (user murray, host alina, hostid da781c18, cores 64)

.
  warnings.warn('GUROBI error: {}.'.format(out))
5.31.1


I understand that this error is related to licensing issues with the Gurobi solver (or at least its API), that is installed on the system, however, why should running Snakemake elicit this error?  Fixing this, or even just squelching the error (and the resulting gurobi.log file which gets created in the directory where the snakemake --version command is run) would be much appreciated.
",-1,-1,-1.0
65545864,Snakemake Cloud execution command parameters (GCP),"I want to use Snakemake on Google Cloud platform, so i used this tutorial : https://snakemake.readthedocs.io/en/stable/executing/cloud.html
I did the first steps, then I need to do this : https://snakemake.readthedocs.io/en/v5.8.0/executing/cluster-cloud.html#executing-a-snakemake-workflow-via-kubernetes
After reading the text below, i created a bucket named mybucketname in a project in my GCP account, and my command is :
snakemake --kubernetes --use-conda --default-remote-provider GS --default-remote-prefix mybucketname

The problem is that I get the error &quot;Error : no Snakefile found, tried Snakefile, snakefile, workflow/Snakefile, workflow/snakefile.&quot;
I tried to change the name of the bucket in my command with another bucket name that doesn't exist, and i get the same error; which means the command doesn't find my bucket in GCP.
I haven't speficied a project name in the console (where my bucket is), so maybe it's the solution ? But i don't know how to specify it.
I have also tried to replace mybucketname with the Link URL and Link for gsutil parameters written in my bucket configuration, but it doesn't work
Can anyone help me ? Thanks
",1,-1,-1.0
65665971,Error while trying to run snakemake in a venv on a protected server,"For a project I made a virtual environment (venv) using Python3. I installed all the necessary dependencies using a simple bash script (see picture below) after I activated my venv. (I verified the  installed packages using: pip3 list and concluded that every dependency was installed succesfully.)

My project uses snakemake, so I ran this snakemake commando:
snakemake --snakefile Snakefile.py all

I get this error:

I know it has to do something with the venv, because without the venv snakemake runs perfectly. I have read the Snakemake installation documents and it says I have to install conda and make &amp; activate a conda venv. But, I do not have the sudo privileges to download and install conda (I work on a protected server).
What is happening and does someone know a fix?
",-1,-1,-1.0
65801953,How to handle ftp links provided in config file in snakemake?,"I am attempting to build a snakemake workflow that will provide a symlink to a local file if it exists or if the file does not exist will download the file and integrate it into the workflow. To do this I am using two rules with the same output with preference given to the linking rule (ln_fastq_pe below) using ruleorder.
Whether the file exists or not is known before execution of the workflow. The file paths or ftp links are provided in tab-delimited config file that is used by the workflow to read in samples.
e.g. the contents of samples.txt:
id      sample_name     fq1     fq2
b       test_paired     resources/SRR1945436_1.fastq.gz resources/SRR1945436_2.fastq.gz
c       test_paired2    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR194/005/SRR1945435/SRR1945435_1.fastq.gz  ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR194/005/SRR1945435/SRR1945435_2.fastq.gz

relevant code from the workflow here:
import pandas as pd
from snakemake.remote.FTP import RemoteProvider as FTPRemoteProvider
FTP = FTPRemoteProvider()

configfile: &quot;config/config.yaml&quot;
samples = pd.read_table(&quot;config/samples.tsv&quot;).set_index(&quot;id&quot;, drop=False)
all_ids=list(samples[&quot;id&quot;])

ruleorder: ln_fastq_pe &gt; dl_fastq_pe
rule dl_fastq_pe:
    &quot;&quot;&quot;
    download file from ftp link
    &quot;&quot;&quot;
    input:
        fq1=lambda wildcards: FTP.remote(samples.loc[wildcards.id, &quot;fq1&quot;], keep_local=True),
        fq2=lambda wildcards: FTP.remote(samples.loc[wildcards.id, &quot;fq2&quot;], keep_local=True)
    output:
        &quot;resources/fq/{id}_1.fq.gz&quot;,
        &quot;resources/fq/{id}_2.fq.gz&quot;
    shell:
        &quot;&quot;&quot;
        mv {input.fq1} {output[0]}
        mv {input.fq2} {output[1]}
        &quot;&quot;&quot;

rule ln_fastq_pe:
    &quot;&quot;&quot;
    link file
    &quot;&quot;&quot;
    input:
        fq1=lambda wildcards: samples.loc[wildcards.id, &quot;fq1&quot;],
        fq2=lambda wildcards: samples.loc[wildcards.id, &quot;fq2&quot;]
    output:
        &quot;resources/fq/{id}_1.fq.gz&quot;,
        &quot;resources/fq/{id}_2.fq.gz&quot;
    shell:
        &quot;&quot;&quot;
        ln -sr {input.fq1} {output[0]}
        ln -sr {input.fq2} {output[1]}
        &quot;&quot;&quot;

When I run this workflow, I receive the following error pointing to the line describing the ln_fastq_pe rule.
WorkflowError in line 58 of /path/to/Snakefile:
Function did not return str or list of str.


I think the error is in how I am describing the FTP links in the samples.txt config file in the dl_fastq_pe rule. What is the proper way to describe FTP links given in a tabular config file so that snakemake will understand them and can download and use the files in a workflow?
Also, is it possible to do what I am trying to do and will this method get me there? I have tried other solutions (e.g. using python code to check if file exists and executing one set of shell commands if it does and the other if it doesn't) to no avail.
",1,-1,-1.0
65839646,how to solve snakemake 5.32.0 env problem,"i had meet a problem when i run snakemake in cluster system, &quot;missingoutputfile&quot; and i had searched trying to solve the problem, maybe because it is &quot;run&quot; derived, not &quot;shell&quot;,   and it is a bug, in the new verision you had solve this problem, when i update it (verison 5.32.0). but another problem is raising up.
Error in rule predict_plasforest:
    jobid: 34
    output: linear_plasmid_genome/DP-Sample058-S54-adapter-phix-moving-sickle-sss_contigs_1kb.csv
    log: log/isolating-linear-contig/DP-Sample058-S54-adapter-phix-moving-sickle-sss_linear_plasforest.out, log/isolating-linear-contig/DP-Sample058-S54-adapter-phix-moving-sickle-sss_linear_plasforest.err (check log file(s) for error message)
    conda-env: /home/projects/ku_00041/apps/wanli/F_pipeline/conda_envs/60d0848d
    shell:
        export PATH=$PATH:/home/projects/ku_00041/apps/wanli/F_pipeline/db/blast/bin;cp /home/projects/ku_00041/apps/wanli/F_pipeline/db/plasforest/plasmid_refseq.* .;cp /home/projects/ku_00041/apps/wanli/F_pipeline/db/plasforest/plasforest.sav .;python3 /home/projects/ku_00041/apps/wanli/F_pipeline/db/plasforest/PlasForest.py -i assmebly_res/DP-Sample058-S54-adapter-phix-moving-sickle-sss_contigs_1kb.fasta -r -b -f --threads 30 -o linear_plasmid_genome/DP-Sample058-S54-adapter-phix-moving-sickle-sss_contigs_1kb.csv 2&gt;log/isolating-linear-contig/DP-Sample058-S54-adapter-phix-moving-sickle-sss_linear_plasforest.err &gt;log/isolating-linear-contig/DP-Sample058-S54-adapter-phix-moving-sickle-sss_linear_plasforest.out
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)
    cluster_jobid: 30135295

when i see the log file:
Traceback (most recent call last):
  File &quot;/home/projects/ku_00041/apps/wanli/F_pipeline/db/plasforest/PlasForest.py&quot;, line 26, in &lt;module&gt;
    from sklearn.ensemble import RandomForestClassifier
ModuleNotFoundError: No module named 'sklearn'

but in the conda-env, it has already install sklean. and when i activate this env and reun the same command, it is working.
did you know how to solve this problem?
",-1,-1,-1.0
65870166,request for clarification in snakemake's documentation regarding 'resources' and 'threads',"I have a question with regards to resources and threads (it's not clear to me from the documentation).
Are the resources per thread ?
That's the case with various HPC job submission systems. E.g.: that's for example how jobs work on LSF's bsub:

If I request 64 threads, with 1024MiB each, bsub will schedule a job with 64 process, each consuming 1024MiB individually, and thus consuming 64GiB in total.
(That total memory may or may not be on the same machine, as the 64 processes may or may not be on the same machine depending on the host[span=n] parameters. For openMPI uses it might well be 64 different machines each allocating it's own local 1024MiB chunk. But with host[span=1], it's going to be a single machine with 64 threads and 64GiB memory).


When looking at the LSF profile, mem_mb seems to passed with only unit versions but otherwise the same value from ressources to bsub
thus it seems that snakemake and LSF both assume that total_memory = threads * mem_mb.
I just wanted to make sure this assumption is correct.

Upon further analysis, the resources accounting in jobs.py is in contactiction of the above.
Filing a bug request
",-1,-1,-1.0
65900514,using snakemake.utils in profile without using conda-not-block-search-path-envvars,"I just updated from snakemake 5.28 to 5.32, and now from snakemake.utils import read_job_properties throws an import error for my job submission script in my snakemake profile:
Traceback (most recent call last):
  File &quot;/ebio/abt3_projects/software/dev/ll_pipelines/llmgqc/bin/ll_pipeline_utils/profiles/sge/sge-submit.py&quot;, line 7, in &lt;module&gt;
    from snakemake.utils import read_job_properties
ModuleNotFoundError: No module named 'snakemake'

Using --conda-not-block-search-path-envvars prevents this import error, but I'd rather not have to use --conda-not-block-search-path-envvars every time I run snakemake.
I don't see anything special that the SLURM snakemake profile uses to get around this import issue, so I really don't know what to do besides using --conda-not-block-search-path-envvars all of the time.
",-1,-1,-1.0
65947348,Snakemake: error when using split command?,"I'm getting an error when I use the Unix split command in the shell part of my Snakemake rule:
rule split:
    input:
            &quot;test_file.txt&quot;
    output:
            directory(&quot;split_test_file&quot;)
    shell:
            '''
            mkdir {output}
            split -1 3 {input} split_
            mv split_* {output}
            '''

This is the error:
Error in rule split:
jobid: 0
output: split_test_file
shell:
    
    mkdir split_test_file
    split -1 3 test_file.txt split_
    mv split_* split_test_file
    
    (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

I think the error occurs in the mv split_* split_test_file line because when I run only the first 2 lines there is no error. I can't seem to find why I can't move all the files that resulted from splitting test_file.txt into the output directory? Thank you so much!
",-1,-1,-1.0
65948047,snakemake - Missing input files for rule all,"I am trying to create a pipeline that will take a user-configured directory in config.yml (where they have downloaded a project directory of .fastq.gz files from BaseSpace), to run fastqc on sequence files. I already have the downstream steps of merging the fastqs by lane and running fastqc on the merged files.
However, the wildcards are giving me problems running fastqc on the original basespace files. The following is my error when I try running snakemake.
Missing input files for rule all:
qc/fastqc_premerge/DEX-13_S9_L001_ngc1838-10_L001_ds.9fd1f6dff0df47ab821125aab07be69b_r1_fastqc.zip
qc/fastqc_premerge/BOMB-3-2-19D_S8_L002_ngc1838-8_L002_ds.b81c308d62ba447b8caf074ffb27917e_r1_fastqc.zip
qc/fastqc_premerge/DEX-13_S9_L002_ngc1838-10_L002_ds.6369bc71fac44f00931eecb9b0a45d59_r1_fastqc.zip

Any suggestions would be greatly appreciated.  Below is minimal code to reproduce this problem.
import glob

configfile: &quot;config.yaml&quot;

wildcard_constraints:
   bsdir = '\w+_L\d+_ds.\w+',
   lanenum = '\d+'

inputdirectory=config[&quot;directory&quot;]
DIRECTORY, SAMPLES, LANENUMS = glob_wildcards(inputdirectory+&quot;/{bsdir}/{sample}_L{lanenum}_R1_001.fastq.gz&quot;)
DIRECTORY, SAMPLES, LANENUMS = glob_wildcards(inputdirectory+&quot;/{bsdir}/{sample}_L{lanenum}_R2_001.fastq.gz&quot;)


##### target rules #####
rule all:
    input:
       #expand('qc/fastqc_premerge/{sample}_L{lanenum}_{bsdir}_r1_fastqc.zip', sample=SAMPLES, bsdir=DIRECTORY, lanenum=LANENUMS)
        expand('qc/fastqc_premerge/{sample}_L{lanenum}_{bsdir}_r1_fastqc.zip', zip, sample=SAMPLES, bsdir=DIRECTORY, lanenum=LANENUMS)  ##Changed to this from commenters suggestion, however, snakemake still wont run


rule fastqc_premerge_r1:
    input:
        f&quot;{config['directory']}/{{bsdir}}/{{sample}}_L{{lanenum}}_R1_001.fastq.gz&quot;
    output:
        html=&quot;qc/fastqc_premerge/{sample}_L{lanenum}_{bsdir}_r1.html&quot;,
        zip=&quot;qc/fastqc_premerge/{sample}_L{lanenum}_{bsdir}_r1_fastqc.zip&quot; # the suffix _fastqc.zip is necessary for multiqc to find the file. If not using multiqc, you are free to choose an arbitrary filename
    params: &quot;&quot;
    log:
        &quot;logs/fastqc_premerge/{sample}_L{lanenum}_{bsdir}_r1.log&quot;
    threads: 1
    wrapper:
        &quot;v0.69.0/bio/fastqc&quot;

Directory structure:
ngc1838-10_L001_ds.9fd1f6dff0df47ab821125aab07be69b/DEX-13_S9_L001_R1_001.fastq.gz
ngc1838-10_L001_ds.9fd1f6dff0df47ab821125aab07be69b/DEX-13_S9_L001_R2_001.fastq.gz
ngc1838-10_L002_ds.6369bc71fac44f00931eecb9b0a45d59/DEX-13_S9_L002_R1_001.fastq.gz
ngc1838-10_L002_ds.6369bc71fac44f00931eecb9b0a45d59/DEX-13_S9_L002_R2_001.fastq.gz
ngc1838-8_L002_ds.b81c308d62ba447b8caf074ffb27917e/BOMB-3-2-19D_S8_L002_R1_001.fastq.gz
ngc1838-8_L002_ds.b81c308d62ba447b8caf074ffb27917e/BOMB-3-2-19D_S8_L002_R2_001.fastq.gz

In this above case, I would like to run fastqc on all 6 input R1/R2 files, then downstream, create a merged file for DEX_13_S9 (for the two inputs to merge) and BOMB-3_2_19D (which will be a copy of the 1 input). Then create 4 fastqc reports on these resulting R1 and R2 files.
EDIT: I had to change the following to get snakemake to run
inputdirectory=config[&quot;directory&quot;]
PROJECTDIR, RANDOMINT, LANENUM1, BSSTRINGS, SAMPLES, LANENUMS = glob_wildcards(inputdirectory+&quot;/{proj}-{randint}_L{lanenum1}_ds.{bsstring}/{sample}_L{lanenum}_R1_001.fastq.gz&quot;, followlinks=True)
PROJECTDIR, RANDOMINT, LANENUM1, BSSTRINGS, SAMPLES, LANENUMS = glob_wildcards(inputdirectory+&quot;/{proj}-{randint}_L{lanenum1}_ds.{bsstring}/{sample}_L{lanenum}_R2_001.fastq.gz&quot;, followlinks=True)


##### target rules #####
rule all:
    input:
       &quot;qc/multiqc_report_premerge.html&quot;




rule fastqc_premerge_r1:
    input:
        f&quot;{config['directory']}/{{proj}}-{{randint}}_L{{lanenum1}}_ds.{{bsstring}}/{{sample}}_L{{lanenum}}_R1_001.fastq.gz&quot;
    output:
        html=&quot;qc/fastqc_premerge/{sample}_L{lanenum}_{proj}-{randint}_L{lanenum1}_ds.{bsstring}_r1.html&quot;,
        zip=&quot;qc/fastqc_premerge/{sample}_L{lanenum}_{proj}-{randint}_L{lanenum1}_ds.{bsstring}_r1_fastqc.zip&quot; # the suffix _fastqc.zip is necessary for multiqc
    params: &quot;&quot;
    log:
        &quot;logs/fastqc_premerge/{sample}_L{lanenum}_{proj}-{randint}_L{lanenum1}_ds.{bsstring}_r1.log&quot;
    threads: 1
    wrapper:
        &quot;v0.69.0/bio/fastqc&quot;

rule fastqc_premerge_r2:
    input:
        f&quot;{config['directory']}/{{proj}}-{{randint}}_L{{lanenum1}}_ds.{{bsstring}}/{{sample}}_L{{lanenum}}_R2_001.fastq.gz&quot;
    output:
        html=&quot;qc/fastqc_premerge/{sample}_L{lanenum}_{proj}-{randint}_L{lanenum1}_ds.{bsstring}_r2.html&quot;,
        zip=&quot;qc/fastqc_premerge/{sample}_L{lanenum}_{proj}-{randint}_L{lanenum1}_ds.{bsstring}_r2_fastqc.zip&quot; # the suffix _fastqc.zip is necessary for multiqc
    params: &quot;&quot;
    log:
        &quot;logs/fastqc_premerge/{sample}_L{lanenum}_{proj}-{randint}_L{lanenum1}_ds.{bsstring}_r2.log&quot;
    threads: 1
    wrapper:
        &quot;v0.69.0/bio/fastqc&quot;

rule multiqc_pre:
    input:
        expand(&quot;qc/fastqc_premerge/{sample}_L{lanenum}_{proj}-{randint}_L{lanenum1}_ds.{bsstring}_r1_fastqc.zip&quot;, zip, sample=SAMPLES, lanenum=LANENUMS, proj=PROJECTDIR, randint=RANDOMINT, lanenum1=LANENUM1, bsstring=BSSTRINGS),
        expand(&quot;qc/fastqc_premerge/{sample}_L{lanenum}_{proj}-{randint}_L{lanenum1}_ds.{bsstring}_r2_fastqc.zip&quot;, zip, sample=SAMPLES, lanenum=LANENUMS, proj=PROJECTDIR, randint=RANDOMINT, lanenum1=LANENUM1, bsstring=BSSTRINGS)
    output:
        &quot;qc/multiqc_report_premerge.html&quot;
    log:
        &quot;logs/multiqc_premerge.log&quot;
    wrapper:
        &quot;0.62.0/bio/multiqc&quot;


",-1,-1,-1.0
65951574,Moving Snakemake's conda environments to another location,"I would like to know if it's possible to move the ./snakemake/conda to a new location without Snakemake trying to reinstall all environments with a new hash (yaml). For example moving /mnt/Serge/.snakemake/conda to /mnt/tools/.snakemake/conda. I've already tried to copy the folder to the new location and adjust --conda-prefix /mnt/Serge/.snakemake/conda to --conda-prefix /mnt/tools/.snakemake/conda but that didn't work out and it started to create new environments next to the old ones. Does this have to do with symlinks and how do I solve this issue without reinstalling all the tools?
",-1,-1,-1.0
66108145,How to handle python version/package conflicts in snakemake?,"I am attempting to build a pipeline using snakemake. I have created a conda environment which uses python v3.9 and also contains the snakemake program. My first couple of rules need v3.9. All the rules work great until I get to my 3rd rule, called rule3, where I want to use pysam, which is currently unavailable with python 3.9, and therefore can't be installed in the main conda environment. I'm aware that snakemake is supposed to be able to navigate these kinds of issues by being able to use multiple conda environments. I therefore included the yaml for a python environment that has both pysam and python v3.8. In rule3 I include the following lines
conda:
        &quot;Env2.yaml&quot; # used for running pysam

run:
        import pysam
        import sys
        import os
        .
        .
        .
        .

When I attempt to run snakemake I am left with the following message:
Conda environments are only allowed with shell, script, notebook, or wrapper directives (not with run).
This means you cannot run python in snakemake with different conda environments.
Given the problem above, my question is: What is the right way to deal with python version/package conflicts given that you can't run python code with the different conda environments in snakemake?
",-1,-1,-1.0
66115492,Change the directory name and fastq file name with snakemake,"I would like to change the folder names and the names of the read files. From here I found something similar (Move and rename files from multiple folders using snakemake):
workdir: &quot;/path/to/workdir/&quot;

import pandas as pd
from io import StringIO

sample_file = StringIO(&quot;&quot;&quot;fastq sampleID
BOB_1234/fastq/BOB_1234.R1.fastq.gz TAG_1/fastq/TAG_1.R1.fastq.gz
BOB_1234/fastq/BOB_1234.R2.fastq.gz TAG_1/fastq/TAG_1.R2.fastq.gz
BOB_3421/fastq/BOB_3421.R1.fastq.gz TAG_2/fastq/TAG_2.R1.fastq.gz
BOB_3421/fastq/BOB_3421.R2.fastq.gz TAG_2/fastq/TAG_2.R2.fastq.gz&quot;&quot;&quot;)

df = pd.read_table(sample_file, sep=&quot;\s+&quot;, header=0)
sampleID = df.sampleID
fastq = df.fastq

rule all:
    input:
       expand(&quot;{sample}&quot;, sample=df.sampleID)

rule move_and_rename_fastqs:
    input: fastq = lambda w: df[df.sampleID == w.sample].fastq.tolist()
    output: &quot;{sample}&quot;
    shell:
        &quot;&quot;&quot;echo mv {input.fastq} {output}&quot;&quot;&quot;

I can an error:
MissingOutputException in line 19 of snakefile:
Job Missing files after 5 seconds:
TAG_1/fastq/TAG_1.R1.fastq.gz

",-1,-1,-1.0
66228374,snakemake rule won't run the complete command,"I am working on this snakemake pipeline where the last rule looks like this:
rule RunCodeml:
input:
                '{ProjectFolder}/{Fastas}/codeml.ctl'
        output:
                '{ProjectFolder}/{Fastas}/codeml.out'
        shell:
                'codeml {input}'

This rule does not run and the error seems to be that the program codeml can't find the .ctl file because it looks for an incomplete path: '/work_beegfs/sunam133/Anas_plasmids/Phylo_chromosome/Acinet_only/Klebs_Esc_SCUG/cluster_536/co'
although the command seems correct:
shell:
        codeml /work_beegfs/sunam133/Anas_plasmids/Phylo_chromosome/Acinet_only/Klebs_Esc_SCUG/cluster_536/codeml.ctl
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)'

And here the output from running with -p option:
error when opening file /work_beegfs/sunam133/Anas_plasmids/Phylo_chromosome/Acinet_only/Klebs_Esc_SCUG/cluster_1083/co
tell me the full path-name of the file? Can't find the file.  I give up.

I find this behavior very strange and I can't figure out what is going on. Any help would be appreciated.
Thanks!
D.
",-1,-1,-1.0
66493234,snakemake --max-jobs-per-second parameter ignored,"I am currently running Snakemake on my department's cluster (SGE). For this I have used a template given by a workshop to run and submit jobs to the scheduler and run my scripts within the different rules. The template for the profile is taken from this snakemake-gridengine repository.
However, I am running into an issue where Snakemake is not submitting the max number of jobs it should be able to the cluster at once.
snakemake --snakefile pop_split_imputation_pipeline.smk \
  -j 1000 --max-status-checks-per-second 0.01 \
  --profile ~/snakemake/profile -f --rerun-incomplete --use-conda

For instance, above is an example of a command used to submit a .smk pipeline to be run, which in theory should generate 1000 jobs per rule. However, within my cluster, only 10-50 jobs at any one time are being submitted. Within my config.yaml I already have set max-jobs-per-second: 1000, so clearly it should be able to submit all these jobs at once yet it doesn't.
Can anyone point to something to improve the submission speed of these jobs?
",-1,-1,-1.0
66502389,snakemake error when running two jobs at once that use same conda environment,"I am encountering an error when executing a Snakemake (6.0.0) workflow such that two jobs are launched simultaneously, on the same node, both of which use the same conda environment. Minimal example is below.
A few observations:

The problem occurs when running the workflow on a node of my institutional cluster, but not on my local machine. (I am running snakemake from within an interactive slurm job with &gt;1 cpu; I am using Miniconda, provided as a module by my cluster sysadmins)
The workflow completes just fine when the tasks are forced to run serially (snakemake --use-conda -j1). The problem only occurs when -j2 or higher (not exceeding the number of cores available in the slurm allocation). The first job seems to run just fine, it's always the second job that croaks.
I can activate the conda environment at issue just fine after it has been created by snakemake (e.g. after running the workflow, conda activate /long_path_to_cluster_project_folder/testing/conda_test/.snakemake/conda/c4751dca works, I can run R from within, etc.)
If I run snakemake --use-conda -j2, the only error I get is (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!) below the shell commands run. If I add --verbose, a lengthy traceback is printed in blue and red, which I've included below. The relevant bit seems to be:
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 505, in prefix_path
    return self.info[&quot;conda_prefix&quot;]
AttributeError: 'Conda' object has no attribute 'info'


Suspecting some kind of race condition, I also tried adding --max-jobs-per-second=0.5 to throttle the jobs so they wouldn't start at the same time, but that appeared to have no effect (jobs start at same time, same error as before. I am not running snakemake with --cluster or --profile or anything; there are no extra slurm jobs being created, just processes spawned on the same compute node)
The same problem occurs if I create two totally different Snakemake rules that end up being executed at once, as long as both rules use the same conda environment.

I'm pretty new to both snakemake and to HPC, but seems like this is somewhere between a system-/configuration-specific problem (since it only occurs on the cluster) and a minor snakemake bug (since snakemake seems to be attributing the problem to my shell script rather than something to do with conda). I'm interested in suggestions for how to troubleshoot further or to work around the problem.
Thanks!
Minimal example:
├── input.txt
├── results
└── workflow
    ├── Snakefile
    └── envs
        └── env1.yaml


workflow/Snakefile:
rule all:
    input:
        'results/output1.txt',
        'results/output2.txt',
        'results/output3.txt',
        'results/output4.txt'

rule rule1:
    input: 'input.txt'
    output:
        'results/output{n}.txt'
    conda: 'envs/env1.yaml'
    shell:&quot;&quot;&quot;
    sleep 5s
    touch {output}
    &quot;&quot;&quot;


workflow/envs/env1.yaml:
channels:
- conda-forge
- bioconda
- defaults
dependencies:
- r-ggplot2



$ snakemake --use-conda -j2 -p --verbose
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 2
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   all
    4   rule1
    5

&lt;&lt; snip &gt;&gt;


[Fri Mar  5 21:01:33 2021]
Error in rule rule1:
    jobid: 2
    output: results/output2.txt
    conda-env: /long_path_to_cluster_project_folder/testing/conda_test/.snakemake/conda/c4751dca
    shell:
        
    sleep 5s
    touch results/output2.txt
    
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Full Traceback (most recent call last):
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 2326, in run_wrapper
    run(
  File &quot;/long_path_to_cluster_project_folder/testing/conda_test/workflow/Snakefile&quot;, line 33, in __rule_rule1
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/shell.py&quot;, line 141, in __new__
    cmd = Conda(container_img).shellcmd(conda_env, cmd)
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 512, in shellcmd
    activate = os.path.join(self.bin_path(), &quot;activate&quot;)
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 508, in bin_path
    return os.path.join(self.prefix_path(), &quot;bin&quot;)
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 505, in prefix_path
    return self.info[&quot;conda_prefix&quot;]
AttributeError: 'Conda' object has no attribute 'info'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 568, in _callback
    raise ex
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/concurrent/futures/thread.py&quot;, line 52, in run
    result = self.fn(*self.args, **self.kwargs)
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 554, in cached_or_run
    run_func(*args)
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 2357, in run_wrapper
    raise RuleException(
snakemake.exceptions.RuleException: AttributeError in line 13 of /long_path_to_cluster_project_folder/testing/conda_test/workflow/Snakefile:
'Conda' object has no attribute 'info'
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 2326, in run_wrapper
  File &quot;/long_path_to_cluster_project_folder/testing/conda_test/workflow/Snakefile&quot;, line 13, in __rule_rule1
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 512, in shellcmd
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 508, in bin_path
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 505, in prefix_path

RuleException:
AttributeError in line 13 of /long_path_to_cluster_project_folder/testing/conda_test/workflow/Snakefile:
'Conda' object has no attribute 'info'
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 2326, in run_wrapper
  File &quot;/long_path_to_cluster_project_folder/testing/conda_test/workflow/Snakefile&quot;, line 13, in __rule_rule1
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 512, in shellcmd
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 508, in bin_path
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 505, in prefix_path
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 568, in _callback
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/concurrent/futures/thread.py&quot;, line 52, in run
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 554, in cached_or_run
  File &quot;/long_path_to_cluster_project_folder/conda_envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 2357, in run_wrapper

",-1,-1,-1.0
66709298,"snakemake: conda env relative path to the specific smk file, not the execution path (i.e. Snakefile with rule all)","I've written a snakemake workflow and to make code more manageable I split the rules into separate smk files. My current directory structure looks like this:
|-- Snakefile # with statements like include: utils/align.smk 
|-- config.yaml
|-- envs
|   `-- vep.yml
`-- utils
    |-- align.smk
    |-- envs
    |   |-- alignment.yml
    |   |-- mutect2.yml
    |   |-- quality_check.yml
    |   `-- vep.yml
    |-- process_reads.smk
    `-- variant_calling.smk

I also wrote the workflow to rely on conda and specified all the requirements to separate yaml files. Now, the location of the files is specified in config.yaml, for example:
CONDA_ALIGNMENT: &quot;utils/envs/alignment.yml&quot;

and then rules in align.smk call those by conda: config['CONDA_ALIGNMENT'].
However, now that I try to run snakemake, I get the following errror:
snakemake -np --profile cluster --jobs 2 --use-conda --force
Building DAG of jobs...
WorkflowError:
Failed to open source file /path/to/exec/utils/utils/envs/alignment.yml
FileNotFoundError: [Errno 2] No such file or directory: '/path/to/exec/utils/utils/envs/alignment.yml'
  File &quot;/path/to/miniconda3/envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 228, in create
  File &quot;/path/to/miniconda3/envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 98, in hash
  File &quot;/path/to/miniconda3/envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 80, in content
  File &quot;/path/to/miniconda3/envs/snakemake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 67, in _get_content

Note that utils directory appears twice in the path for the file snakemake cannot find. All the other files in inputs/params/outputs... work when specified relative to the main Snakefile, rather than the particular smk file. I would be very grateful for any suggestions on how to deal with this in more snakemake way.
The fastest fix I can come up with would be to move the smk files out of utils but then it would be quite messy...
",-1,-1,-1.0
66834154,How are git repositories used by Snakemake via kubernetes?,"The Snakemake documentation here:
https://snakemake.readthedocs.io/en/stable/executing/cloud.html
states the following under the section heading &quot;Executing a Snakemake workflow via kubernetes&quot;:

Currently, this mode requires that the Snakemake workflow is stored in a git repository. Snakemake uses git to query necessary source files (the Snakefile, scripts, config, …) for workflow execution and encodes them into the kubernetes job.

This is confusing to me.  Looking at the example command line given:
snakemake --kubernetes --use-conda --default-remote-provider $REMOTE --default-remote-prefix $PREFIX

I don't see any reference to a git repository.  It seems to me that Snakemake will look for the snakefile on the local host, in the working directory where this command is issued from.  What is this business about a git repository?
",-1,-1,-1.0
66863103,Snakemake Conda environment does not seem to be activating though it says it is,"I am running Snakemake with the --use-conda option. Snakemake successfully creates the environment, which should include pysam. I am able to manually activate this created environment, and within it, run my script split_strands.py, which imports the module pysam, with no problems. However, when running the Snakemake pipeline, I get the following error log:
Activating conda environment: /projects/ps-yeolab3/ekofman/sc_STAMP_pipeline/STAMP/workflow/.snakemake/conda/7c375b6b
/projects/ps-yeolab3/ekofman/sc_STAMP_pipeline/STAMP/workflow/scripts/split_strands.py:166: SyntaxWarning: &quot;is not&quot; with a literal. Did you mean &quot;!=&quot;?
  if args.output_fwd_bam is not '-':
/projects/ps-yeolab3/ekofman/sc_STAMP_pipeline/STAMP/workflow/scripts/split_strands.py:171: SyntaxWarning: &quot;is not&quot; with a literal. Did you mean &quot;!=&quot;?
  if args.output_rev_bam is not '-':
Traceback (most recent call last):
  File &quot;/projects/ps-yeolab3/ekofman/sc_STAMP_pipeline/STAMP/workflow/scripts/split_strands.py&quot;, line 20, in &lt;module&gt;
    import pysam
ModuleNotFoundError: No module named 'pysam'
[Mon Mar 29 16:41:06 2021]
Error in rule split_strands:
    jobid: 0
    output: 1_split_strands/TWA1_possorted_genome_bam_MD-GTCGCGACACGAGGTA-1.bam.fwd.bam, 1_split_strands/TWA1_possorted_genome_bam_MD-GTCGCGACACGAGGTA-1.bam.rev.bam
    conda-env: /projects/ps-yeolab3/ekofman/sc_STAMP_pipeline/STAMP/workflow/.snakemake/conda/7c375b6b
    shell:
        
        python scripts/split_strands.py -i /projects/ps-yeolab3/ekofman/sc_STAMP_pipeline/STAMP/workflow/inputs/TWA1_possorted_genome_bam_MD-GTCGCGACACGAGGTA-1.bam -f 1_split_strands/TWA1_possorted_genome_bam_MD-GTCGCGACACGAGGTA-1.bam.fwd.bam -r 1_split_strands/TWA1_possorted_genome_bam_MD-GTCGCGACACGAGGTA-1.bam.rev.bam
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Nodes:        tscc-1-37

So as you can see, though it says it is &quot;Activating conda environment&quot;, this does not seem to be true as subsequently the module 'pysam' is not found, which I've verified would be found when activating manually.
This is how the rule is specified:
rule split_strands:
    input: 
        input_bam=config[&quot;samples_path&quot;]+&quot;{sample}&quot;,
        index=config[&quot;samples_path&quot;]+&quot;{sample}.bai&quot;
    output: 
        output_fwd=&quot;1_split_strands/{sample}.fwd.bam&quot;,
        output_rev=&quot;1_split_strands/{sample}.rev.bam&quot;
    conda:
        &quot;envs/python2.7.yaml&quot;
    shell:
        &quot;&quot;&quot;
        python scripts/split_strands.py -i {input.input_bam} -f {output.output_fwd} -r {output.output_rev}
        &quot;&quot;&quot;

I have verified that the hash 7c375b6b corresponds to the appropriate env specified in python2.7.yaml.
Any ideas what might be happening? My rules are being run a cluster and submitted via qsub commands.
",-1,-1,-1.0
66868935,Snakemake --use-conda with --cluster and NFS4 storage,"I am using snakemake in cluster mode to submit a simple one rule workflow to the HPCC, which runs Torque with several compute nodes. The NFSv4 storage is mounted on /data. There is a link /PROJECT_DIR -&gt; /data/PROJECT_DIR/
I submit the job using:
snakemake --verbose --use-conda --conda-prefix /data/software/miniconda3-ngs/envs/snakemake \
--rerun-incomplete --printshellcmds --latency-wait 60  \ 
--configfile /PROJECT_DIR/config.yaml -s '/data/WORKFLOW_DIR/Snakefile' --jobs 100 \
--cluster-config '/PROJECT_DIR/cluster.json' \
--cluster 'qsub -j oe -l mem={cluster.mem} -l walltime={cluster.time} \
                      -l nodes={cluster.nodes}:ppn={cluster.ppn}'

The jobs fails with:
Error in rule fastqc1:                                      
    jobid: 1                                          
    output: /PROJECT_DIR/OUTPUT_DIR/SAMPLE_fastqc.html                                    
    conda-env: /data/software/miniconda3-ngs/envs/snakemake/74019bbc                     
    shell: 
                                                                                
        fastqc -o /PROJECT_DIR/OUTPUT_DIR/ -t 4 -f fastq /PROJECT_DIR/INPUT/SAMPLE.fastq.gz 
    (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)
    cluster_jobid: 211078.CLUSTER

    Error executing rule fastqc1 on cluster (jobid: 1, external: 211078.CLUSTER, jobscript:
    PROJECT_DIR/.snakemake/tmp.t5a2dpxe/snakejob.fastqc1.1.sh). For error details see the cluster
    log and the log files of the involved rule(s).

The jobscript submitted looks like this:
Jobscript: 
#!/bin/sh                                             
# properties = {&quot;type&quot;: &quot;single&quot;, &quot;rule&quot;: &quot;fastqc1&quot;, &quot;local&quot;: false, &quot;input&quot;: 
  [&quot;/PROJECT_DIR/INPUT_DIR/SAMPLE.fastq.gz&quot;], &quot;output&quot;: [&quot;/PROJECT_DIR/OUTPUT_DIR/SAMPLE_fastqc.html&quot;],
  &quot;wildcards&quot;: {&quot;sample&quot;: &quot;SAMPLE&quot;, &quot;read&quot;: &quot;1&quot;},
  &quot;params&quot;: {}, &quot;log&quot;: [], &quot;threads&quot;: 4, &quot;resources&quot;: {}, &quot;jobid&quot;: 1, &quot;cluster&quot;: {&quot;nodes&quot;: 1,
  &quot;ppn&quot;: 4, &quot;time&quot;: &quot;01:00:00&quot;, &quot;mem&quot;: &quot;32gb&quot;}}                                         
  
  cd /data/PROJECT_DIR &amp;&amp; \
  PATH='/data/software/miniconda3-ngs/envs/snakemake-5.32.2/bin':$PATH \
  /data/software/miniconda3-ngs/envs/snakemake-5.32.2/bin/python3.8 \ 
  -m snakemake /PROJECT_DIR/OUTPUT_DIR/SAMPLE_fastqc.html --snakefile /data/WORKFLOW_DIR/Snakefile \
  --force -j --keep-target-files --keep-remote --max-inventory-time 0 \                  
  --wait-for-files /data/PROJECT_DIR/.snakemake/tmp.t5a2dpxe \
  /PROJECT_DIR/INPUT/SAMPLE.fastq.gz /data/software/miniconda3-ngs/envs/snakemake/74019bbc --latency-wait 60 \ 
  --attempt 1 --force-use-threads --scheduler ilp \
  --wrapper-prefix https://github.com/snakemake/snakemake-wrappers/raw/ \  
  --configfiles /PROJECT_DIR/config.yaml -p --allowed-rules fastqc1 --nocolor --notemp --no-hooks --nolock \    
  --mode 2  --use-conda --conda-prefix /data/software/miniconda3-ngs/envs/snakemake  \
  &amp;&amp; touch /data/PROJECT_DIR/.snakemake/tmp.t5a2dpxe/1.jobfinished || \
  (touch /data/PROJECT_DIR/.snakemake/tmp.t5a2dpxe/1.jobfailed; exit 1) 

Somehow when using an interactive qsub shell to run the workflow locally on a single computing node, this problem does not occur. It only happens when submitting the job to the entire computing cluster from the login node.
snakemake versions tested:

5.10.0
5.32.2
6.0.5

",-1,-1,-1.0
66882849,snakemake how to use glob_wilcards properly,"I have many paired fastq files and I have a problem on after running trim_galore package, as it named the fastq files with _1_val_1 and _2_val_2, for example:
AD50_CTGATCGTA_1_val_1.fq.gz and
AD50_CTGATCGTA_2_val_2.fq.gz.
I would like continue snakemake and use
import os
import snakemake.io
import glob

DIR=&quot;AD50&quot;
(SAMPLES,READS,) = glob_wildcards(DIR+&quot;{sample}_{read}.fq.gz&quot;)
READS=[&quot;1&quot;,&quot;2&quot;]

rule all:
    input:
        expand(DIR+&quot;{sample}_dedup_{read}.fq.gz&quot;,sample=SAMPLES,read=READS)

rule clumpify:
    input:
        r1=DIR+&quot;{sample}_1_val_1.fq.gz&quot;,
        r2=DIR+&quot;{sample}_2_val_2.fq.gz&quot;
    output:
        r1out=DIR+&quot;{sample}_dedup_1.fq.gz&quot;,
        r2out=DIR+&quot;{sample}_dedup_2.fq.gz&quot;
    shell:
        &quot;clumpify.sh in={input.r1} in2={input.r2} out={output.r1out} out2={output.r2out} dedupe subs=0&quot;

and the error is:
Building DAG of jobs...
MissingInputException in line 13 of /home/peterchung/Desktop/Rerun-Result/clumpify.smk:
Missing input files for rule clumpify:
AD50/AD50_CTGATCGTA_2_val_2_val_2.fq.gz
AD50/AD50_CTGATCGTA_2_val_1_val_1.fq.gz

I tired another way, somehow the closest is that it detected the missing input like
AD50_CTGATCGTA_1_val_2.fq.gz and AD50_CTGATCGTA_2_val_1.fq.gz which is not exist.
I am not sure the glob_wildcards function I used properly since there are many underscore in it. I tired:
 glob_wildcards(DIR+&quot;{sample}_{read}_val_{read}.fq.gz&quot;)

but it did not work as well.
",-1,-1,-1.0
66905263,Snakemake-Wildcards in input files cannot be determined from output files,"I'm kinda new at snakemake and I'm trying to understand how it works.
I tried to pull a simple snakefile
from snakemake.utils import min_version
min_version(&quot;5.3.0&quot;)
max_reads: 250000
sra_id: [&quot;SRR1187735&quot;]
rule all:
    input:
        &quot;DATA/{sra_id}.fastq.gz&quot;
    
rule prefetch:
    output:
        &quot;DATA/{sra_id}.fastq.gz&quot;
    params:
        max_reads = &quot;max_reads&quot;
    version: &quot;1.0&quot;
    shell:
        &quot;conda activate sra-tools-2.10.1 &quot; 
        &quot;&amp;&amp; &quot;
        &quot;fastq-dump {wildcards.sra_id} -X {params.max_reads} --readids \
            --dumpbase --skip-technical --gzip -Z &gt; {output} &quot;
        &quot;&amp;&amp; &quot;
        &quot;conda deactivate &quot;

But I'm getting this error :

WildcardError in line 5 of /save_home/skastalli/test_rule/Snakefile:
Wildcards in input files cannot be determined from output files:
'sra_id'

Can someone help me please ?
",-1,-1,-1.0
66964776,"Snakemake-based software does not work on slurm cluster, while it works in the login node","I am trying to run a snakemake-based software https://github.com/MeHelmy/princess with SLURM on our cluster https://gitlab.com/cigene/computational/orion-support.
In brief,
the issue is that when I run the software on the login node, it works. However, when I submit a slurm script to run this software, it gives me an error that snakemake/temp file is not found. It takes unrealistically long to finish the job without cluster, so I would really appreciate your help.
So far, we, the IT team and the developer have tried:
(1) check yaml file setting with our cluster managers - looks fine
(2) add --latency-wait 120 - no visible change
(3) check if there is a restriction to delete temp files from the cluster - there is no such restriction
What I ran:
(base) [mariesai@cn-4 ~]$ module load PyYAML/5.1.2-GCCcore-8.3.0
(base) [mariesai@cn-4 ~]$ module load Miniconda3/4.7.10
(base) [mariesai@cn-4 ~]$ module load snakemake/5.30.1
(base) [mariesai@cn-4 ~]$ module load HTSlib 

The following have been reloaded with a version change:
  1) GCCcore/8.3.0 =&gt; GCCcore/9.3.0
  2) XZ/5.2.4-GCCcore-8.3.0 =&gt; XZ/5.2.5-GCCcore-9.3.0
  3) binutils/2.32-GCCcore-8.3.0 =&gt; binutils/2.34-GCCcore-9.3.0
  4) bzip2/1.0.8-GCCcore-8.3.0 =&gt; bzip2/1.0.8-GCCcore-9.3.0
  5) zlib/1.2.11-GCCcore-8.3.0 =&gt; zlib/1.2.11-GCCcore-9.3.0

(base) [mariesai@cn-4 princesstest]$ python3.7  /net/fs-1/Transpose/Software/princess/princess all -d $PWD/0406/2.without.e -r ont -s $PWD/test.fastq.gz --chr ssa18 -f /mnt/SCRATCH/kristenl/Reference/Simon_Sept2020.fasta -j 200  --rerun-incomplete --verbose 


Error digest:
[Tue Apr  6 09:05:56 2021]
Error in rule readsStat:
    jobid: 6
    output: /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/statitics/raw_reads/reads_stat.txt
    conda-env: /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/.snakemake/conda/1b58da75
    shell:
        
        python scripts/rawcoverage.py -i /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/test.fastq.gz -o /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/statitics/raw_reads/reads_stat.txt -t 5
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)
    cluster_jobid: 12727829

Error executing rule readsStat on cluster (jobid: 6, external: 12727829, jobscript: /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/.snakemake/tmp.4xwny_7b/snakejob.readsStat.6.sh). For error details see the cluster log and the log files of the involved rule(s).

Log digest
slurm-12727829.out
Waiting at most 5 seconds for missing files.
Missing files after 5 seconds:
/net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/.snakemake/tmp.4xwny_7b

slurm-12727823.out
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 5
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   readsStat
    1
Select jobs to execute...

[Tue Apr  6 09:05:21 2021]
Job 0: Calculating read coverage statitics for: /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/test.fastq.gz


        python scripts/rawcoverage.py -i /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/test.fastq.gz -o /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/statitics/raw_reads/reads_stat.txt -t 5
        
Activating conda environment: /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/.snakemake/conda/1b58da75
Waiting at most 5 seconds for missing files.
MissingOutputException in line 10 of /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/modules/stat.smk:
Job Missing files after 5 seconds:
/net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/statitics/raw_reads/reads_stat.txt
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Job id: 0 completed successfully, but some output files are missing. 0
  File &quot;/cluster/software/snakemake/5.30.1/lib/python3.6/site-packages/snakemake/executors/__init__.py&quot;, line 575, in handle_job_success
  File &quot;/cluster/software/snakemake/5.30.1/lib/python3.6/site-packages/snakemake/executors/__init__.py&quot;, line 254, in handle_job_success
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

slurm-12727825.out
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   readsStat
    1
Select jobs to execute...

[Tue Apr  6 09:05:46 2021]
Job 0: Calculating read coverage statitics for: /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/test.fastq.gz


        python scripts/rawcoverage.py -i /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/test.fastq.gz -o /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/statitics/raw_reads/reads_stat.txt -t 1
        
Activating conda environment: /net/cn-1/mnt/SCRATCH/princesstest/0406/2.without.e/.snakemake/conda/1b58da75
[Tue Apr  6 09:06:02 2021]
Finished job 0.
1 of 1 steps (100%) done


",-1,-1,-1.0
66818700,Acess scripts in external snakemake directory without defining absolute path,"My end goal is to host a snakemake workflow on a GitHub repo that can be accessed as a snakemake module. I'm testing locally before I host it, but I'm running into an issue. I cannot access the scripts in the snakemake module directory. It looks locally in the current snakemake directory for the scripts, which I obviously cannot move locally if my end goal is to host the module remotely.
I don't see this problem when accessing Conda environments in the remote directory. Is there a way to mimic this behavior for a scripts directory? I would be open to an absolute path reference if it can be applied to access a remote script directory. Here's a dummy example reproducing the error:
Snakemake version: 6.0.5
Tree structure:
.
├── external_module
│   ├── scripts
│   │   ├── argparse
│   │   └── print.py
│   └── Snakefile
└── Snakefile


Local snakefile:
module remote_module:
    snakefile: &quot;external_module/Snakefile&quot;

use rule * from remote_module

use rule foo from remote_module with:
    input:
        &quot;complete.txt&quot;

External Snakefile:
rule foo:
    input:
        &quot;complete.txt&quot;

rule bar:
    output:
        touch(temp(&quot;complete.txt&quot;))
    shell:
        &quot;scripts/print.py -i foo&quot;

print.py
import argparse

def get_parser():
    parser = argparse.ArgumentParser(
        description='dummy snakemake function',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument(&quot;-i&quot;, default=None,
                        help=&quot;item to be printed&quot;)
    
    return parser

def main():
    args = get_parser().parse_args()
    print(args.i)

if __name__ == '__main__':
    main()

Snakemake pipeline execution
(base) bobby@SongBird:~/remote_snakemake_test$ snakemake --cores 4
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       bar
        1       foo
        2

[Fri Mar 26 10:12:50 2021]
rule bar:
    output: complete.txt
    jobid: 1

/usr/bin/bash: scripts/print.py: No such file or directory
[Fri Mar 26 10:12:50 2021]
Error in rule bar:
    jobid: 1
    output: complete.txt
    shell:
        scripts/print.py -i foo
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/bobby/remote_snakemake_test/.snakemake/log/2021-03-26T101250.118440.snakemake.log

Any insight would be very appreciated. Thanks!
",-1,-1,-1.0
66801501,Snakemake : subworkflow not playing well with the main DAG,"I have a main Snakefile and several subworkflows running in independent subdirectories (with paths relative to their own directories). I've noticed that if I modify one of the input of a subworkflow, it will rerun correctly but all the following rules that come afterwards are not rerun.
If I understand correctly what is going on, there's a different DAG for the main Snakefile and for each subworkflow. The main DAG is not aware of any modification in a subworkflow and therefore won't trigger a rerun since the output of the subworkflow hasn't been modified yet.
I'd like that all the rules depending of the output of a subworkflow are rerun if there's a modification in that subworkflow. Isn't that what the default behaviour should be ?
I've also tried the other modularisation techniques. Using includes works but is super annoying because I have to modify all the paths to be relative to the main directory (and therefore I can't run snakemake independently in one subdirectory anymore). I've also tried using the new module system coming with snakemake v.6 that is supposed to be replacing subworkflows. Maybe I don't use it correctly, but it doesn't seem to work for my use case. If I import a rule from a subdirectory it complains that there are missing inputs. It doesn't find the scripts because they are in the subdirectory and not in the main directory. So in that sense it works more like an include than a subworkflow.
Do you have any idea on how to solve my issue ?
Here's a small working example with the module implementation:
MainDirectory
| - Snakefile
rule all:
    input: &quot;Subdirectory/file.txt&quot;

module other_workflow:
    snakefile: &quot;Subdirectory/Snakefile&quot;

use rule * from other_workflow as other_*

| - Subdirectory
| | - Snakefile
rule rule_a:
    input:
        script = 'code.py'
    output: 'file.txt'
    shell: 'python {input.script}'

| | - code.py
with open('file.txt', 'w') as f:
    print('This is a test.', file=f)

This doesn't work as the snakefile in the main directory uses all the rules in the same workdir, whereas I would like it to be running the imported rules in their own workdir. I can make it work by modifying all the relative paths in the subdirectory but that's not what I want. I want to be able to run it without modifications.
",1,-1,-1.0
67197779,Snakemake: output files in one output directory,"The program I am running requires only the directory to be specified in order to save the output files. If I run this with snakemake it is giving me error:
IOError: [Errno 2] No such file or directory: 'BOB/call_images/chrX_194_1967_BOB_78.rpkm.png'

My try:
rule all:
    input:
        &quot;BOB/call_images/&quot;

rule plot:
    input:
        hdf=&quot;BOB/hdf5/analysis.hdf5&quot;,
        txt=&quot;BOB/calls.txt&quot;
    output:
        directory(&quot;BOB/call_images/&quot;)
    shell:
        &quot;&quot;&quot;
        python someprogram.py plotcalls --input {input.hdf} --calls {input.txt} --outputdir {output[0]}
        &quot;&quot;&quot;

Neither this version works:
output:
     outdir=&quot;BOB/call_images/&quot;

",-1,-1,-1.0
67316287,"Snakemake: Job preemption can interrupt running jobs on clusters, how to make sure that the task is not considered as failed?","I'm using Snakemake on a cluster, and I don't know how best to handle the fact that some jobs can be preempted.
For more power on the cluster I use, it is possible to have access to the resources of other teams, but with the risk of being preempted, which consists in stopping the job in progress, and rescheduling it. It will be launched again as soon as a resource is available. This is especially advantageous when you have a lot of quick jobs to run. Unfortunately, I don't have the impression that Snakemake supports this properly.
In the example given in the help on the cluster-status feature for Slurm, there is no PREEMPTED in the running_status list (running_status=[&quot;PENDING&quot;, &quot;CONFIGURING&quot;, &quot;COMPLETING&quot;, &quot;RUNNING&quot;, &quot;SUSPENDED&quot;]), which may lead to consider a preempted job has failed. Not a big deal, I’ve added PREEMPTED to this list, but I am led to believe that Snakemake did not consider this scenario.
More annoyingly, even when running Snakemake with the --rerun-incomplete option, when the job is interrupted by the preemption, then restarted, I get the following error:
IncompleteFilesException:
The files below seem to be incomplete. If you are sure that certain files are not incomplete, mark them as complete with

    snakemake --cleanup-metadata &lt;filenames&gt;

To re-generate the files rerun your command with the --rerun-incomplete flag.

I would expect the interrupted job to restart from scratch.
For now, the only solution I have found is to stop using other teams' resources to avoid having my jobs preempted, but I am losing computing power.
How do you use Snakemake in a context where your jobs can be preempted? Anyone see a solution so I don't get the IncompleteFilesException anymore?
Thanks in advance
",1,-1,-1.0
67324889,Snakemake: trimmomatic wrapper attribute error,"I have a snakemake pipeline that looks like this:
configfile: &quot;./config.yaml&quot;
IN_DIR = config[&quot;in_dir&quot;]
SAMPLES = config[&quot;samples&quot;]

rule all:
    input: 
        expand(&quot;{sample}_Aligned.sortedByCoord.out.bam&quot;, sample=SAMPLES)

rule trimmomatic_pe:
    message:
        &quot;&quot;&quot;
        Pre-processing raw reads with trimmomatic. Trimming low quality reads and adapter sequences. Running QC on trimmed reads.
        &quot;&quot;&quot;
    input:
        r1 = expand(&quot;{in_dir}/{{sample}}_R1_001.fastq.gz&quot;, in_dir=IN_DIR),
        r2 = expand(&quot;{in_dir}/{{sample}}_R2_001.fastq.gz&quot;, in_dir=IN_DIR)
    params:
        trimmer = config[&quot;parameters&quot;][&quot;trim&quot;],
        extra = &quot;&quot;
    output:
        r1 = &quot;tmp/{sample}_R1_trimmed.fastq.gz&quot;,
        r2 = &quot;tmp/{sample}_R2_trimmed.fastq.gz&quot;,
        r1_unpaired = &quot;tmp/{sample}_R1_unpaired_trimmed.fastq.gz&quot;,
        r2_unpaired = &quot;tmp/{sample}_R2_unpaired_trimmed.fastq.gz&quot;
    threads:
        2
    wrapper:
        &quot;0.74.0/bio/trimmomatic/pe&quot;

rule map_reads:
    message:
        &quot;&quot;&quot;
        Mapping trimmed reads to host genome
        &quot;&quot;&quot;
    input:
        r1 = &quot;tmp/{sample}_R1_trimmed.fastq.gz&quot;,
        r2 = &quot;tmp/{sample}_R2_trimmed.fastq.gz&quot;
    params:
        annotation = config[&quot;annotation_file&quot;]
    output:
        &quot;{sample}_Aligned.sortedByCoord.out.bam&quot;
    shell:
        &quot;&quot;&quot;
        STAR \
            --runThreadN 16 \
            --sjdbGTFfile {params.annotation} \
            --sjdbOverhang 149 \
            --outFilterType BySJout \
            --outFilterMultimapNmax 10 \
            --alignSJoverhangMin 5 \
            --alignSJDBoverhangMin 1 \
            --outFilterMismatchNmax 999 \
            --outFilterMismatchNoverReadLmax 0.04 \
            --alignIntronMin 20 \
            --alignIntronMax 1000000 \
            --alignMatesGapMax 1000000 \
            --outFilterIntronMotifs RemoveNoncanonicalUnannotated \
            --outFileNamePrefix {wildcards.sample}_ \
            --outSAMtype BAM SortedByCoordinate \
            --runMode alignReads \
            --genomeDir ./index \
            --readFilesIn {input.r1} {input.r2}
        &quot;&quot;&quot;

When I run snakemake -np the DAG is correctly made, but I keep getting this error that I don't know how to interpret when I try to actually run the pipeline with snakemake --cores 2:
[Thu Apr 29 15:12:21 2021]
Job 1: 
        Pre-processing raw reads with trimmomatic. Trimming low quality reads and adapter sequences. Running QC on trimmed reads.
        

Traceback (most recent call last):
  File &quot;/Users/user/Documents/postdoc_projects/invert/.snakemake/scripts/tmp7yzyzwru.wrapper.py&quot;, line 88, in &lt;module&gt;
    input_files, output_files, snakemake.threads
  File &quot;/Users/user/Documents/postdoc_projects/invert/.snakemake/scripts/tmp7yzyzwru.wrapper.py&quot;, line 27, in distribute_threads
    gzipped_input_files = sum(1 for file in input_files if file.endswith(&quot;.gz&quot;))
  File &quot;/Users/user/Documents/postdoc_projects/invert/.snakemake/scripts/tmp7yzyzwru.wrapper.py&quot;, line 27, in &lt;genexpr&gt;
    gzipped_input_files = sum(1 for file in input_files if file.endswith(&quot;.gz&quot;))
AttributeError: 'Namedlist' object has no attribute 'endswith'
[Thu Apr 29 15:12:22 2021]
Error in rule trimmomatic_pe:
    jobid: 1
    output: tmp/4_12hr_Ciliated_4_S4_R1_trimmed.fastq.gz, tmp/4_12hr_Ciliated_4_S4_R2_trimmed.fastq.gz, tmp/4_12hr_Ciliated_4_S4_R1_unpaired_trimmed.fastq.gz, tmp/4_12hr_Ciliated_4_S4_R2_unpaired_trimmed.fastq.gz

RuleException:
CalledProcessError in line 29 of /Users/user/Documents/postdoc_projects/invert/Snakefile:
Command 'set -euo pipefail;  /Users/user/opt/miniconda3/envs/invert/bin/python3.6 /Users/user/Documents/postdoc_projects/invert/.snakemake/scripts/tmp7yzyzwru.wrapper.py' returned non-zero exit status 1.
  File &quot;/Users/user/Documents/postdoc_projects/invert/Snakefile&quot;, line 29, in __rule_trimmomatic_pe
  File &quot;/Users/user/opt/miniconda3/envs/invert/lib/python3.6/concurrent/futures/thread.py&quot;, line 56, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

Is the pipeline not identifying my sample correctly? The attribute error seems like that's the case, but this is my config file structure:
in_dir: data #Directory containing raw fastq files from RNAseq
samples: &quot;4_12hr_Ciliated_4_S4&quot; #sample name prefix
annotation_file: ref_files/Homo_sapiens.GRCh38.103.gtf #Directory containing the viral host genome annotation in .gtf format

parameters:
  trim: [&quot;TRAILING:3 ILLUMINACLIP:ref_files/TruSeq3-PE-2.fa&quot;] #trimmomatic parameters

What am I missing?
",-1,-1,-1.0
67387534,Snakemake multiple wildcards and argparse arguments,"I am new to snakemake and finding it very difficult to do simplest of things it can do. For illustration, I have written a program adding_text.py that takes arguments (argparse) of an input directory, an output directory and index (from os.listdir of the input directory) to process some text files.
This is my file structure:
identity_category1  
|----A.txt -&gt; text A identity  
|----B.txt -&gt; text B identity  
|----C.txt -&gt; text C identity  
identity_category2  
|----P.txt -&gt; text P identity  
|----Q.txt -&gt; text Q identity  
|----R.txt -&gt; text R identity  
identity_category3  
|----X.txt -&gt; text X identity  
|----Y.txt -&gt; text Y identity  
|----Z.txt -&gt; text Z identity  

And this is my code adding_text.py:
import argparse
import os
my_parser = argparse.ArgumentParser(usage='python %(prog)s [-h] input_dir output_dir file_index')
my_parser.add_argument('input_dir', type=str)
my_parser.add_argument('output_dir', type=str)
my_parser.add_argument('file_index', type=int)
args = my_parser.parse_args()

input_dir = args.input_dir
output_dir = args.output_dir
file_index = args.file_index
if not os.path.exists(output_dir):
    os.mkdir(output_dir)

filelist = os.listdir(input_dir)
input_file = open(os.path.join(input_dir, filelist[file_index]), 'r')
output_file = open(os.path.join(output_dir, filelist[file_index].split('.')[0] + '_added.txt'), 'w')
output_file.write(input_file.read() + ' has been added\n')

All I am doing is firing the following commands at console:
python adding_text.py identity_category1 1_added 0
python adding_text.py identity_category1 1_added 1
python adding_text.py identity_category1 1_added 2
python adding_text.py identity_category2 2_added 0
python adding_text.py identity_category2 2_added 1
python adding_text.py identity_category2 2_added 2
python adding_text.py identity_category3 3_added 0
python adding_text.py identity_category3 3_added 1
python adding_text.py identity_category3 3_added 2

And get the following output (structure):
1_added
|----A_added.txt -&gt; text A identity has been added
|----B_added.txt -&gt; text B identity has been added
|----C_added.txt -&gt; text C identity has been added
2_added
|----P_added.txt -&gt; text P identity has been added
|----Q_added.txt -&gt; text Q identity has been added
|----R_added.txt -&gt; text R identity has been added
3_added
|----X_added.txt -&gt; text X identity has been added
|----Y_added.txt -&gt; text Y identity has been added
|----Z_added.txt -&gt; text Z identity has been added

So the python coding isnt the problem. The problem is when I am trying to design a snakemake workflow around the problem, involving multiple wildcards, dependencies etc. My possible_snakefile looks like this
NUM = [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]
SAMPLE = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;]

rule add_text:
    input: 
        expand(&quot;identity_category{num}/{sample}.txt&quot;, num=NUM, sample=SAMPLE)
    output: 
        expand(&quot;{num}_added/{sample}_added.txt&quot;, num=NUM, sample=SAMPLE)
    run:
        for index in range(0,3):
            shell(&quot;python adding_text.py identity_category{num} {num}_added index&quot;)

When I try to specify a target and perform a dry run via snakemake --cores 1 -n -s possible_snakefile 1_added/A_added.txt , it incorrectly maps input directories and respective files and throws me this error:
MissingInputException in line 4 possible_snakefile:
Missing input files for rule add_text:
identity_category3/C.txt
identity_category2/A.txt
identity_category3/B.txt
identity_category2/B.txt
identity_category2/C.txt
identity_category3/A.txt

I am sure its very simple, but I am not just able to get my head around it. i.e. different wildcard specification in possible_snakefile or specifying different target files at command line. I would appreciate help here. Thank you
",-1,-1,-1.0
67393279,How to prevent snakemake from stopping after failed test suite?,"I am attempting to automate integration testing with snakemake (I need the output from some of the files so this seemed like a good tool). However when I need to run two test suites in pytest, the workflow stops if a single test in either suite fails. So for example I have:
rule run_tests:
    run:
        commands = [
            &quot;pytest test_that_should_fail&quot;,
            &quot;pytest test_that_should_succeed&quot;
        ]
        for c in commands:
            shell(c)

And I need the output of the test that should fail for the latter test. Is there a way to prevent snakemake from stopping after running &quot;pytest test_that_should_fail&quot; ? Additionally snakemake stops without any sort of error message just a generic: &quot;Error in rule run_tests: jobid: 0&quot;
",-1,-1,-1.0
67608482,Snakemake input fastq files from each sample directory issue for metagenomics analysis,"I am working on a new snakemake metagenomics pipeline to trim fastq files, and run them through kraken. Each sample has a directory containing the forward and reverse reads.
Sample_1/r1_paired.fq.gz
Sample_1/r2_paired.fq.gz
Sample_2/r1_paired.fq.gz
Sample_2/r2_paired.fq.gz

I am providing a sample sheet that users can upload, that contains the sample names and the read names. I used pandas to parse the sample sheet and provide the names required for the snakefile. Here is my snakefile.
 #Extract sample names from CSV
import pandas as pd
import os
df = pd.read_csv(&quot;sample_table_test.csv&quot;)
print(df)
samples = df.library.to_list()
print(&quot;Samples being processed:&quot;, samples)
R1 = df.r1_file.to_list()
R2 = df.r2_file.to_list()
print(R1,R2)


rule all:
    input:
        expand(&quot;{sample}.bracken&quot;, sample=samples),
        

#Trimmomatic to trim paired end reads
rule trim_reads:
    input:
        &quot;{sample}/{R1}&quot;,
        &quot;{sample}/{R2}&quot;,
    output:
        &quot;{sample}/{R1}_1_trim_paired.fq.gz&quot;,
        &quot;{sample}/{R2}_2_trim_paired.fq.gz&quot;,
    conda:
        &quot;env.yaml&quot;,
    shell:
        &quot;trimmomatic PE -threads 8 {input} {input} {output} {output} SLIDINGWINDOW:4:30 LEADING:2 TRAILING:2 MINLEN:50&quot;

#Kraken2 to bin reads and assign taxonomy
rule kraken2:
    input:
        &quot;{sample}/{R1}_1_trim_paired.fq.gz&quot;,
        &quot;{sample}/{R2}_2_trim_paired.fq.gz&quot;,
    output:
        &quot;{sample}_report.txt&quot;,
        &quot;{sample}_kraken_cseqs#.fq&quot;,
        
    conda:
        &quot;env.yaml&quot;,
    shell:
        &quot;kraken2 --gzip-compressed --paired --classified-out {output} {input} {input} --db database/minikraken2_v1_8GB/ --report {sample}_report.txt --threads 1&quot;

#Bracken estimates abundance of a species within a sample
rule bracken:
    input:
        &quot;{sample}_report.txt&quot;,
    output:
        &quot;{sample}.bracken&quot;,
    conda:
        &quot;env.yaml&quot;,
    shell:
        &quot;bracken -d database/minikraken2_v1_8GB/ -i {input} -o {output} -r 150&quot;

I am receiving the below error and have been struggling to find a better way to write my snakefile to avoid this issue. Any assistance here would be greatly appreciated.
WildcardError in line 19 of /Metagenomics/Metagenomics/snakemake/Snakefile:
Wildcards in input files cannot be determined from output files:
'R1'

Thank you!
",-1,-1,-1.0
67640412,Running external scripts with wildcards in snakemake,"I am trying to run a snakemake rule with an external script that contains a wildcard as noted in the snakemake reathedocs. However I am running into KeyError when running snakemake.
For example, if we have the following rule:
SAMPLE = ['test']

rule all:
    input:
        expand(&quot;output/{sample}.txt&quot;, sample=SAMPLE)

rule NAME:
    input: &quot;workflow/scripts/{sample}.R&quot;
    output: &quot;output/{sample}.txt&quot;,
    script: &quot;workflow/scripts/{wildcards.sample}.R&quot;

with the script workflow/scripts/test.R containing the following code
out.path = snakemake@output[[1]]

out = &quot;Hello World&quot;

writeLines(out, out.path)

I get the following error when trying to execute snakemake.
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
    count   jobs
    1   NAME
    1   all
    2

[Fri May 21 12:04:55 2021]
rule NAME:
    input: workflow/scripts/test.R
    output: output/test.txt
    jobid: 1
    wildcards: sample=test

[Fri May 21 12:04:55 2021]
Error in rule NAME:
    jobid: 1
    output: output/test.txt

RuleException:
KeyError in line 14 of /sc/arion/projects/LOAD/Projects/sandbox/Snakefile:
'wildcards'
  File &quot;/sc/arion/work/andres12/conda/envs/py38/lib/python3.8/site-packages/snakemake/executors/__init__.py&quot;, line 2231, in run_wrapper
  File &quot;/sc/arion/projects/LOAD/Projects/sandbox/Snakefile&quot;, line 14, in __rule_NAME
  File &quot;/sc/arion/work/andres12/conda/envs/py38/lib/python3.8/site-packages/snakemake/executors/__init__.py&quot;, line 560, in _callback
  File &quot;/sc/arion/work/andres12/conda/envs/py38/lib/python3.8/concurrent/futures/thread.py&quot;, line 57, in run
  File &quot;/sc/arion/work/andres12/conda/envs/py38/lib/python3.8/site-packages/snakemake/executors/__init__.py&quot;, line 546, in cached_or_run
  File &quot;/sc/arion/work/andres12/conda/envs/py38/lib/python3.8/site-packages/snakemake/executors/__init__.py&quot;, line 2262, in run_wrapper
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /sc/arion/projects/LOAD/Projects/sandbox/.snakemake/log/2021-05-21T120454.713963.snakemake.log

Does anyone know why this not working correctly?
",-1,-1,-1.0
67805295,snakemake - define input for aggregate rule without wildcards,"I am writing a snakemake to produce Sars-Cov-2 variants from Nanopore sequencing. The pipeline that I am writing is based on the artic network, so I am using artic guppyplex and artic minion.
The snakemake that I wrote has the following steps:

zip all the fastq files for all barcodes (rule zipFq)
perform read filtering with guppyplex (rule guppyplex)
call the artic minion pipeline (rule minion)
move the stderr and stdout from qsub to a folder under the working directory (rule mvQsubLogs)

Below is the snakemake that I wrote so far, which works
barcodes = ['barcode49', 'barcode50', 'barcode51']

rule all:
    input:
        expand([
            # zip fq
            &quot;zipFastq/{barcode}/{barcode}.zip&quot;,

            # guppyplex
            &quot;guppyplex/{barcode}/{barcode}.fastq&quot;,

            # nanopolish
            &quot;nanopolish/{barcode}&quot;,

            # directory where the logs will be moved to    
            &quot;logs/{barcode}&quot;
        ], barcode = barcodes)

rule zipFq:
    input: 
        FQ = f&quot;{FASTQ_PATH}/{{barcode}}&quot;
    output:
        &quot;zipFastq/{barcode}/{barcode}.zip&quot;
    shell:
        &quot;zip {output} {input.FQ}/*&quot;


rule guppyplex:
    input:
        FQ = f&quot;{FASTQ_PATH}/{{barcode}}&quot; # FASTQ_PATH is parsed from config.yaml
    output:
        &quot;guppyplex/{barcode}/{barcode}.fastq&quot;
    shell:
        &quot;/home/ngs/miniconda3/envs/artic-ncov2019/bin/artic guppyplex --skip-quality-check --min-length {MINLENGTHGUPPY} --max-length {MAXLENGTHGUPPY} --directory {input.FQ} --prefix {wildcards.barcode} --output {output}&quot; # variables in CAPITALS are parsed from config.yaml


rule minion:
    input:
        INFQ = rules.guppyplex.output,
        FAST5 = f&quot;{FAST5_PATH}/{{barcode}}&quot;
    params:
        OUTDIR = &quot;nanopolish/{barcode}&quot;
    output:
        directory(&quot;nanopolish/{barcode}&quot;)
    shell:
        &quot;&quot;&quot;
        mkdir {params.OUTDIR};
        cd {params.OUTDIR};
        export PATH=/home/ngs/miniconda3/envs/artic-ncov2019/bin:$PATH;
        artic minion --normalise {NANOPOLISH_NORMALISE} --threads {THREADS} --scheme-directory {PRIMERSDIR} --read-file ../../{input.INFQ} --sequencing-summary {Seq_Sum} --fast5-directory {input.FAST5}  nCoV-2019/{PRIMERVERSION} {wildcards.barcode} # variables in CAPITALS are parsed from config.yaml
        &quot;&quot;&quot;

rule mvQsubLogs:
    input:
        # zipFQ
        rules.zipFq.output,

        # guppyplex
        rules.guppyplex.output,

        # nanopolish
        rules.minion.output
    output:
        directory(&quot;logs/{barcode}&quot;)
    shell:
        &quot;mkdir -p {output} \n&quot;
        &quot;mv {LOGDIR}/{wildcards.barcode}* {output}/&quot;


The above snakemake works and now I am trying to add another rule, but the difference here is that this rule is an aggregate function i.e. it should not be called for every barcode, but only once after all the rules are called for all barcodes
The rule that I am trying to incorporate (catFasta) would cat all {barcode}.consensus.fasta (generated by rule minion) into in a single file, as shown below (incorporated into the snakemake above):
barcodes = ['barcode49', 'barcode50', 'barcode51']

rule all:
    input:
        expand([
            # zip fq
            &quot;zipFastq/{barcode}/{barcode}.zip&quot;,

            # guppyplex
            &quot;guppyplex/{barcode}/{barcode}.fastq&quot;,

            # nanopolish
            &quot;nanopolish/{barcode}&quot;,
            
            # catFasta
            &quot;catFasta/cat_consensus.fasta&quot;,

            # directory where the logs will be moved to    
            &quot;logs/{barcode}&quot;
        ], barcode = barcodes)

rule zipFq:
    input: 
        FQ = f&quot;{FASTQ_PATH}/{{barcode}}&quot;
    output:
        &quot;zipFastq/{barcode}/{barcode}.zip&quot;
    shell:
        &quot;zip {output} {input.FQ}/*&quot;


rule guppyplex:
    input:
        FQ = f&quot;{FASTQ_PATH}/{{barcode}}&quot; # FASTQ_PATH is parsed from config.yaml
    output:
        &quot;guppyplex/{barcode}/{barcode}.fastq&quot;
    shell:
        &quot;/home/ngs/miniconda3/envs/artic-ncov2019/bin/artic guppyplex --skip-quality-check --min-length {MINLENGTHGUPPY} --max-length {MAXLENGTHGUPPY} --directory {input.FQ} --prefix {wildcards.barcode} --output {output}&quot; # variables in CAPITALS are parsed from config.yaml


rule minion:
    input:
        INFQ = rules.guppyplex.output,
        FAST5 = f&quot;{FAST5_PATH}/{{barcode}}&quot;
    params:
        OUTDIR = &quot;nanopolish/{barcode}&quot;
    output:
        directory(&quot;nanopolish/{barcode}&quot;)
    shell:
        &quot;&quot;&quot;
        mkdir {params.OUTDIR};
        cd {params.OUTDIR};
        export PATH=/home/ngs/miniconda3/envs/artic-ncov2019/bin:$PATH;
        artic minion --normalise {NANOPOLISH_NORMALISE} --threads {THREADS} --scheme-directory {PRIMERSDIR} --read-file ../../{input.INFQ} --sequencing-summary {Seq_Sum} --fast5-directory {input.FAST5}  nCoV-2019/{PRIMERVERSION} {wildcards.barcode} # variables in CAPITALS are parsed from config.yaml
        &quot;&quot;&quot;

rule catFasta:
    input:
        expand(&quot;nanopolish/{barcode}/{barcode}.consensus.fasta&quot;, barcode = barcodes)
    output:
        &quot;catFasta/cat_consensus.fasta&quot;
    shell:
        &quot;cat {input} &gt; {output}&quot;

rule mvQsubLogs:
    input:
        # zipFQ
        rules.zipFq.output,

        # guppyplex
        rules.guppyplex.output,

        # nanopolish
        rules.minion.output,

        # catFasta
        rules.catFasta.output
    output:
        directory(&quot;logs/{barcode}&quot;)
    shell:
        &quot;mkdir -p {output} \n&quot;
        &quot;mv {LOGDIR}/{wildcards.barcode}* {output}/&quot;

However, when I call snakemake with
(artic-ncov2019) ngs@bngs05b:/nexusb/SC2/ONT/scripts/SnakeMake&gt; snakemake -np -s Snakefile_v2 --cluster &quot;qsub -q onlybngs05b -e {LOGDIR} -o {LOGDIR} -j y&quot; -j 5 --jobname &quot;{wildcards.barcode}.{rule}.{jobid}&quot; all # LOGDIR parsed from config.yaml

I get:
Building DAG of jobs...
MissingInputException in line 178 of /nexusb/SC2/ONT/scripts/SnakeMake/Snakefile_v2:
Missing input files for rule guppyplex:
/nexus/Gridion/20210521_Covid7/Covid7/20210521_0926_X1_FAL11796_a5b62ac2/fastq_pass/barcode49/barcode49.consensus.fasta

Which I don't find easy to understand: snakemake is complaining about /nexus/Gridion/20210521_Covid7/Covid7/20210521_0926_X1_FAL11796_a5b62ac2/fastq_pass/barcode49/barcode49.consensus.fasta whereas /nexus/Gridion/20210521_Covid7/Covid7/20210521_0926_X1_FAL11796_a5b62ac2/fastq_pass/ is FASTQ_PATH and I am not defining f&quot;{FASTQ_PATH}/{{barcode}}.consensus.fasta&quot; anywhere
A very same problem is described here, though the strategy in the accepted answer (the input for rule catFasta would be expand(&quot;nanopolish/{{barcode}}/{{barcode}}.consensus.fasta&quot;)) does not work for me.
Does anyone know how I can circumvent this?
",-1,1,-1.0
67868616,snakemake - replacing dynamic() with checkpoints,"I have the following snakemake workflow:
rule all:
    input:
        dynamic(os.path.join(config['out_dir'],'{CHR}','{CHR}.fa'))
    output:
        config['out_dir'] + '/all.fasta'
    shell:
        &quot;&quot;&quot;
        cat {input} &gt; {output}
        &quot;&quot;&quot;

rule split_genome_to_chr:
    &quot;&quot;&quot;
    Split the input genome fasta to
    multiple fastas with one chromosome
    in each.
    &quot;&quot;&quot;
    input:
        config['input_genome']
    output:
        dynamic(os.path.join(config['out_dir'],'{CHR}','{CHR}.fa'))
    params:
        out_dir=config['out_dir'],
    conda:
        CONDA_ENV_DIR + '/faSplit.yml'
    shell:
        &quot;&quot;&quot;
        faSplit byname {input} {params.out_dir}/ -outDirDepth=1
        &quot;&quot;&quot;

This works fine, but prints some warnings regarding the deprecation of dynamic() in favor of checkpoints.
I went through the checkpoints documentation, but still couldn't figure out how to change my code so it uses checkpoints instead of dynamic. Here's what I tried:
def aggregate_input(wildcards):
    return checkpoints.split_genome_to_chr.get(chrom=wildcards.CHR).output[0]  

rule all:
    input:
        aggregate_input
    output:
        config['out_dir'] + '/all.fasta'
    shell:
        &quot;&quot;&quot;
        cat {input} &gt; {output}
        &quot;&quot;&quot;

checkpoint split_genome_to_chr:
    &quot;&quot;&quot;
    Split the input genome fasta to
    multiple fastas with one chromosome
    in each.
    &quot;&quot;&quot;
    input:
        config['input_genome']
    output:
        os.path.join(config['out_dir'],'{CHR}','{CHR}.fa')
    params:
        out_dir=config['out_dir'],
    conda:
        CONDA_ENV_DIR + '/faSplit.yml'
    shell:
        &quot;&quot;&quot;
        faSplit byname {input} {params.out_dir}/ -outDirDepth=1
        &quot;&quot;&quot;

but getting the error message:
InputFunctionException in line 51 of workflow.snakefile:
AttributeError: 'Wildcards' object has no attribute 'CHR'
Wildcards:

Any help would be appreciated!
",1,-1,-1.0
67879033,How to access retry attempts in snakemake python code?,"When you execute a snakemake script with --restart-times &gt;= 1 it will try to re-execute a failed run. Upon re-execution it is possible to access the number of execution attempts via a lambda function in &quot;resources&quot;. However, I would like to access the number of attempts in a block of python code outside of my rule. I have tried to pass the attempt variable from the resources block to my python function, but to no avail. My snakemake version is 5.32.1 and a quick test with 6.0.3 looks very similar.
def getTargetFiles(files, attempted):
    do stuff
    return modified-target-files

rule do_things_rule:
    input: 
        ...
    output:
        getTargetFiles(&quot;file/path.txt&quot;, resources.attempt)
    resources:
        attempt=lambda wildcards, attempt: attempt,

This unfortunately yields an error. &quot;NameError in line 172 of xxxx.py: name 'resources' is not defined&quot;
The closest I have come, is to access &quot;workflow.attempt&quot; but this seems to be always set to 1. Perhaps this is the default value for attempts?
rule do_things_rule:
    input: 
        ...
    output:
        getTargetFiles(&quot;file/path.txt&quot;, workflow.attempt)

I was taking a look at the internals of snakemake in the hope of finding a solution. Unfortunately my python knowledge isn't up to the task. There are some variables one can access in place of workflow.attempt, which do not have integer values. Not sure if there is a way of getting the current number of attempts using these slightly differently:
print snakemake.jobs.Job.attempt
&lt;property object at 0x7f4eecba66d0&gt;

print snakemake.jobs.Job._attempt
&lt;member '_attempt' of 'Job' objects&gt;

",1,-1,-1.0
68084019,snakemake - wildcards from python dictionary,"I am writing a snakemake file that has input files on a specific location, with specific folder names (for this example, barcode9[456]). I need to change the naming conventions in these directories so I now want to add a first rule to my snakemake, which would link the folders in the original location (FASTQ_PATH) to an output folder in the snakemake working directory. The names of the link folders in this directory come from a python dictionay d, defined in the snakemake. I would then use these directories as input for the downstream rules.
So the first rule of my snakemake is actually a python script (scripts/ln.py) that maps the naming convention in the original directory to the desired naming conventions, and links the fastqs:
The snakemake looks like so:
FASTQ_PATH = '/path/to/original_location'


# dictionary that maps the subdirectories in FASTQ_PATH (keys) with the directory names that I want in the snakemake working directory (values)
d = {'barcode94': 'IBC_UZL-CV5-04',
 'barcode95': 'IBC_UZL-CV5-42',
 'barcode96': 'IBC_UZL-CV5-100'}



rule all:
    input:
        expand('symLinkFq/{barcode}', barcode = list(d.values())) # for each element in list(d.values()) I want to create a subdirectory that would point to the corresponding path in the original location (FASTQ_PATH)



rule symlink:
    input:
        FASTQ_PATH,
        d
    output:
        'symLinkFq/{barcode}'
    script:
        &quot;scripts/ln.py&quot;

The python script that I am calling to make the links is shown below
import pandas as pd
import subprocess as sp
import os

# parsing variables from Snakefile
d_sampleNames = snakemake.input[1]
fastq_path = snakemake.input[0]


os.makedirs('symLinkFq')
for barcode in list(d_sampleNames.keys()):
    idx = list(d_sampleNames.keys()).index(barcode)
    sampleName = list(d_sampleNames.values())[idx]
    
    sp.run(f&quot;ln -s {fastq_path}/{barcode} symLinkFq/{sampleName}&quot;, shell=True) # the relative path with respect to the working directory should suffice for the DEST in the ln -s command

But when I call snakemake -np -s Snakefile I get
Building DAG of jobs...
MissingInputException in line 15 of /nexusb/SC2/ONT/scripts/SnakeMake/minimalExample/renameFq/Snakefile:
Missing input files for rule symlink:
barcode95
barcode94
barcode96

The error kind of makes sense to me. The only 'input' files that I have are python variables instead of being files that do exist in my system.
I guess the issue that I am having comes down to the fact that the wildcards that I want to use for all rules are not present in any file that can be used as input, so what I can think of using is the dictionary with the correspondence, though it is not working as I tried.
Does anyone know how to get around this, any other different approach is welcome.
",-1,-1,-1.0
68104021,Paths from json file don't expand in Snakemake,"I have a Snakemake pipeline where I get my input/output paths for my file folders from a json file and use the expand function to obtain the paths.
import json

with open('config.json', 'r') as f:
    config = json.load(f)

wildcard = [&quot;1234&quot;, &quot;5678&quot;]

rule them_all:
    input:
        expand('config[&quot;data_input&quot;]/data_{wc}.tab', wc = wildcard)

    output:
        expand('config[&quot;data_output&quot;]/output_{wc}.rda', wc = wildcard)
    shell:
        &quot;Rscript ./my_script.R&quot;

My config.json is
{
&quot;data_input&quot;: &quot;/very/long/path&quot;,
&quot;data_output&quot;: &quot;/slightly/different/long/path&quot;
}

While trying to make a dry run, though, I get the following error:
$ snakemake -np
Building DAG of jobs...
MissingInputException in line 12 of /path/to/Snakefile:
Missing input files for rule them_all:
config[&quot;data_input&quot;]/data_1234.tab
config[&quot;data_input&quot;]/data_5678.tab

The files are there and their path is /very/long/path/data_1234.tab.
This is probably a low-hanging fruit, but what am I doing wrong in the syntax for the expansion? Or is it the way I call the json file?
",-1,-1,-1.0
68185241,"Snakemake with R script, error: snakemake object not found","I'm having issues trying to run a snakemake rule that invokes an R script. The relevant snakemake rule looks like this:
rule summarize_transcripts:
    input:
        lineages = expand(&quot;../final/{seq_run}/{exp}_transcript_lineages.csv&quot;, exp=EXP, seq_run=SEQ_RUN),
        salmon_dir = expand(&quot;../salmon_quant_results/{experiment_id}&quot;, experiment_id=EXP),
    output:
        species_counts = expand(&quot;../final/{seq_run}/{exp}_sp_counts.csv&quot;, exp=EXP, seq_run=SEQ_RUN),
        family_counts = expand(&quot;../final/{seq_run}/{exp}_family_counts.csv&quot;, exp=EXP, seq_run=SEQ_RUN)        
    shell:
        '''
        Rscript ./deseq2.R
        '''

Here's the top of my R script, which fails at the S4 object call:
library('DESeq2')
library('tximport')

#Read in dataframe that contains the trinity ID's linked to lineage information
df &lt;- read.csv(snakemake@input[[&quot;lineages&quot;]], header=TRUE)

The libraries load fine, the error message states:
Error in read.table(file = file, header = header, sep = sep, quote = quote,  : 
  object 'snakemake' not found
Calls: read.csv -&gt; read.table
Execution halted

I'm trying to keep my dependencies organized within a conda .yml file, which looks like this:
name: test_env

channels:
  - conda-forge
  - bioconda
  - r
  - default

dependencies:
  - star =2.7.1a
  - trinity =2.12.0
  - samtools =1.12
  - salmon =1.4.0
  - snakemake
  - pandas
  - r=4.1.0
  - r-essentials
  - bioconductor-deseq2
  - bioconductor-tximport

Why is R not recognizing the snakemake inputs as described in the snakemake docs? I am running snakemake version 6.5.1.
",-1,-1,-1.0
68486202,Snakemake use all samples as one input with porechop,"I'm trying to use porechop on several data with a Snakemake workflow.
In my Snakefile, there are three rules, a fastqc rule and a porechop rule, in addition to the all rule. The fastqc rule works very well, I have all three out for my three fastq. But for porechop, instead of running the command three times, it runs the command once with the -i flag for all three files at the same time:
Error in rule porechop:
    jobid: 2
    output: /ngs/prod/nanocea_project/test/prod/porechop/25022021_2_pore.fastq.gz, /ngs/prod/nanocea_project/test/prod/porechop/02062021_1_pore.fastq.gz, /ngs/prod/nanocea_project/test/prod/porechop/02062021_2_pore.fastq.gz
    conda-env: /ngs/prod/nanocea_project/test/.snakemake/conda/a72fb141b37718b7c37d9f32d597faeb
    shell:
        porechop -i /ngs/prod/nanocea_project/test/reads/25022021_2.fastq.gz /ngs/prod/nanocea_project/test/reads/02062021_1.fastq.gz /ngs/prod/nanocea_project/test/reads/02062021_2.fastq.gz -o /ngs/prod/nanocea_project/test/prod/porechop/25022021_2_pore.fastq.gz /ngs/prod/nanocea_project/test/prod/porechop/02062021_1_pore.fastq.gz /ngs/prod/nanocea_project/test/prod/porechop/02062021_2_pore.fastq.gz -t 40 --discard_middle
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

However, when I use it with a single sample, the program works.
Here my code:
import glob
import os

###Global Variables###

FORMATS=[&quot;zip&quot;, &quot;html&quot;]
DIR_FASTQ=&quot;/ngs/prod/nanocea_project/test/reads&quot;

###FASTQ Files###

def list_samples(DIR_FASTQ):
        SAMPLES=[]
        for file in glob.glob(DIR_FASTQ+&quot;/*.fastq.gz&quot;):
                base=os.path.basename(file)
                sample=(base.replace('.fastq.gz', ''))
                SAMPLES.append(sample)
        return(SAMPLES)

SAMPLES=list_samples(DIR_FASTQ)

###Rules###
rule all:
        input:
                expand(&quot;/ngs/prod/nanocea_project/test/stats/fastqc/{sample}_fastqc.{ext}&quot;, sample=SAMPLES, ext=FORMATS),
                expand(&quot;/ngs/prod/nanocea_project/test/prod/porechop/{sample}_pore.fastq.gz&quot;, sample=SAMPLES)
rule fastqc:
        input:
                expand(DIR_FASTQ+&quot;/{sample}.fastq.gz&quot;, sample=SAMPLES)
        output:
                expand(&quot;/ngs/prod/nanocea_project/test/stats/fastqc/{sample}_fastqc.{ext}&quot;, sample=SAMPLES, ext=FORMATS)
        threads:
                16
        conda:
                &quot;envs/fastqc.yaml&quot;
        shell:
                &quot;fastqc {input} -o /ngs/prod/nanocea_project/test/stats/fastqc/ -t {threads}&quot;

rule porechop:
        input:
                expand(DIR_FASTQ+&quot;/{sample}.fastq.gz&quot;, sample=SAMPLES)
        output:
                expand(&quot;/ngs/prod/nanocea_project/test/prod/porechop/{sample}_pore.fastq.gz&quot;, sample=SAMPLES)
        threads:
                40
        conda:
                &quot;envs/porechop.yaml&quot;
        shell:
                &quot;porechop -i {input} -o {output} -t {threads} --discard_middle&quot;


Do you have any idea what's wrong?
Thanks !
",-1,-1,-1.0
68582955,snakemake: pass input that does not exist (or pass multiple params),"I am trying and struggling mightily to write a snakemake pipeline to download files from an aws s3 instance.
Because the organization and naming of my files on s3 is inconsistent, I do not want to use snakemake's remote options.  Instead, I use a mix of grep and python to enumerate the paths I want on s3, and put them in a text file:
#s3paths.txt
s3://path/to/sample1.bam
s3://path/to/sample2.bam

In my config file I specify the samples I want to work with:
#config.yaml
samplesToDownload: [sample1, sample3, sample18]

I want to make a pipeline where the first rule downloads files from s3 who contain a string present in config['samplesToDownload'].  A runtime code snippet does this for me:
pathsToDownload: [path for path in s3paths.txt if path contains string in samplesToDownload]

All this works fine, and I am left with a global variable pathsToDownload that looks something like this:
pathsToDownload: ['s3://path/to/sample1.bam', 's3://path/to/sample3.bam', 's3://path/to/sample18.bam']


Now I try to get snakemake involved and struggle. If I try to put the python variable in inputs, snakemake refuses because the file does not exist locally:
rule download_bams_from_s3:
   input: 
      s3Path = pathsToDownload
   output:
      expand(where/I/want/file/{sample}.bam, sample=config['samplesToDownload'])
   shell:
       aws s3 cp {input.s3Path} where/I/want/file/{sample}.bam


This fails because input.s3Path cannot be found as it is a path on s3, not a local path.  I then try to do the same but with the pathsToDownload as a param:
rule download_bams_from_s3:
   params: 
      s3Path = pathsToDownload
   output:
      expand(where/I/want/file/{sample}.bam, sample=config['samplesToDownload'])
   shell:
       aws s3 cp {params.s3Path} where/I/want/file/{sample}.bam


This doesn't produce an error, but it produces the wrong type of shell command.  Instead of producing what I want, which is 3 total shell commands:
shell: aws s3 cp path/to/sample1 where/I/want/file/sample1.bam
shell: aws s3 cp path/to/sample3 where/I/want/file/sample3.bam
shell: aws s3 cp path/to/sample18 where/I/want/file/sample18.bam

it produces one shell command with all three paths:
shell: aws s3 cp path/to/sample1 path/to/sample3 path/to/sample18 where/I/want/file/sample1.bam where/I/want/file/sample3.bam where/I/want/file/sample18.bam

Even if I were able to properly construct one massive shell command it is not what I want because I want separate shell commands to take advantage of snakemakes parallelization and ability to not redownload the same file if it already exists.
I feel like this use case for snakemake is not a big ask but I have spent hours trying to construct something workable to no avail.  A clean solution is much appreciated!
",-1,-1,-1.0
68583785,Expand multiple pandas columns in snakemake to iterrate rows,"I'm trying to use 2 columns in the same pandas dataframe as the basis to generate output files via snakemake.
A minimal example of my code is like this:
import pandas as pd

df = pd.DataFrame.from_dict({'col1':[1,2,3], 'col2':['A','B','C']})

rule all:
    input:
        expand('data/{c1}/{c2}.tsv', c1 = df['col1'], c2 = df['col2'])

rule make_files:
    output:
        &quot;data/{c1}/{c2}.tsv&quot;
    shell:
        &quot;touch {output}&quot;

The issue I have is that my desired output is 3 files that would be the result of iterating row by row (i.e., 1/A.tsv, 2/B.tsv, 3/C.tsv), however this current script gives is 9 files that are the permutations of my rows (i.e., 1/A.tsv, 1/B.tsv, 1/C.tsv, 2/A.tsv, 2/B.tsv, 2/C.tsv, 3/A.tsv, 3/B.tsv, 3/C.tsv).
Any help would be appreciated
",1,1,-1.0
68637394,Snakemake report with Rmarkdown and custom.css file,"Below an example of Snakemake Rmd report with a custom.css.
---
title: &quot;Test Report&quot;
date: &quot;`r format(Sys.time(), '%d %B, %Y')`&quot;
params:
   rmd: &quot;report.Rmd&quot;
output:
  html_document:
     css: &quot;custom.css&quot; 

---

## R Markdown

This is an R Markdown document.
Test include from snakemake `r snakemake@input`.

This specific example does not work because Snakemake moves the .rmd file to a temporary location.

File custom.css not found in resource path
Error: pandoc document conversion failed with error 99

The trivial solution would be moving the custom.css to where report.rmd is rendered, but we don't have this location. In addition, we can not use the snakemake directive on the header, as it is only available after it.
Does anyone have a solution for this issue? The only solution I can think is patching Snakemake to accept specific header parameters.
",-1,-1,-1.0
68673666,snakemake (or parallel) for multiple machines over ssh,"Say you have a snakemake file. This file produces something like 50000 jobs, however, these are small jobs that take a few seconds to run.
From the head node, you have access to multiple servers named:
machine01
machine02
machine03
machine04
machine05
machine06

To make matters more interesting, each machine has an uneven number of cores. What is the best way to send the different jobs to different machines for parallel execution? I tried the batch option in snakemake but it does not seem to be doing but I thought it did.
",-1,-1,-1.0
68681883,"Snakemake ""Auto-scaling Azure Kubernetes cluster without shared filesystem"" fails with dependency error","I am trying to run the https://snakemake.readthedocs.io/en/stable/executor_tutorial/azure_aks.html tutorial on snakemake.
Now the pipeline starts, but fails on an import error in python/snakemake.
WorkflowError:
The Python 3 package 'azure-storage-blob' need to be installed to use Azure Storage remote() file functionality. No module named 'azure'
  File &quot;/opt/conda/envs/snakemake/lib/python3.9/importlib/__init__.py&quot;, line 127, in import_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1030, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1007, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 986, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 680, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 850, in exec_module
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 228, in _call_with_frames_removed

In python, importing azure seems possible.
Python 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:39:48) 
[GCC 9.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__
&gt;&gt;&gt; print(&quot;Azure Blob Storage v&quot; + __version__ + &quot; - Python quickstart sample&quot;)
Azure Blob Storage v12.8.1 - Python quickstart sample
&gt;&gt;&gt; 


I tried putting python + azure-storage-blob to the env files in snakemake, but doing this resulted in new problems.
I also tried older versions of python and at some point a later version of kubernetes.
",-1,-1,-1.0
68701481,How to avoid Snakemake rule from using incomplete output file from other rules,"rule rule1:
    output: tsv = &quot;...&quot;
    input: faa = &quot;...&quot;
    shell:
        &quot;&quot;&quot;
        awk ... &gt; {output.tsv}
        some commands {input.faa} | awk ... &gt;&gt; {output.tsv}
        &quot;&quot;&quot;
rule rule2:
    output:
        tsv = &quot;...&quot;
    input:
        tsv = rules.rule1.output.tsv,
    shell:
        &quot;&quot;&quot;
        awk ... {input.tsv} &gt; {output.tsv}
        &quot;&quot;&quot;


As it illustrated above, rule2 takes input file from rule1.
According to the official docs, since the output file in rule1 is created successfully by awk, Snakemake assumes everything worked fine, even if my output file is incomplete, because awk is going to append to that file. Snakemake just ran rule2 and took the incomplete file from rule1. Actually, the second awk command in rule1 have not being executed, leaving the output file incomplete.
",-1,-1,-1.0
68716950,How can I execute a single Snakemake rule without triggering input rules?,"In a Snakemake workflow, I would like to run a rule without it triggering any of the rules that produce its input.
A sample scenario is as follows: I have a rule A that is costly and produces many output files from input files:
rule A:
  input: &quot;{name}.in&quot;
  output: &quot;{name}.out&quot;
  shell: &quot;touch {input} {output}&quot; #just a dummy, replace with actual costly task

A second rule B takes the output files and uploads them to a server:
rule B:
  input: &quot;{name}.out&quot;
  output: touch(&quot;{name}.up&quot;)
  shell: &quot;curl -F 'data={input}' http://google.com/upload

The third rule C is just a usual all rule that acts as the terminal rule to trigger all input ones:
names = [&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;] # dummy for long list
rule C:
  input: expand(&quot;{name}.up&quot;,name=names)

Assume there was an error in rule B such that the expensive rule A completed, but rule B has not.
I would like to trigger rule B and rule C only, in such a way that rule A is not.
The problem is that for some reason, rule A will always run, despite many x1.out being present. This shouldn't be the case but it is.
I'm now looking for a Snakemake CLI option that allows me to prevent rule A from being run.
I could find a CLI option --until which does exactly the opposite, it runs all rules up to a certain rule. I would like to be able to do the opposite, something like --from which starts at B and fails if inputs cannot be found.
I don't know exactly why rule A gets triggered. The input files have not been updated. Nonetheless A is run (in fact it's much more complicated, the above is simplified a lot).
In short: is there a CLI option that allows me to specify a rule that should be run, including all downstream rules, but none of the upstream rules? Or is this impossible?
",-1,-1,-1.0
68776818,Why are my wildcard attributes not being filled in Snakemake?,"I am following the tutorial in the documentation (https://snakemake.readthedocs.io/en/stable/tutorial/advanced.html) and have been stuck on the &quot;Step 4: Rule parameter&quot; exercise. I would like to access a float from my config file using a wildcard in my params directive.
I seem to be getting the same error whenever I run snakemake -np in the command line:
InputFunctionException in line 46 of /mnt/c/Users/Matt/Desktop/snakemake-tutorial/Snakefile:
Error:
  AttributeError: 'Wildcards' object has no attribute 'sample'
Wildcards:

Traceback:
  File &quot;/mnt/c/Users/Matt/Desktop/snakemake-tutorial/Snakefile&quot;, line 14, in get_bcftools_call_priors

This is my code so far
import time
configfile: &quot;config.yaml&quot;

rule all:
    input:
        &quot;plots/quals.svg&quot;

def get_bwa_map_input_fastqs(wildcards):
    print(wildcards.__dict__, 1, time.time()) #I have this print as a check
    return config[&quot;samples&quot;][wildcards.sample]

def get_bcftools_call_priors(wildcards):
    print(wildcards.__dict__, 2, time.time()) #I have this print as a check
    return config[&quot;prior_mutation_rates&quot;][wildcards.sample]

rule bwa_map:
    input:
        &quot;data/genome.fa&quot;,
        get_bwa_map_input_fastqs
        #lambda wildcards: config[&quot;samples&quot;][wildcards.sample]
    output:
        &quot;mapped_reads/{sample}.bam&quot;
    params:
        rg=r&quot;@RG\tID:{sample}\tSM:{sample}&quot;
    threads: 2
    shell:
        &quot;bwa mem -R '{params.rg}' -t {threads} {input} | samtools view -Sb - &gt; {output}&quot;

rule samtools_sort:
    input:
        &quot;mapped_reads/{sample}.bam&quot;
    output:
        &quot;sorted_reads/{sample}.bam&quot;
    shell:
        &quot;samtools sort -T sorted_reads/{wildcards.sample} &quot;
        &quot;-O bam {input} &gt; {output}&quot;

rule samtools_index:
    input:
        &quot;sorted_reads/{sample}.bam&quot;
    output:
        &quot;sorted_reads/{sample}.bam.bai&quot;
    shell:
        &quot;samtools index {input}&quot;

rule bcftools_call:
    input:
        fa=&quot;data/genome.fa&quot;,
        bam=expand(&quot;sorted_reads/{sample}.bam&quot;, sample=config[&quot;samples&quot;]),
        bai=expand(&quot;sorted_reads/{sample}.bam.bai&quot;, sample=config[&quot;samples&quot;])
        #prior=get_bcftools_call_priors
    params:
        prior=get_bcftools_call_priors
    output:
        &quot;calls/all.vcf&quot;
    shell:
        &quot;samtools mpileup -g -f {input.fa} {input.bam} | &quot;
        &quot;bcftools call -P {params.prior} -mv - &gt; {output}&quot;

rule plot_quals:
    input:
        &quot;calls/all.vcf&quot;
    output:
        &quot;plots/quals.svg&quot;
    script:
        &quot;scripts/plot-quals.py&quot;


and here is my config.yaml
samples:
  A: data/samples/A.fastq
  #B: data/samples/B.fastq
  #C: data/samples/C.fastq

prior_mutation_rates:
  A: 1.0e-4
  #B: 1.0e-6

I don't understand why my input function call in bcftools_call says that the wildcards object is empty of attributes, yet an almost identical function call in bwa_map has the attribute sample that I want. From the documentation it seems like the wildcards would be propogated before anything is run, so why is it missing?
This is the full output of the commandline call snakemake -np:
{'_names': {'sample': (0, None)}, '_allowed_overrides': ['index', 'sort'], 'index': functools.partial(&lt;function Namedlist._used_attribute at 0x7f91b1a58f70&gt;, _name='index'), 'sort': functools.partial(&lt;function Namedlist._used_attribute at 0x7f91b1a58f70&gt;, _name='sort'), 'sample': 'A'} 1 1628877061.8831172
Job stats:
job               count    min threads    max threads
--------------  -------  -------------  -------------
all                   1              1              1
bcftools_call         1              1              1
bwa_map               1              1              1
plot_quals            1              1              1
samtools_index        1              1              1
samtools_sort         1              1              1
total                 6              1              1


[Fri Aug 13 10:51:01 2021]
rule bwa_map:
    input: data/genome.fa, data/samples/A.fastq
    output: mapped_reads/A.bam
    jobid: 4
    wildcards: sample=A
    resources: tmpdir=/tmp

bwa mem -R '@RG\tID:A\tSM:A' -t 1 data/genome.fa data/samples/A.fastq | samtools view -Sb - &gt; mapped_reads/A.bam

[Fri Aug 13 10:51:01 2021]
rule samtools_sort:
    input: mapped_reads/A.bam
    output: sorted_reads/A.bam
    jobid: 3
    wildcards: sample=A
    resources: tmpdir=/tmp

samtools sort -T sorted_reads/A -O bam mapped_reads/A.bam &gt; sorted_reads/A.bam

[Fri Aug 13 10:51:01 2021]
rule samtools_index:
    input: sorted_reads/A.bam
    output: sorted_reads/A.bam.bai
    jobid: 5
    wildcards: sample=A
    resources: tmpdir=/tmp

samtools index sorted_reads/A.bam

[Fri Aug 13 10:51:01 2021]
rule bcftools_call:
    input: data/genome.fa, sorted_reads/A.bam, sorted_reads/A.bam.bai
    output: calls/all.vcf
    jobid: 2
    resources: tmpdir=/tmp

{'_names': {}, '_allowed_overrides': ['index', 'sort'], 'index': functools.partial(&lt;function Namedlist._used_attribute at 0x7f91b1a58f70&gt;, _name='index'), 'sort': functools.partial(&lt;function Namedlist._used_attribute at 0x7f91b1a58f70&gt;, _name='sort')} 2 1628877061.927639
InputFunctionException in line 46 of /mnt/c/Users/Matt/Desktop/snakemake-tutorial/Snakefile:
Error:
  AttributeError: 'Wildcards' object has no attribute 'sample'
Wildcards:

Traceback:
  File &quot;/mnt/c/Users/Matt/Desktop/snakemake-tutorial/Snakefile&quot;, line 14, in get_bcftools_call_priors

If anyone knows what is going wrong I would really appreciate an explaination. Also if there is a better way of getting information out of the config.yaml into the different directives, I would gladly appreciate those tips.
Edit:
I have searched around the internet quite a bit, but have yet to understand this issue.
",-1,-1,-1.0
68890702,Is it possible to have optionally empty Snakemake wildcards?,"Is it possible to have optionally empty wildcards? It seems like it was possible a few years ago (https://groups.google.com/g/snakemake/c/S7fTL4jAYIM), but the described method didn't work for a user last year and now is not working for me.
My Snakefile looks something like this (abbreviated for clarity):
wildcard_constraints:
    udn_id=&quot;ID.+&quot;,
    compound=&quot;(no_)*compound(_genome|_exome)*&quot;

rule all:
    input: expand(&quot;file/path/{id}/{compound}{.*}.html&quot;,
            id=[config[&quot;id&quot;]], compound=compound_list, freq=freq_list)

rule create_html:
    output: &quot;file/path/{id}/{compound}{freq,.*}.html&quot;
    input: &quot;/oak/stanford/groups/euan/UDN/output/AnnotSV/AnnotSV_3.0.5/{udn_id}/WGS_blood_&quot;+hg+&quot;/gateway_hpo/{udn_id}.{comp_het}{cohort_freq,.*}.annotated.tsv&quot;
    shell: #Run shell commands

rule append_freq:
    output: &quot;file/path/{id}/{compound}.ha_freq.tsv&quot;
    input: &quot;file/path/{id}/{compound}.tsv&quot;
    script: &quot;file/path/get_ha_freq.py&quot;

I get the error
No values given for wildcard ''.
File file/path, line 6 in &lt;module&gt;

when I run this.
I also tried implementing a wildcard constraint like this:
wildcard_constraints:
    udn_id=&quot;ID.+&quot;,
    compound=&quot;(no_)*compound(_genome|_exome)*&quot;
    freq=&quot;.*&quot;

rule all:
    input: expand(&quot;file/path/{id}/{compound}{freq}.html&quot;,
            id=[config[&quot;id&quot;]], compound=compound_list, freq=freq_list)

rule create_html:
    output: &quot;file/path/{id}/{compound}{freq}.html&quot;
    input: &quot;/oak/stanford/groups/euan/UDN/output/AnnotSV/AnnotSV_3.0.5/{udn_id}/WGS_blood_&quot;+hg+&quot;/gateway_hpo/{udn_id}.{comp_het}{cohort_freq}.annotated.tsv&quot;
    shell: #Run shell commands

rule append_freq:
    output: &quot;file/path/{id}/{compound}.ha_freq.tsv&quot;
    input: &quot;file/path/{id}/{compound}.tsv&quot;
    script: &quot;file/path/get_ha_freq.py&quot;

but I received the error,
No values given for wildcard 'freq'.
File file/path, line 7 in &lt;module&gt; 

when I did this.
What error am I making?
",-1,-1,-1.0
68891127,snakemake: Ambiguous rule not detected?,"The following Snakefile fails with AmbiguousRuleException:
library_id = ['S1']
run_id = ['R1']

samples = dict(zip(library_id, run_id))

rule all:
    input:
        expand('{library_id}.bam', library_id= library_id),

rule bwa:
    output:
        '{run_id}.bam',

rule merge_bam:
    input:
        lambda wc: '%s.bam' % samples[wc.library_id],
    output:
        '{library_id}.bam',

Gives:

    AmbiguousRuleException:
    Rules bwa and merge_bam are ambiguous for the file S1.bam.
    Consider starting rule output with a unique prefix, constrain your wildcards, or use the ruleorder directive.
    Wildcards:
        bwa: run_id=S1
        merge_bam: library_id=S1
    Expected input files:
        bwa: 
        merge_bam: R1.bamExpected output files:
        bwa: S1.bam
        merge_bam: S1.bam

That's expected and it's ok. However, if library_id and run_id have the same value the ambiguity is not detected and only the first rule is executed:
library_id = ['S1']
run_id = ['S1'] # Same as library_id!

samples = dict(zip(library_id, run_id))

rule all:
    input:
        expand('{library_id}.bam', library_id= library_id),

rule bwa:
    output:
        '{run_id}.bam',

rule merge_bam:
    input:
        lambda wc: '%s.bam' % samples[wc.library_id],
    output:
        '{library_id}.bam',

Dry-run execution:
Job counts:
    count   jobs
    1   all
    1   bwa
    2

[Mon Aug 23 11:27:39 2021]
localrule bwa:
    output: S1.bam
    jobid: 1
    wildcards: run_id=S1

[Mon Aug 23 11:27:39 2021]
localrule all:
    input: S1.bam
    jobid: 0

Job counts:
    count   jobs
    1   all
    1   bwa
    2
This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.

Is this a bug or am I missing something? The second example should give AmbiguousRuleException just like the first and it's even more obvious.
This is with snakemake 6.4.1
",-1,-1,-1.0
69001097,"Conda 4.10.3 and Snakemake >5 ""__conda_exe"" problem","I have the following problem, when I used the command below, it appears that snakemake cannot resolve the creation of the environment using the --use-conda option.
If I not use this option of snakemake and launch the snakefile in an appropriate environment created by conda and not by snakemake, the command execution is ok.
Did someone have the same problem ?
Thanks,
Command:
snakemake -p -d ./ -s 00_Quality_Check.smk -j 4 --use-conda

Versions:

conda 4.10.3
python 3.9
snakemake 5.11

Error message:
Building DAG of jobs...
CreateCondaEnvironmentException:
Unable to check conda version:
environment: ligne 10: __conda_exe : commande introuvable

  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 232, in create
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 343, in __new__
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 356, in __init__
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 410, in _check

Version:

conda 4.10.3
python 3.9
snakemake 6.7

Error message:
Building DAG of jobs...
environment: ligne 10: __conda_exe : commande introuvable
Traceback (most recent call last):
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/site-packages/snakemake/__init__.py&quot;, line 699, in snakemake
    success = workflow.execute(
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/site-packages/snakemake/workflow.py&quot;, line 933, in execute
    dag.create_conda_envs(
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/site-packages/snakemake/dag.py&quot;, line 304, in create_conda_envs
    env.create(dryrun)
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 281, in create
    conda = Conda(self._container_img)
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/site-packages/snakemake/deployment/conda.py&quot;, line 433, in __init__
    shell.check_output(self._get_cmd(&quot;conda info --json&quot;))
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/site-packages/snakemake/shell.py&quot;, line 63, in check_output
    return sp.check_output(cmd, shell=True, executable=executable, **kwargs)
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/subprocess.py&quot;, line 424, in check_output
    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
  File &quot;/home/usr/miniconda3/envs/snake/lib/python3.9/subprocess.py&quot;, line 528, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command 'conda info --json' returned non-zero exit status 127.

",-1,-1,-1.0
69109610,Escape a period properly in snakemake expand function,"I am writing a snakemake pipeline where I download various files whose filenames contain many periods, and I am having a devil of a time getting it to properly understand the file names.  I have essentially two rules, a download rule and a target rule.  Here they are simplified below.

rule download_files_from_s3:
    input:
        some input
    params:
        some params
    output:
        expand(&quot;destinationDir/{{sampleName}}\.{{suffix}}&quot;)
    shell:
        shell:
        &quot;aws s3 cp input destinationDir&quot;

rule targets:
    input:
        expand(&quot;destinationDir/{sampleName}\.{suffix}&quot;, sampleName=sampleNames)



In this formulation snakemake compiles successfully, and properly downloads the files from s3 to where I want them.  However, it is unable to find them and &quot;says waiting at most 5 seconds for missing files&quot;.  I can see when I run the snakemake in dry run mode, the snakemake expects files of the form &quot;destinationDir/sampleName\.suffix&quot; when in reality they exist without a backslash: &quot;destinationDir/sampleName.suffix&quot;. My first though was to ditch the backslash, changing my rules to the form:
expand(&quot;destinationDir/{sampleName}.{suffix}&quot;, sampleName=sampleNames)

This however creates an overly greedy regular expression.  My value for suffix should be &quot;.params.txt&quot;.  When I run the no backslash version snakemake evaluates the wildcard sampleName as &quot;sampleName.params&quot; and the wildcard suffix as &quot;txt&quot;. How ought I to best go about this either by forcing the regular expression matching in expand to behave or to have snakemake properly interpret the '' character?  My efforts so far haven't been successful.
",-1,-1,-1.0
69182967,"How to configure multiple directories, sub-directories in Snakemake?","I have SAM files that are in the following structure:
MyFolder
├── 534897345_Lane1
│   ├── Sample9729
│   │   └── Aligned.out.sam
│   ├── Sample082340
│   │   └── Aligned.out.sam
│   ├── Sample098490220ABN
│       └── Aligned.out.sam
└── 534897345_Lane2
    ├── XY97289
    │   └── Aligned.out.sam
    ├── IJBND97824
    │   └── Aligned.out.sam
    ├── something90784
        └── Aligned.out.sam

I need to pass each SAM file into its own FastQC job, and its own HTSeq-Count job. Later on, I will also be merging these sam files together to call variants.
I'm having a hard time figuring out how to programmatically access the sam files while keeping their lane and sample name identity in Snakemake. Here is what I have right now (doesn't work):
lanes, samples = glob_wildcards('path/to/MyFolder/' + '{lane}/{sample}/Aligned.out.sam')

rule all: 
    input:
        expand('logs/fastqc/{lane}/{sample}.log', lane=lanes, sample=samples)

rule fastqc: 
    input:

    output:
        html='/fastqc/{lane}/{sample}.html',
        zip='/fastqc/{lane}/{sample}_fastqc.zip'
    threads: 1
    log: 'logs/fastqc/{lane}/{sample}.log'
    resources: mem_mb=8000, cpus=1
    wrapper: 
        '''
        0.77.0/bio/fastqc
        '''

It seems like the expand() function tries to construct every combination of files, so for example 534897345_Lane1/XY97289/Aligned.out.sam comes up as missing as it should. How do I go about constructing the samples in Snakemake? Thanks!
",-1,-1,-1.0
69346283,MissingRule Exception in Snakemake,"I am new to snakemake workflow management and I'm struggling to grasp how the wildcards input works. I tried to do QC of some SRR data but the snakemake is giving the &quot;MissingRuleException error&quot;.
my config file(config.yaml) contain the content:
samples: sample.csv
path: /Users/path/Bioinformatics/srr_practice
sample.csv is
sample_name,fq1

A,SRR11412215

B,SRR11412216

C,SRR11412217

D,SRR11412218

E,SRR11412219

Snakefile
import os
import pandas as pd

configfile:&quot;config.yaml&quot;

samples=pd.read_csv(config[&quot;samples&quot;], sep=&quot;,&quot;).set_index(&quot;sample_name&quot;, drop=False)


def get_fastq(wildcards):

        units=samples.loc[wildcards.sample]
fq=units[&quot;fq1&quot;]
        return expand(os.path.join(config[&quot;path&quot;], &quot;{fq}.fastq.gz&quot;), fq=fq)

rule all:

    input:

        expand(os.path.join(config[&quot;path&quot;], &quot;fastq_output/{sample}.fastqc.html&quot;),sample=samples[&quot;fq1&quot;].to_list()),
        expand(os.path.join(config[&quot;path&quot;], &quot;fastq_output/{sample}_fastqc.zip&quot;), sample=samples[&quot;fq1&quot;].to_list())

rule fastq:

    input:

            get_fastq,
    output:

            zip=os.path.join(config[&quot;path&quot;], &quot;fastq_output/{wildcards.sample_name}_fastqc.zip&quot;),
                html=os.path.join(config[&quot;path&quot;], &quot;fastq_output/{wildcards.sample_name}_fastqc.html&quot;)

    wrapper:
            &quot;0.78.0/bio/fastqc&quot;

snakemake -np Snakefile
Error
Building DAG of jobs...
MissingRuleException:
No rule to produce Snakefile (if you use input functions make sure that they don't raise unexpected exceptions).
",-1,-1,-1.0
69392147,Can't execute shell script in snakemake,"I recently started using snakemake and would like to run a shell script in my snakefile. However, I'm having trouble accessing input, output and params. I would appreciate any advice!
Here the relevant code snippets:

from my snakefile

rule ..:
    input:
        munged = 'results/munged.sumstats.gz'
    output:
        ldsc = 'results/ldsc.txt'
    params:
        mkdir = 'results/ldsc_results/',
        ldsc_sumstats = '/resources/ldsc_sumstats/',
    shell:
        'scripts/run_gc.sh'


and the script:

chmod 770 {input.munged}
mkdir -p {params.mkdir}

ldsc=$(ls {params.ldsc_sumstats})
for i in $ldsc; do
...

I get the following error message:
...
chmod: cannot access '{input.munged}': No such file or directory
ls: cannot access '{params.ldsc_sumstats}': No such file or directory
...

",-1,-1,-1.0
69401600,How to give explicitly input and output to Snakemake file?,"So I have a pipeline written in shell which loops over three folders and then an inside loop which loops over files inside the folder.
For next step, I have a snakemake file, which takes an input folder and output folder. For trial run I gave the folder path inside the snakemake file.
So I was wondering is there any way I can give input and output folder path explicitly.
For e.g.
snakemake --cores 30 -s trial.snakemake /path/to/input /path/to/output
Since I want to change the input and output according to the main loop.
I tried import sys and using sys.argv[1] and sys.argv[2] inside the snakemake file but its not working.
Below is the snippet of my pipeline, it takes three folder for now, ABC_Samples, DEF_Samples, XYZ_Samples
for folder in /path/to/*_Samples
do
  folderName=$(basename $folder _Samples)
  mkdir -p /path/to/output/$fodlerName
  for files in $folder/*.gz
  do
    /
     do something
    /
  done
  snakemake --cores 30 -s trial.snakemake /path/to/output/$fodlerName /path/to/output2/
done

But the above doesn't work. So is there any way I can do this. I am really new to snakemake.
Thank you in advance.
",-1,-1,-1.0
69443047,Snakemake: Conda env download/install extremely slow on GKE,"I am currently experimenting executing my snakemake workflow no GKE. I have noticed huge time discrepancies in the time it takes for the same conda environment to be setup on different pods.
For example, on this pod the env fastq2bam took almost 40 minutes to download and install.
2021-10-04T21:30:12.013016030ZBuilding DAG of jobs...
2021-10-04T21:30:14.088986292ZCreating conda environment workflow/envs/fastq2bam.yml...
2021-10-04T21:30:14.089894547ZDownloading and installing remote packages.
2021-10-04T22:09:06.496554997ZEnvironment for workflow/envs/fastq2bam.yml created (location: .snakemake/conda/a6dd1d801207b0464f4fb8c9ad01dfa5)

While, on a different pod it only took about 3 minutes to build the same env.
2021-10-04T21:32:41.695487553ZBuilding DAG of jobs...
2021-10-04T21:32:43.602866802ZCreating conda environment workflow/envs/fastq2bam.yml...
2021-10-04T21:32:43.603565305ZDownloading and installing remote packages.
2021-10-04T21:35:03.559349719ZEnvironment for workflow/envs/fastq2bam.yml created (location: .snakemake/conda/a6dd1d801207b0464f4fb8c9ad01dfa5)

The only real difference I can think of between the pods is the amount of CPUs being requested by them. The slow pod is requesting 1 core, while the quicker one has 10 cores. But I don't think that CPUs could be the issue here?
",-1,-1,-1.0
69693713,snakemake only runs the first rule not all,"My snakefile looks like this.
rule do00_download_step01_download_:
    input:
        
    output:
        &quot;data/00_download/scores.pqt&quot;
    run:
        from lib.do00_download import do00_download_step01_download_
        do00_download_step01_download_()
rule do00_download_step02_get_the_mean_:
    input:
        &quot;data/00_download/scores.pqt&quot;
    output:
        &quot;data/00_download/cleaned.pqt&quot;
    run:
        from lib.do00_download import do00_download_step02_get_the_mean_
        do00_download_step02_get_the_mean_()
rule do01_corr_step01_correlate:
    input:
        &quot;data/00_download/cleaned.pqt&quot;
    output:
        &quot;data/01_corr/corr.pqt&quot;
    run:
        from lib.do01_corr import do01_corr_step01_correlate
        do01_corr_step01_correlate()
rule do95_plot_step01_correlations:
    input:
        &quot;data/01_corr/corr.pqt&quot;
    output:
        &quot;plot/heatmap.png&quot;
    run:
        from lib.do95_plot import do95_plot_step01_correlations
        do95_plot_step01_correlations()
rule do95_plot_step02_plot_dist:
    input:
        &quot;data/00_download/cleaned.pqt&quot;
    output:
        &quot;plot/dist.png&quot;
    run:
        from lib.do95_plot import do95_plot_step02_plot_dist
        do95_plot_step02_plot_dist()
rule do99_figures_step01_make_figure:
    input:
        &quot;plot/dist.png&quot;
        &quot;plot/heatmap.png&quot;
    output:
        &quot;figs/fig01.svg&quot;
    run:
        from lib.do99_figures import do99_figures_step01_make_figure
        do99_figures_step01_make_figure()
rule all:
    input:
        &quot;figs/fig01.svg&quot;

I have arranged the rules in a sequential manner, hoping that this will make sure all the steps will be run in that order. However, when I run snakemake, it only runs the first rule and then it exits.
I have individually checked all the steps (functions that I import) if they work well, the paths of the input and output files. Everything looks ok. So I am guessing that the issue is with how I formatted the snakefile. I am new to snakemake (beginer-level). So it would be very helpful if somebody points out how I should fix this issue.
",1,-1,-1.0
69708470,File name problem when running snakemake with Tibanna,"Given a simple rule to join two files,
rule run_A:
    input: 
        &quot;A.txt&quot;, 
        &quot;B.txt&quot;, 
        &quot;C.txt&quot;
    
    output: &quot;Z.txt&quot;
    
    resources:
        disk_mb=100
    shell:
        &quot;cat {input} &gt; {output}&quot;

Local execution with
snakemake -j 1 Z.txt

runs with no problems.
Notice that input files are available at the s3 bucket,
(snakemake) pedro@Gen83-ubuntu:~/projects/sandbox/test$ aws s3 ls s3://grim-penguin/test/
2021-10-25 14:28:49          0 A.txt
2021-10-25 14:28:54          0 B.txt
2021-10-25 14:28:57          0 C.txt

Simply adding --tibanna, fails locally,
(snakemake) pedro@Gen83-ubuntu:~/projects/sandbox/test$ snakemake --tibanna -j 1 Z.txt
Error: --tibanna must be combined with --default-remote-prefix to provide bucket name and subdirectory (prefix) (e.g. 'bucketname/projectname')

Adding --default-remote-prefix also fails locally,
(snakemake) pedro@Gen83-ubuntu:~/projects/sandbox/test$ snakemake --tibanna --default-remote-prefix grim-penguin/test -j 1 Z.txt
Building DAG of jobs...
MissingRuleException:
No rule to produce Z.txt (if you use input functions make sure that they don't raise unexpected exceptions).

Prepending the remote prefix to the target now works locally,
(snakemake) pedro@Gen83-ubuntu:~/projects/sandbox/test$ snakemake --tibanna --default-remote-prefix grim-penguin/test -j 1 grim-penguin/test/Z.txt

but fails remotely with missing rule exception,
...
Building DAG of jobs...
MissingRuleException:
No rule to produce grim-penguin/test/Z.txt (if you use input functions make sure that they don't raise unexpected exceptions).

Local versions are,
(snakemake) pedro@Gen83-ubuntu:~/projects/sandbox/test$ snakemake --version
6.9.1
(snakemake) pedro@Gen83-ubuntu:~/projects/sandbox/test$ tibanna --version
tibanna 1.7.1

Paths without prefix fail locally and paths with prefix fail remotely. What am I missing here?
",-1,-1,-1.0
69470051,Snakemake Singularity with Local Resources / Questions about Snakemake with --use-singularity,"I'm starting to experiment with using containers with Snakemake, and I have a question about what needs to be pre-build into the container and what doesn't.  For example:
I want to run a python script (stored in workflow_root/scripts/myScript.py, for example) in a container with a pipe in from another program.  Do I need to build the python script into the container, declare it as an input file, or is that accessible from within the container (and how do I point to it)?  My current rule looks something like:
rule myRule:
    params:
        sample = get_sample,
        basePath = sys.path[0]
    input:
        in1=get_in1,
        in2=get_in2
    output:
        out1 = &quot;{runPath}/{sample}_read1_dcs.fq.gz&quot;,
        out2 = &quot;{runPath}/{sample}_read2_dcs.fq.gz&quot;
    priority: 50
    conda:
       &quot;envs/myEnv.yaml&quot;
    log:
        &quot;{runPath}/logs/{sample}_myRule.log&quot;
    shell:
        &quot;&quot;&quot;
        set -e
        set -o pipefail
        set -x
        {{
        picard FastqToSam \
        F1={input.in1} \
        F2={input.in2} \
        O=/dev/stdout \
        SM={params.sample} \
        TMP_DIR=picardTempDir \
        SORT_ORDER=unsorted \
        | python3 {params.basePath}/scripts/myScript.py \
        --input /dev/stdin \
        --prefix {wildcards.sample}
        }} 2&gt;&amp;1 | tee -a {log}
        &quot;&quot;&quot;

I want to run bwa, where I have a sizable user-provided reference that I need to use.  Can I do this, or would I need to build that reference into the container?  (I'd also like to use ensemble-VEP, which has its own sizable reference database to deal with).
I suppose what my question boils down to is: what files / locations are mounted to the container by Snakemake, and where do I find them when I'm writing rules involving shell commands?  The documentation doesn't seem to be very clear on this, and it would be nice to be able to figure it out without having to do a bunch of experimentation.
",1,-1,-1.0
69271992,"snakemake --profile PROFILE_NAME --dry-run results in ""No config.yaml found"", despite profile being located under .config/snakemake","I recently created a Snakemake profile using the guide at Snakemake-Profiles/slurm. I was able to get the profile installed successfully, and it does work when calling the path directly. However, when using the profile name, such as
snakemake --profile slurm --dry-run

I get the error:
Error: profile given but no config.yaml found. Profile has to be given as 
either absolute path, relative path or name of a directory available in either 
/etc/xdg/snakemake or /home/GROUP/USERNAME/.config/snakemake.

I have indeed installed the profile under ~/.config/snakemake. Here is the tree of this directory:
/home/GROUP/USERNAME/.config/snakemake
.
└── slurm
    ├── cluster_config.yaml
    ├── config.yaml
    ├── CookieCutter.py
    ├── __pycache__
    │   ├── CookieCutter.cpython-39.pyc
    │   └── slurm_utils.cpython-39.pyc
    ├── settings.json
    ├── slurm-jobscript.sh
    ├── slurm-status.py
    ├── slurm-submit.py
    └── slurm_utils.py

2 directories, 10 files

I can continue to specify the path to this profile when running Snakemake, but it would be useful to simply give it the name of the profile. Does anyone happen to know why Snakemake doesn't seem to be picking up that the profile slurm exists?
",-1,-1,-1.0
70024383,Missing input file for rule all [snakemake],"I've read some other questions with the same topic but I cannot solve it at all...
I'm trying to concatenate some files when I have got a second one if not, do nothing... table:
name              | path                                                                        | path2
554_MO_GEM12_r070 | data/171219_NB501241_0070_AHCHYNBGX5/fastq/554_MO_GEM12_r070_S5_R1_001.fastq.gz  |
693_SP_GEM12_r070 | data/171219_NB501241_0070_AHCHYNBGX5/fastq/693_SP_GEM12_r070_S21_R1_001.fastq.gz | data/200914_NB501241_0451_AHFNHMBGXG/fastq/693_MO_reseq70_r451_S1_R1_001.fastq.gz
866_MO_GEM12_r070 | data/171219_NB501241_0070_AHCHYNBGX5/fastq/866_MO_GEM12_r070_S10_R1_001.fastq.gz |
708_MO_GEM12_r070 | data/171219_NB501241_0070_AHCHYNBGX5/fastq/708_MO_GEM12_r070_S9_R1_001.fastq.gz  | data/180201_NB501241_0088_AHJ2GHBGX5/fastq/708_MO_GEM12_reseq070_r088_S5_R1_001.fastq.gz

This is the (simplified) Snakefile...
import os
import pandas as pd
import subprocess

### loading samples
SAMPLES = pd.read_csv(&quot;prueba_snakemake.csv&quot;)
SAMPLES.name = SAMPLES.name.astype(str)
SAMPLES = SAMPLES.set_index(&quot;name&quot;)

### including rules
include: &quot;rules/testings.smk&quot;

rule all:
    input:
        expand([&quot;data/processed/{sample}.test&quot;], sample=SAMPLES.index)

... and this is the testings.smk:
def concatenate_fastq(sample, sample_df):
    res_file = f&quot;data/processed/{sample}_concatenated.fastq.gz&quot;
    if not os.path.isfile(res_file):
        cmd = f&quot;cat {[sample_df['path']][0]} {[sample_df['path2']][0]} &gt; {res_file}&quot;
        subprocess.run(cmd, shell=True)
    return [res_file]

def get_fastq_files(wildcards):
    sample_df = SAMPLES.loc[wildcards.sample]
    if pd.isna(sample_df[&quot;path2&quot;]):
        reads = [sample_df[&quot;path&quot;]]
    else:
        reads = concatenate_fastq(wildcards.sample, sample_df)
    print(reads)
    return reads

rule test_rule:
    input:
        reads = get_fastq_files
    output:
        &quot;data/processed/{sample}.test&quot;
    shell:
        &quot;touch {output}&quot;

But something is not working well,

when concatenation is not performed none file is generated (and it'd be expected to be some touch files),
new concatenated files are correctly stored in the folder but they are not detected (?) by the rule all:

Building DAG of jobs...
['data/RUNs/171219_NB501241_0070_AHCHYNBGX5/fastq/554_MO_GEM12_r070_S5_R1_001.fastq.gz']
['data/processed/693_SP_GEM12_r070_concatenated.fastq.gz']
MissingInputException in line 18 of snake_flow/workflows/rules/trimming.smk:
Missing input files for rule test_rule:
data/processed/693_SP_GEM12_r070_concatenated.fastq.gz

I think is not a wildcard problem nor different output's paths. Any idea what I'm missing, please? Thanks.
",-1,-1,-1.0
70029504,Optional output files for snakemake?,"Essentially, I am trying to make a snakemake rule for trimming for both paired-end and single-end reads. My problem is that for unpaired reads, there is 1 output, but for paired reads, there are 2 outputs (technically 4 but for my rule, I've specified 2). The error I get I think has to do with my output.... I'll just show what I have first.
config.yaml:
sample_file: &quot;sample.tab&quot;

FASTQ_DIR: &quot;/dir/data/fastq_files&quot;
TRIMMED_DIR: &quot;/dir/data/trimmed&quot;

sample.tab:
Sample  Layout
SRR11213896     SE
ERR3887380      PE

Snakefile:
configfile: &quot;config.yaml&quot;
FASTQ_DIR = config[&quot;FASTQ_DIR&quot;]
TRIMMED_DIR = config[&quot;TRIMMED_DIR&quot;]
import pandas as pd
sample_file = config[&quot;sample_file&quot;]
samples_df = pd.read_table(sample_file).set_index(&quot;Sample&quot;, drop = True)
srr_samples = list(samples_df.index)
srr_unpaired = list(samples_df[samples_df[&quot;Layout&quot;] == &quot;SE&quot;].index)
srr_paired = list(samples_df[samples_df[&quot;Layout&quot;] == &quot;PE&quot;].index)

def get_reads(wc):
        tag = samples_df.loc[samples_df.index == wc.sample, 'Layout'].iloc[0]
        if tag == &quot;SE&quot;:
                return FASTQ_DIR + &quot;/&quot; + wc.sample + &quot;_1M.fastq&quot;
        if tag == &quot;PE&quot;:
                return FASTQ_DIR + &quot;/&quot; + wc.sample + &quot;_1M_R1.fastq&quot;, FASTQ_DIR + &quot;/&quot; + wc.sample + &quot;_1M_R2.fastq&quot;

rule all:
        input:
                expand(TRIMMED_DIR + &quot;/{sample}_trimmed.fastq&quot;, sample = srr_unpaired),
                expand(TRIMMED_DIR + &quot;/{sample}_R1_trimmed.fastq&quot;, sample = srr_paired),
                expand(TRIMMED_DIR + &quot;/{sample}_R2_trimmed.fastq&quot;, sample = srr_paired)

rule trimmed:
        input:
                reads = get_reads
        output:
                unpaired = TRIMMED_DIR + &quot;/{sample}_trimmed.fastq&quot;,
                paired_r1 = TRIMMED_DIR + &quot;/{sample}_R1_trimmed.fastq&quot;,
                paired_r2 = TRIMMED_DIR + &quot;/{sample}_R2_trimmed.fastq&quot;
        
        params:
                tag = lambda wc: samples_df.loc[samples_df.index == wc.sample, 'Layout'].iloc[0],
                to_trim = &quot;TRAILING:30 SLIDINGWINDOW:4:15 MINLEN:15&quot;,
                read = lambda wc: samples_df.loc[samples_df.index == wc.sample].index[0],
                dir = TRIMMED_DIR + &quot;/&quot;,
                read_dir = FASTQ_DIR + &quot;/&quot;
        run: 
                if params.tag == &quot;SE&quot;:
                        shell(&quot;trimmomatic {params.tag} {input.reads} {output.unpaired} {params.to_trim}&quot;)
                if params.tag == &quot;PE&quot;:
                        shell(&quot;trimmomatic {params.tag} {input.reads} {output.paired_r1} {params.dir}{params.read}_R1_unpaired.fastq {output.paired_r2} {params.dir}{params.read}_R2_unpaired.fastq {params.to_trim}&quot;)

Running snakemake -n by itself gives no errors, but running snakemake I get this error, here is the snakemake log:
Building DAG of jobs...
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
        count   jobs
        1       all
        2       trimmed
        3

rule trimmed:
    input: /dir/data/fastq_files/ERR3887380_1M_R1.fastq, /dir/data/fastq_files/ERR3887380_1M_R2.fastq
    output: /dir/data/trimmed/ERR3887380_trimmed.fastq, /dir/data/trimmed/ERR3887380_R1_trimmed.fastq, /dir/data/trimmed/ERR3887380_R2_trimmed.fastq
    jobid: 2
    wildcards: sample=ERR3887380

Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/snakemake.log

I SUSPECT it's because in the output: section of my snakemake.log, there appears to be 3 outputs when for this read, there should only be 2. Does anyone have any ideas around this? Much help would be appreciated !!!!!
",-1,-1,-1.0
70116422,Snakemake with unknown names of multiple outputs from seaborn plots in python,"I am working snakemake and using seaborn in python.
I have come to a point in the pipeline where snakemake gives missing output.
Pseudocode for snakemake would look a bit like this:
WC1 = Wildcard1
WC2 = Wildcard2

rule all:
expand(&quot;/path/to/outputs{wc1}_{wc2}.png&quot;, wc1=WC1, wc2=WC2)

checkpoint seaborn:
    input:
        &quot;/OtherPath/to/file1.csv&quot;
        &quot;/AnotherPath/to/file2.tsv&quot;
    output:
        &quot;/path/to/outputs{wc1}_{wc2}.png&quot;
    shell:
        &quot;python SeabornPlot.py&quot;

The python script, as somewhat pseudocode, would be like this:
import matplotlib.pyplot as plt
import seaborn as sns

CSV1 = pd.read_csv(snakemake.input[O], delimiter=&quot;,&quot;)
CSV2 = pd.read_csv(snakemake.input[1], delimiter=&quot;,&quot;)

for column in CSV1:
    g = sns.barplot(x=column, data=CSV2)
    fig = g.get_figure()
    fig.savefig(snakemake.output[0]) 
    plt.clf()

This does not work. The python script works outside of snakemake (with the correct path and name for the file). I suspect it has something to do with using &quot;snakemake.output[0]&quot;, but what can I use instead?
I am quite sure checkpoint is correct here, instead of rule. But please correct me if I am wrong.
Finally, I know something is missing in rule all. I probably should add a function with a new wildcard for each plot.
But the main problem I have is to get python to output the plots to the correct path.
Thanks in advance!
",-1,-1,-1.0
70203493,"Snakemake with 3 different programs, 2 input-directorys and one pre-defined .txt file with desired outputs","This will be a bit of a demanding question but death starring the sankemake doku so far has not yielded the desired result and I hope someone more experienced can walk me through.
Let me describe what I have:
1: Three Programs on the Shell, which execute like this:
./program_1 $(cat file/from/dir_1/foo.fa) $(cat file/from/dir_2/bar.fa) &gt; output.txt
./program_2 $(cat file/from/dir_1/foo.fa) $(cat file/from/dir_2/bar.fa) &gt; output.txt
./program_3 $(cat file/from/dir_1/foo.fa) $(cat file/from/dir_2/bar.fa) &gt; output.txt

Yes the cat is necessary, they use strings from within the files, not files themselves.

2: Two directories that look something like this:
hsa-let-7f-5p.fa
hsa-let-7g-5p.fa
hsa-miR-100-3p.fa
hsa-miR-100-5p.fa
hsa-miR-101-2-5p.fa

...(more)
NM_000044.fa
NM_000059.fa
NM_000075.fa
NM_000088.fa
NM_000103.fa

...(more)

3: A .txt file with all desired output combinations:
hsa-miR-29b-3p__NM_138473_programm_1.txt
hsa-miR-29b-3p__NM_138473_programm_2.txt
hsa-miR-29b-3p__NM_138473_programm_3.txt
hsa-miR-545-3p__NM_002332_programm_1.txt
hsa-miR-545-3p__NM_002332_programm_2.txt
hsa-miR-545-3p__NM_002332_programm_3.txt

...(more)

Not all files from the two directories are used, some are used multiple times, I dont want all combination, just the once specified. The output files should be separate and named according to the above specified .txt. The final sankefile should parallelize nicely on a cluster.

The workflow is pretty simple in words:
1.Read the output file names.
2.Retrieve the required inputs for the combinations from both directories.
3.Run them on all 3 programs and output the pre-defined txt. by piping the program stdout into a file.
Done

But well ... how to tell that to Snakemake? I was told this is conveniently possible but not luck so far. If there are question pls ask. Thank You in Advance (:


I adjusted the code below to my best knowledge for the situation. I'm reading out from the  file now, the &quot;programs&quot; on top and &quot;dirs&quot; in &quot;rule one&quot; have their full paths.  I also switched the two inputs in the shell command because I was confused, and they go this way. Yea, I know the file names are suboptimal.
out = []
f = open(&quot;snakemake_output_small.txt&quot;, &quot;r&quot;)
for line in f:
    out.append(line.replace('\n', ''))   

# Get distinct filenames
hsa = set([x.split('__')[0] for x in out])
nm = [x.split('__')[1] for x in out]
nm = set([re.sub('_program_.*', '', x) for x in nm])
program = ['full/path/to/program_1', 'full/path/to/program_2', 'full/path/to/program_3']

# Force snakemake to use exactly these wildcard values
# i.e. do not match by regex
wildcard_constraints:
    hsa= '|'.join([re.escape(x) for x in hsa]),
    nm= '|'.join([re.escape(x) for x in nm]),
    program= '|'.join([re.escape(x) for x in program]),


rule all:
    input:
        out,


rule one:
    input:
        hsa= 'full/path/to/dir1/{hsa}.fa',
        nm= 'full/path/to/dir2/{nm}.fa',
    output:
        '{hsa}__{nm}_{program}.txt',
    shell:
        r&quot;&quot;&quot;
        {wildcards.program} {input.nm} {input.hsa} &gt; {output}
        &quot;&quot;&quot;


Now it is telling me:
Building DAG of jobs...
MissingInputException in line 21 of Snakefile:
Missing input files for rule all:
hsa-miR-545-3p__NM_002332_program_3.txt
hsa-miR-29b-3p__NM_138473_program_1.txt
hsa-miR-29b-3p__NM_138473_program_1.txt
hsa-miR-545-3p__NM_002332_program_2.txt
hsa-miR-29b-3p__NM_138473_program_3.txt
hsa-miR-545-3p__NM_002332_program_2.txt


This is 100% a user Error, but what am I missing. Input dir is correct if i'm not totally blind.

I also forgot to mention, that two of the program have their own parameters that have to be included. A -d for program 2. and a -P here/a/parameter/file for program 3. so the shell commands might need to be separated. Sry this is all very messy.
Like before if there are questions, please ask .
",-1,-1,-1.0
70246545,Snakemake limit the memory usage of jobs,"I need to run 20 genomes with a snakemake. So I am using basic steps like alignment, markduplicates, realignment, basecall recalibration and so on in snakemake. The machine I am using has up to 40 virtual cores and 70G memory and I run the program like this.
snakemake -s Snakefile -j 40

This works fine, but as soon as It runs markduplicates along other programs, it stops as I think it overloads the 70 available giga and crashes.
Is there a way to set in snakemake the memory limit to 60G in total for all programs running? I would like snakemake runs less jobs in order to stay under 60giga, is some of the steps require a lot of memory. The command line below crashed as well and used more memorya than allocated.
snakemake -s Snakefile -j 40 --resources mem_mb=60000

",1,-1,-1.0
70262630,Snakemake installing R package in rule environment: error cannot move 00LOCK permission denied,"I'm writing a Snakemake pipeline with a rule that will run an R script. This rule has its own environment that looks like this:
channels:
 - conda-forge
 - r
 - bioconda
dependencies:
 - r-base = 4.1.1
 - r-ggplot2 = 3.3.5
 - r-biocmanager = 1.30.16

Next to the above packages, I also need ggbio, which can be installed with biocmanager:
if(!require(ggbio, quietly=TRUE)){  # if the package is not there, install it
  BiocManager::install(&quot;ggbio&quot;)
}

When executing this rule via Snakemake, initially everything goes right, and most dependencies of the ggbio package get installed. However, after some while, I get the following error:
...
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** checking absolute paths in shared objects and dynamic libraries
mv: cannot move '/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/00LOCK-xml2/00new/xml2' to '/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/xml2': Permission denied
ERROR:   moving to final location failed
ERROR: dependency ‘xml2’ is not available for package ‘biomaRt’
* removing ‘/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/biomaRt’
ERROR: dependency ‘biomaRt’ is not available for package ‘GenomicFeatures’
* removing ‘/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/GenomicFeatures’
ERROR: dependency ‘GenomicFeatures’ is not available for package ‘VariantAnnotation’
* removing ‘/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/VariantAnnotation’
ERROR: dependency ‘GenomicFeatures’ is not available for package ‘OrganismDbi’
* removing ‘/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/OrganismDbi’
ERROR: dependency ‘GenomicFeatures’ is not available for package ‘ensembldb’
* removing ‘/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/ensembldb’
ERROR: dependencies ‘GenomicFeatures’, ‘VariantAnnotation’, ‘ensembldb’ are not available for package ‘biovizBase’
* removing ‘/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/biovizBase’
ERROR: dependencies ‘biovizBase’, ‘VariantAnnotation’, ‘GenomicFeatures’, ‘OrganismDbi’, ‘ensembldb’ are not available for package ‘ggbio’
* removing ‘/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/ggbio’

The downloaded source packages are in
        ‘/tmp/RtmpiQdboR/downloaded_packages’
Updating HTML index of packages in '.Library'
Making 'packages.html' ... done
Old packages: 'fansi'
Warning messages:
1: In .inet_warning(msg) :
  installation of package ‘xml2’ had non-zero exit status
2: In .inet_warning(msg) :
  installation of package ‘biomaRt’ had non-zero exit status
3: In .inet_warning(msg) :
  installation of package ‘GenomicFeatures’ had non-zero exit status
4: In .inet_warning(msg) :
  installation of package ‘VariantAnnotation’ had non-zero exit status
5: In .inet_warning(msg) :
  installation of package ‘OrganismDbi’ had non-zero exit status
6: In .inet_warning(msg) :
  installation of package ‘ensembldb’ had non-zero exit status
7: In .inet_warning(msg) :
  installation of package ‘biovizBase’ had non-zero exit status
8: In .inet_warning(msg) :
  installation of package ‘ggbio’ had non-zero exit status
Error in library(ggbio) : there is no package called ‘ggbio’
Execution halted

The error seems to originate from this part:
mv: cannot move '/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/00LOCK-xml2/00new/xml2' to '/mnt/c/Users/nicks/Documents/variantcallingpipeline/.snakemake/conda/b98e3353bb11024e3652b19a833d9dc8/lib/R/library/xml2': Permission denied

I get this error only when I run Snakemake from the Ubuntu app on my Windows. If I run it on the server, it works:
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** checking absolute paths in shared objects and dynamic libraries
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (xml2)
... (and then it continues to the other packages)

The versions are the same on the server and local, which is 1.3.3 for xml2. I've seen other questions relating to this lock issue (here and here), and most of them suggest doing something like this:
install.packages(&quot;Rcpp&quot;, dependencies = TRUE, INSTALL_opts = '--no-lock')

But this doesn't work for me because the package doesn't get installed with install.packages(). Adding options(&quot;install.lock&quot;=FALSE) before the install also doesn't work. Also, when I check the R library folder in the environment location, I do not see the 00LOCK directory.
Any tips? Or is this just a Windows thing that's not easily fixable? Running the installment code in Rstudio does work, but I need the package to get installed in the conda environment, not my standard Rstudio library.
",-1,-1,-1.0
70297171,define input and output as list elements in snakemake,"Hi I am new to snakemake and have some problems. I have defined two rules below.
myrule1 works and does what I want but I am playing around with the syntax and try to make it a bit more advanced but have some issues. These are found in myrule2.
Is it not possible to define input and output variables as list elements?
Thanks, I know it is a simple stupid questions, but I have some problems knowing what to search for.
FILES = &quot;file.r1&quot;,&quot;file.r2&quot;
EXT=&quot;.p1.gz&quot;,&quot;.p2.gz&quot;,&quot;.p3.gz&quot;

print(&quot;files: &quot;+str(FILES),&quot;\n&quot;)

rule all:
    input:
        expand(&quot;OUT{ext}&quot;,file=FILES,ext=EXT)
   

rule myrule1:
    input:
        R1 = FILES[0],
        R2 = FILES[1]
    output:
        r1 = &quot;OUT&quot;+EXT[0],
        r2 = &quot;OUT&quot;+EXT[1],
        c = &quot;OUT&quot;+EXT[2]
    shell:
        &quot;myprog -in1 {input.R1} -in2 {input.R2} -out OUT&quot;

##below doesnt work
rule myrule2:
    input:
        R1,R2 = FILES
    output:
        r1,r2,c = [&quot;OUT&quot;+x for x in EXT]

    shell:
        &quot;myprog -in1 {input.R1} -in2 {input.R2} -out OUT&quot;

",-1,-1,-1.0
70324411,snakemake truncating shell codes,"I'm trying to change the chromosome number notation from [0-9XY] to Chr[0-9XY] using the samtools reheader in the shell command of the snakemake.
rule rename:
    input:
        os.path.join(config[&quot;input&quot;], &quot;{sample}.bam&quot;),
    output:
        os.path.join(config[&quot;output&quot;], &quot;new_sample/{sample}_chr.bam&quot;)
    log:
        os.path.join(config[&quot;log&quot;], &quot;samtools/{sample}&quot;)
    shell:
        &quot;samtools view -H {input} | sed  -e 's/SN:\([0-9XY]*\)/SN:chr\1\/' -e 's/SN:MT/SN:chrM/' |samtools reheader - {input} &gt; {output}&quot;

The code ran successfully in the terminal but when I used the code in snakemake it gave the error:
shell:
        samtools view -H /Users/EGA_dataset/cnvkit_snakemake/input/EGAF00000788153_PD11458c.bam | sed  -e 's/SN:\([0-9XY]*\)/SN:chr/' -e 's/SN:MT/SN:chrM/' |samtools reheader - /Users/cnvkit_snakemake/input/EGAF00000788153_PD11458c.bam &gt; /Users/EGA_dataset/cnvkit_snakemake/output/new_sample/EGAF00000788153_PD11458c_chr.bam
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

When I looked into the error I found that the snakemake read the sed  -e 's/SN:([0-9XY]*)/SN:chr\1/ as 's/SN:([0-9XY]*)/SN:chr/'. That is, it truncated the code for some reasons I do not understand.
",-1,-1,-1.0
70367776,How can I continue a snakemake workflow from a subset of files created at some point in the workflow? (tried checkpoint),"At some point in my workflow, I'm selecting a subset of files. I want to use that subset of files to continue with the next steps in my workflow:
rule all:
    input:
        &quot;results/selected_files.tsv&quot;,

checkpoint select_by_size:
    input:
        &quot;results/selected_seqs&quot;
    output:
        directory(&quot;results/selected_seqs_by_size&quot;)
    shell:
        &quot;&quot;&quot;
        mkdir -p {output[0]} 
        # The second -size is only for testing, remove it for a real run
        find {input} -size +302c -size -304c -exec cp {{}} {output[0]} \;
        &quot;&quot;&quot;

def get_selected_files(wildcards):
    ck_output = checkpoints.select_by_size.get(**wildcards).output[0]
    GENES,  = glob_wildcards(join(ck_output, &quot;{gene}.fasta&quot;))
    return expand(join(ck_output, &quot;{GENE}.fasta&quot;), GENE=GENES)

rule create_table_sel:
    input:
        get_selected_files
    output:
        &quot;results/selected_files.tsv&quot;
    run:
        with open(output[0], 'w') as fh:
            for f in input:
               print(Path(f).stem, f, file=fh, sep='\t')

The rule create_table_sel creates a table that look like this:
JLEOKLFN_00589  results/selected_seqs_by_size/JLEOKLFN_00589.fasta
JLEOKLFN_01812  results/selected_seqs_by_size/JLEOKLFN_01812.fasta

If I try to use this table to get single files into a following rule, it fails (adding &quot;results/a3ms.txt&quot; to rule all)
def get_seqs(wildcards):
    df = pd.read_csv(&quot;results/selected_files.tsv&quot;, sep=&quot;\t&quot;, header=None, index_col=0)
    return df.loc[wildcards.gene].values[0]

rule hhblits_msa:
    input:
        get_seqs
        # &quot;results/selected_seqs_by_size/{gene}.fasta&quot;
    output:
        &quot;results/hhblits_msa/{gene}.a3m&quot;
    log:
        &quot;logs/hhblits/{gene}_msa.log&quot; 
    params:
        db=config[&quot;msa_db&quot;],
        n_iter= 2
    threads: 4
    shell:
        &quot;&quot;&quot;
        mkdir -p results/hhblits_msa
        hhblits -i {input[0]} -d {params.db} -oa3m {output[0]} -cpu {threads}  -n {params.n_iter} \
        &amp;&gt; {log}
        &quot;&quot;&quot;

# The intention is to use the above rule as input
rule agg:
    input:
        &quot;results/hhblits_msa/{gene}.a3m&quot;
    output:
        &quot;results/a3ms.txt&quot;
    shell:
        &quot;echo {input} &gt;&gt; {output}&quot;

Wildcards in input files cannot be determined from output files:
'gene'

If I use something like snakemake --cores 4 results/hhblits_msa/JLEOKLFN_00589.a3m, it runs without problems, but of course I want to use this workflow without the need to specify an output file in the command line.
How can I add the rule hhblits_msa to my workflow? How can I continue any workflow from a subset of files created at some point in the same workflow?
Also, someway related, is it possible to use the checkpoints object to return a single file and not only return all the files with expand? I tried a few thigs and I fails because the is no way to get the wildcards. The idea would be to use that single file as input of any rule, like hhblits_msa.
ADDED
The problem is to use the files created in select_by_size as input to hhblits_msa. Given that those files are created in the same workflow, and I don't know them beforehand, I cannot include them in all. When I use this workflow or the one suggested by Jianyu, the rule hhblits_msa doesn't get executed.
",-1,-1,-1.0
70420780,Snakemake. Set of experiments to be run sequentially on different resources,"Excuse the length of the question - the problem is a bit complex and I think it is interesting, so I wanted to illustrate it clearly and in full detail, providing working code that can be tested, including my (not very fruitful) attempts.
The problem
Assume that I have a set of experiments to do on different Virtual Machines with different characteristics, for example, to determine the runtime of different workloads. In each machine there is a set of experiments depending on other parameters, which vary the workload, as for example the name of the application to run, the size of the problem to solve, etc.
To simplify, assume only the following parameters that define each experiment:

The type of the virtual machine (eg: small, medium, large)
The name of the application to run (eg: eratostenes, fibonacci)
The size of the problem to solve (eg: 100, 500, 1000)

With these parameters, the number of experiments to run is 3 x 2 x 3 = 18
As a result I want 18 files, each one containing different statistics about the experiment. The names of the files will use the pattern VM_{type}_APP_{app_name}_SIZE_{size}.data as for example &quot;VM_small_APP_eratostenes_SIZE_100.data&quot; and so on.
Naive approach
So far, so good. I can easily write a Snakefile that perform these tasks:
vm_types = [&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;]
applications = [&quot;eratostenes&quot;, &quot;fibonacci&quot;]
sizes = [100, 500, 1000]
all_files = expand(&quot;VM_{type}_APP_{app_name}_SIZE_{size}.data&quot;, type=vm_types, app_name=applications, size=sizes)

def perform_experiment(params):
    print(&quot;Faking experiment with params&quot;, params)
    with open(&quot;VM_{type}_APP_{app_name}_SIZE_{size}.data&quot;.format(**params), &quot;w&quot;) as f:
        f.write(&quot;Dummy data&quot;)

rule all:
    input: all_files

rule perform_experiment:
    output: &quot;VM_{type}_APP_{app_name}_SIZE_{size}.data&quot;
    run:
        perform_experiment(wildcards)

In fact the problem is slightly more complex, because the VMs required for the experiments have to be created and started before the experiment can take place. So I need to add extra tasks for starting up the virtual machine, and include dependencies among tasks. For example:
vm_types = [&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;]
applications = [&quot;eratostenes&quot;, &quot;fibonacci&quot;]
sizes = [100, 500, 1000]
all_files = expand(&quot;VM_{type}_APP_{app_name}_SIZE_{size}.data&quot;, type=vm_types, app_name=applications, size=sizes)

def perform_experiment(params):
    print(&quot;Faking experiment with params&quot;, params)
    with open(&quot;VM_{type}_APP_{app_name}_SIZE_{size}.data&quot;.format(**params), &quot;w&quot;) as f:
        f.write(&quot;Dummy data&quot;)

def start_vm(t):
    print(&quot;Starting VM&quot;, t)

rule all:
    input: all_files

rule perform_experiment:
    input: &quot;VM_{type}_start.done&quot;
    output: &quot;VM_{type}_APP_{app_name}_SIZE_{size}.data&quot;
    run:
        perform_experiment(wildcards)

rule start_vm:
    output: touch(&quot;VM_{type}_start.done&quot;)
    run:
        start_vm(wildcards.type)

The real problem
The problem is that the DAG of this workflow has no dependencies between VMs, so all experiments could be done in parallel, in principle. Even if I enforce --jobs 1 to avoid parallelism, since the experiments are independent, snakemake can choose to start by any experiment in any of the VM types, and then perform the next experiment in a different VM type, etc.
This is not what I want. Asssume that I don't have the resources to run several VM in parallel, so I want to perform the tests in each VM sequentially. In addition, starting up and shutting down a machine takes time, so I want to leverage the fact that one VM is started to perform all required experiments inside that machine. That is:
For each virtual machine type:
  - Set up that vm
  - Perform all experiments that must be done in that virtual machine
  - Stop that virtual machine

How would be the best way to accomplish this?
My attempt
I thought about introducing additional tasks to stop the VM, which should also produce a file like &quot;VM_small_stop.done&quot;, and initiate the start-up of a new VM type depending on the stopping of the previous one. In addition, the &quot;stop_vm&quot; task can only be run after all experiments in that VM are done, so I need to introduce dependencies between &quot;stop_vm&quot; and &quot;perform_experiment&quot;.
Also, I need to &quot;manually&quot; delete the file VM_{type}_start.done when a machine is stopped, as part of the code which stops it (there is way to specify as output the absence of a file so that Snakemake deletes it? Kind of touch(filename) but in reverse... delete(filename)).
And I also need to add to the rule all a dependency on the last machine stop, or othervwise that last machine will not be stopped.
This quickly makes the Snake file very complex, as for example:
vm_types = [&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;]
applications = [&quot;eratostenes&quot;, &quot;fibonacci&quot;]
sizes = [100, 500, 1000]
all_files = expand(&quot;VM_{type}_APP_{app_name}_SIZE_{size}.data&quot;, type=vm_types, app_name=applications, size=sizes)

def perform_experiment(params):
    print(&quot;Faking experiment with params&quot;, params)
    with open(&quot;VM_{type}_APP_{app_name}_SIZE_{size}.data&quot;.format(**params), &quot;w&quot;) as f:
        f.write(&quot;Dummy data&quot;)

def start_vm(t):
    print(&quot;Starting VM&quot;, t)

import os
def stop_vm(t):
    print(&quot;Stopping VM&quot;, t)
    os.unlink(&quot;VM_{}_start.done&quot;.format(t))

def previous_vm_type(wildcards):
    if wildcards.type == &quot;small&quot;: return []
    return &quot;VM_{}_stop.done&quot;.format(vm_types[vm_types.index(wildcards.type)-1])

rule all:
    input: all_files, &quot;VM_{}_stop.done&quot;.format(vm_types[-1])

rule perform_experiment:
    input: &quot;VM_{type}_start.done&quot;
    output: &quot;VM_{type}_APP_{app_name}_SIZE_{size}.data&quot;
    run:
        perform_experiment(wildcards)

rule start_vm:
    input: previous_vm_type
    output: touch(&quot;VM_{type}_start.done&quot;)
    run:
        start_vm(wildcards.type)

rule stop_vm:
    input: expand(&quot;VM_{type}_APP_{app_name}_SIZE_{size}.data&quot;, app_name=applications, size=sizes, allow_missing=True)
    output: touch(&quot;VM_{type}_stop.done&quot;)
    run:
        stop_vm(wildcards.type)

This kind of works. It produces the following DAG which looks ok:

But nevertheless it has some problems:

The approach looks clumsy. It must be a better way!
If I want to re-run one of the experiments, say for example VM_large_APP_eratostenes_SIZE_500.data, how shoud I proceed?


If I simply delete the file VM_large_APP_eratostenes_SIZE_500.data and re-run snakemake, what happens is that not only that experiment is performed, but all experiments in that machine. I guess that it is because all of them depend on VM_large_start.done, and that file is refreshed when the machine starts.
Even worse if the deleted file is one of the first VM in the DAG (small machine), then all the experiments are re-run, because after running all experiments in &quot;small&quot; machine, the file VM_small_stop.done is updated, and that triggers the starting of the next VM type, and so on...
Putting ancient() around the &quot;done&quot; files in the rules has no effect.

The question
So my &quot;solution&quot; defeates the purpose of using snakemake in the first place. If I want to re-run only one experiment, but this Snakefile will cause to run all of them anyway, better I would have written a sequential python script.
So, what would be a good way to address this problem using Snakemake?
",1,-1,-1.0
70288899,Snakemake with multiple output directories,"I'm new to using snakemake. I've written a bash script to analyse some paired end amplicon sequencing data. I've been advised to use snakemake to iterate my script over the paired reads for each sample. My script generates lots of different output files (various bams, vcfs, html and txt files) in different directories (qc, alignment, variants, consensus and repeats).
I've read the tutorial, and tried writing a Snakefile, as below. I didn't know what to put in output as there are many different output directories and files created, so I thought having input and script would be enough.
rule othello_call:
    input:
        R1=&quot;/Users/michaelflower/Documents/ACL/Research/Projects/Illumina amplicon sequencing/2021.03.23 MiSeq test/run_othello/data/{sample}_R1_001.fastq.gz&quot;
        R2=&quot;/Users/michaelflower/Documents/ACL/Research/Projects/Illumina amplicon sequencing/2021.03.23 MiSeq test/run_othello/data/{sample}_R2_001.fastq.gz&quot;
        GEN=&quot;hg38&quot;
        REF=&quot;/Users/michaelflower/refs/hg38/hg38.fa&quot;
        REPREF=&quot;/Users/michaelflower/refs/monckton/Supplementary_Data.fasta&quot;
        CON=&quot;/Users/michaelflower/opt/anaconda3&quot;
        OUT=&quot;/Users/michaelflower/Documents/ACL/Research/Projects/Illumina amplicon sequencing/2021.03.23 MiSeq test/run_othello&quot;
    output:
    shell:
        ./othello.sh R1={input.R1} \
            R2={input.R2} \
            GEN={input.GEN} \
            REF={input.REF} \
            REPREF={input.REPREF} \
            CON={input.CON} \
            OUT={input.OUT}

But when I run it I get:
$ snakemake -np
SyntaxError in line 8 of /Users/michaelflower/bin/othello/Snakefile:
invalid syntax (Snakefile, line 8)

Any help setting up the input and output rules would be much appreciated!
",-1,-1,-1.0
70702438,Snakemake temp(),"I have Snakemake hooked up to an S3 account, and I'm wanting to delete certain temp() files after processing our pipeline.
I have a rule that designates certain files as temp(). Below is an example of one:
 #Split rep element mapped bam file into subfiles
rule split_rep_bam:
  input:
    'rep_element_pipeline/{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam'
  output:
    temp('rep_element_pipeline/AA.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/AC.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/AG.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/AN.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/AT.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/CA.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/CC.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/CG.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/CN.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/CT.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/GA.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/GC.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/GG.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/GN.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/GT.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/NA.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/NC.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/NG.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/NN.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/NT.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/TA.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/TC.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/TG.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/TN.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp'),
    temp('rep_element_pipeline/TT.{sample}.fastq.gz.mapped_vs_' + config[&quot;ref&quot;][&quot;bt2_index&quot;] + '.sam.tmp')
  conda:
    '../envs/rep_element.yaml'
  params:
    fp=full_path
  shell:
    'perl ../scripts/split_bam_to_subfiles.pl {input[0]} '
    '{params.fp}/rep_element_pipeline/'

I've been running Snakemake as such:
snakemake --default-remote-provider S3 --default-remote-prefix '$s3' --use-conda --cores 32 --rerun-incomplete --printshellcmds --delete-temp-output

Where I've specified the --delete-temp-output option.
When running this rule, it appears snakemake is deleting these files. However, these files still persist in S3. Does anyone know why these files aren't being deleted in S3?
Building DAG of jobs...
Deleting 20211222-rp030/rep_element_pipeline/AA.APP-01_rep_1_Ribo_R1.fastq.gz.mapped_vs_bt2_hg38.sam.tmp
Deleting 20211222-rp030/rep_element_pipeline/AC.APP-01_rep_1_Ribo_R1.fastq.gz.mapped_vs_bt2_hg38.sam.tmp

",-1,-1,-1.0
70857130,snakemake: only run the rules that connect one upstream rule (point A) with one downstream rule (point B),"I have a snakemake workflow in which made a change in only one rule, so that its output is not slightly different than what was earlier. Now I need to re-run the downstream rules that depended on this rule. For those rules, I want to know a functionality in snakemake that (1) identifies the rules to rerun i.e. all the downstream rules that depend on the rule I changed and
(2) simply re-runs them while overwriting their output files.
For this, I searched through the snakemake's documentation. I could find an option that seemed relevant:

--forceall, -F
Force the execution of the selected (or the first) rule and all rules it is dependent on regardless of already created output.

But it was not useful, because this command would run the rules that are upstream (dependent) of the rule I changed. This is exactly the opposite of what I wanted. If I provide the downstream rule to this option, then I guess it will re-run all of its dependent rules, some of which are needed to be re-run.
Question: Is there a way in snakemake to set a &quot;start&quot; rule (point A) and &quot;end&quot; rule (point B) and only re-run the rules that are connecting the two?
Possible solution (but an undesirable one):
I could manually figure out which rules I want to re-run and provide them to --forcerun option.

--forcerun, -R
Force the re-execution or creation of the given rules or files. Use this option if you changed a rule and want to have all its output in your workflow updated.

But manual interventions like this would not be good for the reproducibility of the workflow. Ideally, I would like if snakemake could do this by itself.
",-1,-1,-1.0
70949510,snakemake and conda environments,"Question: how do I specify a conda environment with a distinct prefix in a snakemake rule without it crashing upon trying to deactivate the env?
background:
I have conda environments stored in an alternate location (I made a given conda environment with conda create --prefix ./envname) than my baseline miniconda.
I call these conda environments in their respective snakemake rules, as advised in the documentation.
rule hello:
input: ...
output: ...
conda: &quot;/path/to/conda/env&quot;
shell: &quot;...&quot;
I am unable to supply .yaml files to my Snakefile.
All my snakemake rules run beautifully.
However, snakemake then tries to exit and does it using --name not --prefix
subprocess.CalledProcessError: Command 'conda env export --name '/path/to/env'' returned non-zero exit status     1.

Of course this will fail because conda environments referred to by 'name' can't have '/' in them.
So my snakemake crashes. My rules work great in the command line but I'm working with hundreds of samples so I need snakemake.
Also, I know people have said online that you can't do 'conda activate env' and that the workaround is to put a header saying 'source $HOME/miniconda3/etc/profile.d/conda.sh' which I have but this isn't fixing the issue because its within the Snakefile.
help! thank you!
",-1,-1,-1.0
70958188,Using snakemake to rename files according to defined mapping,"I'm trying to use snakemake to download a list of files, and then rename them according to mapping given in the file. I first read a dictionary from a file that has the form of {ID_for_download : sample_name}, and I pass the list of its keys to first rule for download (because downloading is taxing, I'm just using a dummy script to generate empty files). For every file in the list, two files are downloaded in the form of {file_1.fastq} and {file_2.fastq} When those files are downloaded, I then rename them using the second rule - here I take advantage of being able to run python code in a rule using run key word. When I do a dry-run using -n flag, everything works. But when I do an actual run, I get an error of the form
Job Missing files after 5 seconds [list of files]
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Job id: 0 completed successfully, but some output files are missing. 0
Exiting because a job execution failed. Look above for error message
Removing output files of failed job rename_srafiles_to_samples since they might be corrupted: [list of all files]

What happens is that a directory to store my files is created, and then my files are &quot;downloaded&quot;, and then are renamed. Then when it reaches the last file, I get this error and everything is deleted. The snakemake file is below:
import csv
import os
SRA_MAPPING = read_dictionary() #dictionary read from a file
SRAFILES = list(SRA_MAPPING.keys())[1:] #list of sra files
SAMPLES = [SRA_MAPPING[key] for key in SRAFILES] #list of sample names
rule all:
    input:
        expand(&quot;raw_samples/{samples}_1.fastq&quot;,samples=SAMPLES),
        expand(&quot;raw_samples/{samples}_2.fastq&quot;,samples=SAMPLES),
rule download_srafiles:
    output:
        expand(&quot;raw_samples/{srafiles}_1.fastq&quot;,srafiles=SRAFILES),
        expand(&quot;raw_samples/{srafiles}_2.fastq&quot;,srafiles=SRAFILES)
    shell:
        &quot;bash dummy_download.sh&quot;
rule rename_srafiles_to_samples:
    input:
        expand(&quot;raw_samples/{srafiles}_1.fastq&quot;,srafiles=SRAFILES),
        expand(&quot;raw_samples/{srafiles}_2.fastq&quot;,srafiles=SRAFILES)
    output:
        expand(&quot;raw_samples/{samples}_1.fastq&quot;,samples=SAMPLES),
        expand(&quot;raw_samples/{samples}_2.fastq&quot;,samples=SAMPLES)
    run:
        os.chdir(os.getcwd()+r&quot;/raw_samples&quot;)
        for file in os.listdir():
                old_name=file[:file.find(&quot;_&quot;)]
                sample_name=SRA_MAPPING[old_name]
                new_name=file.replace(old_name,sample_name)
                os.rename(file,new_name)

I've separately tried to run download_srafiles and it worked. I also separately tried to run rename_srafiles_to_samples and it worked. But when I run those files in conjunction, I get the error. For completeness, the script dummy_download.sh is below:
#!/bin/bash
read -a samples &lt;&lt;&lt; $(cut -d , -f 1 linker.csv | tail -n +2)
for file in &quot;${samples[@]}&quot;
do
touch raw_samples/${file}_1.fastq
touch raw_samples/${file}_2.fastq
done

(linker.csv is a file in one column has ID_for_download and in other column has sample_name)
What am I doing wrong?
EDIT: Per user dariober, the change of directories via python's os in the rule rename_srafiles_to_samples &quot;confused&quot; snakemake. Snakemake's logic is sound - if I change the directory to enter raw_samples, it tries to find raw_samples in itself and fails. To that extend, I tested different versions.
Version 1
Exactly as dariober explained. Important bits of code:
for file in os.listdir('raw_samples'):
     old_name= file[:file.find(&quot;_&quot;)]
     sample_name=SRA_MAPPING[old_name]
     new_name= file.replace(old_name,sample_name)
     os.rename('raw_samples/' + file, 'raw_samples/' + new_name)

It lists files in &quot;raw_samples&quot; directory, and then renames them. Crucial thing to do is to add prefix of directory (raw_samples/) to each rename.
Version 2
The same as my original post, but instead of leaving working directory, I exit it at the end of the loop. It works.
os.chdir(os.getcwd()+r&quot;/raw_samples&quot;)
for file in os.listdir():
     old_name= file[:file.find(&quot;_&quot;)]
     sample_name=SRA_MAPPING[old_name]
     new_name= file.replace(old_name,sample_name)
     os.rename(file,new_name)
os.chdir(&quot;..&quot;)

Version 3
Same as my original post, but instead of modifying anything in the run segment, I modify the output to exclude file directory. This means that I have to modify my rule all too. It didn't work. Code is below:
rule all:
input:
    expand(&quot;{samples}_1.fastq&quot;,samples=SAMPLES),
    expand(&quot;{samples}_2.fastq&quot;,samples=SAMPLES),

rule download_srafiles:
    output:
        expand(&quot;raw_samples/{srafiles}_1.fastq&quot;,srafiles=SRAFILES),
        expand(&quot;raw_samples/{srafiles}_2.fastq&quot;,srafiles=SRAFILES)
    shell:
        &quot;touch {output}&quot;

rule rename_srafiles_to_samples:
    input:
        expand(&quot;raw_samples/{srafiles}_1.fastq&quot;,srafiles=SRAFILES),
        expand(&quot;raw_samples/{srafiles}_2.fastq&quot;,srafiles=SRAFILES)
    output:
        expand(&quot;{samples}_1.fastq&quot;,samples=SAMPLES),
        expand(&quot;{samples}_2.fastq&quot;,samples=SAMPLES)
    run:
        os.chdir(os.getcwd()+r&quot;/raw_samples&quot;)
        for file in os.listdir():
             old_name= file[:file.find(&quot;_&quot;)]
             sample_name=SRA_MAPPING[old_name]
             new_name= file.replace(old_name,sample_name)
             os.rename(file,new_name)

The error it gives is:
MissingOutputException in line 24
...
Job files missing

The files are actually there. So I don't know if I made some error in the code or is this some bug.
Conclusion
I wouldn't say that this is a problem with snakemake. It's more of a problem with my poorly thought out process. In retrospect, it makes perfect sense that entering directory messes up output/input process of snakemake. If I want to use os module in snakemake to change directories, I have to be very careful. Enter wherever I need to, but ultimately go back to my original starting place. Many thanks to /u/dariober and /u/SultanOrazbayev
",-1,-1,-1.0
71075669,snakemake parameter exploration: how to pass options to a command in shell directive?,"I'm performing parameter exploration using Paramspace utility as described here.  I've read parameters in a pandas dataframe and next I wish to pass these as values of options of a shell command but can't figure out how.
In the below minimal example, I wish to pass parameter s (read in dataframe df) as the value of option -n for head command in the shell directive.
from snakemake.utils import Paramspace
import pandas as pd

df = pd.DataFrame(
    {'s' : [1, 2, 3]},
    index = [1, 2, 3]
    )

paramspace_empty = Paramspace(df)

rule all:
    input:
        expand(&quot;results/{params}.tsv&quot;, params=paramspace_empty.instance_patterns)

rule simulate_empty:
    output:
        f&quot;results/{paramspace_empty.wildcard_pattern}.tsv&quot;
    params:
        simulation=paramspace_empty.instance
    shell: &quot;&quot;&quot;
        head input.txt &gt; {output}    
    &quot;&quot;&quot;

I tried the below and similar variations but nothing worked.
shell: &quot;&quot;&quot;
    head -n {params.simulation['s']} input.txt &gt; {output}
&quot;&quot;&quot;

The above example is extracted (and modified a bit) from the Snakefile here which tests paramspace utility.
I seem to be missing something fundamental or trivial. Thank you in advance for any help.
",1,-1,-1.0
71116406,Snakemake input rule defintion via lambda + Pandas dataframe,"Sorry if this is gonna be probably a duplication of other questions, but I couldn't figure to debug what's going on in my case.
Got a dataframe like this:
       Sample  gender phenotype subject_id
0  ERR35175    male     tumor         13
1  ERR35176    male   control         13
2  ERR35177  female     tumor         14
3  ERR35178  female   control         14
4  ERR35179    male     tumor         16
5  ERR35180    male   control         16

Given a subject_id, I need to give in input either the tumor and the control sample from the dataframe, concatenating with path from config and file termination, to produce an output which uses the subject_id. To do so, I've written this rule (under file snv_calling.smk):
rule Mutect2:
    input:
        tumor=lambda wc: config[&quot;datadirs&quot;][&quot;BQSR_2&quot;]+'/%s_recal.pass2.bam' %df[(df.phenotype == &quot;tumor&quot;) &amp; (df.subject_id == [wc.patient])].Sample.values[0],
        normal=lambda wc: config[&quot;datadirs&quot;][&quot;BQSR_2&quot;]+'/%s_recal.pass2.bam' %df[(df.phenotype == &quot;control&quot;) &amp; (df.subject_id == [wc.patient])].Sample.values[0]
    output:
        vcf=config[&quot;datadirs&quot;][&quot;VCF&quot;]+&quot;/{patient}.vcf&quot;
    shell:
    &quot;&quot;&quot;
    gatk Mutect2 \
    -I {input.tumor} \
    -I {input.normal} \
    -O {output.vcf}
    &quot;&quot;&quot;

Inside the Snakefile:
PATIENT=['13','14','16']
rule all:
    input:
        expand(config[&quot;datadirs&quot;][&quot;VCF&quot;]+&quot;/&quot;+&quot;{patient}.vcf&quot;, patient=PATIENT)

It gives me this error, where line 37 is the first input argument:
InputFunctionException in line 37 of ../rules/snv_calling.smk:
Error:
  ValueError: ('Lengths must match to compare', (0,), (1,))
Wildcards:
  patient=13
Traceback:
  File &quot;../rules/snv_calling.smk&quot;, line 45, in &lt;lambda&gt;

I'm struggle to understand what's going on, because it seems that the wildcards patient is assigned properly from the error. If I run the function outside of Snakemake there's no error against the PATIENT list.
",-1,-1,-1.0
71162082,How to use pandas within snakemake pipelines,"I would like to improve the reproducibility of some python codes I made by transforming some codes into a data pipeline. I am used to targets in R and would like to find an equivalent in Python. I have the impression that snakemake is quite close to that.
I don't understand how we can use pandas to import an input in a snakemake task, modify it and then write output.
Let's take the easiest pipeline I can think of: we take a csv and write a copy somewhere else.
The pipeline works fine when using bash script:
rule trying_snakemake:
    input:
        path=&quot;untitled.txt&quot;
    output:
        &quot;test-snakemake.csv&quot;
    run:
        shell(&quot;cp {input.path} {output}&quot;)

I wanted to use the equivalent approach with pandas (of course here using pandas does not seem necessary but this is to understand the logic):
rule trying_snakemake:
    input:
        path=&quot;untitled.txt&quot;
    output:
        &quot;test-snakemake.csv&quot;
    run:
        import pandas as pd
        df = pd.read_csv({input.path})
        df.to_csv({output}, header=False)

snakemake -c1

Invalid file path or buffer object type: &lt;class 'set'&gt;
  File &quot;/home/jovyan/work/label-openfood/Snakefile&quot;, line 19, in __rule_trying_snakemake
  File &quot;/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py&quot;, line 311, in wrapper
  File &quot;/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py&quot;, line 586, in read_csv
  File &quot;/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py&quot;, line 482, in _read
  File &quot;/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py&quot;, line 811, in __init__
  File &quot;/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py&quot;, line 1040, in _make_engine
  File &quot;/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py&quot;, line 51, in __init__
  File &quot;/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py&quot;, line 222, in _open_handles
  File &quot;/opt/conda/lib/python3.9/site-packages/pandas/io/common.py&quot;, line 609, in get_handle
  File &quot;/opt/conda/lib/python3.9/site-packages/pandas/io/common.py&quot;, line 396, in _get_filepath_or_buffer
  File &quot;/opt/conda/lib/python3.9/concurrent/futures/thread.py&quot;, line 52, in run
Exiting because a job execution failed. Look above for error message

I think the error comes at the read_csv step but I don't understand what it means (I am used to situations where pandas works like a charm)
",-1,-1,-1.0
71345313,Snakemake: How to force the creation of all conda environments,"I am aware that by adding the option --conda-create-envs-only you are able to create the conda environments for the workflow. However, would it be possible to force the creation of all conda environments under workflow/envs/ without knowing the workflow DAG in advance?
The reason is that I am planning to run snakemake on an HPC, and the compute nodes have no internet. As such I have to set up the environment in a build node with internet. The problem is that I can only access my input data in the compute nodes.
",-1,-1,-1.0
71485105,Cannot open pickle file generated by snakemake pipeline,"I have a data analysis pipeline that consists of multiple steps. I have generated a snakemake pipeline (new for me) and the output of every task (and input of the next task) is a pickle file containing either a DataFrame or a list of DataFrames. Everything is fine except I cannot open the pickle files manually. Of note, the pipeline uses a dedicated conda environment.
import _pickle
with open(&quot;testb/first/out/stacks.pkl&quot;, &quot;rb&quot;) as f:
    data = _pickle.load(f)

I get this error:
AttributeError: Can't get attribute '_unpickle_block' on &lt;module 'pandas._libs.internals' from 'C:\\Users\\sebde\\anaconda3\\envs\\dbm\\lib\\site-packages\\pandas\\_libs\\internals.cp39-win_amd64.pyd'

Python 3.10.2,
Snakemake-minimal 7.0.4 (as per documentation, I'm on Windows),
Pandas 1.4.1
",-1,-1,-1.0
71170084,Snakemake is unable to match wildcard although it's defined and even suggested,"I am still very confused about the wildcards concept despite reading the full docs and a few examples, so maybe someone can shed light on this weird behaviour. It might be a bug but it's such a basic example that I am pretty sure I am doing or understanding something wrong.
Here is my Snakefile which should generate a bunch of files defined in a dictionary where the location of the files is stored (those can be served by all kinds of data providers like iRODS, XRootD etc., but it's not important now).
import os

some_files = {
    &quot;foo&quot;: &quot;some_location/foo&quot;,
    &quot;bar&quot;: &quot;another_location/bar&quot;,
    &quot;baz&quot;: &quot;yet_another_loc/baz&quot;
}

rule all:
    input: [&quot;raw/&quot; + os.path.basename(f) for f in some_files.keys()]

rule generate_files:
    output:
        temp(&quot;raw/{fname}&quot;)
    shell:
        &quot;echo grabbed file from {some_files[wildcards.fname]} &gt; {output}&quot;

As you can see, I need to use a similar &quot;trick&quot; which was proposed in my previous question (Array of values as input in Snakemake workflows) to force the recognition of the files by adding a rule and listing those (in rule all), which works nicely.
The rule generate_files should then generate (retrieve) those by using the corresponding URL and protocol defined in some_files. For the sake of simplicity, it's now just echoing the origin into the output file.
To achieve this, I thought I can simply use the wildcards.fname in the shell section but I when I run the workflow, I get:
░ tamasgal@silentbox-(2):PhD/snakemake  master ●●● snakemake took 16s
░ 08:47:35 &gt; snakemake -c1
Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job               count    min threads    max threads
--------------  -------  -------------  -------------
all                   1              1              1
generate_files        3              1              1
total                 4              1              1

Select jobs to execute...

[Fri Feb 18 08:47:38 2022]
rule generate_files:
    output: raw/bar
    jobid: 2
    wildcards: fname=bar
    resources: tmpdir=/var/folders/84/mcvklq757tq1nfrkbxvvbq8m0000gn/T

RuleException in line 12 of /Users/tamasgal/Dev/PhD/snakemake/Snakefile:
NameError: The name 'wildcards.fname' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}

If I use fname (and not wildcards.fname), Snakemake proposes to use wildcards.fname, which again, does not work. Here is the output when running with fname in output:
[Fri Feb 18 08:47:48 2022]
rule generate_files:
    output: raw/bar
    jobid: 2
    wildcards: fname=bar
    resources: tmpdir=/var/folders/84/mcvklq757tq1nfrkbxvvbq8m0000gn/T

RuleException in line 12 of /Users/tamasgal/Dev/PhD/snakemake/Snakefile:
NameError: The name 'fname' is unknown in this context. Did you mean 'wildcards.fname'?

Why is this happening? The output of the workflow clearly shows that wildcards: fname=bar, so it exists and is defined. Is this a bug?
",-1,-1,-1.0
71168492,Activating conda environment by name from within snakemake workflow,"Detailed info here, tl;dr can be found towards the end...
I've got a bioinformatics workflow I run using snakemake, with a python3 wrapper script using the snakemake API so that the snakemake command is simplified (https://github.com/charlesfoster/covid-illumina-snakemake). Most necessary programs are installed in a 'master' conda environment, while other programs with incompatible dependencies are installed in dedicated environments using conda directives within snakemake rules.
However, some programs cannot be easily included in this manner because they have a more complex installation. An example is pangolin (https://github.com/cov-lineages/pangolin), which requires the pangolin repo to be cloned, a conda environment created, then a &quot;pip install .&quot;. Then, to run pangolin within the workflow, I have the following rule:
rule pangolin:
    input:
        fasta=os.path.join(RESULT_DIR, &quot;{sample}/variants/{sample}.consensus.fa&quot;),
    output:
        report=os.path.join(RESULT_DIR, &quot;{sample}/pangolin/{sample}.lineage_report.csv&quot;),
    shell:
        &quot;&quot;&quot;
        set +eu
        eval &quot;$(conda shell.bash hook)&quot; &amp;&amp; conda activate pangolin &amp;&amp; pangolin --outfile {output.report} {input.fasta} &amp;&gt; /dev/null
        set -eu
        &quot;&quot;&quot;

I've also tried the new named conda environment directive as of snakemake version ~6.15.5:
rule pangolin:
    input:
        fasta=os.path.join(RESULT_DIR, &quot;{sample}/variants/{sample}.consensus.fa&quot;),
    output:
        report=os.path.join(RESULT_DIR, &quot;{sample}/pangolin/{sample}.lineage_report.csv&quot;),
    conda:
        &quot;pangolin&quot;
    shell:
        &quot;&quot;&quot;
pangolin --outfile {output.report} {input.fasta} &amp;&gt; /dev/null

        &quot;&quot;&quot;

Steps to run the workflow:

conda activate CIS
CIS [options] directory_name/

While this works on my main development PC, when I try to install the pipeline on a new computer, I end up getting the following error:
Could not find conda environment: pangolin
You can list all discoverable environments with `conda info --envs`.

If I run conda info --envs manually within the terminal, I get the following:
$USER/Programs/covid-illumina-snakemake/.snakemake/conda/520fff074cd181af7ee385f2520fdd81
$USER/Programs/covid-illumina-snakemake/.snakemake/conda/cb6755e5de757f643e542e3ec52055b7
base                     $USER/miniconda3
CIS                   *  $USER/miniconda3/envs/CIS
pangolin                 $USER/miniconda3/envs/pangolin


If I run conda info --envs within the snakemake workflow itself, I get the following:
$USER/Programs/covid-illumina-snakemake/.snakemake/conda/520fff074cd181af7ee385f2520fdd81
$USER/Programs/covid-illumina-snakemake/.snakemake/conda/cb6755e5de757f643e542e3ec52055b7
$USER/miniconda3
base                  *  $USER/miniconda3/envs/CIS
$USER/miniconda3/envs/pangolin


(username redacted here in both for brevity)
So, as you can see, the names of the environments are no longer detected within the snakemake workflow, and the 'CIS' environment is incorrectly thought to be 'base'. Therefore, the pangolin conda environment cannot be activated by name with eval &quot;$(conda shell.bash hook)&quot; &amp;&amp; conda activate pangolin.
tl;dr: conda info --envs has unexpected and different behaviour when invoked from within a snakemake workflow, which is 'driven' by a python script within a 'master' conda env.
Does anyone know why this might be, and/or how to fix it? Is there a better way to activate a named conda environment within a snakemake workflow?
Thanks!
snakemake version: 6.15.5
conda version: 4.11.0
",1,-1,-1.0
71111051,Processing two lists of files with snakemake,"I want to use snakemake to do bowtie2 mapping of split read files to a reference genome, and I'd like that rule to be integrated in the general workflow.
For that purpose, I first defined a rule to create a bowtie index
rule build_bowtie_index:
    input:
        referenceGenomeFasta
    output:
        expand(&quot;{name}.{index}.bt2&quot;, index=range(1,5), name = referenceGenomeName),
        expand(&quot;{name}.rev.{index}.bt2&quot;, index=range(1,3), name = referenceGenomeName)
    params:
        btindex = referenceGenomeName
    shell:
        &quot;bowtie2-build {input} {params.btindex}&quot;

referenceGenomeFasta is reference genome fasta file, while referenceGenomeName is the same file without .fna suffix. The goal was to take foo.fna reference genome file and produce a list of index files foo.1.bt2, foo.2.bt2., foo.rev.bt1 etc. because bowtie2 requires that index (-X flag) be the name of the folder up to final dot, so I figured I'd store the referenceGenomeName as a global configurable variable.
Now I have a list of files containing split reads in the form of sampleFoo_1.fastq,sampleFoo_2.fastq, sampleBar_1.fastq,sampleBar_2.fastq etc. When I used bash scripts for mapping, I used the command:
The rule for creating bowtie2 index  is the following:
bowtie2 -x {myindex} -1 {sample}_1.fastq -2 {sample}_2.fastq | samtools view -bS &gt; {sample}.aligned.bam

I also used parallel processing via Sun Grid Engine to speed up the computation as I have a lot of samples. Now I want to transfer the workflow into snakemake. My rule for bowtie2 mapping must include my samples and the outputs of build_bowtie_index rule. The total thing looks like this:
SAMPLES = [&quot;Foo&quot;, &quot;Bar&quot;]
rule align_via_bowtie2: 
    input:
        firstPair=expand(&quot;{rawSampleFolder}/{samples}_1.fastq&quot;,samples=SAMPLES, rawSampleFolder=rawSampleFolder),
        secondPair=expand(&quot;{rawSampleFolder}/{samples}_2.fastq&quot;,samples=SAMPLES, rawSampleFolder=rawSampleFolder),
        btindex1=expand(&quot;{name}.{index}.bt2&quot;, index=range(1,5), name = referenceGenomeName),
        btindex2=expand(&quot;{name}.rev.{index}.bt2&quot;, index=range(1,3), name = referenceGenomeName)
    params:
        btindex = referenceGenomeName
    output:
        expand(&quot;{proccSampleFolder}/{samples}.aligned.bam&quot;, samples=SAMPLES, proccSampleFolder=proccSampleFolder)
    shell:
        &quot;bowtie2 -x {params.btindex} -1 {input.firstPair} -2 {input.secondPair} | samtools view -bS -&gt; {output}&quot;

When I run this script with SAMPLES=[&quot;Foo&quot;] the script works perfectly, and what's run is:
 bowtie2 -x ref_genome -1 raw_samples/Foo_1.fastq -2 raw_samples/Foo_2.fastq | samtools view -bS -&gt; proccessed_samples/Foo.aligned.bam

But when I run the script with SAMPLES=[&quot;Foo&quot;,&quot;Bar&quot;], what snakemake executes is:
 bowtie2 -x ref_genome -1 raw_samples/Foo_1.fastq raw_samples/Bar_1.fastq -2 raw_samples/Foo_2.fastq raw_samples/Bar_2.fastq | samtools view -bS -&gt; proccessed_samples/Foo.aligned.bam proccessed_samples/Bar.aligned.bam

In other words, instead of mapping first Foo, and Bar second, it maps Foo Bar and I get an error. My problem is that I want to process a list of files as a separate job - or two separate lists of files containing split reads. What am I doing wrong?
",-1,1,-1.0
71080292,Snakemake Error with MissingOutputException,"I am trying to run STAR with snakemake in a server,
My smk file is that one :
import pandas as pd

configfile: 'config.yaml'

#Read sample to batch dataframe mapping batch to sample (here with zip)
sample_to_batch = pd.read_csv(&quot;/mnt/DataArray1/users/zisis/STAR_mapping/snakemake_STAR_index/all_samples_test.csv&quot;, sep = '\t')

#rule spcifying output
rule all_STAR:
    input:
        #expand(&quot;{sample}/Aligned.sortedByCoord.out.bam&quot;, sample = sample_to_batch['sample'])
        expand(config['path_to_output']+&quot;{sample}/Aligned.sortedByCoord.out.bam&quot;, sample = sample_to_batch['sample'])

rule STAR_align:
    #specify input fastq files
    input:
        fq1 = config['path_to_output']+&quot;{sample}_1.fastq.gz&quot;,
        fq2 = config['path_to_output']+&quot;{sample}_2.fastq.gz&quot;
    params:
        #location of indexed genome andl location to save the ouput
        genome = directory(config['path_to_reference']+config['ref_assembly']+&quot;.STAR_index&quot;),
        prefix_outdir = directory(config['path_to_output']+&quot;{sample}/&quot;)
    threads: 12
    output:
        config['path_to_output']+&quot;{sample}/Aligned.sortedByCoord.out.bam&quot;
    log:
        config['path_to_output']+&quot;logs/{sample}.log&quot;
    message:
        &quot;--- Mapping STAR---&quot;
    shell:
        &quot;&quot;&quot;
        STAR --runThreadN {threads} \
        --readFilesCommand zcat \
        --readFilesIn {input} \
        --genomeDir {params.genome} \
        --outSAMtype BAM SortedByCoordinate \
        --outSAMunmapped Within \
        --outSAMattributes Standard
        &quot;&quot;&quot;

While STAR starts normally at the end i have this error:
Waiting at most 5 seconds for missing files.
MissingOutputException in line 14 of /mnt/DataArray1/users/zisis/STAR_mapping/snakemake/STAR_snakefile_align.smk:
Job Missing files after 5 seconds:
/mnt/DataArray1/users/zisis/STAR_mapping/snakemake/001_T1/Aligned.sortedByCoord.out.bam
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Job id: 1 completed successfully, but some output files are missing. 1
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

I tried --latency-wait but is not working.
In order to execute snake make i run the command
users/zisis/STAR_mapping/snakemake_STAR_index$ snakemake --snakefile STAR_new_snakefile.smk --cores all --printshellcmds

Technically i am in my directory with full access and permissions
Do you think that this is happening due to strange rights in the execution of snakemake or when it tries to create directories ?
It creates the directory and the files but i can see that there is a files Aligned.sortedByCoord.out.bamAligned.sortedByCoord.out.bam .
IS this the problem ?
",-1,-1,-1.0
71666920,Set global snakemake variables from within a rule,"In snakemake, I can define global variables before declaring my rules. This is nifty when I have a list of files that I need to download via some tool - then I can ensure the name of those files are wildcards for my subsequent steps. For example, when I have a text file (srarunsacc.txt) containing lines of SRA111111,SRA111112,SRA111113 I can do the following (pseudocode):
def read_textfile(name):
....
SRAFILES=read_textfile(&quot;srarunsacc.txt&quot;)

rule download_srafiles:
    output:
        &quot;%s{srafiles}_1.fastq&quot; % &quot;raw_samples/&quot;,
        &quot;%s{srafiles}_2.fastq&quot; % &quot;raw_samples/&quot;
    shell:
        &quot;fasterq-dump {wildcards.srafiles} -O raw_samples&quot;

This ensures that I can then process my downloaded files within snakemake, by linking input and output within rules. But suppose I do not start with the text file srarunsacc.txt and that it is instead generated within the snakemake process. Now I can't define my SRAFILES global variable up top, and I'm stumped at what to do. Ideally, I would be able to make a SRAFILES global variable from within one of my rules, but I haven't managed to do that.

I've created a dummy process meant to mimic my workflow :
rule create_textfile:
    output:
        &quot;list_of_files.txt&quot;
    run:
        files=[f&quot;TESTFILE_{i}.test&quot; for i in range(1,10)]
        files=&quot;\n&quot;.join(files)
        with open(output[0],&quot;w&quot;) as out:
            out.write(files)

In rule create_textfile I create a .txt file that looks like this:
TESTFILE_1.test
TESTFILE_2.test
etc.

Normally, I won't know the exact name of those files, just their extension - which in this case is .test. The next step is :
rule create_files:
    input:
        &quot;list_of_files.txt&quot;
    output:
        &quot;placeholder.plc&quot;
    shell:
        &quot;&quot;&quot;
        mkdir raw_samples
        cat {input} | parallel touch raw_samples/{{}} &gt;&gt; {output}
        &quot;&quot;&quot;

In rule create_files I create an empty file from every line of my created .txt file, so the result is this:
raw_samples
    |____TESTFILE_1.test
    |____TESTFILE_2.test
     .etc

Now, in my &quot;experiment&quot;, I won't know how the files are named, only their resulting extension, say .test. I want to use those created files as an input to my next rule. I've been using the log output trick to link my rules together. Now I want to process every created file separately. For example:
rule process_files:
input:
    plc=&quot;placeholder1.plc&quot;,
    sample=&quot;raw_samples/{sample}.test&quot;
output:
    &quot;placeholder2.plc&quot;
shell:
    &quot;&quot;&quot;
    echo {input.sample} &gt;&gt; {output}
    &quot;&quot;&quot;

The above doesn't work, and gives the error:
Wildcards in input files cannot be determined from output files:'sample'

Which makes sense, as in nowhere above I define output as of raw_samples/{sample}.test
So I've tried modifying the rule create_files like this:
 output:
    plc=&quot;placeholder1.plc&quot;,
    sample=&quot;raw_samples/{sample}.test&quot;

That didn't work, and the error was
Not all output, log and benchmark files of rule create_files contain the same wildcards. This is crucial though, in order to avoid that two or more jobs write to the same file.


Tl;dr - A bunch of files (names unknown prior to process, only their extension) is generated via one snakemake rule. I want to process those files with subsequent snakemake rules, but do not know how to link up input-output / set up wildcards.
",-1,1,-1.0
71668662,How to use wildcards in input for snakemake rule,"I am having rel hard time understanding what I am doing in the following code. Could someone kindly help me with issue?
Partial Snakefile:
# load config file
configfile: &quot;config.yaml&quot;
import sys
import os

sample=config[&quot;Sample_Name&quot;]
trainingset=config[&quot;trainingset&quot;]
    
if config[&quot;batch_analysis&quot;]:
    sample = []
    for name in os.listdir(config[&quot;base_dir&quot;]):
        sample.append(name)
    sample.sort()
    input_dir = expand(config[&quot;base_dir&quot;] + &quot;/{sample}&quot;, sample=sample)
else:
    input_dir = config[&quot;FAST5_basedir&quot;]
    
    out_dir = config[&quot;output_dir&quot;]
    
    files_needed = [&quot;modified_bases.5mC.bed&quot;, &quot;basecalls.fastq&quot;, 
                    &quot;mappings.sorted.bam&quot;, &quot;mappings.sorted.bam.bai&quot;]
    
rule all:
    input:
        expand(&quot;{out_dir}/{sample}/reports/{sample}_{trainingset}_WGS_report.pdf&quot;,
               out_dir=config[&quot;output_dir&quot;], sample=sample, 
               trainingset=config[&quot;trainingset&quot;])
    
rule megalodon:
    input:
        fast5_dir = input_dir,
        ref = config[&quot;ref_index&quot;],
    output:
        touch(expand(&quot;{out_dir}/{sample}/megalodon/{files}&quot;,
                     files=files_needed, sample=sample, 
                     out_dir=config[&quot;output_dir&quot;]))
    threads: 16
    conda:
        workflow.basedir + &quot;/envs/megalodon.yaml&quot;
    params:
        guppybasecallserver = config[&quot;megalodon_guppy_basecall_server&quot;],
        guppy_params = config[&quot;megalodon_guppy_params&quot;],
        additional = config[&quot;megalodon_additional_commands&quot;],
        outputs = config[&quot;megalodon_outputs&quot;],
        liveprocess = &quot;--live-processing&quot; if config[&quot;live_acquisition&quot;] else &quot;&quot;,
        overwrite = &quot;--overwrite&quot; if config[&quot;megalodon_overwrite&quot;] else &quot;&quot;,
    priority: 100
    shell:
        &quot;&quot;&quot;
        megalodon '{input.fast5_dir}'
        --outputs {params.outputs}
        --reference {input.ref}
        --mod-motif m CG 0
        --devices 0
        --processes {threads}
        --sort-mappings
        --guppy-server-path {params.guppybasecallserver}
        --guppy-params &quot;{params.guppy_params}&quot;
        --output-directory &quot;{out_dir}/{sample}/megalodon/&quot;
        {params.liveprocess} {params.overwrite} {params.additional}
        &quot;&quot;&quot;

Output from terminal: snakemake --snakefile megalodon_pipeline -c -n

[Tue Mar 29 22:40:17 2022] 
rule megalodon: 
    input: 
        /home/monib/MagDx/Testing_Folder/data/Test1, 
        /home/monib/MagDx/Testing_Folder/data/Test2, 
        /home/monib/MagDx/Testing_Folder/data/Test3, 
        /home/monib/MagDx/static/hg19.mmi  
    output:  
        /home/monib/MagDx_result/Test1/megalodon/modified_bases.5mC.bed,  
        /home/monib/MagDx_result/Test2/megalodon/modified_bases.5mC.bed,  
        /home/monib/MagDx_result/Test3/megalodon/modified_bases.5mC.bed,  
        /home/monib/MagDx_result/Test1/megalodon/basecalls.fastq, 
        /home/monib/MagDx_result/Test2/megalodon/basecalls.fastq,  
        /home/monib/MagDx_result/Test3/megalodon/basecalls.fastq,  
        /home/monib/MagDx_result/Test1/megalodon/mappings.sorted.bam,  
        /home/monib/MagDx_result/Test2/megalodon/mappings.sorted.bam,  
        /home/monib/MagDx_result/Test3/megalodon/mappings.sorted.bam,  
        /home/monib/MagDx_result/Test1/megalodon/mappings.sorted.bam.bai,  
        /home/monib/MagDx_result/Test2/megalodon/mappings.sorted.bam.bai,  
        /home/monib/MagDx_result/Test3/megalodon/mappings.sorted.bam.bai  
    jobid: 3  
    priority: 100  
    threads: 12  
    resources: tmpdir=/tmp 


The above rule does not iterate, instead the input is being used as concatenated string, instead of base_dir and sample being a wildcards.
Rest of the rules come up fine; it is just this rule I cannot get it to behave like a wildcard base_dir and sample in the input.
",1,-1,-1.0
71689090,How to run a bash script inside a snakemake pipeline,"I would like to run a bash script inside the snakemake pipeline. But I do not know how to call the input and output of snakemake in a bash script.
snakemake:
rule xxx:
    input:
        &quot;input.vcf&quot;
    output:
        &quot;output.tab&quot;
    shell:
        &quot;&quot;&quot;
        some_bash.sh {input} {output}
        &quot;&quot;&quot;

bash script:
#!/bin/bash

paste &lt;(bcftools snakemake@input[0] |\
    awk -F&quot;\t&quot; 'BEGIN {print &quot;CHR\tPOS\tID\tREF\tALT\tFILTER&quot;} \
      !/^#/ {print $1&quot;\t&quot;$2&quot;\t&quot;$3&quot;\t&quot;$4&quot;\t&quot;$5&quot;\t&quot;$6}') \
    \
  &lt;(bcftools query -f '[\t%SAMPLE=%GT]\n' snakemake@input[0] |\
    awk 'BEGIN {print &quot;nHet&quot;} {print gsub(/0\|1|1\|0|0\/1|1\/0/, &quot;&quot;)}') \
    \
  &lt;(bcftools query -f '[\t%SAMPLE=%GT]\n' snakemake@input[0] |\
    awk 'BEGIN {print &quot;nHomAlt&quot;} {print gsub(/1\|1|1\/1/, &quot;&quot;)}') \
    \
  &lt;(bcftools query -f '[\t%SAMPLE=%GT]\n' snakemake@input[0] |\
    awk 'BEGIN {print &quot;nHomRef&quot;} {print gsub(/0\|0|0\/0/, &quot;&quot;)}') \
    \
  &lt;(bcftools snakemake@input[0] | awk -F&quot;\t&quot; '/^#CHROM/ {split($0, header, &quot;\t&quot;); print &quot;HetSamples&quot;} \
    !/^#CHROM/ {for (i=10; i&lt;=NF; i++) {if (gsub(/0\|1|1\|0|0\/1|1\/0/, &quot;&quot;, $(i))==1) {printf header[i]&quot;,&quot;}; if (i==NF) {printf &quot;\n&quot;}}}') \
    \
  &lt;(bcftools snakemake@input[0] | awk -F&quot;\t&quot; '/^#CHROM/ {split($0, header, &quot;\t&quot;); print &quot;HomSamplesAlt&quot;} \
    !/^#CHROM/ {for (i=10; i&lt;=NF; i++) {if (gsub(/1\|1|1\/1/, &quot;&quot;, $(i))==1) {printf header[i]&quot;,&quot;}; if (i==NF) {printf &quot;\n&quot;}}}') \
    \
  | sed 's/,\t/\t/g' | sed 's/,$//g' &gt; snakemake@output[0]

Error I get:
[E::main] unrecognized command 'snakemake@input[0]'
[E::main] unrecognized command 'snakemake@input[0]'
[E::main] unrecognized command 'snakemake@input[0]'
[E::hts_open_format] [E::hts_open_format] Failed to open file &quot;snakemake@input[0]&quot; : No such file or directoryFailed to open file &quot;snakemake@input[0]&quot; : No such file or directory

",-1,-1,-1.0
71736002,"Snakemake - How to produce multiple outputs from single input, multiple times with different seeds?","I am trying to create a snakemake rule that produces 3 output files for each given seed. I currently have the following:
SIM_OUTPUT = [&quot;summary&quot;, &quot;tripinfo&quot;, &quot;vehroute&quot;]
SEEDS = [1,2]
single_seed = 1
rule sumo_sim_1:
    input:
        config = &quot;two-hours-ad-hoc.sumo.cfg&quot;
    output:
        expand(&quot;xml/{file}.{seed}.xml&quot;, seed = single_seed, file=SIM_OUTPUT)
    shell:
        &quot; sumo -c {input.config} --seed {single_seed}&quot; 
        &quot;--summary-output {output[0]} &quot;
        &quot;--tripinfo-output {output[1]} &quot; 
        &quot;--vehroute-output {output[2]} &quot;

The above code works for a single seed, but I cant get/think of a way to work for multiple seeds.
",-1,1,-1.0
71738307,Snakemake not recognizing multiple files as input,"I'm having some trouble running snakemake. I want to perform quality control of some RNA-Seq bulk samples using FastQC. I've written the code in a way that all files following the pattern {sample}_{replicate}.fastq.gz should be used as input, where {sample} is the sample id (i.e. SRR6974023) and {replicate} is 1 or 2. My little scripts follows:
configfile: &quot;config.yaml&quot;

rule all:
  input:
    expand(&quot;raw_qc/{sample}_{replicate}_fastqc.{extension}&quot;, sample=config[&quot;samples&quot;], replicate=[1, 2], extension=[&quot;zip&quot;, &quot;html&quot;])
    

rule fastqc:
  input:
    rawread=expand(&quot;raw_data/{sample}_{replicate}.fastq.gz&quot;, sample=config[&quot;samples&quot;], replicate=[1, 2])
  
  output:
    compress=expand(&quot;raw_qc/{sample}_{replicate}_fastqc.zip&quot;, sample=config[&quot;samples&quot;], replicate=[1, 2]),
    net=expand(&quot;raw_qc/{sample}_{replicate}_fastqc.html&quot;, sample=config[&quot;samples&quot;], replicate=[1, 2])
  
  threads: 
    8
  
  params:
    path=&quot;raw_qc/&quot;
  
  shell:
    &quot;fastqc -t {threads} {input.rawread} -o {params.path}&quot; 

Just is case, the config.yaml is:
samples:
  SRR6974023
  SRR6974024

The raw_data directory with my files look like this:
SRR6974023_1.fastq.gz  SRR6974023_2.fastq.gz  SRR6974024_1.fastq.gz  SRR6974024_2.fastq.gz

Finally, when I run the script, I always see the same error:
Building DAG of jobs...
MissingInputException in line 8 of /home/user/path/Snakefile:
Missing input files for rule fastqc:
raw_data/SRR6974023 SRR6974024_2.fastq.gz
raw_data/SRR6974023 SRR6974024_1.fastq.gz


It see correctly only the last files, in this case SRR6974024_1.fastq.gz and SRR6974024_2.fastq.gz. Whatsoever, the other one it's only seen as SRR6974023. How can I solve this issue? I appreciate some help. Thank you all!
",-1,-1,-1.0
71825092,Snakemake with integrated conda env not properly installed,"I have a rule in my snakemake pipeline to run multiqc :
    rule summaryReport:
        input:
            fastqc_forward = expand(final_path + &quot;/fastqc/{sample}_R1_fastqc.html&quot;, sample = samples),
            fastqc_rev = expand(final_path + &quot;/fastqc/{sample}_R2_fastqc.html&quot;, sample = samples)
        output:
            report = final_path + &quot;/fastqc/report_quality_control.html&quot;
        params:
            path = final_path + &quot;/fastqc&quot;
        conda:
            &quot;multiqc.yaml&quot;
        shell:
            &quot;multiqc {params.path} --filename {output.report}&quot;

with the conda env file multiqc.yaml :
name: multiqc 
channels:
  - conda-forge
  - bioconda
dependencies:
  - multiqc=1.12

When I run the pipeline I've got the following error :
Activating conda environment: /XXXX/.snakemake/conda/e8f3e6def45259d12dddd42fcd679657
Traceback (most recent call last):
  File &quot;/XXXX/.snakemake/conda/e8f3e6def45259d12dddd42fcd679657/bin/multiqc&quot;, line 6, in &lt;module&gt;
    from multiqc.__main__ import multiqc
ModuleNotFoundError: No module named 'multiqc'

I've tested to activate the conda environment manually and indeed there is the same error when running multiqc.
I've tested to create a conda env thanks to &quot;multiqc.yaml&quot; outside snakemake, and in this case multiqc is running correctly.
If someone has an idea about this it would help me a lot!
Thank you in advance
",-1,-1,-1.0
72046255,Error trying to use multiple file as input in Snakemake,"I'm trying to use some files as input for a bioinformatics tool. My basic code follows:
configfile: &quot;config.yaml&quot;


WORK_DATA = config[&quot;WORK_DATA&quot;]

rule all:
  input:
    expand(WORK_DATA + &quot;{sample}_{read}_quality-pass.fastq&quot;, sample=config[&quot;samples&quot;], read=[1, 2])
     

rule filter:
  input:
    R1 = expand(WORK_DATA + &quot;{sample}_1.fastq&quot;, sample=config[&quot;samples&quot;]),
    R2 = expand(WORK_DATA + &quot;{sample}_2.fastq&quot;, sample=config[&quot;samples&quot;])
  
  output:
    expand(WORK_DATA + &quot;{sample}_{read}_quality-pass.fastq&quot;, sample=config[&quot;samples&quot;], read=[1, 2])
 
  params:
    outname1 = expand(&quot;{sample}_1&quot;, sample=config[&quot;samples&quot;]),
    outname2 = expand(&quot;{sample}_2&quot;, sample=config[&quot;samples&quot;])

  shell:
    &quot;FilterSeq.py quality -s {input.R1} -q 20 --outname {params.outname1} --log FS1.log\n&quot;
    &quot;FilterSeq.py quality -s {input.R2} -q 20 --outname {params.outname2} --log FS2.log&quot;

However, I get an error looking something like this:
Error in rule filter:
    jobid: 1
    output: /home/path/SRR1383456_1_quality-pass.fastq, /home/path/SRR1383456_2_quality-pass.fastq, /home/path/SRR1383457_1_quality-pass.fastq, /home/path/SRR1383457_2_quality-pass.fastq
    shell:
FilterSeq.py quality -s /home/path/SRR1383456_1.fastq /home/path/SRR1383457_1.fastq -q 20 --outname SRR1383456_1 SRR1383457_1 --log FS1.log
FilterSeq.py quality -s /home/path/SRR1383456_2.fastq /home/path/SRR1383457_2.fastq -q 20 --outname SRR1383456_2 SRR1383457_2 --log FS2.log
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2022-04-28T113611.757898.snakemake.log


I don't know exactly the cause of this error, but it seems that those files are being placed as input (input.R1/R2) at the same time. Consequently, the FilterSeq.py program get's confused since it accepts only one input per run. I wonder how could I solve this issue, so that I could run all my intended files smoothly. Any help on this will be appreciated!
",-1,-1,-1.0
72251940,Snakemake runs rule too many times using config.yaml,"I'm trying to create this snakemake workflow which would evaluate raw reads quality using FastQc and create a raport using MultiQC. I use 4 input files and get expected results, however I just noticed that each rule gets run 4 times and takes all 4 inputs each time and I'm not sure how to fix that. Could anyone help me figure out how to:

Run the rule 4 times but use only one input from config.yaml at a time?
Run the rule 1 time but use all 4 inputs?

I'm trying to follow the snakemake tutorial but no luck so far.
Snakefile:
configfile: &quot;config.yaml&quot;

rule all:
    input:
       expand(&quot;outputs/multiqc_report_1/{sample}_multiqc_report_1.html&quot;, sample=config[&quot;samples&quot;])
        
rule raw_fastqc:
    input:
        expand(&quot;data/samples/{sample}.fastq&quot;, sample=config[&quot;samples&quot;])
    output:
        &quot;outputs/fastqc_1/{sample}_fastqc.html&quot;,
        &quot;outputs/fastqc_1/{sample}_fastqc.zip&quot;
    shell:
        &quot;fastqc {input} -o outputs/fastqc_1/&quot;

rule raw_multiqc:
    input:
        expand(&quot;outputs/fastqc_1/{sample}_fastqc.html&quot;, sample=config[&quot;samples&quot;]),
        expand(&quot;outputs/fastqc_1/{sample}_fastqc.zip&quot;, sample=config[&quot;samples&quot;])
    output:
        &quot;outputs/multiqc_report_1/{sample}_multiqc_report_1.html&quot;
    shell:
        &quot;multiqc ./outputs/fastqc_1/ -n {output}&quot;

config.yaml file:
samples:
    Collibri_standard_protocol-HBR-Collibri-100_ng-2_S1_L001_R1_001: data/samples/Collibri_standard_protocol-HBR-Collibri-100_ng-2_S1_L001_R1_001.fastq
    Collibri_standard_protocol-HBR-Collibri-100_ng-2_S1_L001_R2_001: data/samples/Collibri_standard_protocol-HBR-Collibri-100_ng-2_S1_L001_R2_001.fastq
    KAPA_mRNA_HyperPrep_-UHRR-KAPA-100_ng_total_RNA-3_S8_L001_R1_001: data/samples/KAPA_mRNA_HyperPrep_-UHRR-KAPA-100_ng_total_RNA-3_S8_L001_R1_001.fastq
    KAPA_mRNA_HyperPrep_-UHRR-KAPA-100_ng_total_RNA-3_S8_L001_R2_001: data/samples/KAPA_mRNA_HyperPrep_-UHRR-KAPA-100_ng_total_RNA-3_S8_L001_R2_001.fastq

I run the snakemake using command:
snakemake -s Snakefile --core 1

Each rule is run 4 times:
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job            count    min threads    max threads
-----------  -------  -------------  -------------
all                1              1              1
raw_fastqc         4              1              1
raw_multiqc        4              1              1
total              9              1              1

But each time all 4 inputs are used:
[Sun May 15 23:06:22 2022]
rule raw_fastqc:
    input: data/samples/Collibri_standard_protocol-HBR-Collibri-100_ng-2_S1_L001_R1_001.fastq, data/samples/Collibri_standard_protocol-HBR-Collibri-100_ng-2_S1_L001_R2_001.fastq, data/samples/KAPA_mRNA_HyperPrep_-UHRR-KAPA-100_ng_total_RNA-3_S8_L001_R1_001.fastq, data/samples/KAPA_mRNA_HyperPrep_-UHRR-KAPA-100_ng_total_RNA-3_S8_L001_R2_001.fastq
    output: outputs/fastqc_1/Collibri_standard_protocol-HBR-Collibri-100_ng-2_S1_L001_R2_001_fastqc.html, outputs/fastqc_1/Collibri_standard_protocol-HBR-Collibri-100_ng-2_S1_L001_R2_001_fastqc.zip
    jobid: 3
    wildcards: sample=Collibri_standard_protocol-HBR-Collibri-100_ng-2_S1_L001_R2_001
    resources: tmpdir=/tmp

",-1,-1,-1.0
72531922,Correctly consuming a multiline config file in snakemake as an input,"For various reasons I would like to be able to define my inputs in a separate config file. My current version without using a config file looks like:
rule test:
   input:
     labs = &quot;data/labs.csv&quot;
     demo = &quot;data/demo.csv&quot;
   output:
     &quot;outputs/output.txt&quot;
   script:
     &quot;programs/myprogram.py&quot;

Instead of this I would like my config file to be something like:
{
 &quot;inputs&quot;: {
        &quot;labs&quot; : &quot;data/labs.csv&quot;,
         &quot;demo&quot;: &quot;data/demo.csv&quot;
  }
}

And then my snakemake file would be:
rule test:
   input:
     config[&quot;inputs&quot;]
   output:
     &quot;outputs/output.txt&quot;
   script:
     &quot;programs/myprogram.py&quot;

However, I get an error telling me that I have missing input files for the rule, with note of affected files labs and demo.
I imagine I could parse this into a list that perhaps inputs could understand, but I would like my inputs to ideally retain their names. Unfortunately it is not at all clear to me how to achieve this.
",-1,-1,-1.0
72537393,Snakemake expand+zip function unexpected behavior,"I am trying to use Snakemake to process calls to the rnaQUAST tool with multiple inputs delineated by two sets of different, but paired keywords. I do not want all combinations of these keywords, only specific combinations. It is my understanding that I need to specify the use of zip within the expand() call in my rule all as below. However, in interpreting the variables snakemake appears to populate the {sample} and {reference} wildcards in an unexpected way:
samples_rnaQUAST = [&quot;TMW3250_15&quot;,&quot;TMW3250_20&quot;,&quot;TMW3256_15&quot;,&quot;TMW3256_20&quot;,&quot;TMW3261_15&quot;,&quot;TMW3261_20&quot;,
                    &quot;TMW3673_15&quot;,&quot;TMW3673_20&quot;,&quot;TMW3285_15&quot;,&quot;TMW3285_20&quot;,&quot;TMW3275_15&quot;,&quot;TMW3275_20&quot;,
                    &quot;TMW3681_15&quot;,&quot;TMW3681_20&quot;,&quot;TMW3287_15&quot;,&quot;TMW3287_20&quot;]
references_rnaQUAST = [&quot;German_ale&quot;,&quot;German_ale&quot;,&quot;German_ale&quot;,&quot;German_ale&quot;,
                       &quot;English_ale&quot;,&quot;English_ale&quot;,&quot;American_ale&quot;,&quot;American_ale&quot;,
                       &quot;Frohberg&quot;,&quot;Frohberg&quot;,&quot;Frohberg&quot;,&quot;Frohberg&quot;,&quot;Saaz&quot;,
                       &quot;Saaz&quot;,&quot;Saaz&quot;,&quot;Saaz&quot;]

rule all:
    input:
        expand(&quot;rnaquast/{sample}{reference}/short_report.txt&quot;, zip, sample=samples_rnaQUAST, reference=references_rnaQUAST)

rule rnaQUAST:
    input:
        transcriptome=&quot;trinity/{sample}/default_by_condition_trinity/Trinity.fasta&quot;,
        reference=&quot;genomes/{reference}_genome.fasta&quot;,
        gtf=&quot;genomes/AUGUSTUS_annotations/{reference}.gtf&quot;
    output:
        report=&quot;rnaquast/{sample}{reference}/short_report.txt&quot;
    threads: 16
    shell:&quot;&quot;&quot;
    /home/user/miniconda3/envs/rnaquast/share/rnaquast-1.5.1-0/rnaQUAST.py \
    --transcripts {input.transcriptome} \
    --reference {input.reference} \
    --gtf {input.gtf} \
    -t {threads} \
    -o rnaquast/{output.report}
    &quot;&quot;&quot;

With snakemake 5.10.0, I am receiving the following output populating {sample} and {reference} wildcards:
Building DAG of jobs...
MissingInputException in line 65 of /home/user/analyses/Snakefile:
Missing input files for rule rnaQUAST:
genomes/e_genome.fasta
trinity/TMW3250_15German_al/default_by_condition_trinity/Trinity.fasta
genomes/AUGUSTUS_annotations/e.gtf

Why is snakemake splitting the inputs in this unexpected way, and misallocating portions of the strings input to wildcards in rule all?
",-1,-1,-1.0
72587108,Apply snakemake rule on all generated files,"I want to run a simple script &quot;script.py&quot;, which will run some caculayions and periodically spit out a step_000n.txt file with n being dependent on the total file execution time. I would then like snakemake to run another rule on all generated files. What would be the proper Snakefile input?
ie
1. run scipt.py
2. get step_000{1,2,3,4 ..}.txt (n being variable and not determined)
3. apply `process.py -in step_000{n}.txt -out step_000{n}.png` on all step_000{1,2,3,4 ..}.txt

My obviously wrong attempt is below

rule all:
    input: expand(&quot;{step}.png&quot;, step=list(map(lambda x: x.split(&quot;.&quot;)[0], glob.glob(&quot;model0*.txt&quot;))))

rule txt:
    input: &quot;{step}.txt&quot;
    output: &quot;{step}.png&quot;
    shell:
        &quot;process.py -in {input} -out {output}&quot;

rule first:
    output: &quot;{step}.txt&quot;
    script: &quot;script.py&quot;


I could not figure out how to define output target here.
",-1,1,-1.0
72329213,Snakemake fails to produce output,"I'm trying to run fastqc on two paired files (1.fq.gz and 2.fq.gz). Running:
snakemake --use-conda -np newenv/1_fastqc.html
...produces what looks like a sensible DAG:
Building DAG of jobs...
Job stats:
job           count    min threads    max threads
----------  -------  -------------  -------------
curlewtest        1              1              1
total             1              1              1


[Sat May 21 11:27:40 2022]
rule curlewtest:
    input: 1.fq.gz, 2.fq.gz
    output: newenv/1_fastqc.html, newenv/2_fastqc.html
    jobid: 0
    resources: tmpdir=/tmp

fastqc 1.fq.gz 2.fq.gz
Job stats:
job           count    min threads    max threads
----------  -------  -------------  -------------
curlewtest        1              1              1
total             1              1              1

This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.

When I run the job snakemake --use-conda --cores all newenv/1_fastqc.html
, the analysis runs, but the output files fail to appear. Snakemake also throws the following error:
Waiting at most 5 seconds for missing files.
MissingOutputException in line 2 of /mnt/data/kcollier/snakemake-workspace/snakefile:
Job Missing files after 5 seconds. This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait:
newenv/1_fastqc.html
newenv/2_fastqc.html completed successfully, but some output files are missing. 0
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

Increasing latency does not help. The output directory I created beforehand (newenv) also disappears. Does anyone else know why this is?
",-1,-1,-1.0
72311985,Using modules in Snakemake,"I have a Snakefile which I want to use rules from another Snakefile as a module. The problem is, the first rule doesn't get it's input modified and Snakemake throws a MissingInputException
Here's my master Snakefile:
from snakemake.utils import min_version
min_version(&quot;6.0&quot;)

container: &quot;docker://condaforge/mambaforge:4.12.0-0&quot;
pepfile: &quot;pep/config.yaml&quot;
include: &quot;modules/import/Snakefile&quot;

module velocity:
    snakefile: &quot;modules/velocity/Snakefile&quot;


use rule * from velocity as velocity_*

use rule preprocessing from velocity as velocity_preprocessing with:
    input:
        filtered = &quot;results/preprocessing/{pat}.h5ad&quot;

And everything runs correctly when the module Snakefile looks like this:
rule preprocessing:
    input:
        filtered = &quot;results/preprocessing/{pat}.h5ad&quot;
    output:
        processed = &quot;results/velocity/processed/{pat}.h5ad&quot;
    conda: &quot;envs/velocity.yml&quot;
    script: &quot;python/velocity_preprocessing.py&quot;

but not when the input is set to filtered = &quot;in&quot; where I get the following Exception:
Building DAG of jobs...
MissingInputException in line 1 of [My Directory]/workflow/modules/velocity/Snakefile:
Missing input files for rule velocity_preprocessing:
    output: results/velocity/processed/PAWAMB.h5ad
    wildcards: pat=PAWAMB
    affected files:
        in

even when the use rule ... from ... as ... with: chunk is present. Am I doing something wrong? I would expect both to build the same DAG.
",-1,-1,-1.0
72634410,Snakemake fasterq-dump wrapper AttributeError: 'Wildcards' object has no attribute 'accession',"I am trying to use fasterq-dump wrapper in my snakemake workflow to download paired-end fastq.gz files. Here is my snakefile:
# read a .txt file including many SRR* accession number
import pandas as pd
df = pd.read_csv('SraRunTable.txt', sep=',', header=0)

# append all accession number to a list
SAMPLES = []

for i in df['Run']:
    SAMPLES.append(i)

# snakemake workflow starts here
rule all:
    input:
        expand(&quot;/data/fastq/{sample}_1.fastq.gz&quot;, sample=SAMPLES)

rule get_fastq_pe_gz:
    output:
        # the wildcard name must be accession
        &quot;/data/fastq/{sample}_1.fastq.gz&quot;,
        &quot;/data/fastq/{sample}_2.fastq.gz&quot;,
    log:
        &quot;/data/logs/{sample}.log&quot;
    params:
        extra=&quot;--skip-technical&quot;
    threads: 20
    wrapper:
        &quot;v1.7.0/bio/sra-tools/fasterq-dump&quot;

After executing it using conda, snakemake -s fasterq-dump.snake --cores 20 --use-conda, I received an AttributeError which I cannot figure it out. Any suggestions or solutions are appreciated!
Here is the complete log including the error message:
Building DAG of jobs...
Creating conda environment https://github.com/snakemake/snakemake-wrappers/raw/v1.7.0/bio/sra-tools/fasterq-dump/environment.yaml...
Downloading and installing remote packages.
Environment for https://github.com/snakemake/snakemake-wrappers/raw/v1.7.0/bio/sra-tools/fasterq-dump/environment.yaml created (location: .snakemake/conda/fab035359fa42a09dfad78160e9b8543)
Using shell: /usr/bin/bash
Provided cores: 20
Rules claiming more threads will be scaled down.
Job stats:
job                count    min threads    max threads
---------------  -------  -------------  -------------
all                    1              1              1
get_fastq_pe_gz      422             20             20
total                423              1             20

Select jobs to execute...

[Wed Jun 15 17:10:30 2022]
rule get_fastq_pe_gz:
    output: /data/scratch/yaochung/Khrameeva/fastq/SRR8750458_1.fastq.gz, /data/scratch/yaochung/Khrameeva/fastq/SRR8750458_2.fastq.gz
    log: /data/scratch/yaochung/Khrameeva/logs/SRR8750458.log
    jobid: 62
    reason: Missing output files: /data/scratch/yaochung/Khrameeva/fastq/SRR8750458_1.fastq.gz
    wildcards: sample=SRR8750458
    threads: 20
    resources: tmpdir=/tmp

Activating conda environment: .snakemake/conda/fab035359fa42a09dfad78160e9b8543
Activating conda environment: .snakemake/conda/fab035359fa42a09dfad78160e9b8543
Traceback (most recent call last):
  File &quot;/data/scratch/yaochung/TEKRABber_thesis/pipelines/fasterq-dump/.snakemake/scripts/tmp4ip6wnot.wrapper.py&quot;, line 45, in &lt;module&gt;
    shell(
  File &quot;/home/yaochung41/anaconda3/envs/snakemake/lib/python3.10/site-packages/snakemake/shell.py&quot;, line 139, in __new__
    cmd = format(cmd, *args, stepout=2, **kwargs)
  File &quot;/home/yaochung41/anaconda3/envs/snakemake/lib/python3.10/site-packages/snakemake/utils.py&quot;, line 430, in format
    return fmt.format(_pattern, *args, **variables)
  File &quot;/data/scratch/yaochung/TEKRABber_thesis/pipelines/fasterq-dump/.snakemake/conda/fab035359fa42a09dfad78160e9b8543/lib/python3.10/string.py&quot;, line 161, in format
    return self.vformat(format_string, args, kwargs)
  File &quot;/data/scratch/yaochung/TEKRABber_thesis/pipelines/fasterq-dump/.snakemake/conda/fab035359fa42a09dfad78160e9b8543/lib/python3.10/string.py&quot;, line 165, in vformat
    result, _ = self._vformat(format_string, args, kwargs, used_args, 2)
  File &quot;/data/scratch/yaochung/TEKRABber_thesis/pipelines/fasterq-dump/.snakemake/conda/fab035359fa42a09dfad78160e9b8543/lib/python3.10/string.py&quot;, line 205, in _vformat
    obj, arg_used = self.get_field(field_name, args, kwargs)
  File &quot;/data/scratch/yaochung/TEKRABber_thesis/pipelines/fasterq-dump/.snakemake/conda/fab035359fa42a09dfad78160e9b8543/lib/python3.10/string.py&quot;, line 276, in get_field
    obj = getattr(obj, i)
AttributeError: 'Wildcards' object has no attribute 'accession'
[Wed Jun 15 17:10:34 2022]
Error in rule get_fastq_pe_gz:
    jobid: 62
    output: /data/scratch/yaochung/Khrameeva/fastq/SRR8750458_1.fastq.gz, /data/scratch/yaochung/Khrameeva/fastq/SRR8750458_2.fastq.gz
    log: /data/scratch/yaochung/Khrameeva/logs/SRR8750458.log (check log file(s) for error message)
    conda-env: /data/scratch/yaochung/TEKRABber_thesis/pipelines/fasterq-dump/.snakemake/conda/fab035359fa42a09dfad78160e9b8543

RuleException:
CalledProcessError in line 25 of /data/scratch/yaochung/TEKRABber_thesis/pipelines/fasterq-dump/fasterq-dump.snake:
Command 'source /home/yaochung41/anaconda3/bin/activate '/data/scratch/yaochung/TEKRABber_thesis/pipelines/fasterq-dump/.snakemake/conda/fab035359fa42a09dfad78160e9b8543'; set -euo pipefail;  python /data/scratch/yaochung/TEKRABber_thesis/pipelines/fasterq-dump/.snakemake/scripts/tmp4ip6wnot.wrapper.py' returned non-zero exit status 1.
  File &quot;/data/scratch/yaochung/TEKRABber_thesis/pipelines/fasterq-dump/fasterq-dump.snake&quot;, line 25, in __rule_get_fastq_pe_gz
  File &quot;/home/yaochung41/anaconda3/envs/snakemake/lib/python3.10/concurrent/futures/thread.py&quot;, line 58, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2022-06-15T170843.109776.snakemake.log

",-1,-1,-1.0
72698720,Using Docker with Snakemake (as motivated by the Snakemake documentation) does not provide a true docker container?,"In order to improve the distribution of my snakemake workflow, I am wanting to Docker-ize it.  The way I usually deploy my software stack is through conda environments (saved as .yaml files within an envs/ directory of my project folder).  They are then called by each rule through the conda: directive and --use-conda flag (at execution), for example:
rule exampleRule:
    input: input.file
    output: output.file
    conda:
        os.path.join(workflow.basedir, &quot;envs/&lt;nameOfEnv&gt;.yaml&quot;)
    shell:  &quot;some shell command&quot;

Following the snakemake documentation, I am aware that I can Dockerize my full workflow using the command: snakemake --containerize.
The resulting Dockerfile is something like this (this is a contrived example, in reality I have seven environments that are all built inside this container):
FROM condaforge/mambaforge:latest
LABEL io.github.snakemake.containerized=&quot;true&quot;
LABEL io.github.snakemake.conda_env_hash=&quot;dc12f3c8b1fb3caed02ab3305e24859bd63f4f1ea0c1ed29d71c857e7d0baaf5&quot;

# Step 1: Retrieve conda environments

# Conda environment:
#   source: envs/nameOfEnvironment.yaml
#   prefix: /conda-envs/4e57ed29df8b6f849000ab15b5c719f2
#   channels:
#    - conda-forge
#    - bioconda
#    - anaconda
#    - defaults
#   dependencies:
#     - wget
#     - packageX == 5.3.0
#     - packageY == 2.5.5
#     - packageZ &gt;= 1.1.1

RUN mkdir -p /conda-envs/4e57ed29df8b6f849000ab15b5c719f2
COPY envs/nameOfEnvironment.yaml /conda-envs/4e57ed29df8b6f849000ab15b5c719f2/environment.yaml

# Step 2: Generate conda environments
RUN mamba env create --prefix /conda-envs/4e57ed29df8b6f849000ab15b5c719f2 --file /conda-envs/4e57ed29df8b6f849000ab15b5c719f2/environment.yaml &amp;&amp; \
    mamba clean --all -y


I have already succeeded in following this method, and built an image that I host on Dockerhub that is then referenced in the Snakefile, e.g. container: &quot;docker://myDockerHub/myWorkflow_dockerimage:latest&quot;
Then, when executing with the --use-singularity flag, snakemake will pull this image and build the environments.
The problem is that the rest of the execution seems to follow a similar procedure as if I had not used singularity/docker whatsoever.
Meaning that in order to run the workflow in this way, I still require some base packages being installed, most especially Snakemake itself.  Doesn't this sort of defeat the purpose of containerizing the workflow in the first place, or do I misunderstand something fundamental?
The only real solution I can think of is to create an image/container with Snakemake installed.  Then I would run this image using singularity - but from then on I would be running the workflow from within the snakemake image as if I had not containerized the environments themselves.  When executing snakemake from within this image, I cannot use the --use-singularity flag, as singularity itself is not available from within the image.
It seems completely counterintuitive to run snakemake without the --use-singularity flag, when my intention is to utilize singularity.
",-1,-1,-1.0
72701330,Missing input files error for expanded input in Snakemake,"I have been having a strange error that I cannot fix. I am using snakemake 7.8.2 in a conda environment on an Ubuntu 20.04 machine (have also tried running this on our cluster and the same error occurred). For some odd reason one of my rules that has wildcards cannot be used as an expanded input for the following rule and continuously gives me an error that the input files are missing when they are very clearly created in the previous rule.
Here are my 2 rules:
### Use R to generate figures of each run's sequencing summary

rule rejected_seq_figures:
  input:
    seq_summary = &quot;resources/{RUNS}/guppy_outputs/sequencing_summary.txt&quot;,
    rejected_ids = &quot;resources/{RUNS}/raw_reads/unblocked_read_ids.txt&quot;
  output:
    rejected_pie = report(&quot;results/rejected/seq_summary/{RUNS}_pie_chart.pdf&quot;, caption=&quot;report/rejected_pie.rst&quot;, category=&quot;Rejected Reads Sequence Summary&quot;, subcategory=&quot;{RUNS}&quot;),
    histo_read_len = report(&quot;results/rejected/seq_summary/{RUNS}_histogram.pdf&quot;, caption=&quot;report/histo_read_len.rst&quot;, category=&quot;Rejected Reads Sequence Summary&quot;, subcategory=&quot;{RUNS}&quot;),
    barcode_boxplot = report(&quot;results/rejected/seq_summary/{RUNS}_boxplot.pdf&quot;, caption=&quot;report/barcode_boxplot.rst&quot;, category=&quot;Rejected Reads Sequence Summary&quot;, subcategory=&quot;{RUNS}&quot;)
  script:
    &quot;scripts/rejected_seq_summary_figures.R&quot;

### Use R to create a summary table of all runs sequencing summaries

rule rejected_seq_table:
  input:
    sum_file_list = &quot;results/rejected/sum_file_list.tsv&quot;,
    ids_file_list = &quot;results/rejected/ids_file_list.tsv&quot;,
    rejected_pie = expand(&quot;results/rejected/seq_summary/{run}_pie_chart.pdf&quot;, run=RUNS)
  output:
    report(&quot;results/rejected/seq_summary/rejected_seq_summary_table.tsv&quot;, caption=&quot;report/rejected_seq_summary_table.rst&quot;, category=&quot;Rejected Reads Sequence Summary&quot;, subcategory=&quot;All Runs&quot;)
  script:
    &quot;scripts/rejected_seq_summary_table.R&quot;

The error snakemake produces as it tries to build the DAG:
MissingInputException in line 39 of /home/639893/Adaptive_Sequencing_Analysis_Workflow/workflow/rules/rejected_seq_summary.smk:
Missing input files for rule rejected_seq_table:
    output: results/rejected/seq_summary/rejected_seq_summary_table.tsv
    affected files:
        results/rejected/seq_summary/ONT_skin1_adap2_pie_chart.pdf
        results/rejected/seq_summary/ONT_skin1_adap_pie_chart.pdf

Those are exactly the expected outputs for the rule so the wildcard is working, there are not any typos or path mistakes either. The R script has been tested and the snakemake inputs and outputs work well, but when I add the rule to my workflow it continuously produces this error.
I have already tried: taking all three outputs for the figure rule as expanded inputs, changing the orders of the inputs for the following rule, putting the expanded input into the rule all rather than the following rule (I am just trying to get this rule to run), creating a temporary and touched text file rather than a pdf, making the path shorter and longer, changing the snakemake version to 7 different versions, just having one output for the first rule, and removing the report code from the outputs. All resulted in the same error and the rest of my workflow works well.
Additionally, I have another rule where I perform the same action (expanded output from a previous rule with wildcards as the input for the next) and that works perfectly fine. The only difference between the rules is that the one causing an error (above) uses an R script and the working rule just uses shell commands.
I would really appreciate any help because I need to be able to create figures like this and add them to the snakemake report. Thank you in advance.
",-1,-1,-1.0
72776216,"Snakemake: Mismatched Wildcards Variable Values for ""output"" Rule","I am encountering a problem that doesn't seem to occur consistently between folders.
Essentially, I thought I had a Snakemake pipeline that would work to copy files into folders (with different destinations for different subfolders).  I am currently accomplishing this with some Python dictionaries as well as 2 wildcard values.
However, I am currently encountering a problem that I believe is due to a mismatch between the {outf} and {sample} wildcards values.
Brief Description
I believe that the wildcards are defined with rule all:
rule all:
    input:
        expand(os.path.join(&quot;{outf}&quot;,&quot;{sample}&quot;,&quot;methods.txt&quot;), outf=OUTPREFIXES, sample=SAMPLES)

In the example that I will describe below:

Pairing of {outf} and {sample} is correct for input
Pairing of {outf} and {sample} is not correct in the log output for output
Pairing of {outf} and {sample} is not correct in the log output for wildcards

Additional Details
I am removing some details related to the exact formatting, but the code is basically as follows:
import pandas as pd
import os
import re

data = pd.read_csv(&quot;mapping_list.csv&quot;).set_index('Subfolder', drop=False)
SAMPLES = data[&quot;Subfolder&quot;].tolist()
OUTPREFIXES = data[&quot;Output&quot;].tolist()

def get_input_folder(wildcards):
    return data.loc[wildcards.sample][&quot;Input&quot;]

def get_output_folder(wildcards):
    return data.loc[wildcards.sample][&quot;Output&quot;]
    
rule all:
    input:
        expand(os.path.join(&quot;{outf}&quot;,&quot;{sample}&quot;,&quot;methods.txt&quot;), outf=OUTPREFIXES, sample=SAMPLES)

rule copy_folders:
    input:
        infolder = directory(get_input_folder),
        outfolder = directory(get_output_folder),
    output:
        os.path.join(&quot;{outf}&quot;,&quot;{sample}&quot;,&quot;methods.txt&quot;),
    resources:
        mem_mb=2000,
        cpus=1
    shell:
        '''
        SHOUT1={input.outfolder}
        ...
        cp -R {input.infolder} $SHOUT1
        
        TEMPSAMPLE=$(basename {input.infolder})
        SHEND={input.outfolder}/$TEMPSAMPLE
        ...
        cp ../methods.txt $SHEND
        '''

I am receiving the following error message:
Waiting at most 5 seconds for missing files.
MissingOutputException in line 22 of /path/to/Snakefile:
Missing files after 5 seconds:
[Variable Destination Folder B]/[Sample A]/methods.txt

I believe that I can see the problem in an earlier part of the log :
rule copy_folders:
    input: /common/folder/path/[Sample A], [Variable Destination Folder A]
    output: [Variable Destination Folder B]/[Sample A]/methods.txt
    jobid: 171
    wildcards: outf=[Variable Destination Folder B], sample=[Sample A]
    resources: mem_mb=2000, cpus=1

I have a sample sheet where various folders are paired with a unique sample ID.  On a given line, you would find [Sample A] and [Variable Destination Folder A].  On a different line, you would find [Sample B] and [Variable Destination Folder B], etc..
In other words, the mismatch for the wildcards at the earlier step matches the error message in that it describes a file that should not be created at that point (because the values for {outf} and {sample} are not matched correctly, for different lines &quot;A&quot; and &quot;B&quot;).
The methods.txt file is not strictly needed.  However, I encountered problems when trying to use a directory as the endpoint, so I copied an extra file and I used that as the endpoint.  If it helps, I can share the earlier code.  However, for 1 different folder with a smaller number of subfolders to copy and less complicated destination folders, something similar to the current code appeared to work successfully.
I had an earlier version of the code to try and make sure that the shell environment variables were &quot;local&quot; to each folder.  I think the use of &quot;local&quot; caused a problem in itself, which an error message indicating that can only be used within a function.
However, if use the similarly simplified portion of the shell code, then the paths were filled in as follows:
        local SHOUT1=[Variable Destination Folder A]
        ...
        cp -R /common/folder/path/[Sample A] $SHOUT1
        
        local TEMPSAMPLE=$(basename /common/folder/path/[Sample A])
        local SHEND=[Variable Destination Folder A]/$TEMPSAMPLE
        ...
        cp ../methods.txt $SHEND

In other words, it looks like the paths for the shell command were correct (all for line &quot;A&quot; in the sample mapping file).  I assume this is because they only used input wildcards values, because I noticed a problem with the variable mismatching.  Some troubleshooting was added to be able to handle a folder with a space in the name where different parts of the same script need to use &quot;\ &quot; versus &quot; &quot; to run correctly), but I am excluding those folders to try and simplify the most immediate troubleshooting.  However, I can't run the Snakemake script if I can't specify the output value correctly.
Any assistance with troubleshooting would be greatly appreciated!
I thought this should be a relatively simple example to start learning Snakemake for what is basically cp -R $INPUTSUBFOLDER $OUTPUTFOLDER, but perhaps there are more complications than I realized.
Sincerely,
Charles
",1,-1,-1.0
72794939,how to quickly identify if a rule in Snakemake needs an input function,"I'm following the snakemake tutorial on their documentation page and really got stuck on the concept of input functions https://snakemake.readthedocs.io/en/stable/tutorial/advanced.html#step-3-input-functions
Basically they define a config.yaml as follows:
samples:
  A: data/samples/A.fastq
  B: data/samples/B.fastq

and the Snakefile as follows without any input function:
configfile: &quot;config.yaml&quot;

rule all:
    input:
        &quot;plots/quals.svg&quot;

rule bwa_map:
    input:
        &quot;data/genome.fa&quot;,
        &quot;data/samples/{sample}.fastq&quot;
    output:
        &quot;mapped_reads/{sample}.bam&quot;
    threads: 12
    shell:
        &quot;bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}&quot;

rule samtools_sort:
    input:
        &quot;mapped_reads/{sample}.bam&quot;
    output:
        &quot;sorted_reads/{sample}.bam&quot;
    shell:
        &quot;samtools sort -T sorted_reads/{wildcards.sample} -O bam {input} &gt; {output}&quot;

rule samtools_index:
    input:
        &quot;sorted_reads/{sample}.bam&quot;
    output:
        &quot;sorted_reads/{sample}.bam.bai&quot;
    shell:
        &quot;samtools index {input}&quot;

rule bcftools_call:
    input:
        fa = &quot;data/genome.fa&quot;,
        bam = expand(&quot;sorted_reads/{sample}.bam&quot;,sample=config['samples']),
        bai = expand(&quot;sorted_reads/{sample}.bam.bai&quot;,sample=config['samples'])
    output:
        &quot;calls/all.vcf&quot;
    shell:
        &quot;bcftools mpileup -f {input.fa} {input.bam} | &quot;
        &quot;bcftools call -mv - &gt; {output}&quot;

rule plot_quals:
    input:
        &quot;calls/all.vcf&quot;
    output:
        &quot;plots/quals.svg&quot;
    script:
        &quot;scripts/plot-quals.py&quot;

In the tutorial they mention that this expansion happens in the initialization step:
bam = expand(&quot;sorted_reads/{sample}.bam&quot;,sample=config['samples']),
bai = expand(&quot;sorted_reads/{sample}.bam.bai&quot;,sample=config['samples'])

and that the FASTQ paths cannot be determined for rule bwa_map in this phase. However the code works if we run as is, why is that ?
Then they recommend using an input function to defer bwa_map to the next phase (DAG phase) as follows:
def get_bwa_map_input_fastqs(wildcards):
    return config[&quot;samples&quot;][wildcards.sample]

rule bwa_map:
    input:
        &quot;data/genome.fa&quot;,
        get_bwa_map_input_fastqs
    output:
        &quot;mapped_reads/{sample}.bam&quot;
    threads: 8
    shell:
        &quot;bwa mem -t {threads} {input} | samtools view -Sb - &gt; {output}&quot;

I'm really confused when an input function makes sense and when it does not ?
",-1,-1,-1.0
72828955,How to log error messages arrising from scripts in snakemake,"I am trying to build a snakemake pipeline with custom python scripts.
Some of my scripts run into errors, leading to a shutdown of the pipeline.
However, while in the shell output I can barely see the end of the python error message that leads to the shutdown, this error is not logged anywhere. It is not logged in the snakemake.log that gets automatically created (only stating which script failed without giving the error message), and adding a &quot;log: &quot; with a folder to the rule that fails only creates an empty log.
Is there a way to access the error message, so I can solve the underlying issue?
Edit:
my current snakemake rule looks like this:
rule do_X:
    input: &quot;{Wildcard}_a&quot;
    output: &quot;{wildcard}_b&quot;
    log: out = &quot;{Wildcard}_stdout.log&quot;
         err = &quot;{Wildcardd}_stderr.err&quot;
    shell: python script x.py {input}{output}

If the script fails, I recive empty logs,
",-1,-1,-1.0
72849888,"How to use Snakemake ""allow_missing""=True properly? Partial wild_cards?","I have a list of input files that are in different subfolders and each folder have different number of files, with two wildcards SAMPLE and id. For the output, these names will also be present:
SAMPLE=set([&quot;x&quot;,&quot;y&quot;,&quot;z&quot;])
with open(config[&quot;path&quot;]+&quot;barcodes.txt&quot;) as f:
id = [line.rstrip() for line in f]
rule all:
    input:
        expand(config[&quot;path&quot;]+ &quot;{sample}/remap/filtered.{sample}.R1.clean.id_{ID}.fq.bam&quot;, sample=SAMPLE, ID=id, allow_missing=True)



rule map_again:
    output:
        config[&quot;path&quot;]+ &quot;{sample}/remap/filtered.{sample}.R1.clean.id_{ID}.fq.bam&quot;
    input:
        expand(config[&quot;path&quot;]+ &quot;{{sample}}/map/filtered.{{sample}}.R1.clean.id_{ID}.fq.gz&quot;, sample=SAMPLE, allow_missing=True)
    shell:
        &quot;squire Map -1 {input} -r 150 -p 10 &quot;

However, I still got warnings from Snakemake that certain combination of the wildcards don't exist, although I hoped it to ignore these ones...
How could I correct this?
Thank you very much!
",-1,-1,-1.0
72932409,How to load enviroment variable script inside singularity container in snakemake,"Consider the following snakemake rule:
rule all:
    singularity: &quot;./ubuntu.sif&quot;
    output: &quot;tmp.txt&quot;
    shell: &quot;&quot;&quot;
    python --version &gt;&gt; tmp.txt
    &quot;&quot;&quot;

when I run it as snakemake -c1 --use-singularity it gives an error because the python interpreter in not in PATH in the container.
Activating singularity image ./ubuntu.sif
/usr/bin/bash: line 2: python: command not found
[Sun Jul 10 17:49:04 2022]
Error in rule all:
    jobid: 0

How can I make snakemake run a &quot;source /some/path/to/env.sh&quot; command on the singularity container?

I know I can run the rule as
rule all:
    output: &quot;tmp.txt&quot;
    shell: &quot;&quot;&quot;
    singularity exec ./ubuntu.sif /path/to/python --version &gt;&gt; tmp.txt
    &quot;&quot;&quot;

But it seems much more hacky solution than providing simple &quot;singularity&quot; parameter. And that is how people are using it currently. I do not have root access so i cannot modify the container's .singularity.d content.
",1,-1,-1.0
72948655,Multiple Globe Wildcards Snakemake,"I Have a problem in snakemake.
I Have 2 glob wildcards, one for my annotations files, and the other for the nucleotide file. My analysis require both of these files which are stored in two different pathways. My pipline works perfectly when i run it for one chromosomes or two. But when i use more, The first rules of my pipline which use the nucleotide files, works very well for all chromosomes, but when it comes for annotation files, it starts to be confusing. it merge files for example:
when i want to extract feature genes from all files. what i want is to store the results in their specific repertoire. like i want extract feature genes for chromosomes 12 and 5 and 8 . the pipeline does this extraction but he stores the results like this, feature genes12 in repertoire chromosome 5 and vise versa.
I'll show you a part of my code.
&gt; configfile : &quot;config.yml&quot;
import os
from os.path import join
import yaml 
import subprocess
from snakemake.utils import report
from glob import glob
# a list of Nucleotide and Annotation files we are using.
gff, = glob_wildcards(&quot;data/Hap1/Gff/{gff}.gff3&quot;)
chromosome, = glob_wildcards(&quot;data/Hap1/Genome/{chromosome}.fasta&quot;)
rule all: 
    input:
        expand(config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/NLR-Annotator/NBS_LRR/NBS_LRR_{chromosome}.gff&quot;, chromosome=chromosome),
        **expand(config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Nucleotide/Id_seq_{gff}.txt&quot;, zip, gff=gff, chromosome=chromosome),**
        expand(config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Nucleotide/gene_{gff}.fasta&quot;, zip, chromosome=chromosome, gff=gff),
        expand(config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Proteome/gene/gene_{gff}.gff&quot;, zip, chromosome=chromosome, gff=gff),

rule Extraction_Attributs:
input:
&quot;data/Hap1/Gff/{gff}.gff3&quot;
output:
m = config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Proteome/mRNA/mRNA_{gff}.gff&quot;,
gene = config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Proteome/gene/gene_{gff}.gff&quot;
message:
&quot; STEP 4: mRNA feature EXTRACTION &quot;
benchmark:
config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Proteome/gene/Benchmark/gene_{gff}_benchmark.tsv&quot;
shell:
&quot;grep 'mRNA' {input} &gt; {output.m} |&quot;
&quot;grep 'gene' {input} &gt; {output.gene} &quot;
#---------------------------STEP 5: ID mRNA FEATURE EXTRACTION -------------------

rule Extraction_Identifiants_mRNA:
input:
config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Proteome/mRNA/mRNA_{gff}.gff&quot;
output:
temp(config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Proteome/mRNA/mRNA_ID_{gff}.gff&quot;)
message:
&quot;STEP 5: ID mRNA FEATURE EXTRACTION&quot;
benchmark:
config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Proteome/mRNA/Benchmark/mRNA_ID_{gff}.tsv&quot;
shell:
'''
awk '$9 {{print $9}}' {input} | cut -d &quot;;&quot; -f1 | sed -r &quot;s/ID=/&gt;/&quot; &gt; {output}
'''

#---------------------------STEP 6: ID gene FEATURE EXTRACTION -------------------

rule Extraction_Identifiants_Gene:
input:
config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Proteome/gene/gene_{gff}.gff&quot;
output:
config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Proteome/gene/geneID/gene_ID_{gff}.txt&quot;
message:
&quot;STEP 6: ID gene FEATURE EXTRACTION&quot;
benchmark:
config[&quot;Results&quot;] + &quot;Hap1/{chromosome}/Proteome/gene/geneID/Benchmark/gene_ID_{gff}.tsv&quot;
shell:
'''
awk '$9 {{print $9}}' {input} | sed -r &quot;s/ID=/&gt;/&quot; &gt; {output}
'''
Part of my
The error starts from that expand in bold font.
Thank you in advance for your help.
",1,-1,-1.0
72961251,Importing local modules into Snakemake files,"How do you import local files (.py files) into Snakemake files (.smk files)? I have the following example structure:
parent_dir
├── dir1
│   └── a.py
└── dir2
    └── b.smk

I would like to import a.py into b.smk. I tried the following:
sys.path.insert(0, Path(__file__).parent.parent)
import dir1.a

but had no success. It results in a ModuleNotFoundError. Is there a way around this? I do not want to change the extension of a.py to .smk.
",-1,-1,-1.0
72969900,Snakemake checkpoint output unknown number of files with no subsequent aggregation but instead rules that peform actions on individual files?,"Thanks for any help ahead of time.
I'm trying to use the Snakemake checkpoint functionality to produce an unknown number of files in a directory, which I've gotten to work using the pattern described in the docs, but then I don't want to do any kind of aggregation rule afterwards, I want to have rules that do actions on each individual file (of course inherently in parallel via wildcards).
Here's a simple reproducible example of my problem:
from os.path import join


rule all:
    input:
        &quot;aggregated.txt&quot;,


checkpoint create_gzip_file:
    output:
        directory(&quot;my_directory/&quot;),
    shell:
        &quot;&quot;&quot;
        mkdir my_directory/
        cd my_directory
        for i in 1 2 3; do gzip &lt; /dev/null &gt; $i.txt.gz; done
        &quot;&quot;&quot;


rule gunzip_file:
    input:
        join(&quot;my_directory&quot;, &quot;{i}.txt.gz&quot;),
    output:
        join(&quot;my_directory&quot;, &quot;{i}.txt&quot;),
    shell:
        &quot;&quot;&quot;
        gunzip -c {input} &gt; {output}
        &quot;&quot;&quot;


def gather_gunzip_input(wildcards):
    out_dir = checkpoints.create_gzip_file.get(**wildcards).output[0]
    i = glob_wildcards(join(out_dir, &quot;{i}.txt.gz&quot;))
    return expand(f&quot;{out_dir}/{{i}}&quot;, i=i)


rule aggregate:
    input:
        gather_gunzip_input,
    output:
        &quot;aggregated.txt&quot;,
    shell:
        &quot;cat {input} &gt; {output}&quot;

I'm getting the following error:
$ snakemake --printshellcmds --cores all
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 16
Rules claiming more threads will be scaled down.
Job stats:
job                 count    min threads    max threads
----------------  -------  -------------  -------------
aggregate               1              1              1
all                     1              1              1
create_gzip_file        1              1              1
total                   3              1              1

Select jobs to execute...

[Wed Jul 13 14:57:09 2022]
checkpoint create_gzip_file:
    output: my_directory
    jobid: 2
    reason: Missing output files: my_directory
    resources: tmpdir=/tmp
Downstream jobs will be updated after completion.


        mkdir my_directory/
        cd my_directory
        for i in 1 2 3; do gzip &lt; /dev/null &gt; $i.txt.gz; done
        
[Wed Jul 13 14:57:09 2022]
Finished job 2.
1 of 3 steps (33%) done
MissingInputException in line 20 of /home/hermidalc/projects/github/hermidalc/test/Snakefile:
Missing input files for rule gunzip_file:
    output: my_directory/['1', '2', '3'].txt
    wildcards: i=['1', '2', '3']
    affected files:
        my_directory/['1', '2', '3'].txt.gz

",-1,-1,-1.0
72975255,"Error with snakemake when combining --use-singularity and --use-conda: ""/bin/sh: 1: conda: not found""","I have an analysis pipeline using snakemake where one rule requires a singularity container, and several others require conda environments. I specify the &quot;container:&quot; and &quot;conda:&quot; directives where needed within the Snakefile.
The way snakemake is invoked is a little complex. For various reasons, I invoke the snakemake API via a custom script within a conda environment, with both &quot;use_conda=True&quot; and &quot;use_singularity=True&quot; specified in the config. The pipeline has completed with no issues in the past, but someone recently reinstalled the pipeline, and now I get the following error:
/bin/sh: 1: conda: not found
Traceback (most recent call last):
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/site-packages/snakemake/__init__.py&quot;, line 726, in snakemake
    success = workflow.execute(
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/site-packages/snakemake/workflow.py&quot;, line 978, in execute
    dag.create_conda_envs(
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/site-packages/snakemake/dag.py&quot;, line 313, in create_conda_envs
    env.create(dryrun)
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/site-packages/snakemake/deployment/conda.py&quot;, line 391, in create
    pin_file = self.pin_file
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/site-packages/snakemake/common/__init__.py&quot;, line 192, in __get__
    value = self.method(instance)
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/site-packages/snakemake/deployment/conda.py&quot;, line 107, in pin_file
    f&quot;.{self.conda.platform}.pin.txt&quot;
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/site-packages/snakemake/common/__init__.py&quot;, line 192, in __get__
    value = self.method(instance)
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/site-packages/snakemake/deployment/conda.py&quot;, line 102, in conda
    return Conda(self._container_img)
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/site-packages/snakemake/deployment/conda.py&quot;, line 648, in __init__
    shell.check_output(
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/site-packages/snakemake/shell.py&quot;, line 63, in check_output
    return sp.check_output(cmd, shell=True, executable=executable, **kwargs)
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/subprocess.py&quot;, line 420, in check_output
    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
  File &quot;/home/cfos/miniconda3/envs/CIS/lib/python3.10/subprocess.py&quot;, line 524, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command ' singularity --quiet --silent exec --home /home/cfos/Programs/COVID_Illumina_Snakemake  /home/cfos/Programs/COVID_Illumina_Snakemake/.snakemake/singularity/1ad1f7122bf7704a6b02b6359ede7533.simg sh -c 'conda info --json'' returned non-zero exit status 127.

I get the same error if I run the snakemake command 'manually' (outside of my script) like so:
snakemake -j20 --use-conda --use-singularity -s /path/to/Snakefile --config [OPTIONS]

It appears like snakemake is checking for the presence of conda within a singularity container that I have only specified for one rule. This is not the desired behaviour, since conda is not within that container. Instead, snakemake should be detecting and using the system installation of miniconda, and the the installation of mamba within the conda environment from which I run the pipeline. This is the way the pipeline used to run with no problems.
Does anyone have a solution to this problem? The confusing thing is that I've tried various versions of snakemake, including ones that worked well previously, and I still get the error.
Details
snakemake v7.8.5
conda 4.11.0
If it helps to understand the way I invoke the pipeline: https://github.com/charlesfoster/covid-illumina-snakemake
Thanks!
",1,-1,-1.0
72826479,Snakemake rule log/benchmark wildcards dont match output wildcards after checkpoint,"I am running a Snakemake workflow with a checkpoint at some point from which I gather the previously unknown number of output files. Snakemake should then create a number of tasks based on the file number with the next rule, using some part of the gathered checkpoints files as wildcards for that rules wildcards. It all works fine, unless I want that rule to also create log and/or benchmark files, at which point is throws:
SyntaxError:
Not all output, log and benchmark files of rule plasmid_spades contain the same wildcards. This is crucial though, in order to avoid that two or more jobs write to the same file.
  File &quot;/path/to/Snakefile&quot;, line N, in &lt;module&gt;

These are the relevant parts of the workflow:
WCS = None

...

def gather_checkpoint_output(wildcards):
    ck_output = checkpoints.checkpoint_rule.get(**wildcards).output[0]
    global WCS
    WCS, = glob_wildcards(os.path.join(ck_output, &quot;{wc}&quot;, &quot;{wc}.file&quot;))
    return expand(os.path.join(ck_output, &quot;{wc}&quot;, &quot;{wc}.file&quot;), wc=WCS)


def gather_some_rule_after_checkpoint_out(wildcards):
    rule_output = checkpoints.checkpoint_rule.get(**wildcards).output[0]
    WCS2, = glob_wildcards(os.path.join(rule_output, &quot;{wc}&quot;, &quot;{wc}.file&quot;))
    return expand(os.path.join(&quot;some&quot;, &quot;{wc}&quot;, &quot;path&quot;, &quot;output.file&quot;), wc=WCS2)

...

localrules: all
rule all:
    input:
        gather_checkpoint_output,
        gather_some_rule_after_checkpoint_out

...

rule some_rule_after_checkpoint:
    input:
        input = gather_checkpoint_output
    output:
        out_dir = directory(expand(os.path.join(&quot;some&quot;, &quot;{wc}&quot;, &quot;dir&quot;), wc=WCS)),
        output = expand(os.path.join(&quot;some&quot;, &quot;{wc}&quot;, &quot;path&quot;, &quot;output.file&quot;), wc=WCS)
    log:
         os.path.join(&quot;logs&quot;, &quot;some&quot;, &quot;path&quot;, &quot;{wc}_rule.log&quot;)
    benchmark:
         os.path.join(&quot;logs&quot;, &quot;some&quot;, &quot;path&quot;, &quot;{wc}_rule_benchmark.tsv&quot;)
...

Is the problem, that it evaluates the logs/benchmarks wildcard in the beginning (WCS = None), while the output will be reevaluated with the checkpoint functions? Although, a rules wildcards are based off of the outputs wildcards, I think. I tried lambda functions, expand(), etc., to specifically get the wildcards from (the hopefully reevaluated WCS) for the logs, but that is apparently not permitted. Am I overlookig something obvious here or is the entire construction wrong somehow?
",-1,-1,-1.0
73017815,snakemake rule error. How can I select rule order?,"I run the snakemake for RNA-seq analysis.
I made snakefile for running, and some error occurred in terminal.
I set rule salmon quant reads at last order but it is running at first.
So snakemake showed the error in rule salmon quant reads.
salmon quant reads must run after salmon index finished.
Error in rule salmon_quant_reads:
    jobid: 173
    output: salmon/WT_Veh_11/quant.sf, salmon/WT_Veh_11/lib_format_counts.json
    log: logs/salmon/WT_Veh_11.log (check log file(s) for error message)
    conda-env: /home/baelab2/LEEJUNEYOUNG/7.Colesevelam/RNA-seq/.snakemake/conda/ff908de630224c1a4118f5dc69c8a761

RuleException:
CalledProcessError in line 111 of /home/baelab2/LEEJUNEYOUNG/7.Colesevelam/RNA-seq/Snakefile_2:
Command 'source /home/baelab2/miniconda3/bin/activate '/home/baelab2/LEEJUNEYOUNG/7.Colesevelam/RNA-seq/.snakemake/conda/ff908de630224c1a4118f5dc69c8a761'; set -euo pipefail;  /home/baelab2/miniconda3/envs/snakemake/bin/python3.10 /home/baelab2/LEEJUNEYOUNG/7.Colesevelam/RNA-seq/.snakemake/scripts/tmpr6r8ryk9.wrapper.py' returned non-zero exit status 1.
  File &quot;/home/baelab2/LEEJUNEYOUNG/7.Colesevelam/RNA-seq/Snakefile_2&quot;, line 111, in __rule_salmon_quant_reads
  File &quot;/home/baelab2/miniconda3/envs/snakemake/lib/python3.10/concurrent/futures/thread.py&quot;, line 58, in run

How can I fix it?
Here is the my snakefile info.
SAMPLES = [&quot;KO_Col_5&quot;, &quot;KO_Col_6&quot;, &quot;KO_Col_7&quot;, &quot;KO_Col_8&quot;, &quot;KO_Col_9&quot;, &quot;KO_Col_10&quot;, &quot;KO_Col_11&quot;, &quot;KO_Col_15&quot;, &quot;KO_Veh_3&quot;, &quot;KO_Veh_4&quot;, &quot;KO_Veh_5&quot;, &quot;KO_Veh_9&quot;, &quot;KO_Veh_11&quot;, &quot;KO_Veh_13&quot;, &quot;KO_Veh_14&quot;, &quot;WT_Col_1&quot;, &quot;WT_Col_2&quot;, &quot;WT_Col_3&quot;, &quot;WT_Col_6&quot;, &quot;WT_Col_8&quot;, &quot;WT_Col_10&quot;, &quot;WT_Col_12&quot;, &quot;WT_Veh_1&quot;, &quot;WT_Veh_2&quot;, &quot;WT_Veh_4&quot;, &quot;WT_Veh_7&quot;, &quot;WT_Veh_8&quot;, &quot;WT_Veh_11&quot;, &quot;WT_Veh_14&quot;]

rule all:
    input:
        expand(&quot;raw/{sample}_1.fastq.gz&quot;, sample=SAMPLES),
        expand(&quot;raw/{sample}_2.fastq.gz&quot;, sample=SAMPLES),
        expand(&quot;qc/fastqc/{sample}_1.before.trim_fastqc.zip&quot;, sample=SAMPLES),
        expand(&quot;qc/fastqc/{sample}_2.before.trim_fastqc.zip&quot;, sample=SAMPLES),
        expand(&quot;trimmed/{sample}_1.fastq.gz&quot;, sample=SAMPLES),
        expand(&quot;trimmed/{sample}_2.fastq.gz&quot;, sample=SAMPLES),
        expand(&quot;qc/fastqc/{sample}_1.after.trim_fastqc.zip&quot;, sample=SAMPLES),
        expand(&quot;qc/fastqc/{sample}_2.after.trim_fastqc.zip&quot;, sample=SAMPLES),
        expand(&quot;salmon/{sample}/quant.sf&quot;, sample=SAMPLES),
        expand(&quot;salmon/{sample}/lib_format_counts.json&quot;, sample=SAMPLES)

rule fastqc_before_trim_1:
    input:
        &quot;raw/{sample}.fastq.gz&quot;,
    output:
        html=&quot;qc/fastqc/{sample}.before.trim.html&quot;,
        zip=&quot;qc/fastqc/{sample}.before.trim_fastqc.zip&quot;,
    log:
        &quot;logs/fastqc/{sample}.before.log&quot;
    threads: 10    
    priority: 1
    wrapper:
        &quot;v1.7.0/bio/fastqc&quot;

rule cutadapt:
    input:
        r1 = &quot;raw/{sample}_1.fastq.gz&quot;,
        r2 = &quot;raw/{sample}_2.fastq.gz&quot;
    output:
        fastq1=&quot;trimmed/{sample}_1.fastq.gz&quot;,
        fastq2=&quot;trimmed/{sample}_2.fastq.gz&quot;,
        qc=&quot;trimmed/{sample}.qc.txt&quot;
    params:
        adapters = &quot;-a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT&quot;,
        extra = &quot;--minimum-length 1 -q 20&quot;    
    log:
        &quot;logs/cutadapt/{sample}.log&quot;
    threads: 10    
    priority: 2
    wrapper:
        &quot;v1.7.0/bio/cutadapt/pe&quot;

rule fastqc_after_trim_2:
    input:
        &quot;trimmed/{sample}.fastq.gz&quot;
    output:
        html=&quot;qc/fastqc/{sample}.after.trim.html&quot;,
        zip=&quot;qc/fastqc/{sample}.after.trim_fastqc.zip&quot;
    log:
        &quot;logs/fastqc/{sample}.after.log&quot;
    threads: 10
    priority: 3
    wrapper:
        &quot;v1.7.0/bio/fastqc&quot;

rule salmon_index:
    input:
        sequences=&quot;raw/Mus_musculus.GRCm39.cdna.all.fasta&quot;
    output:
        multiext(
            &quot;salmon/transcriptome_index/&quot;,
            &quot;complete_ref_lens.bin&quot;,
            &quot;ctable.bin&quot;,
            &quot;ctg_offsets.bin&quot;,
            &quot;duplicate_clusters.tsv&quot;,
            &quot;info.json&quot;,
            &quot;mphf.bin&quot;,
            &quot;pos.bin&quot;,
            &quot;pre_indexing.log&quot;,
            &quot;rank.bin&quot;,
            &quot;refAccumLengths.bin&quot;,
            &quot;ref_indexing.log&quot;,
            &quot;reflengths.bin&quot;,
            &quot;refseq.bin&quot;,
            &quot;seq.bin&quot;,
            &quot;versionInfo.json&quot;,
        ),
    log:
        &quot;logs/salmon/transcriptome_index.log&quot;,
    threads: 10
    priority: 10
    params:
        # optional parameters
        extra=&quot;&quot;,
    wrapper:
        &quot;v1.7.0/bio/salmon/index&quot;

rule salmon_quant_reads:
    input:
        # If you have multiple fastq files for a single sample (e.g. technical replicates)
        # use a list for r1 and r2.
        r1 = &quot;trimmed/{sample}_1.fastq.gz&quot;,
        r2 = &quot;trimmed/{sample}_2.fastq.gz&quot;,
        index = &quot;salmon/transcriptome_index&quot;
    output:
        quant = &quot;salmon/{sample}/quant.sf&quot;,
        lib = &quot;salmon/{sample}/lib_format_counts.json&quot;
    log:
        &quot;logs/salmon/{sample}.log&quot;
    params:
        # optional parameters
        libtype =&quot;A&quot;,
        extra=&quot;--validateMappings&quot;
    threads: 10
    priority: 20
    wrapper:
        &quot;v1.7.0/bio/salmon/quant&quot;

",-1,-1,-1.0
73028206,Workflow with Snakemake & Conda: Placeholder of length '80' too short in package,"I have followed the Snakemake best practices here to create a workflow, where different steps activate different Conda environments. For example, a rule that collects statistics:
rule per_samp_stats: 
  input:
    ref_path='/labs/jandr/walter/tb/data/refs/{ref}.fa',
    bam='results/{batch}/{samp}/bams/{samp}_{mapper}_{ref}_sorted.bam'
  log:
    'results/{batch}/{samp}/stats/{samp}_{mapper}_{ref}_cov_stats.log'
  conda: 'envs/picard.yaml'
  params:
    prefix='results/{batch}/{samp}/stats/{samp}'
  shell:    
    '''
    {config[scripts_dir]}cov_stats.sh {input.ref_path} 
    '''   

I am running into the error:
# CreateCondaEnvironmentException:
# Could not create conda environment from /oak/stanford/scg/lab_xx/xx/tb/mtb_tgen/workflow/envs/mtb.yaml:
# Command:
# mamba env create --quiet --file &quot;/oak/stanford/scg/lab_xx/xx/tb/mtb_tgen/.snakemake/conda/5b3e765eb8210c23d169553fd7853814.yaml&quot; --prefix &quot;/oak/stanford/scg/lab_xx/xx/tb/mtb_tgen/.snakemake/conda/5b3e765eb8210c23d169553fd7853814&quot;
# Output:
# Preparing transaction: ...working... done
# Verifying transaction: ...working... failed
# 
# PaddingError: Placeholder of length '80' too short in package /oak/stanford/scg/lab_xx/xx/tb/mtb_tgen/.snakemake/conda/5b3e765eb8210c23d169553fd7853814/bin/Rscript.
# The package must be rebuilt with conda-build &gt; 2.0.


I'm wondering if you have suggestions how to deal with this; the problem seems to occur because of the long full path to the location where Snakemake rebuilds the environment to run a specific rule. Any help would be fantastic. Thank you!
",1,-1,-1.0
73062821,Snakemake scatter-gather with wildcard AmbiguousRuleException,"My problem is when using Snakemake scatter-gather feature the documentation is basic and i modified my code according to mentioned in this link:
rule fastq_fasta:
    input:rules.trimmomatic.output.out_file
    output:&quot;data/trimmed/{sample}.fasta&quot;
    shell:&quot;sed -n '1~4s/^@/&gt;/p;2~4p' {input} &gt; {output}&quot;

rule split:
    input:
        &quot;data/trimmed/{sample}.fasta&quot;
    params:
        scatter_count=config[&quot;scatter_count&quot;],
        scatter_item = lambda wildcards: wildcards.scatteritem
    output:
        temp(scatter.split(&quot;data/trimmed/{{sample}}_{scatteritem}.fasta&quot;))
    script:
        &quot;scripts/split_files.py&quot;
        
rule process:
    input:&quot;data/trimmed/{sample}_{scatteritem}.fasta&quot;
    output:&quot;data/processed/{sample}_{scatteritem}.csv&quot;
    script:
        &quot;scripts/process.py&quot;

rule gather:
    input:
        gather.split(&quot;data/processed/{{sample}}_{scatteritem}.csv&quot;)
    output:
        &quot;data/processed/{sample}.csv&quot;
    shell:
        &quot;cat {input} &gt; {output}&quot;

I added wildcard option but, I got:
AmbiguousRuleException: Rules fastq_to_fasta(which is previous rule) and split are ambiguous for the file data/trimmed/Ornek_411-of-81-of-81-of-81-of-81-of-81-of-81-of-81-of-81-of-8.fasta

I tried lots of things but either rules are not calling or take AmbiguousRuleException. What am i missing, can someone help?
",-1,-1,-1.0
73112948,Snakemake 'run' directive produces no error message,"When I use the run directive in snakemake (using python code) it doesn't produce any kind of error message for troubleshooting.
Is this desired behavior? Am I missing something?
Here a minimal example using snakemake 7.8.3 and python 3.9.13.
I invoked snakemake with the -p option which in shell directive outputs the exact code as passed to the shell (but doesn't do anything for run directive I guess).
Snakefile:
def useless_function():
    return[thisVariableAlsoDoesntExist]

rule all:
    input: &quot;final.txt&quot;

rule test:
    output: &quot;final.txt&quot;
    run:
        print(thisVariableDoesNotExist)
        useless_function()

Stdout:
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job      count    min threads    max threads
-----  -------  -------------  -------------
all          1              1              1
test         1              1              1
total        2              1              1

Select jobs to execute...

[Mon Jul 25 18:59:13 2022]
rule test:
    output: final.txt
    jobid: 1
    reason: Missing output files: final.txt
    resources: tmpdir=/tmp

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2022-07-25T185913.188760.snakemake.log


Expected error message (when function and print command are executed directly on python console):
&gt;&gt;&gt; print(thisVariableDoesNotExist)
Traceback (most recent call last):                       
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;                    
NameError: name 'thisVariableDoesNotExist' is not defined


&gt;&gt;&gt; useless_function()
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;&lt;stdin&gt;&quot;, line 2, in useless_function
NameError: name 'thisVariableAlsoDoesntExist' is not defined

",-1,-1,-1.0
73140792,"How to use "".yaml"" to load modules on cluster using ""envmodules"" and ""--use-envmodules"" in snakemake","I have a snakefile like this (only for dep:
rule test:
    input:
        text='catthis.txt'
    output:
        &quot;test.txt&quot;
    envmodules: 
        &quot;modules.yaml&quot;
    shell:
        &quot;cat {input.text} &gt; ./{output}&quot;

My modules.yaml file contains this:
modules:
    &quot;StdEnv/2020&quot;,
    &quot;gcc/9.3.0&quot;

So in the end, I'd like to have something like this, when snakemake is called:
rule test:
    input:
        text='catthis.txt'
    output:
        &quot;test.txt&quot;
    envmodules: 
        &quot;StdEnv/2020&quot;,
        &quot;gcc/9.3.0&quot;
    shell:
        &quot;cat {input.text} &gt; ./{output}&quot;

Perhaps this is not possible, but I found nowhere on the snakemake website here that would allow this. But I'd be much more practical for me to have one file to call rather than pasting the modules to be loaded in all the rules (here I'm showing one, but imagine I have 50 rules...)
When running the snakemake (assuming everything is in the same directory)
snakemake -p --cores 1 --use-envmodules

it doesn't work (using the modules.yaml), but it does work if the modules are put directly in the snakefile.
The catthis.txt contains only this text Lorem ipsum dolor sit amet, again for demonstration.
",-1,-1,-1.0
73143215,Error submitting jobscript (exit code 127) when invoking snakemake through cron job on SLURM cluster,"I'm trying to run a snakemake pipeline through crontab on a SLURM cluster. Here is the bash script that I used to send to the slurm.
#!/bin/bash
#SBATCH --job-name=nextstrain

snakemake --configfile config.yaml --jobs 100 --keep-going --rerun-incomplete --latency-wait 360 --cluster 'sbatch' -r -p --useconda

This scrip runs as intended. However, when I run the script through crontab as so:
0 8 * * 1 /bin/bash /home/user/snakemake_automate.sh

I get the error:
Error submitting jobscript (exit code 127):

I am not sure what I should do to fix this error.
",-1,-1,-1.0
73153995,snakemake parse output file and use as wildcard in the next rule,"I have a rule that produces files with sample names in every line of the file:
FAMILY = ['fam1','fam2','fam3']

rule extract_individuals:
    input:
        vcf = 'muscle/{family}.vcf.gz'
    output:
        vcf_ind = 'output/{family}.txt'
    shell:
        'bcftools query -l {input.vcf} -o {output.vcf_ind}' 

This rule will produce files with individual names:
sample1
sample2
sample3

I want to have another rule that for every line of these files, uses the string representation of the line as the wildcard output of another rule; for example:
rule get_samples:
    input: 'output/{family}.txt'
    output: 'output/{individual}.vcf.gz'
    shell: 'python -c &quot;for line in {input} print(line)&quot; |  xargs -I {{}} bcftools view -O z -s {{}} -o {ouput} {input}'

Note that snakemake complains here that the output from input files cannot be determined from output files.
",-1,-1,-1.0
73163437,Snakemake input and output according to a dictionary,"I am trying to rename some files in the snakemake pipeline.
Let's say I have three files: &quot;FileA.txt&quot;, &quot;FileB.txt&quot;, &quot;FileC.txt&quot; and I want them renamed according to a dictionary dict = {&quot;A&quot;: &quot;0&quot;, &quot;B&quot;: &quot;1&quot;, &quot;C&quot;: &quot;2&quot;} to get &quot;RenamedFile0.txt&quot;, &quot;RenamedFile1.txt&quot;, and &quot;RenamedFile2.txt&quot;. How would one write a rule for this?
This is how my pipeline looks like (I've tried with a function but doesn't work)
SAMPLES = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;]
RENAMED_SAMPLES = [&quot;0&quot;, &quot;1&quot;, &quot;2&quot;]

rename = {&quot;0&quot;: &quot;A&quot;, &quot;1&quot;: &quot;B&quot;, &quot;2&quot;: &quot;C&quot;}

def mapFile(wildcards):
    file = &quot;results/EditedFile&quot; + str(rename[wildcards]) + &quot;.txt&quot;
    return(file)

rule all:
    input:
        &quot;results/Combined.txt&quot;

rule cut:
    input:
        &quot;data/File{sample}.txt&quot;
    output:
        &quot;results/EditedFile{sample}.txt&quot;
    shell:
        &quot;cut -f1 {input} &gt; {output}&quot;

rule rename:
    input:
        mapFile
    output:
        &quot;results/RenamedFile{renamedSample}.txt&quot;
    shell:
        &quot;cp {input} {output}&quot;


rule combine:
    input:
        expand(&quot;results/RenamedFile{renamedSample}.txt&quot;, renamedSample = RENAMED_SAMPLES)
    output:
        &quot;results/Combined.txt&quot;
    shell:
        &quot;cat {input} &gt; {output}&quot;

I get the following error:
KeyError: ['2']
Wildcards:
renamedSample=2

Thanks!!!
",-1,1,-1.0
73231283,how to speed up task performing using snakemake,"Machine: 48 cores, 96 threads, RAM 256GB
System: Ubuntu 20.04
Python: 3.9

I have a python script for some data processing and analysis and an input dataset containing around 40,000 files for the job.
The script could be run by python a.py -i sample_list.txt -o /path/to/outdir.
The sample_list.txt contains the file prefixes of all 40000 files in that dataset. Each prefix represented a sample id. In python, it is imported as a list. /path/to/outdir define the output directory. The software will create a new folder in output directory based on the prefixes first, then the generated data will be put in these new folders sample-by-sample.
I found that this script is analyzing data one by one. I estimated the time, it needs nearly 240 days to finish all the jobs for this dataset! It is unacceptable. I think parallelization for job submission could speed up. That's when snakemake comes into my sight.
I did some study on snakemake. In my case, I can provide three things for snakemake file:
input: &quot;sample_list.txt&quot;
output: &quot;/path/to/outdir&quot;
shell: &quot;python a.py -i {input} -o {output}&quot;

But I have a question:
If I provided a sample_list file as input, the script will read the file prefix, instead of checking input file pattern e.g. {input}_1.txt and import it as input directly. Is it possible to parallelize the jobs based on sample_list.txt? Or I must define the input file pattern for input of snakemake?
Thanks
Additional:
I will print an example for you.
The filename looks like: sample1_1.fq, sample1_2.fq, sample2_1.fq, sample2_2.fq, etc.
The software requires a list: name = ['sample1','sample2','sample3','sample4']
To get all the sample names (file prefix) I extracted the sample name and stored them in sample_list.txt:
sample1
sample2
sample3
sample4

How to parallelize the jobs?
Edited:
def multiprocess(data_path, out_dir, run_name):
  max_cores = multiprocessing.cpu_count() - 2
  pool = multiprocessing.Pool(processes = max_cores) #, maxtasksperchild = 4
  
  sample_list = []
  for i in os.listdir(data_path):
      #print(i)
      portion = os.path.splitext(i)
      tmp_out = portion[0][0:portion[0].rfind('_')]
      sample_list.append(tmp_out)
  sample_list = sorted(list(set(sample_list)))
  for sample in sample_list:
    pool.apply_async(main, args=(data_path, out_dir, run_name, [sample]))
  print('Wait for all subprocesses done...')
  pool.close()
  pool.join()
  print('All subprocesses done!')
  

if __name__ == '__main__':
  parser = argparse.ArgumentParser(description = &quot;Run single sample&quot;)
  parser.add_argument('-d','--data',action = 'store', dest = 'data', help = &quot;Data path&quot;,required = True)
  parser.add_argument('-o','--outdir',action = 'store', dest = 'outdir', help = &quot;Output directory&quot;,required = True)
  parser.add_argument('-r','--runname',action = 'store', dest = 'runname', help = &quot;Define the group&quot;,required = True)
  args = parser.parse_args()
  multiprocess(data_path = args.data,out_dir = args.outdir, run_name = args.runname)

I loaded 11 samples. It quickly caused RAM leakage. Is there a way to improve this?
",-1,1,-1.0
73248359,Snakemake Uploading output to azure blob storage with sas token,"I'm trying to use snakemake.remote.AzBlob to work with input &amp; output from/to Azure blob storage. It works fine when I pass the account key as credentials. However, when I use sas token with all permissions (rawcl) I can download the inputs and run the script, but it's failing in uploading to the remote directory. I have tested the sas token and I can write to storage account with other tools. Here is my snakefile:
from snakemake.remote.AzBlob import RemoteProvider as AzureRemoteProvider
AS = AzureRemoteProvider(account_url= &quot;https://{accountname}.blob.core.windows.net?{sas-token}&quot;)

rule bwa_map:
    input:
        AS.remote(&quot;snakemake/data/genome.fa&quot;,keep_local=True),
        AS.remote(&quot;snakemake/data/samples/A.fastq&quot;)
    output:
        AS.remote(&quot;snakemake/mapped_reads_sas_ud/A.bam&quot;)
    shell:
        &quot;bwa mem {input} | samtools view -Sb - &gt; {output}&quot;

and the error message is the following:
Uploading to remote: snakemake/mapped_reads_sas_ud/A.bam HttpResponseError: This request is not authorized to perform this operation.

I'm using snakemake version 7.12.0
Any idea what I could be doing wrong?
",-1,-1,-1.0
73288514,Perl installed via conda snakemake shell error: Can´t open perl script .. no such file or directory,"I´m currently writing a snakemake pipeline, for which I want to include a perl script.
The script is not written by me, but from a github page. I never worked with perl before.
I installed perl (5.32.1) via conda. I have installed miniconda and am working on my universities unix server.
The code for my perl script rule looks like this:
rule r1_filter5end:
input:
    config[&quot;arima_mapping&quot;] + &quot;unprocessed_bam/{sample}_R1.sam&quot;
output:
    config[&quot;arima_mapping&quot;] + &quot;filtered_bam/{sample}_R1.bam&quot;
params:
conda:
    &quot;../envs/arima_mapping.yaml&quot;
log:
    config[&quot;logs&quot;] + &quot;arima_mapping/r1_filter5end/{sample}_R1.log&quot;
threads:
    12
shell:
    &quot;samtools view --threads {threads} -h {input} -b | perl ../scripts/filter_five_end.pl | samtools -b -o {output} 2&gt; log&quot;

When I run this I receive the following error:

Can't open perl script &quot;../scripts/filter_five_end.pl&quot;: no such file
or directory found

From what I learned while researching is that the 1. line of a perl script sets the path to my perl executable. The script I downloaded had the following path:
#!/usr/bin/perl

And since I use perl installed via conda this is probably wrong. So I set the path to:
#!/home/mi/my_user/miniconda3/bin/perl

However this did still not work, regardless of if I call
perl ../scripts/filter_five_end.pl

or
../scripts/filter_five_end.pl

Maybe it´s just not possible to run perl scripts via snakemake?
Anyone who had encountered this specific similar case?^^
",-1,-1,-1.0
73313592,Manually created snakemake wildcards not used/recognized,"I've been trying to manually create snakemake wildcards by importing a tab-delimited file that looks as follows:

dataset   sample  species frr
PRJNA493818_GSE120639_SRP162872    SRR7942395_GSM3406786_sAML_Control_1    Homo_sapiens    1
PRJNA493818_GSE120639_SRP162872   SRR7942395_GSM3406786_sAML_Control_1    Homo_sapiens    2
PRJNA362883_GSE93946_SRP097621    SRR5195524_GSM2465521_KrasT_45649_NoDox Mus_musculus    1
PRJNA362883_GSE93946_SRP097621    SRR5195524_GSM2465521_KrasT_45649_NoDox Mus_musculus    2

This is how my snakemake file looks like (minimal example):
import pandas as pd
import os

# --- Importing Configuration Files --- #
configfile: &quot;/DATA/config/config.yaml&quot;

table_cols = ['dataset','sample','species','frr']
table_samples = pd.read_table('/DATA/config/samples.tsv', header=0, sep='\t', names=table_cols)
DATASET = table_samples.dataset.values.tolist()
SAMPLE = table_samples['sample'].values.tolist()
SPECIES = table_samples.species.values.tolist()
FRR = table_samples.frr.values.tolist()
print(DATASET,SAMPLE,SPECIES,FRR)

rule all:
        input:
                expand(config[&quot;project_path&quot;]+&quot;results/{dataset}/rawQC/{sample}_{species}_RNA-Seq_{frr}_fastqc.html&quot;, zip, dataset=DATASET, sample=SAMPLE, species=SPECIES, frr=FRR)

## fastq files quality control
rule rawFastqc:
        input:
                rawread=config[&quot;project_path&quot;]+&quot;resources/raw_datasets/{dataset}/{sample}_{species}_RNA-Seq_{frr}.fastq.gz&quot;
        output:
                zip=config[&quot;project_path&quot;]+&quot;results/{dataset}/rawQC/{sample}_{species}_RNA-Seq_{frr}_fastqc.zip&quot;,
                html=config[&quot;project_path&quot;]+&quot;results/{dataset}/rawQC/{sample}_{species}_RNA-Seq_{frr}_fastqc.html&quot;
        threads:
                12
        params:
                path=config[&quot;project_path&quot;]+&quot;results/{dataset}/rawQC/&quot;
        conda:
                &quot;envs/bulkRNAseq.yaml&quot;
        shell:
                &quot;&quot;&quot;
                fastqc {input.rawread} --threads {threads} -o {params.path}
                &quot;&quot;&quot;

When I run:
snakemake -s test --use-conda -n -p

This is the output:
['PRJNA493818_GSE120639_SRP162872', 'PRJNA493818_GSE120639_SRP162872', 'PRJNA362883_GSE93946_SRP097621', 'PRJNA362883_GSE93946_SRP097621'] ['SRR7942395_GSM3406786_sAML_Control_1', 'SRR7942395_GSM3406786_sAML_Control_1', 'SRR5195524_GSM2465521_KrasT_45649_NoDox', 'SRR5195524_GSM2465521_KrasT_45649_NoDox'] ['Homo_sapiens', 'Homo_sapiens', 'Mus_musculus', 'Mus_musculus'] [1, 2, 1, 2]
Building DAG of jobs...
Job counts:
    count   jobs
    1   all
    4   rawFastqc
    5

[Thu Aug 11 00:57:30 2022]
rule rawFastqc:
    input: /DATA/resources/raw_datasets/PRJNA362883_GSE93946_SRP097621/SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus_musculus_RNA-Seq_1.fastq.gz
    output: /DATA/results/PRJNA362883_GSE93946_SRP097621/rawQC/SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus_musculus_RNA-Seq_1_fastqc.zip, /DATA/results/PRJNA362883_GSE93946_SRP097621/rawQC/SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus_musculus_RNA-Seq_1_fastqc.html
    jobid: 3
    wildcards: dataset=PRJNA362883_GSE93946_SRP097621, sample=SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus, species=musculus, frr=1
    threads: 12


        fastqc /DATA/resources/raw_datasets/PRJNA362883_GSE93946_SRP097621/SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus_musculus_RNA-Seq_1.fastq.gz --threads 12 -o /DATA/results/PRJNA362883_GSE93946_SRP097621/rawQC/
        

[Thu Aug 11 00:57:30 2022]
rule rawFastqc:
    input: /DATA/resources/raw_datasets/PRJNA493818_GSE120639_SRP162872/SRR7942395_GSM3406786_sAML_Control_1_Homo_sapiens_RNA-Seq_1.fastq.gz
    output: /DATA/results/PRJNA493818_GSE120639_SRP162872/rawQC/SRR7942395_GSM3406786_sAML_Control_1_Homo_sapiens_RNA-Seq_1_fastqc.zip, /DATA/results/PRJNA493818_GSE120639_SRP162872/rawQC/SRR7942395_GSM3406786_sAML_Control_1_Homo_sapiens_RNA-Seq_1_fastqc.html
    jobid: 1
    wildcards: dataset=PRJNA493818_GSE120639_SRP162872, sample=SRR7942395_GSM3406786_sAML_Control_1_Homo, species=sapiens, frr=1
    threads: 12


        fastqc /DATA/resources/raw_datasets/PRJNA493818_GSE120639_SRP162872/SRR7942395_GSM3406786_sAML_Control_1_Homo_sapiens_RNA-Seq_1.fastq.gz --threads 12 -o /DATA/results/PRJNA493818_GSE120639_SRP162872/rawQC/
        

[Thu Aug 11 00:57:30 2022]
rule rawFastqc:
    input: /DATA/resources/raw_datasets/PRJNA362883_GSE93946_SRP097621/SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus_musculus_RNA-Seq_2.fastq.gz
    output: /DATA/results/PRJNA362883_GSE93946_SRP097621/rawQC/SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus_musculus_RNA-Seq_2_fastqc.zip, /DATA/results/PRJNA362883_GSE93946_SRP097621/rawQC/SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus_musculus_RNA-Seq_2_fastqc.html
    jobid: 4
    wildcards: dataset=PRJNA362883_GSE93946_SRP097621, sample=SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus, species=musculus, frr=2
    threads: 12


        fastqc /DATA/resources/raw_datasets/PRJNA362883_GSE93946_SRP097621/SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus_musculus_RNA-Seq_2.fastq.gz --threads 12 -o /DATA/results/PRJNA362883_GSE93946_SRP097621/rawQC/
        

[Thu Aug 11 00:57:30 2022]
rule rawFastqc:
    input: /DATA/resources/raw_datasets/PRJNA493818_GSE120639_SRP162872/SRR7942395_GSM3406786_sAML_Control_1_Homo_sapiens_RNA-Seq_2.fastq.gz
    output: /DATA/results/PRJNA493818_GSE120639_SRP162872/rawQC/SRR7942395_GSM3406786_sAML_Control_1_Homo_sapiens_RNA-Seq_2_fastqc.zip, /DATA/results/PRJNA493818_GSE120639_SRP162872/rawQC/SRR7942395_GSM3406786_sAML_Control_1_Homo_sapiens_RNA-Seq_2_fastqc.html
    jobid: 2
    wildcards: dataset=PRJNA493818_GSE120639_SRP162872, sample=SRR7942395_GSM3406786_sAML_Control_1_Homo, species=sapiens, frr=2
    threads: 12


        fastqc /DATA/resources/raw_datasets/PRJNA493818_GSE120639_SRP162872/SRR7942395_GSM3406786_sAML_Control_1_Homo_sapiens_RNA-Seq_2.fastq.gz --threads 12 -o /DATA/results/PRJNA493818_GSE120639_SRP162872/rawQC/
        

[Thu Aug 11 00:57:30 2022]
localrule all:
    input: /DATA/results/PRJNA493818_GSE120639_SRP162872/rawQC/SRR7942395_GSM3406786_sAML_Control_1_Homo_sapiens_RNA-Seq_1_fastqc.html, /DATA/results/PRJNA493818_GSE120639_SRP162872/rawQC/SRR7942395_GSM3406786_sAML_Control_1_Homo_sapiens_RNA-Seq_2_fastqc.html, /DATA/results/PRJNA362883_GSE93946_SRP097621/rawQC/SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus_musculus_RNA-Seq_1_fastqc.html, /DATA/results/PRJNA362883_GSE93946_SRP097621/rawQC/SRR5195524_GSM2465521_KrasT_45649_NoDox_Mus_musculus_RNA-Seq_2_fastqc.html
    jobid: 0

Job counts:
    count   jobs
    1   all
    4   rawFastqc
    5
This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.

It's clear that print(DATASET,SAMPLE,SPECIES,FRR) produces my desired wildcard values:
['PRJNA493818_GSE120639_SRP162872', 'PRJNA493818_GSE120639_SRP162872', 'PRJNA362883_GSE93946_SRP097621', 'PRJNA362883_GSE93946_SRP097621'] ['SRR7942395_GSM3406786_sAML_Control_1', 'SRR7942395_GSM3406786_sAML_Control_1', 'SRR5195524_GSM2465521_KrasT_45649_NoDox', 'SRR5195524_GSM2465521_KrasT_45649_NoDox'] ['Homo_sapiens', 'Homo_sapiens', 'Mus_musculus', 'Mus_musculus'] [1, 2, 1, 2]

However subsequently snakemake does not take these into account and produces the wrong wildcard values, despite the fact I'm not using glob_wildcards.
I'm clearly missing something, but I can't figure out what I'm doing wrong. I've also looked into the following post: Manually create snakemake wildcards .
Help would be much appreciated!
",1,-1,-1.0
73314292,Unable to install snakemake via conda,"I am trying to install snakemake as instructed by the tutorial:
conda install -n base -c conda-forge mamba
conda activate base
mamba create -c conda-forge -c bioconda -n snakemake snakemake

But after the third line I get this error:
Traceback (most recent call last):
  File &quot;/kusers/ancillary/mradzieta/anaconda3/bin/mamba&quot;, line 11, in &lt;module&gt;
    sys.exit(main())
  File &quot;/kusers/ancillary/mradzieta/anaconda3/lib/python3.8/site-packages/mamba/mamba.py&quot;, line 848, in main
    from conda.common.compat import ensure_text_type, init_std_stream_encoding
ImportError: cannot import name 'init_std_stream_encoding' from 'conda.common.compat' (/kusers/ancillary/mradzieta/anaconda3/lib/python3.8/site-packages/conda/common/compat.py)

",-1,-1,-1.0
73390841,Can I run a try-except block (or similar) inside a Snakemake rule?,"I trying to run a snakemake workflow. What I want to do is that if a given rule (rule a) works out send an email with a small text saying so, and if it fails then send another different email indicating an ERROR. I was wondering, is there a way in snakemake to run something similar to a try-except python block?
I have already tried out the try-except block with some commands inside a shell() directive, but it seems that I am not allowed to run any python code (except part of the block) once I have already written a shell() directive.
",-1,-1,-1.0
73394619,"Snakemake ""Function did not return str or list of str.""","I'm trying to run a RNAseq snakemake pipeline. I'm getting stuck on my input function.
import pandas as pd
import os
import fnmatch
import re
 
# --- Importing Configuration Files --- #
configfile: &quot;/DATA/config/config.yaml&quot;
 
table_cols = ['dataset','sample','species','frr','gtf_version','fa_version']
table_samples = pd.read_table('/DATA/config/samples.tsv', header=0, sep='\t', names=table_cols)
DATASET = table_samples.dataset.values.tolist()
SAMPLE = table_samples['sample'].values.tolist()
SPECIES = table_samples.species.values.tolist()
FRR = table_samples.frr.values.tolist()
GTF_VERSION = table_samples.gtf_version.values.tolist()
FA_VERSION = table_samples.fa_version.values.tolist()
 
print(DATASET,SAMPLE,SPECIES,FRR,GTF_VERSION,FA_VERSION)
 
 
rule all:
        input:
                directory(expand(config[&quot;project_path&quot;]+&quot;resources/starIndex_{species}_{fa_version}_{gtf_version}&quot;,zip, species=SPECIES, fa_version=FA_VERSION, gtf_version=GTF_VERSION)),
                expand(config[&quot;project_path&quot;]+&quot;results/{dataset}/star_aligned_1pass/{sample}_{species}_Aligned.sortedByCoord.out.bam&quot;, zip, dataset=DATASET, sample=SAMPLE, species=SPECIES)
                
wildcard_constraints:
        dataset=&quot;|&quot;.join([re.escape(x) for x in DATASET]),
        sample=&quot;|&quot;.join([re.escape(x) for x in SAMPLE]),
        species=&quot;|&quot;.join([re.escape(x) for x in SPECIES]),
        gtf_version=&quot;|&quot;.join([re.escape(x) for x in GTF_VERSION]),
        fa_version=&quot;|&quot;.join([re.escape(x) for x in FA_VERSION])
 
 
## rule starIndex ##  Create star index if it does not exist yet
rule starIndex:
        priority: 1
        input:
                fasta=expand(config[&quot;project_path&quot;]+&quot;resources/{species}.{fa_version}.dna.primary_assembly.fa&quot;,zip, species=SPECIES, fa_version=FA_VERSION),
                gtf=expand(config[&quot;project_path&quot;]+&quot;resources/{species}.{gtf_version}.gtf&quot;,zip, species=SPECIES, gtf_version=GTF_VERSION)
        output:
                directory(config[&quot;project_path&quot;]+&quot;resources/starIndex_{species}_{fa_version}_{gtf_version}&quot;)
        threads:
                20
        params:
                directory(config[&quot;project_path&quot;]+&quot;resources/starIndex_{species}_{fa_version}_{gtf_version}&quot;)
        conda:
                &quot;envs/DTPedia_bulkRNAseq.yaml&quot;
        shell:
                &quot;&quot;&quot;
                STAR --runThreadN {threads} --runMode genomeGenerate --genomeDir {output} --genomeFastaFiles {input.fasta} --sjdbGTFfile {input.gtf}
                &quot;&quot;&quot;
 
rule star_1pass_alignment:
        priority: 4
        input:
                read1=config[&quot;project_path&quot;]+&quot;resources/raw_datasets/{dataset}/{sample}_{species}_RNA-Seq_1.fastq.gz&quot;,
                read2=config[&quot;project_path&quot;]+&quot;resources/raw_datasets/{dataset}/{sample}_{species}_RNA-Seq_2.fastq.gz&quot;,
                index=determine_species,
                prefix=config[&quot;project_path&quot;]+&quot;results/{dataset}/star_aligned_1pass/{sample}_{species}_&quot;
        output:
                bam=config[&quot;project_path&quot;]+&quot;results/{dataset}/star_aligned_1pass/{sample}_{species}_Aligned.sortedByCoord.out.bam&quot;,
                log=config[&quot;project_path&quot;]+&quot;results/{dataset}/star_aligned_1pass/{sample}_{species}_Log.final.out&quot;,
                sj_1pass=config[&quot;project_path&quot;]+&quot;results/{dataset}/star_aligned_1pass/{sample}_{species}_SJ.out.tab&quot;
        threads:
                12
        conda:
                &quot;envs/DTPedia_bulkRNAseq.yaml&quot;
        shell:
                &quot;&quot;&quot;
                STAR --runMode alignReads --genomeDir {input.index} --genomeLoad LoadAndKeep --outSAMtype BAM SortedByCoordinate --limitBAMsortRAM 10000000000 --limitGenomeGenerateRAM 20000000000 --readFilesIn {input.read1} {input.read2} --runThreadN {threads} --readFilesCommand gunzip -c --outFileNamePrefix {input.prefix}
                &quot;&quot;&quot;

This is the error:
['PRJNA493818_GSE120639_SRP162872', 'PRJNA493818_GSE120639_SRP162872', 'PRJNA362883_GSE93946_SRP097621', 'PRJNA362883_GSE93946_SRP097621'] ['SRR7942395_GSM3406786_sAML_Control_1', 'SRR7942395_GSM3406786_sAML_Control_1', 'SRR5195524_GSM2465521_KrasT_45649_NoDox', 'SRR5195524_GSM2465521_KrasT_45649_NoDox'] ['Homo_sapiens', 'Homo_sapiens', 'Mus_musculus', 'Mus_musculus'] [1, 2, 1, 2] ['GRCh38.106', 'GRCh38.106', 'GRCm39.107', 'GRCm39.107'] ['GRCh38', 'GRCh38', 'GRCm39', 'GRCm39']

Building DAG of jobs...
WorkflowError in line 113 of /DATA/workflow/snakefileV21:
Function did not return str or list of str.

I for example tried modifying the line after return without success and the same error output:
# function determine_species_fasta # function for determining fasta input of correct species to rule starIndex
def determine_species(wildcards):
        read1 = config[&quot;project_path&quot;]+&quot;resources/raw_datasets/{wildcards.dataset}/{wildcards.sample}_{wildcards.species}_RNA-Seq_1.fastq.gz&quot;
        if fnmatch.fnmatch(read1, '*Homo_sapiens*'):
                return &quot;/DATA/resources/starIndex_Homo_sapiens_GRCh38_GRCh38.106&quot;
        elif fnmatch.fnmatch(read1, '*Mus_musculus*'):
                return &quot;/DATA/resources/starIndex_Mus_musculus_GRCm39_GRCm39.107&quot;  

Perhaps the wildcards in read1 = config[&quot;project_path&quot;]+&quot;resources/raw_datasets/{wildcards.dataset}/{wildcards.sample}_{wildcards.species}_RNA-Seq_1.fastq.gz&quot; are not properly filled in? I also tried unpack() without succes https://snakemake.readthedocs.io/en/v6.0.0/snakefiles/rules.html#input-functions-and-unpack.
I hope you could help (:
EDIT 1
I've changed the code to this after suggestions from @SultanOrazbayev. This snakemake pipeline analyzes RNAseq data from mice and humans. This python input function determines which species' starIndex to use and pastes the relevant directory (not file) that was outputted in the rule starIndex.:
# function determine_species_fasta # function for determining fasta input of correct species to rule starIndex
def determine_species(wildcards):
        read1 = config[&quot;project_path&quot;]+&quot;resources/raw_datasets/{wildcards.dataset}/{wildcards.sample}_{wildcards.species}_RNA-Seq_1.fastq.gz&quot;
        if fnmatch.fnmatch(read1, '*Homo_sapiens*'):
                return &quot;/DATA/resources/starIndex_Homo_sapiens_GRCh38_GRCh38.106&quot;
        elif fnmatch.fnmatch(read1, '*Mus_musculus*'):
                return &quot;/DATA/resources/starIndex_Mus_musculus_GRCm39_GRCm39.107&quot;
        else:
                raise ValueError(f&quot;Wildcards do not match the desired pattern: {wildcards}&quot;)


now give this error:
(base) @darwin:/DATA/workflow$ snakemake -s snakefileV21 --use-conda 
['PRJNA493818_GSE120639_SRP162872', 'PRJNA493818_GSE120639_SRP162872', 'PRJNA362883_GSE93946_SRP097621', 'PRJNA362883_GSE93946_SRP097621'] ['SRR7942395_GSM3406786_sAML_Control_1', 'SRR7942395_GSM3406786_sAML_Control_1', 'SRR5195524_GSM2465521_KrasT_45649_NoDox', 'SRR5195524_GSM2465521_KrasT_45649_NoDox'] ['Homo_sapiens', 'Homo_sapiens', 'Mus_musculus', 'Mus_musculus'] [1, 2, 1, 2] ['GRCh38.106', 'GRCh38.106', 'GRCm39.107', 'GRCm39.107'] ['GRCh38', 'GRCh38', 'GRCm39', 'GRCm39']

Building DAG of jobs...
InputFunctionException in line 115 of /DATA/workflow/snakefileV21:
ValueError: Wildcards do not match the desired pattern: PRJNA493818_GSE120639_SRP162872 SRR7942395_GSM3406786_sAML_Control_1 Homo_sapiens
Wildcards:
dataset=PRJNA493818_GSE120639_SRP162872
sample=SRR7942395_GSM3406786_sAML_Control_1
species=Homo_sapiens

So I suspect this line is not properly executed read1 = config[&quot;project_path&quot;]+&quot;resources/raw_datasets/{wildcards.dataset}/{wildcards.sample}_{wildcards.species}_RNA-Seq_1.fastq.gz&quot;
Any suggestions to correct this?
",1,-1,-1.0
73525834,snakemake: Problem with --rerun-triggers flag and bash variable,"I have a problem when I try to provide the --rerun-triggers flag as a bash variable.
My command is
snakemake $snakemake_extra -pr --snakefile Snakefile --configfile config.yaml -c 20 -n

and snakemake_extra is a bash variable defined as
snakemake_extra=&quot;--rerun-triggers {mtime,input,params}&quot;

I get the following error:

snakemake: error: argument --rerun-triggers: invalid choice: '{mtime,input,params}' (choose from 'mtime', 'params', 'input', 'software-env', 'code')

The problem seems to be that snakemake(?) adds single-quotes before and after the {}.
When I insert the --rerun-triggers flag directly (without bash variable) it works fine. I need the bash variable however and can also not use a snakemake profile yaml.
Is there any possible workaround?
I am using snakemake version 7.12.1.
Thanx,
Carlus
",-1,-1,-1.0
73593220,Snakemake Salsa2 CreateCondaEnvironmentException with python 2.7. and openssl,"I´m trying to integrate Salsa2 into my workflow. Since this tool runs on python 2.7 I definitely want to create a separate conda environment for this purpose. My yaml file looks like this:
channels:
  - conda-forge
  - anaconda
  - bioconda

dependencies:
  - python = 2.7.18
  - salsa2 = 2.3.
  - boost = 1.70.0
  - boost-cpp = 1.74.0
  - networkx = 1.11

However when I run my code snakemake --use-conda --cores 15 I receive the following error:
Creating conda environment envs/salsa2.yaml...
Downloading and installing remote packages.
CreateCondaEnvironmentException:
Could not create conda environment from /buffer/path/to/my/assembly_downstream/workflow/rules/../envs/salsa2.yaml:
Command:
mamba env create --quiet --file &quot;/buffer/path/to/my/assembly_downstream/workflow/.snakemake/conda/bf0438d2754e7799a3cb353c72e2ed4b.yaml&quot; --prefix &quot;/buffer/path/to/my/assembly_downstream/workflow/.snakemake/conda/bf0438d2754e7799a3cb353c72e2ed4b&quot;
Output:
Encountered problems while solving:
  - nothing provides openssl &gt;=1.1.1,&lt;1.1.2.0a0 needed by python-3.6.7-h0371630_0

The simple solution would probably be to add openssl 1.1.1 to my yaml. Like this:
channels:
  - conda-forge
  - anaconda
  - bioconda
dependencies:
  - python = 2.7.18
  - salsa2 = 2.3.
  - boost = 1.70.0
  - boost-cpp = 1.74.0
  - networkx = 1.11
  - openssl = 1.1.1

But this fixes nothing and I still receive the error. Creating this exact environment manually works as expected, therefore I guess it's a conflict with snakemake since it runs on python3. But my normal snakemake environment is properly configured and I thought the conda envs are only used for separated rules?
Does anyone know what could cause this error?
",-1,-1,-1.0
73612458,"Snakemake, numerous rules with different sets of wildcards","I would like to write a series of rules, but some rules do not use the same wildcards as other rules. When I try this, Snakemake cannot determine the wildcards.
Suppose I have a workflow like this:

generate separate data files for 30 chromosomes
run a program called &quot;algorithm1&quot; with parameter &quot;foo&quot; on each of these 30 chromosomes
run a different program called &quot;algorithm2&quot; with parameter &quot;bar&quot; on each of these 30 chromosomes
Algorithm 1 and 2 are not related; I want algorithm 1 to run then afterwards algortihm2.

An illustrative workflow which you can run at home (changing user) looks like this:
rule all:
    input:
        [f'/home/user/snakemake_test_220905/algorithm2/parameter1_{alg2_param1}/littlefile{chrom}_analysis.txt' for chrom in range(0,10) for alg1_param1 in ['foo'] for alg2_param1 in ['bar']]

    
rule write_separate_vcf:
    input: 
        infile = '/home/user/snakemake_test_220905/big_file.vcf'
    output: 
        outfile = '/home/user/snakemake_test_220905/littlefile{chrom}.vcf'
    run:
        shell(
            f'echo &quot;file{wildcards.chrom}&quot; &gt; {output.outfile}'
        )

rule algorithm_1:
    input: 
        infile = '/home/user/snakemake_test_220905/littlefile{chrom}.vcf'
    output:
        outfile = '/home/user/snakemake_test_220905/algorithm1/parameter1_{alg1_param1}/littlefile{chrom}_analysis.txt'
    run:
        shell(
            f'echo &quot;parameter_{wildcards.alg1_param1}_{input.infile}&quot; &gt; {output.outfile}'
        )

rule algorithm_2:
    input: 
        infile = '/home/user/snakemake_test_220905/littlefile{chrom}.vcf',
        inile_flag = '/home/user/snakemake_test_220905/algorithm1/parameter1_{alg1_param1}/littlefile{chrom}_analysis.txt' # force dependency
    output:
        outfile = '/home/user/snakemake_test_220905/algorithm2/parameter1_{alg2_param1}/littlefile{chrom}_analysis.txt'
    run:
        shell(
            f'echo &quot;parameter_{wildcards.alg2_param1}_{input.infile}&quot; &gt; {output.outfile}'
        )

(use for i in {1..10}; do echo vcf$i &gt;&gt; big_file.vcf; done to create the big_file.vcf)
This of course fails, the returned error is:
WildcardError in line 29 of /home/user/snakemake_test_220905/Snakefile:
Wildcards in input files cannot be determined from output files:
'alg1_param1'

I would like all the files to be written at once, but can't figure out an elegant way to do it. Any ideas?
",-1,-1,-1.0
73656752,snakemake program not running all functions?,"configfile: &quot;config.yaml&quot;
DATA = config['DATA_DIR']
BIN = config['BIN']
JASPAR = config['DATA_DIR']
RESULTS = config['RESULTS']
# JASPAR = &quot;{0}/JASPAR2020&quot;.format(DATA)
JASPARS, ASSEMBLIES, BATCHES, TFS, BEDS = glob_wildcards(os.path.join(DATA, &quot;{jaspar}&quot;, &quot;{assembly}&quot;, &quot;TFs&quot;, &quot;{batch}&quot;, &quot;{tf}&quot;, &quot;{bed}.bed&quot;))
rule all:
input:
    expand (os.path.join(RESULTS, &quot;{jaspar}&quot;, &quot;{assembly}&quot;, &quot;LOLA_dbs&quot;, &quot;JASPAR2020_LOLA_{batch}.RDS&quot;), jaspar = JASPARS, assembly = ASSEMBLIES, batch = BATCHES)

rule createdb:
input:
    files = expand(os.path.join(RESULTS, &quot;{jaspar}&quot;, &quot;{assembly}&quot;, &quot;data&quot;, &quot;{batch}&quot;, &quot;{tf}&quot;, &quot;regions&quot;, &quot;{bed}.bed&quot;), zip, jaspar = JASPARS, assembly = ASSEMBLIES, batch = BATCHES, tf = TFS, bed = BEDS)
output:
    os.path.join(RESULTS, &quot;{jaspar}&quot;, &quot;{assembly}&quot;, &quot;LOLA_dbs&quot;, &quot;JASPAR2020_LOLA_{batch}.RDS&quot;)
    
shell:
    &quot;&quot;&quot;
    R --vanilla --slave --silent -f {BIN}/create_lola_db.R \
    --args {RESULTS}/{wildcards.jaspar}/{wildcards.assembly}/data/{wildcards.batch} \
    {output};
    &quot;&quot;&quot;

Why my snakemake program is not considering &quot;createdb&quot; rule. It is only considering &quot;all&quot;. Can someone please help me with this?
",1,-1,-1.0
73685754,Can Snakemake run cluster jobs on a user-defined subset of data?,"I was wondering whether there is any possibility to create cluster jobs with snakemake which will run a batch of tasks.
Assuming the following input:
image_00.tif
image_01.tif
...
image_99.tif

I would like to create the following output:
meas_image_00.txt
meas_image_02.txt
...
meas_image_99.txt

But instead of creating 100 jobs, I would like to submit 5 jobs with 20 input files each, creating 20 output files.
I thought about the group flag but I am not sure whether this is really doing what I want.
The only other way I can imagine is to additionally create a batch file as output and ignore that there will be per image outputs. The expected files then would be something like this:
batch_1.summary
batch_2.summary
batch_3.summary
batch_4.summary
batch_5.summary

But I would loose control if the meas_image*.txt are created at all.
Does anybody have an advice how to solve that?
Edit:
I played around with an example using rules generated by loops. I just concentrated on creating output in batches for now but something goes wrong. This is my snakefile:
data=list(range(0,100))
samples = [&quot;image_&quot; + str(i).zfill(2) for i in data]
batch_size = 5


def chunks(lst, n):
    &quot;&quot;&quot;Yield successive n-sized chunks from lst.&quot;&quot;&quot;
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def chunklist():
    chunk_dict = {}
    for i, chunk in enumerate(chunks(samples, batch_size)):
        chunk_dict[i] = chunk
    return chunk_dict

cd = chunklist()

rule all:
  input: 'rule_3.out'
  
for i,chunk in cd.items():
    rule:  # batch rule
        name: &quot;my_rule_{}&quot;.format(i)
        output: expand(&quot;{sample}.txt&quot;, sample=chunk)
        group: 'batch_group'
        run: 
            print(output)
            print(chunk)
            for file in chunk:
                print(&quot;Creating {} of {}&quot;.format(file, chunk))
                touch('{}.txt'.format(file))

for i in cd:
    rule:  # batch rule
        name: &quot;batch_rule_{}&quot;.format(i)
        input: expand('{sample}.txt', sample=cd[i])
        group: 'batch_group'
        output: touch('{}.group'.format(i))


rule rule3:
  input: 
    files=expand('{sample}.txt', sample=samples),
    batches=expand('{i}.group', i=range(len(samples) // batch_size - 1))  # should be ceil in general
  output:
    'rule_3.out'
  shell: 'cat {input.files} &gt; {output}'

And this is the problem, I get:
rule my_rule_10:
    output: image_50.txt, image_51.txt, image_52.txt, image_53.txt, image_54.txt
    jobid: 12
    reason: Missing output files: image_51.txt, image_54.txt, image_53.txt, image_52.txt, image_50.txt
    resources: tmpdir=/tmp

image_50.txt image_51.txt image_52.txt image_53.txt image_54.txt
['image_95', 'image_96', 'image_97', 'image_98', 'image_99']
Creating image_95 of ['image_95', 'image_96', 'image_97', 'image_98', 'image_99']
Creating image_96 of ['image_95', 'image_96', 'image_97', 'image_98', 'image_99']
Creating image_97 of ['image_95', 'image_96', 'image_97', 'image_98', 'image_99']
Creating image_98 of ['image_95', 'image_96', 'image_97', 'image_98', 'image_99']
Creating image_99 of ['image_95', 'image_96', 'image_97', 'image_98', 'image_99']

It's strange to me as output is generated by chunk but then differs from the variable chunk...
I have no clue what goes wrong. This example should be executable for everybody. I would be happy to get help.
",1,-1,-1.0
73686094,Snakemake doesn't infer wildcards with input function,"I'm trying to write a snakemake workflow that where the wildcards are 'swapped' using an input function.  Essentially, there are two conditions ('A' and 'B'), and a number of files are generated for each (A/1.txt, A/2.txt, etc, B/1.txt, B/2.txt, etc)  The number of files is always the same between the conditions, but unknown at the start of the workflow.  There is an intermediate step, and then I want to use the intermediate files from one condition with the original files from the other condition.  I wrote a simple snakefile that illustrates what I want to do:
dirs = (&quot;A&quot;, &quot;B&quot;)

wildcard_constraints:
  DIR='|'.join(dirs),
  sample='\d+'

rule all:
    input:
        expand(&quot;{DIR}_done&quot;, DIR=dirs)

checkpoint create_files:
    output:
        directory(&quot;{DIR}/&quot;)
    shell:
        &quot;&quot;&quot;
        mkdir {output};
          N=10;
        for D in $(seq $N); do
            touch {output}/$D.txt
        done
        &quot;&quot;&quot;

rule intermediate:
  input:
    &quot;{DIR}/{SAMPLE}.txt&quot;
  output:
    intermediate = &quot;{DIR}_intermediate/{SAMPLE}.txt&quot;
  shell:
    &quot;touch {intermediate}&quot;

def swap_input(wildcards):
  
  if wildcards.DIR == 'A':
    return f'B_intermediate/{wildcards.SAMPLE}.txt'
    
  if wildcards.DIR == 'B':
    return f'A_intermediate/{wildcards.SAMPLE}.txt'

rule swap:
  input:
    original = &quot;{DIR}/{SAMPLE}.txt&quot;,
    intermediate = swap_input
  output:
    swapped=&quot;{DIR}_swp/{SAMPLE}.txt&quot;
  shell:
    &quot;touch {output}&quot;

def get_samples(wildcards):

    checkpoint_dir = checkpoints.create_files.get(**wildcards).output[0]

    samples = glob_wildcards(os.path.join(checkpoint_dir, &quot;{sample}.txt&quot;)).sample
    return samples

rule done:
  input:
    lambda wildcards: expand(&quot;{DIR}_swp/{SAMPLE}.txt&quot;, SAMPLE=get_samples(wildcards), allow_missing=True)
  output:
    &quot;{DIR}_done&quot;
  shell:
    &quot;touch {output}&quot;

However, snakemake doesn't appear to be correctly inferring the wildcards.  Perhaps I am misunderstanding something about the way snakemake infers wildcards. I get the error:
MissingInputException in rule intermediate in line 38 of Snakefile:
Missing input files for rule intermediate:
    output: A_intermediate/1.txt
    wildcards: DIR=A, SAMPLE=1
    affected files:
        A/1.txt

But the file A/1.txt should be created by the first rule create_files.
I thought perhaps this might be something to do with the checkpoint not being completed, but if I add checkpoint_dir = checkpoints.create_files.get(**wildcards).output[0] at the start of the input function swap_input, the error is still the same.
Is there a way to get this workflow to work?
",-1,-1,-1.0
73773325,Using multiple wildcards in snakemake for differentiate replicates in a tsv file,"I've successfully implemented a Snakemake workflow and now want to add the possibility to merge specific samples, whether they are technical or biological replicates. In this case, I was told I should use multiple wildcards and work with a proper data frame. However, I'm pretty unsure what the syntax would even look like and how I can differentiate between entries properly.
My samples.tsv used to look like this:




sample
fq1
fq2




bCalAnn1_1_1
path/to/file
path/to/file


bCalAnn1_1_2
path/to/file
path/to/file


bCalAnn2_1_1
path/to/file
path/to/file


bCalAnn2_1_2
path/to/file
path/to/file


bCalAnn2_2_1
path/to/file
path/to/file


bCalAnn2_3_1
path/to/file
path/to/file




Now it looks like this:




sample
bio_unit
tech_unit
fq1
fq2




bCalAnn1
1
1
path/to/file
path/to/file


bCalAnn1
1
2
path/to/file
path/to/file


bCalAnn2
1
1
path/to/file
path/to/file


bCalAnn2
1
2
path/to/file
path/to/file


bCalAnn2
2
1
path/to/file
path/to/file


bCalAnn2
3
1
path/to/file
path/to/file




Where bio_unit contains the index of the library for one sample and tech_unit is the index of the lanes for one library.
My Snakefile looks like this:
import pandas as pd
import os
import yaml

configfile: &quot;config.yaml&quot;

samples = pd.read_table(config[&quot;samples&quot;], index_col=&quot;sample&quot;)


rule all:
    input:
        expand(config[&quot;arima_mapping&quot;] + &quot;{sample}_{unit_bio}_{unit_tech}_R1.bam&quot;, 
               sample=samples.sample, unit_bio=samples.unit_bio, unit_tech=samples.unit_tech)



rule r1_mapping:
    input:
        read1 = lambda wc: 
                samples[(samples.sample == wc.sample) &amp; (samples.unit_bio == wc.unit_bio) &amp; (samples.unit_tech == wc.unit_tech)].fq1.iloc[0],
        ref = config[&quot;PacBio_assembly&quot;],
        linker = config[&quot;index&quot;] + &quot;pac_bio_assembly.fna.amb&quot;
    output:
        config[&quot;arima_mapping&quot;] + &quot;unprocessed_bam/({sample},{unit_bio},{unit_tech})_R1.bam&quot;
    params:
    conda:
        &quot;../envs/arima_mapping.yaml&quot;
    log:
        config[&quot;logs&quot;] + &quot;arima_mapping/mapping/({sample},{unit_bio},{unit_tech})_R1.log&quot;
    threads:
        12
    shell:
        &quot;bwa mem -t {threads} {input.ref} {input.read1} | samtools view --threads {threads} -h -b -o {output} 2&gt; {log}&quot;


r1_mapping is basically my first rule which requires no differentiating between replicates. Therefore, I would want to use it for every row. I experimented a little bit with the syntax but ultimately ran into this error:
MissingInputException in rule r1_mapping in line 20 of /home/path/to/assembly_downstream/workflow/rules/arima_mapping.smk:
Missing input files for rule r1_mapping:
    output: /home/path/to/assembly_downstream/intermed/arima_mapping/unprocessed_bam/('bCalAnn1', 1, 1)_R1.bam
    wildcards: sample='bCalAnn1', unit_bio= 1, unit_tech= 1
    affected files:
        /home/path/to/assembly_downstream/data/arima_HiC/('bCalAnn1', 1, 1)_R1.fastq.gz

Do I even read the table properly? I´m currently very confused, can anyone give me a hint on where to start fixing this? 
Fixed by changing:
samples = pd.read_table(config[&quot;samples&quot;], index_col=[&quot;sample&quot;,&quot;unit_bio&quot;,&quot;unit_tech&quot;])

to
samples = pd.read_table(config[&quot;samples&quot;], index_col=&quot;sample&quot;)

EDIT:
New error
Missing input files for rule all:
    affected files:
        /home/path/to/assembly_downstream/intermed/arima_mapping/&lt;bound method NDFrame.sample of           unit_bio  ...                                                fq2
sample              ...                                                   
bCalAnn1         1  ...  /home/path/to/assembly_downstream/data/ari...
bCalAnn1         1  ...  /home/path/to/assembly_downstream/data/ari...
bCalAnn2         1  ...  /home/path/to/assembly_downstream/data/ari...
bCalAnn2         1  ...  /home/path/to/assembly_downstream/data/ari...
bCalAnn2         2  ...  /home/path/to/assembly_downstream/data/ari...
bCalAnn2         3  ...  /home/path/to/assembly_downstream/data/ari...

[6 rows x 4 columns]&gt;_3_2_R1.bam
        /home/path/to/assembly_downstream/intermed/arima_mapping/&lt;bound method NDFrame.sample of           unit_bio  ...                                                fq2
sample              ...                                                   
bCalAnn1         1  ...  /home/path/to/assembly_downstream/data/ari...
bCalAnn1         1  ...  /home/path/to/assembly_downstream/data/ari...
bCalAnn2         1  ...  /home/path/to/assembly_downstream/data/ari...
bCalAnn2         1  ...  /home/path/to/assembly_downstream/data/ari...
bCalAnn2         2  ...  /home/path/to/assembly_downstream/data/ari...
bCalAnn2         3  ...  /home/path/to/assembly_downstream/data/ari...

[6 rows x 4 columns]&gt;_3_1_R1.bam
        /home/path/to/assembly_downstream/intermed/arima_mapping/&lt;bound method NDFrame.sample of           unit_bio  ...  

This goes on to a total of 6 times which sounds like I´m currently getting all rows 6 times each red. I guess this has something to do with how expand() works..? I´m gonna keep researching for now.
",-1,-1,-1.0
73820507,How to get Snakemake and CellRanger Count to work with multiple samples,"I have a snakemake rule that is trying to pull from this directory called merged. This contains two separate scRNA datasets. I want to utilize snakemake in conjunction with cellranger to run any number of samples. I am getting an error below in my cellranger_count rule that I am not understanding and google isnt helping. Could someone please make this a teachable moment?
merged
SC111111-TTATTCGAGG-AGCAGGACAG_merged_R1.fastq.gz  SC222222-TGCGCGGTTT-TTTATCCTTG_merged_R1.fastq.gz
SC111111-TTATTCGAGG-AGCAGGACAG_merged_R2.fastq.gz  SC222222-TGCGCGGTTT-TTTATCCTTG_merged_R2.fastq.gz


rule cellranger_count:
    input: rules.merge_fastqs.output
    output:
        maxtrix_h5 = '{sampleID}_TenXAnalysis/outs/raw_feature_bc_matrix.h5',
        metrics = '{sampleID}_TenXAnalysis/outs/metrics_summary.csv',
        barcodes = '{sampleID}_TenXAnalysis/outs/raw_feature_bc_matrix/barcodes.tsv.gz',
        features = '{sampleID}_TenXAnalysis/outs/raw_feature_bc_matrix/features.tsv.gz',
        matrix = '{sampleID}_TenXAnalysis/outs/raw_feature_bc_matrix/matrix.mtx.gz'
    threads: 16
    params:
        ref = 'PATH/yard/apps/refdata-gex-GRCh38-2020-A',
        #sample_id = '{sampleID}_merged'
    ## id = unique run ID string
    ## fastqs = Path to data
    ## sample = Sample names as specified in the sample sheet
    ## transcriptome = Path to Cell Ranger compatible transcritpome reference
    ## localcores = tells cellragner how many cores to use
    ## localmem = how much mem to use
    shell: &quot;&quot;&quot;
    module load cellranger/6.1.2

    rm -rf {wildcards.sampleID}_TenXAnalysis

    cellranger count --id={wildcards.sampleID}_TenXAnalysis \
        --fastqs={input} \
        --sample={wildcards.sampleID} \
        --transcriptome={params.ref} \
        --localcores={threads} \
        --localmem=128
    &quot;&quot;&quot;

logfile error:
error: Found argument 'merged/SC111111-TTATTCGAGG-AGCAGGACAG_merged_R2.fastq.gz' which wasn't expected, or isn't valid in this context

If you tried to supply `merged/SC111111-TTATTCGAGG-AGCAGGACAG_merged_R2.fastq.gz` as a PATTERN use `-- merged/SC111111-TTATTCGAGG-AGCAGGACAG_merged_R2.fastq.gz

",1,-1,-1.0
73978737,How do I use snakemake --cluster-cancel?,"I currently have a snakemake pipeline running with multiple jobs on a cluster. I want to cancel my jobs early, and the snakemake documentation says that I can use the --cluster-cancel option. However, it doesn't have any example of how to use it. The cluster I am using cancels jobs with qdel. So, I tried using snakemake --cluster-cancel &quot;qdel&quot;, but when I do this it returns the error
snakemake: error: unrecognized arguments: --cluster-cancel
",-1,-1,-1.0
74154402,Snakemake ImportError on Windows 10,"I am trying to set up and run snakemake on Windows 10 following the instructions in the documentation.
I have installed snakemake with conda.
I verify the installation with snakemake --version.
When I try to run a simple workflow (eg. the one in the tutorial) I get the following error:
ImportError: DLL load failed while importing _ctypes: The specified module could not be found.
Can anyone help?
Many thanks,
Martha
snakemake version 7.16.0
conda verison 22.9.0
Snakefile:
rule bwa_map:
    input:
        &quot;data/genome.fa&quot;,
        &quot;data/samples/A.fastq&quot;
    output:
        &quot;mapped_reads/A.bam&quot;
    shell:
        &quot;bwa mem {input} | samtools view -Sb - &gt; {output}&quot;

",-1,-1,-1.0
74226320,"Snakemake tutorial error: ""AttributeError: 'str' object has no attribute 'name' ""","When I try running the first command (snakemake -np mapped_reads/A.bam) from the Step 1 of Snakemake tutorial (https://snakemake.readthedocs.io/en/stable/tutorial/basics.html) I get the following error:
snakemake -np mapped_reads/A.bam
Building DAG of jobs...
Traceback (most recent call last):
  File &quot;/home/jeremi1504/mambaforge/envs/snakemake-tutorial/lib/python3.8/site-packages/snakemake/__init__.py&quot;, line 701, in snakemake
    success = workflow.execute(
  File &quot;/home/jeremi1504/mambaforge/envs/snakemake-tutorial/lib/python3.8/site-packages/snakemake/workflow.py&quot;, line 1066, in execute
    logger.run_info(&quot;\n&quot;.join(dag.stats()))
  File &quot;/home/jeremi1504/mambaforge/envs/snakemake-tutorial/lib/python3.8/site-packages/snakemake/dag.py&quot;, line 2191, in stats
    yield tabulate(rows, headers=&quot;keys&quot;)
  File &quot;/home/jeremi1504/mambaforge/envs/snakemake-tutorial/lib/python3.8/site-packages/tabulate/__init__.py&quot;, line 2048, in tabulate
    list_of_lists, headers = _normalize_tabular_data(
  File &quot;/home/jeremi1504/mambaforge/envs/snakemake-tutorial/lib/python3.8/site-packages/tabulate/__init__.py&quot;, line 1471, in _normalize_tabular_data
    rows = list(map(lambda r: r if _is_separating_line(r) else list(r), rows))
  File &quot;/home/jeremi1504/mambaforge/envs/snakemake-tutorial/lib/python3.8/site-packages/tabulate/__init__.py&quot;, line 1471, in &lt;lambda&gt;
    rows = list(map(lambda r: r if _is_separating_line(r) else list(r), rows))
  File &quot;/home/jeremi1504/mambaforge/envs/snakemake-tutorial/lib/python3.8/site-packages/tabulate/__init__.py&quot;, line 107, in _is_separating_line
    (len(row) &gt;= 1 and row[0] == SEPARATING_LINE)
  File &quot;/home/jeremi1504/mambaforge/envs/snakemake-tutorial/lib/python3.8/site-packages/snakemake/rules.py&quot;, line 1138, in __eq__
    return self.name == other.name and self.output == other.output
AttributeError: 'str' object has no attribute 'name'

I followed all the commands from the set-up section (https://snakemake.readthedocs.io/en/stable/tutorial/setup.html) and everything seems to be installed properly. The required environment is active and I had no problems activating it. I also build my snakefile according to the instructions.
What can be the source of the problem? I understand what causes the usual 'AttributeError' but I do not know why it occurs in a tutorial that is supposed to work without any modifications from the user.
How can I overcome this error? Is it a matter of installing additional dependencies? If so, which ones? Or is the tutorial broken/outdated/malformed?
",-1,-1,-1.0
74340191,How to get the number of total scattergather items in Snakemake scatter/gather?,"I'm trying out Snakemake's scatter/gather inbuilts but am stumbling over how to get the number of total splits configured.
The documentation doesn't mention how I can access that variable as defined in the workflow or passed through CLI.
Docs say I should define a scattergather directive:
scattergather:
    split=8

But how do I get the value of split which is 8 in this case inside my split rule where I would assign it to params.split_total?
rule split:
    input: &quot;input.txt&quot;
    output: scatter.split(&quot;splitted/{scatteritem}.txt&quot;)
    params: split_total = config[&quot;scattergather&quot;][&quot;split&quot;]
    shell: &quot;split -l {params.split_total} input&quot;

This fails with: KeyError 'scattergather'
Am I missing something obvious? This is the docs I'm looking at: KeyError in line 48 of /Users/corneliusromer/code/ncov-ingest/workflow/snakemake_rules/curate.smk:
2 'scattergather'
",-1,-1,-1.0
74343982,module 'asyncio' has no attribute 'coroutine' on snakemake,"I followed the following code to install snakemake on my linux machine :
conda activate base
mamba create -c conda-forge -c bioconda -n snakemake snakemake
conda activate snakemake

Everything was fine, but when I try to run the following code :
snakemake -s Snakemake_job -j n

I get this error :
Building DAG of jobs...
Traceback (most recent call last):
  File &quot;/home/debian/.conda/envs/snakemake/lib/python3.11/site-packages/snakemake/__init__.py&quot;, line 747, in snakemake
    success = workflow.execute(
              ^^^^^^^^^^^^^^^^^
  File &quot;/home/debian/.conda/envs/snakemake/lib/python3.11/site-packages/snakemake/workflow.py&quot;, line 949, in execute
    self.scheduler = JobScheduler(
                     ^^^^^^^^^^^^^
  File &quot;/home/debian/.conda/envs/snakemake/lib/python3.11/site-packages/snakemake/scheduler.py&quot;, line 109, in __init__
    from ratelimiter import RateLimiter
  File &quot;/home/debian/.conda/envs/snakemake/lib/python3.11/site-packages/ratelimiter.py&quot;, line 36, in &lt;module&gt;
    class RateLimiter(object):
  File &quot;/home/debian/.conda/envs/snakemake/lib/python3.11/site-packages/ratelimiter.py&quot;, line 127, in RateLimiter
    __aexit__ = asyncio.coroutine(__exit__)
                ^^^^^^^^^^^^^^^^^
AttributeError: module 'asyncio' has no attribute 'coroutine'

Does someone have an idea of what is going on? I tried to re-install snakemake, or to pip install asyncio without sucess...
",-1,-1,-1.0
74344912,Snakemake with one input but multiple parameters permutations,"I have been trying to wrap my head around this problem which probably has a very easy solution.
I am running a bioinformatics workflow where I have one file as input and I want to run a program on it. However I want that program to be run with multiple parameters. Let me explain.
I have file.fastq and I want to run cutadapt (in the shell) with two flags: --trim and -e. I want to run trim with values --trim 0 and --trim 5. Also I want -e with values -e 0.1 and -e 0.5
Thererfore I want to run the following:
cutadapt file.fastq --trim0 -e0.5  --output ./outputs/trim0_error0.5/trimmed_file.fastq
cutadapt file.fastq --trim5 -e0.5  --output ./outputs/trim5_error0.5/trimmed_file.fastq
cutadapt file.fastq --trim0 -e0.1  --output ./outputs/trim0_error0.1/trimmed_file.fastq
cutadapt file.fastq --trim5 -e0.1  --output ./outputs/trim5_error0.1/trimmed_file.fastq
I thought snakemake would be perfect for this. So far I tried:
E = [0.1, 0.5]
TRIM = [5, 0]

rule cutadapt:
    input:
        &quot;file.fastq&quot;

    output:
        expand(&quot;../outputs/trim{TRIM}_error{E}/trimmed_file.fastq&quot;, E=E, TRIM=TRIM)
    
    params:
        trim = TRIM,
        e = E

    shell:
        &quot;cutadapt {input} -e{params.e} --trim{params.trim} --output {output}&quot;

However  I get an error like  this:
shell:
cutadapt file.fastq -e0.1 0.5 --trim0 5 --output {output}
(one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

So, as you can see, snakemake is not taking each argument of the TRIM and E variables, but putting them together like a string. How could I solve this problem? Thank you in advance
",1,-1,-1.0
74347235,apply a snakemake rule on a list of values in parallel,"I'm currently exploring using snakemake as a workflow tool.
In my specific use case I don't start from a list of files but rather from a list of values that should result in the creation of a list of files.
In my example, I create the files with a small python snippet which works fine but when I want to use those files in parallel in a second rule, they are concatenated into one parameter:

rule all:
    input:
        expand('{file}.bar', file=data)

rule foo:
    output:
        expand('{file}.foo', file=data)
    run:
        for item in data:
            with open(f'{item}.foo', 'w') as fout:
                fout.write('foo')

rule bar:
    input:
        file=expand('{file}.foo', file=data)
    output:
        outfile=expand('{file}.bar', file=data)
    shell:
        &quot;&quot;&quot;echo {output.outfile};echo bar &gt; {output.outfile} &quot;&quot;&quot;

the example prints
&quot;one.bar two.bar three.bar&quot;
at once, so the rule is applied only once,
and then raises an error because the expected output files are not created.
",-1,1,-1.0
74461500,missing input file in snakemake,"below is my snakefile script.
import os
import glob
import yaml

##load config file
configfile: &quot;config/config.yaml&quot;

##ref and query locations
referance = config[&quot;reference_loc&quot;]
query = config[&quot;query_loc&quot;]

##output file location
output = config[&quot;output&quot;]   

##sample file name
suffix = [&quot;.fsa&quot;, &quot;.fasta&quot;, &quot;.fa&quot;, &quot;.fna&quot;, &quot;.fas&quot;]
sample_file = glob.glob(query+&quot;*&quot;+str(suffix))
sample1 = [item.removeprefix(query) for item in sample_file]
sample2 = [item.removesuffix(&quot;.fasta&quot;) for item in sample1]
sample3 = [item.removesuffix(&quot;.fas&quot;) for item in sample2]
sample4 = [item.removesuffix(&quot;.fsa&quot;) for item in sample3]
sample5 = [item.removesuffix(&quot;.fna&quot;) for item in sample4]
sample = [item.removesuffix(&quot;.fa&quot;) for item in sample5]

##referance file name
ref_file = glob.glob(referance+&quot;*&quot;+str(suffix))
ref1 = [item.removeprefix(referance) for item in ref_file]
ref2 = [item.removesuffix(&quot;.fasta&quot;) for item in ref1]
ref3 = [item.removesuffix(&quot;.fas&quot;) for item in ref2]
ref4 = [item.removesuffix(&quot;.fsa&quot;) for item in ref3]
ref5 = [item.removesuffix(&quot;.fna&quot;) for item in ref4]
ref = [item.removesuffix(&quot;.fa&quot;) for item in ref5]

##rules
rule all:
    input:
        expand(output+&quot;final/{ref}_merged.csv&quot;, ref = ref)

rule blast:
    input:
        expand(output+'{sample}_{ref}.txt', sample = sample, ref = ref)

related rules are also found below
rule blast:
    input:
        query=query+'{sample}'+str(suffix),
        ref=referance+'{ref}'+str(suffix)
    output:
        output+'{sample}_{ref}.txt'
    threads:
        workflow.cores
    conda:
        'envs/blast.yaml'
    shell:
        &quot;blastp -subject {input.ref} -query {input.query} -outfmt '6 qacc sacc pident length qcovs evalue mismatch gaps qseq sseq qlen slen sstart send' -out {output} -num_threads {threads}&quot;

rule merge_data:
    input:
        rules.blast.output
    output:
        output+&quot;final/{ref}_merged.csv&quot;
    conda:
        'envs/pandas.yaml'
    scripts:
        ../scripts/pd_blastresults.py'

i get a return message with the following message,
snakemake --cores 14 --use-conda all
Building DAG of jobs...
MissingInputException in line 39 of /home/user/Desktop/newtest/workflow/Snakefile:
Missing input files for rule all:
    affected files:
        /home/user/Desktop/data/results/final/GlpTReferenceNC_000913.3_merged.csv

and if i use the other rule i also get a similar message
snakemake --cores 14 --use-conda blast
Building DAG of jobs...
MissingInputException in line 43 of /home/user/Desktop/newtest/workflow/Snakefile:
Missing input files for rule blast:
    affected files:
        /home/user/Desktop/data/results/GlpTCP014497.1_GlpTReferenceNC_000913.3.txt
        /home/user/Desktop/data/results/GlpTCP063774.1_GlpTReferenceNC_000913.3.txt
        /home/user/Desktop/data/results/GlpTCP014488_GlpTReferenceNC_000913.3.txt
        /home/user/Desktop/data/results/GlpTCP103710.1_GlpTReferenceNC_000913.3.txt
        /home/user/Desktop/data/results/GlpTCP014522.1_GlpTReferenceNC_000913.3.txt

And i have trying so much to figure out what is happening, i don't seem to find the issue. Help is much appreciated. Thank you.
what i tried to do is:

specify locations for each files and not through wildcards, and make the script work on each alone, also got the same errors.

kept sample wildcards and only specified ref location, still same issue

re-wrote the whole snakefile, same error.


i did not uninstall snakemake or conda yet, i want to find a solution before i try this way.
Edited after euronion answer,
that helped me figure out the issue. i added all the rules to the main sankefile and found out what was happening there.
    import os
    import sys
    import glob
    import yaml
    import pandas as pd
    
    ##load config file
    configfile: &quot;config/config.yaml&quot;
    
    ##ref and query locations
    referance = config[&quot;reference_loc&quot;]
    query = config[&quot;query_loc&quot;]
    
    ##output file location
    output = config[&quot;output&quot;]   
    
    ##sample file name
    suffix = [&quot;.fsa&quot;, &quot;.fasta&quot;, &quot;.fa&quot;, &quot;.fna&quot;, &quot;.fas&quot;]
    sample_file = glob.glob(query+&quot;*&quot;+str(suffix))
    sample1 = [item.removeprefix(query) for item in sample_file]
    sample2 = [item.removesuffix(&quot;.fasta&quot;) for item in sample1]
    sample = [item.removesuffix(&quot;.fa&quot;) for item in sample2]
    print(sample)
    
    ##rules
    rule all:
        input:
            expand(output+&quot;final/{sample}_merged.csv&quot;, sample = sample)
    
    rule blast:
        input:
            query=query+'{sample}.fasta',
            ref=referance
        output:
            output+'{sample}.txt'
        threads:
            workflow.cores
        conda:
            'envs/blast.yaml'
        shell:
            &quot;blastp -subject {input.ref} -query {input.query} -outfmt '6 qacc sacc pident length qcovs evalue mismatch gaps qseq sseq qlen slen sstart send' -out {output} -num_threads {threads}&quot;
    
    rule merge_data:
        input:
            rules.blast.output
        output:
            output1=output+&quot;final/{sample}_merged.csv&quot;
        conda:
            'envs/pandas.yaml'
        script:
            &quot;../scripts/pd_blastresults.py&quot;


but now i got into another issue is specifying my different reference files, i'll just figure that later.
",1,-1,-1.0
74475638,Snakemake MissingOutputException when writing list to file,"I'm having a MissingOutputException with what I think is a very basic rule. I'm trying to print a list given through the config file into a file using some Python commands but Snakemake keeps throwing MissingOutputException error:
# --- Importing Configuration Files --- #

configfile: &quot;config.yaml&quot;

# -------------------------------------------------

scaffolds = config[&quot;Scaffolds&quot;]

localrules: all, MakeScaffoldList

# -------------------------------------------------

rule all:
    input:
        LIST = &quot;scaffolds.list&quot;

# -------------------------------------------------

rule MakeScaffoldList:
    output:
        LIST = &quot;scaffolds.list&quot;
    params:
        SCAFFOLDS = scaffolds
    run:
        &quot;&quot;&quot;
        with open(output.LIST, 'w') as f:
            for line in params.SCAFFOLDS:
                f.write(f&quot;{line}\n&quot;)
        &quot;&quot;&quot;

Error:
[Thu Nov 17 14:08:33 2022]
localrule MakeScaffoldList:
    output: scaffolds.list
    jobid: 1
    resources: mem_mb=27200, disk_mb=1000, tmpdir=/scratch, account=snic2022-22-156, partition=core, time=12:00:00, threads=4

MissingOutputException in line 37 of test.smk:
Job Missing files after 5 seconds. This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait:
scaffolds.list completed successfully, but some output files are missing. 0
Exiting because a job execution failed. Look above for error message

What am I doing wrong? Is it the Python code wrong?
",-1,-1,-1.0
74479965,Snakemake doesn't activate conda environment correctly,"I have a Python module modulename installed in a conda environment called myenvname.
My snakemake file consists of one simple rule:
rule checker2:
    output:
        &quot;tata.txt&quot;
    conda:
        &quot;myenvname&quot;
    script:
        &quot;scripts/test2.py&quot;

The contents of the test2.py are the following:
import modulename
with open(&quot;tata.txt&quot;,&quot;w&quot;) as _f:
    _f.write(modulename.__version__)

When I run the above snakemake file with the command snakemake -j 1 --use-conda --conda-frontend conda I get ModuleNotFoundError, which would imply that there is no modulename in my specified environment. However, when I do the following :
conda activate myenvname
python workflow/scripts/test2.py

... everything works perfectly. I have no idea what's going on.
The full error is pasted below, with some info omitted for privacy.
Traceback (most recent call last):
  File &quot;/OMITTED/.snakemake/scripts/tmpheaxuqjn.test2.py&quot;, line 13, in &lt;module&gt;
    import cnvpytor as cnv
ModuleNotFoundError: No module named 'MODULENAME'
[Thu Nov 17 18:27:22 2022]
Error in rule checker2:
    jobid: 0
    output: tata.txt
    conda-env: MYENVNAME

RuleException:
CalledProcessError in line 12 of /OMITTED/workflow/snakefile:
Command 'source /apps/qiime2/miniconda3/bin/activate 'MYENVNAME'; set -euo pipefail;  /OMITTED/.conda/envs/snakemake/bin/python3.1 /OMITTED/.snakemake/scripts/tmpheaxuqjn.test2.py' returned non-zero exit status 1.
  File &quot;/OMITTED/workflow/snakefile&quot;, line 12, in __rule_checker2
  File &quot;/OMITTED/.conda/envs/snakemake/lib/python3.10/concurrent/futures/thread.py&quot;, line 58, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /OMITTED/.snakemake/log/2022-11-17T182715.495739.snakemake.log

EDIT:
Typo in script fixed, the typo isn't in the script I'm running so it's not the issue here.
EDIT2:
I've tried two different attempts from comments. All three attempts
are run with the same CLI command snakemake -j 1 --use-conda --conda-frontend conda
Attempt 1
Rule in the snakemake:
rule checker3:
    output:
        &quot;tata.txt&quot;
    conda:
        &quot;myenvname&quot;
    shell:
        &quot;&quot;&quot;
        conda env list &gt;&gt; {output}
        conda list &gt;&gt; {output}
        &quot;&quot;&quot;

In the output file I had the following (I have lots of environs and packages I've cut out):
# conda environments:
#
...
myenvname              *  /OMITTED/.conda/envs/myenvname
...
# packages in environment at /OMITTED/.conda/envs/myenvname:
#
# Name                    Version                   Build  Channel
...
modulename                  1.2                       dev_0    &lt;develop&gt;
...

This attempt proves that the conda environment is activated and that this environment has modulename.
Attempt 2
Same as running the script, but I've modified the script to include
import time; time.sleep(30); import modulename

So I can snag a runnable script before it's deleted. The script has the following inserted at the start:
######## snakemake preamble start (automatically inserted, do not edit) ########
import sys; sys.path.extend(['/OMITTED/.conda/envs/snakemake/lib/python3.10/site-packages', '/OMITTED/MYWORKINGDIRECTORY/workflow/scripts']); import pickle; snakemake = pickle.loads(####a lot of stuff here###); from snakemake.logging import logger; logger.printshellcmds = False; __real_file__ = __file__; __file__ = '/OMITTED/MYWORKINGDIRECTORY/workflow/scripts/test3.py';
######## snakemake preamble end #########

I have no idea what to do with this information.
Attempt 3
Instead of running script, I've ran a shell command that runs a python script.
rule checker4:
    output:
        &quot;tata.txt&quot;
    conda:
        &quot;myenvname&quot;
    shell:
        &quot;python workflow/scripts/test3.py&quot;

It worked (showed no errors), and when I open &quot;tata.txt&quot; I find &quot;1.2&quot; which is the version of of my module.
Conclusions
The snakemake actually activates proper environment, but the problem is in script part. I have no idea why this is.
There is a similar question here, so this is a duplicate question.
",-1,-1,-1.0
74658260,Snakemake not interpreting wildcard correctly?,"I am trying to run a snakemake file but it is producing a weird result


refseq = 'refseq.fasta'
reads = '_R1_001'
reads2 = '_R2_001'
configfile: &quot;config.yaml&quot;
## Add config

def getsamples():
    import glob
    test = (glob.glob(&quot;*.fastq.gz&quot;))
    samples = []
    for i in test:
        samples.append(i.rsplit('_', 2)[0])
    print(samples)
    return(samples)

def getbarcodes():
    with open('unique.barcodes.txt') as file:
        lines = [line.rstrip() for line in file]
    return(lines)

rule all:
    input:
        expand(&quot;called/{barcodes}{sample}_called.vcf&quot;, barcodes=getbarcodes(), sample=getsamples()),
        expand(&quot;mosdepth/{barcodes}{sample}.mosdepth.summary.txt&quot;, barcodes=getbarcodes(), sample=getsamples())


rule fastq_grep:
    input:
        R1 = &quot;{sample}_R1_001.fastq.gz&quot;,
        R2 = &quot;{sample}_R2_001.fastq.gz&quot;
    output:
        &quot;grepped/{barcodes}{sample}_R1_001.plate.fastq&quot;,
        &quot;grepped/{barcodes}{sample}_R2_001.plate.fastq&quot;
    shell:
        &quot;fastq-grep -i '{wildcards.barcodes}' {input.R1} &gt; {output} &amp;&amp; fastq-grep -i '{wildcards.barcodes}' {input.R2} &gt; {output}&quot;




I have files in my directory with *.fastq.gz on the end of them but I get this:

Missing input files for rule fastq_grep:
0_R1_001.fastq.gz
0_R2_001.fastq.gz

Those two files do not exist, where is it getting them from?
I would expect to see a lot of fastq files that are in my directory but it is only listing one file that does not exist.
",-1,-1,-1.0
74686484,Snakemake pipeline not attempting to produce output?,"I have a relatively simple snakemake pipeline but when run I get all missing files for rule all:


refseq = 'refseq.fasta'
reads = ['_R1_001', '_R2_001']

def getsamples():
    import glob
    test = (glob.glob(&quot;*.fastq&quot;))
    print(test)
    samples = []
    for i in test:
        samples.append(i.rsplit('_', 2)[0])
    return(samples)

def getbarcodes():
    with open('unique.barcodes.txt') as file:
        lines = [line.rstrip() for line in file]
    return(lines)

rule all:
    input:
        expand(&quot;grepped/{barcodes}{sample}_R1_001.plate.fastq&quot;, barcodes=getbarcodes(), sample=getsamples()),
        expand(&quot;grepped/{barcodes}{sample}_R2_001.plate.fastq&quot;, barcodes=getbarcodes(), sample=getsamples())
    wildcard_constraints:
        barcodes=&quot;[a-z-A-Z]+$&quot;


rule fastq_grep:
    input:
        R1 = &quot;{sample}_R1_001.fastq&quot;,
        R2 = &quot;{sample}_R2_001.fastq&quot;
    output:
        out1 = &quot;grepped/{barcodes}{sample}_R1_001.plate.fastq&quot;,
        out2 = &quot;grepped/{barcodes}{sample}_R2_001.plate.fastq&quot;
    
    wildcard_constraints:
        barcodes=&quot;[a-z-A-Z]+$&quot;
    shell:
        &quot;fastq-grep -i '{wildcards.barcodes}' {input.R1} &gt; {output.out1} &amp;&amp; fastq-grep -i '{wildcards.barcodes}' {input.R2} &gt; {output.out2}&quot;

The output files that are listed by the terminal seem correct, so it seems it is seeing what I want to produce but the shell is not making anything at all.
I want to produce a list of files that have grepped the list of barcodes I have in a file. But I get &quot;Missing input files for rule all:&quot;
",-1,-1,-1.0
74731618,Honour module's `config.yml` when dynamically importing modules into Snakemake workflow,"Background
I'm new to scientific workflows, and am building my first more complex Snakemake workflow.
Setup &amp; what I've tried
I'm importing modules dynamically. The scenario is that I have a root config.yml file that includes:
subworkflows:
 - subwf-1
 - subwf-2
 - other-subwf

In the root Snakefile I'm doing:
configfile: 'config.yml'

ALL_SUBWFS = config['subworkflows']

# &lt;omitted&gt; Check for min version 6.0

for MODULE in ALL_SUBWFS:

    module MODULE:
        snakefile:
            f'subworkflows/{MODULE}/Snakefile'

    use rule * from MODULE as f'{MODULE}_*'

This works fine so far. However, I'd like to be able to configure the different submodules each in their own config.yml.
Assume the following directory structure:
.
├── Snakefile
├── config.yml
└── subworkflows/
    ├── subwf-1/
    │   ├── Snakefile
    │   └── config.yml
    ├── subwf-2/
    │   ├── Snakefile
    │   └── config.yml
    └── other-subwf/
        ├── Snakefile
        └── config.yml

As far as I understand, this isn't supported, and neither of these options work:

Define configfile: config.yml in main workflow and configfile: cf in subworkflows, where I've tried three options for cf:

cf = str(workflow.source_path('config.yml'))  # 1
cf = f'{workflow.basedir}/config.yml'  # 2
cf = 'config.yml'  # 3

# With each of these options
configfile: cf

All give me KeyError in &lt;...&gt;/Snakefile: 'config'.

Using config: config in module import, and subworkflows' Snakefiles including something like VALUES = config['values'] gives me KeyError in &lt;...&gt;/Snakefile: 'values' using each option.

Actual question
Am I right in assuming that it isn't possible to honour the configfiles for modules at all, and that instead, I need to use a global config file, e.g. with keys for each subworkflow imported as config: config['&lt;key-for-subworkflow-config-YAML-map&gt;']?
",1,-1,-1.0
74769073,Snakemake WorkflowError: Target rules may not contain wildcards,"rule all:
        input:
                &quot;../data/A_checkm/{genome}&quot;

rule A_checkm:
    input:
      &quot;../data/genomesFna/{genome}_genomic.fna.gz&quot;
    output:
        directory(&quot;../data/A_checkm/{genome}&quot;)
    threads:
        16
    resources:
        mem_mb = 40000
    shell:
        &quot;&quot;&quot;
        # setup a tmp working dir
        tmp=$(mktemp -d)
        mkdir $tmp/ref
        cp {input} $tmp/ref/genome.fna.gz
        cd $tmp/ref
        gunzip -c genome.fna.gz &gt; genome.fna
        cd $tmp

        # run checking
        checkm lineage_wf -t {threads} -x fna ref out &gt; stdout

        # prepare output folder
        cd {config[project_root]}
        mkdir -p {output}
        # copy results over
        cp -r $tmp/out/* {output}/
        cp $tmp/stdout {output}/checkm.txt
        # cleanup
        rm -rf $tmp
        &quot;&quot;&quot;


Thank you in advance for your help!
I would like to run checkm on a list of ~600 downloaded genome files having the extension '.fna.gz'. Each downloaded file is saved in a separate folder having the same name as the genome. I would like also to have all the results in a separate folder for each genome and that's why my output is a directory.
When I run this code with 'snakemake -s Snakefile --cores 10 A_checkm', I get the following error:
WorkflowError: Target rules may not contain wildcards. Please specify concrete files or a rule without wildcards at the command line, or have a rule without wildcards at the very top of your workflow (e.g. the typical &quot;rule all&quot; which just collects all results you want to generate in the end).
Anyone could help me identifying the error, please?
",1,1,-1.0
74771653,"Snakemake, producing list of files that are created within the pipeline","This is my first snakemake workflow, so it might be that I'm overcomplicating things.
My workflow takes as input the 'database query' for downloading some files, which is specified in my 'config.yaml'. It means that I do not know the names of the files that will be downloaded before running the pipeline.
# configfile: &quot;config.yaml&quot;
# DATABASE = config[&quot;database&quot;]    
# database: '(&quot;Apis&quot;[Organism] OR Apis[All Fields]) AND (latest[filter] AND &quot;representative genome&quot;[filter] AND all[filter] NOT anomalous[filter])'

DATABASE = '(&quot;Apis&quot;[Organism] OR Apis[All Fields]) AND (latest[filter] AND &quot;representative genome&quot;[filter] AND all[filter] NOT anomalous[filter])'

What I want to do is to:

Create a genome list: call a database with my query and extract the links to the files (create_genome_list). (Here, I use entrez)
Next, I want to download the files using the collected links (download_genome)
Files are zipped, so I want to unzip them (unzip_genome)
Finally, I would like to create a list of all downloaded and unzipped files... and here I struggle. (make_summary_table)

I can run my snakemake on steps 1-3 when I call one of the expected output files with the following:

snakemake -p database/GCA_000184785.2_Aflo_1.1_genomic/GCA_000184785.2_Aflo_1.1_genomic.fna  --use-conda
It gives me links to all expected files (5) in folder /temp,
and 1 downloaded and unzipped file: /database/GCA_000184785.2_Aflo_1.1_genomic/GCA_000184785.2_Aflo_1.1_genomic.fna

My snakemake for steps 1-3 looks like this:
rule create_genome_list:
output: touch(&quot;temp/{genome}&quot;)

conda:  &quot;entrez_env.yaml&quot;
message: &quot;Creating the genomes list...&quot;

shell:
    r&quot;&quot;&quot;
    esearch -db assembly -query '{DATABASE}' \
    | esummary \
    | xtract -pattern DocumentSummary -element FtpPath_GenBank \
    | while read -r line ; 
    do
        fname=$(echo $line | grep -o 'GCA_.*' | sed 's/$/_genomic.fna.gz/');
        wildcard=$(echo $fname | sed -e 's!.fna.gz!!');

        echo &quot;$line/$fname&quot; &gt; temp/$wildcard;
        #echo $wildcard &gt;&gt; list_of_genomes.txt

    done
   
    &quot;&quot;&quot;   


rule download_genome:
    output: touch(&quot;database/{genome}/{genome}.fna.gz&quot;)

    input:  &quot;temp/{genome}&quot;

    shell:
       r&quot;&quot;&quot;
       GENOME_LINK=$(cat {input})
       GENOME=&quot;${{GENOME_LINK##*/}}&quot;
       wget -P ./database/{wildcards.genome}/ $GENOME_LINK 
       &quot;&quot;&quot;


rule unzip_genome:
   output: touch(&quot;database/{genome}/{genome}.fna&quot;)

   input:  &quot;database/{genome}/{genome}.fna.gz&quot;

   shell: &quot;gunzip {input}&quot;        

My problem starts when I want to create the final rule, which will wrap up the results of my pipeline. In my real pipeline, I do some additional analyses with downloaded genomes, and at the end, I want to join all partial results obtained per single genome into one table. Here I post a toy example, which I believe reflects my problem the best.
I guess there is some way to extract the genomes' names so I could call them in the final summarising rule's input.
I approached it in an ugly way by listing files in temp/ and using them in expand() like follow:
GENOMES = os.listdir(&quot;temp/&quot;)

rule make_summary_table:
    output: &quot;summary_table.txt&quot;

    input:  expand(&quot;database/{genome}/{genome}.fna&quot;, genome = GENOMES)

    shell:
        &quot;&quot;&quot;
        echo {input} &gt;&gt; {output}
        echo &quot; &quot; &gt;&gt; {output}
        &quot;&quot;&quot;

But it works only when /temp exists before running the pipeline. And it produces the summary_table.txt with 5 positions only when I run steps 1-3 before (otherwise, it produces an empty file).
I am also afraid that in my real pipeline, it might happen that not all genomes will produce partial results on time the last summarising rule will be called. But maybe Snakemake handles it somehow (by waiting?) once all the inputs are specified.
-----------------------------EDIT-----------------------------------------
I have tried to implement checkpoint as a possible solution as follow:
DATABASE = '(&quot;Apis&quot;[Organism] OR Apis[All Fields]) AND (latest[filter] AND &quot;representative genome&quot;[filter] AND all[filter] NOT anomalous[filter])'

rule all:
    input:    &quot;summary_table.txt&quot;
    
checkpoint create_genome_list:
    output: directory(&quot;temp/&quot;)

    conda:  &quot;entrez_env.yaml&quot;
    
    shell:
        r&quot;&quot;&quot;
        esearch -db assembly -query '{DATABASE}' \
        | esummary \
        | xtract -pattern DocumentSummary -element FtpPath_GenBank \
        | while read -r line ; 
        do
            fname=$(echo $line | grep -o 'GCA_.*' | sed 's/$/_genomic.fna.gz/');
            wildcard=$(echo $fname | sed -e 's!.fna.gz!!');

            echo &quot;$line/$fname&quot; &gt; temp/$wildcard;
            #echo $wildcard &gt;&gt; list_of_genomes.txt

        done
       
        &quot;&quot;&quot;   
   
rule download_genome:
    output: touch(&quot;database/{genome}/{genome}.fna.gz&quot;)
    
    input:  &quot;temp/{genome}&quot;

    
    shell:
        r&quot;&quot;&quot;
        GENOME_LINK=$(cat {input})
        GENOME=&quot;${{GENOME_LINK##*/}}&quot;
        wget -P ./database/{wildcards.genome}/ $GENOME_LINK 
        &quot;&quot;&quot;

rule unzip_genome:
    output: &quot;database/{genome}/{genome}.fna&quot;

    input:  &quot;database/{genome}/{genome}.fna.gz&quot;
    
    shell:
        r&quot;&quot;&quot;
        gunzip {input}
        &quot;&quot;&quot;        

def aggregate_input(wildcards):
      checkpoint_output = checkpoints.create_genome_list.get(**wildcards).output[0]
      return expand(&quot;database/{genome}/{genome}.fna&quot;,
                  i=glob_wildcards(os.path.join(checkpoint_output, &quot;{genome}.fna&quot;)).genome)

rule make_summary_table:
    output: &quot;summary_table.txt&quot;

    input:  aggregate_input

    shell:
        &quot;&quot;&quot;
        echo {input} &gt;&gt; {output}
        echo &quot; &quot; &gt;&gt; {output}
        &quot;&quot;&quot;

But cannot overcome the error: InputFunctionException in line 73 (rule make_summary_table) of ~/snakemake_test/Snakefile: WildcardError: No values given for wildcard 'genome'. Wildcards: 
",-1,-1,-1.0
74822755,snakemake - dynamically set log for each rule,"I'd like to set the log file (log directive) for each of the workflow rules dynamically, so it contains the rule name. I tried to use the set_log() function like I do with set_params(), but am getting a cryptic error. Here is what I tried:
logs_dir = config['logs_dir']

rule1:
    ...

rule2:
    ...

for r in workflow.rules:
    r.set_log(os.path.join(logs_dir, r.name + '.log'))

and getting: AssertionError in file ... in line ...
I am probably misusing set_log, but couldn't figure it out. Any ideas?
UPDATE
Following the suggestions in the answers, I tried the following:
rule all:
    input:
        'b'

rule myrule:
    input:
        'a'
    output:
        'b'
    log:
        dummy = 'dummy.log'
    shell:
        &quot;&quot;&quot;
        cp {input} {output}
        &quot;&quot;&quot;

for r in workflow.rules:
    r.set_log(real=r.name + '.log')

Unfortunately, running it with snakemake -s snakefile -j 1 resulted again in an unspecified Assertion Error.
",-1,-1,-1.0
74388036,"Snakemake job missing files, even though the only output has been completed","So I am trying to create a Snakemake RNA-seq workflow, that begins with fastqs, aligns them with STAR and counts those with Salmon. I am having an issue with the latter: Snakemake reports the only output file as completed in the log, but states that some output files are missing, even though there are not other expected output files for this particular rule.
The relevant rules :
rule all:
    input:
        expand(&quot;quants/{sample}/quants.sf&quot;, sample=SAMPLES),

...



rule salmon_quant:
    input:
        bam=&quot;{sample}Aligned.toTranscriptome.out.bam&quot;,
        trans=&quot;reference/Mus_musculus.GRCm39.transcriptome.fasta&quot;
    output:
        &quot;quants/{sample}/quants.sf&quot;
    threads: 20
    params:
        dir = &quot;quants/{sample}&quot;
    shell:
        &quot;salmon quant -t {input.trans} -l ISF -p {threads} -a {input.bam} -o {params.dir}&quot;


The beginning of Snakemake output :

Select jobs to execute...

[Thu Nov 10 11:37:33 2022]
rule salmon_quant:
    input: PBS3_1ng_NEB_S17Aligned.toTranscriptome.out.bam, reference/Mus_musculus.GRCm39.transcriptome.fasta
    output: quants/PBS3_1ng_NEB_S17/quants.sf
    jobid: 31
    reason: Missing output files: quants/PBS3_1ng_NEB_S17/quants.sf
    wildcards: sample=PBS3_1ng_NEB_S17
    threads: 15
    resources: tmpdir=/tmp

salmon quant -t reference/Mus_musculus.GRCm39.transcriptome.fasta -l ISF -p 15 -a PBS3_1ng_NEB_S17Aligned.toTranscriptome.out.bam -o quants/PBS3_1ng_NEB_S17
Version Info: This is the most recent version of salmon.
# salmon (alignment-based) v1.9.0


the end
[2022-11-10 11:38:11.630] [jointLog] [info] writing output

Freeing memory used by read queue . . . 
Joined parsing thread . . . &quot;PBS3_1ng_NEB_S17Aligned.toTranscriptome.out.bam&quot; 
Closed all files . . . 
Emptied frag queue. . . 
Emptied Alignment Group Pool. . 
Emptied Alignment Group Queue. . . done
Waiting at most 60 seconds for missing files.
MissingOutputException in rule salmon_quant  in line 68 of /home/etheimer/Benchs/Snakefile:
Job Missing files after 60 seconds. This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait:
quants/PBS3_1ng_NEB_S17/quants.sf completed successfully, but some output files are missing. 31
Shutting down, this might take some time.

As you can see, I have extended the wait time, without success.
It is probably something obvious, but I can't figure out why snakemake begins a rule with one reason (reason: Missing output files: quants/PBS3_1ng_NEB_S17/quants.sf), completes it (quants/PBS3_1ng_NEB_S17/quants.sf completed successfully) but is waiting for something else.
",-1,-1,-1.0
74217388,Why does Snakemake redirect output to a non-existing folder?,"I have a rule in Snakemake which shell section translates to something like that:
echo &quot;Hello&quot; &gt; output_folder/output_file; cat input_file &gt;&gt; output_folder/output_file

Note that output_folder is a child folder for the one where Snakemake runs.
That worked well, but I've decided to use another folder for output, and that folder is not a child of the current one. I have to provide an absolute path. Now my pipeline fails:
/usr/bin/bash: line 1 /absolute/path/output_file: No such file or directory
/usr/bin/bash: line 1 /absolute/path/output_file: No such file or directory
...
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Indeed, after running the pipeline with the --printshellcmds flag and repeating the same sequence of commands in bash it produces the same error.
So now I'm confused: why does it work for the child directory? I've repeated the sequence of commands for the child directory and bash produces the error (which is expected), while Snakemake successfully redirects output to the file in a non-existing folder.
Update: removing the cat ... &gt;&gt; part doesn't help and the behavior is the same. I've tried to replace absolute path with the relative one. Now I'm getting another error:
WorkflowError:
Error recording metadata for finished job ([Error 2] No such file or directory: '&lt;current dir&gt;\\.snakemake\\metadata\\Li4vLi4vLi4vZmxhc2hiYWNrYm9uZV93YXRjaC9kb2N0cmluZXMvQ2Fycm9sbC9iYWNrYXBwX3JlZmFjdG9yL1NwYW5pc2gvaW5zZXJ0X2ludG9fcGFyYWRpZ20udmVyYi50eXBlLmhpZXJhcmNoeV9zZWxlY3RfcGFyYWRpZ20udmVyYi50eXBlLmhpZXJhcmNoeXRtcC5zcWw='). Please ensure write permissions for the directory &lt;current dir&gt;\.snakemake')

I'm running it on Windows under MSYS2 environment.

",-1,-1,-1.0
74025045,Safely shut down snakemake programmatically,"I am looking for a way to shutdown/exit/halt a running snakemake workflow programmatically - essentially with a python function that is called in the workflow but may run into an unrecoverable error requiring the workflow to stop for human intervention.

What I am actually trying to do: I start (guppy basecaller) jobs on GPU nodes, and have to specify in the command which cuda core to use. The function checks if lock files exist to specify which cores are in use and which are available. The files are created and removed as part of the shell command of the basecaller. Using a resource the number of parallel gpu jobs is limited to the available number of cores. This works, but I want to be able to catch unexpected problems if e.g. a gpu_lock file got removed or not cleaned.

The function is called in the workflow to specify a parameter, e.g. as the dummy below:

def get_fromel(wildcards):
  if some_number &lt; 0.05:
    sys.exit(&quot;yieeeks&quot;)
  else:
    return &quot;hiyaaa&quot;

rule foo:
  input: bar.txt
  output: baz.txt
  params: 
     fromel = get_fromel
  shell:
     &quot;fizz -f {params.fromel} {input} &gt; {output}



Do I simply call sys.exit(&quot;my message&quot;)? I am worried that it will not clean up incomplete files etc
",-1,1,-1.0
74903088,How do I use this python function within the params section of my Snakemake rule?,"I'm trying to figure out how to extract the read-group lane information from a fastq file, and then use this string within my GATK AddOrReplaceReadGroups Snakemake below (below). I've written a short Python function (at the top of the rule) to do this operation, but can't quite figure out how to actually use it in the script such that I can access the string output (e.g., &quot;ABCHGSX35:2&quot;) of the function on a given fastq file in my GATK shell command. Basically, I want to feed {params.PathToReadR1} into the extract_lane_info function, but can't figure out how to integrate them correctly.
Open to any solutions to the problem posed, or if there's an entirely more efficiently and different way to achieve the same result (getting the read-group lane info as a param value), that'd be great, too. Thanks for any help.
def extract_lane_info(file_path):
    # Run the bash command using subprocess.run()
    elements = subprocess.run([&quot;zcat&quot;, file_path], stdout=subprocess.PIPE).stdout.split(b&quot;\n&quot;)[0].split(b&quot;:&quot;)[2:4]
    # Extract the lane information from the output of the command using a regular expression
    read_group = elements[0].decode().strip(&quot;'&quot;)
    string_after = elements[1].decode().strip(&quot;'&quot;)
    elements = read_group + &quot;:&quot; + string_after
    return(elements)  

rule AddOrReplaceReadGroups:
    input:
         &quot;results/{species}/bwa/merged/{read}_pese.bam&quot;,
 
     output:
        &quot;results/{species}/GATK/AddOrReplace/{read}.pese.bwa_mem.addRG.bam&quot;,

    threads: config[&quot;trimmmomatic&quot;][&quot;cpu&quot;]
    
    log:
        &quot;results/{species}/GATK/AddOrReplace/log/{read}_AddOrReplace.stderrs&quot;
    
    message:
        &quot;Running AddOrReplaceReadGroups on {wildcards.read}&quot;

    conda: 
        config[&quot;CondaEnvs&quot;]
    
    params:
        ReadGroupID = lambda wildcards: final_Prefix_ReadGroup[wildcards.read],
        PathToReadR1 = lambda wildcards: final_Prefix_RawReadPathR1[wildcards.read],
        LIBRARY = lambda wildcards: final_Prefix_ReadGroupLibrary[wildcards.read],
    
    shell:&quot;gatk AddOrReplaceReadGroups -I {input} -O {output} -ID {params.ReadGroupID}.1 -LB {params.LIBRARY} -PL illumina -PU {input.lane_info}:{params.ReadGroupID} -SM {params.ReadGroupID} --SORT_ORDER 'coordinate' --CREATE_INDEX true 2&gt;&gt; {log}&quot;


Basically, {params.PathToReadR1} would be &quot;path/to/file.fastq.gz&quot;, and I want this file to be inputted into the extract_lane_info function, and then the output of this function to be used in the -PU section of the shell command (e.g., {params.lane_info}. I keep getting all types of errors as I've messed around with it, and am unsure how to move forward.
",1,-1,-1.0
74994613,MissingOutputException snakemake,"I am getting an MissingOutputException from my snakemake workflow, snakemake creates the required output in the desired directory but keeps looking for it and exits.
this is my snakefile.
rule all:
    input:  
        expand('/home/stud9/NAS/results/qc_reports/fastqc/trimmed_{sample}_1_fastqc.html', sample=SAMPLES),
        expand('/home/stud9/NAS/results/qc_reports/fastqc/trimmed_{sample}_2_fastqc.html', sample=SAMPLES),
        expand('home/stud9/NAS/results/non_aligned/{sample}_nm2cov.bam', sample=SAMPLS)




rule nm2cov:
    input: 
        '/home/stud9/NAS/results/aligned/to_cov/{sample}_cov.sorted.bam'
    output:
        'home/stud9/NAS/results/non_aligned/{sample}_nm2cov.bam'
    shell:
        &quot;cd /home/stud9/NAS/results/non_aligned &amp;&amp; samtools view -b -f 4 {input} &gt; {wildcards.sample}_nm2cov.bam&quot;

I have used cd before the actual cmd because I want my results there otherwise they would show in the snakefile directory.
This is the messsage I am getting:
Waiting at most 10 seconds for missing files.
MissingOutputException in rule nm2cov  in line 50 of /home/stud9/NAS/scripts/wf_1:
Job Missing files after 10 seconds. This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait:
home/stud9/NAS/results/non_aligned/148_nm2cov.bam completed successfully, but some output files are missing. 55
Shutting down, this might take some time.

sorry if my post is a little bit messy but this is the first time I post here
tried changing --latency-wait to 15 still no response.
",-1,-1,-1.0
75004129,Snakemake/miniforge: Nothing provides libgcc-ng >=12 needed by freebayes-1.3.6-hbfe0e7f_2,"I am running a Snakemake NGS data analysis pipeline, one rule uses Freebayes, it's env looks like this:
name: freebayes
channels:
  - bioconda
dependencies:
 - freebayes=1.3.6

While creating the env this error occurs:
Building DAG of jobs...
Your conda installation is not configured to use strict channel priorities. This is however crucial for having robust and correct environments (for details, see https://conda-forge.org/docs/user/tipsandtricks.html). Please consider to configure strict priorities by executing 'conda config --set channel_priority strict'.
Creating conda environment envs/freebayes.yaml...
Downloading and installing remote packages.
CreateCondaEnvironmentException:
Could not create conda environment from /home/nlv24077/temp/test_legacy_pipeline/rules/../envs/freebayes.yaml:
Command:
mamba env create --quiet --file &quot;/home/nlv24077/mpegg/snaqs_required_files/snakemake_envs/08937c429b94df5250c66c66154d19b9.yaml&quot; --prefix &quot;/home/nlv24077/mpegg/snaqs_required_files/snakemake_envs/08937c429b94df5250c66c66154d19b9&quot;
Output:
Encountered problems while solving:
  - nothing provides libgcc-ng &gt;=12 needed by freebayes-1.3.6-hbfe0e7f_2

If I set the channel to conda-forge the error changes to:
- nothing provides requested freebayes 1.3.6**

How could I solve this?
",-1,-1,-1.0
75075092,Snakemake cannot activate an existing conda environment,"I understood from the Snakemake docs that the conda directive can take the name of an existing conda environment. However, I am faced with the error EnvironmentNameNotFound: Could not find conda environment. Why can Snakemake not find the existing conda environment? Please see my example below. I am using Miniconda3 and Snakemake v7.19.1, through Ubuntu 20.04 on WSL2.
My Snakefile contains:
rule test:
    output: temp(&quot;test.txt&quot;)
    conda: &quot;test-env&quot;
    shell: &quot;touch {output}&quot;

The test-env environment was created as follows:
conda create -n test-env -y

Running conda info --envs shows that test-env exists: test-env   /home/elh605/miniconda3/envs/test-env.
My snakemake command and the full output:
snakemake -j1 --use-conda


Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job      count    min threads    max threads
-----  -------  -------------  -------------
test         1              1              1
total        1              1              1

Select jobs to execute...

[Tue Jan 10 19:28:50 2023]
rule test:
    output: test.txt
    jobid: 0
    reason: Missing output files: test.txt
    resources: tmpdir=/tmp

Activating conda environment: test-env

EnvironmentNameNotFound: Could not find conda environment: test-env
You can list all discoverable environments with `conda info --envs`.


[Tue Jan 10 19:28:52 2023]
Finished job 0.
1 of 1 steps (100%) done
Removing temporary output test.txt.
Complete log: .snakemake/log/2023-01-10T192850.160679.snakemake.log

I tried using the --conda-prefix option to point Snakemake to my base conda environment, however Snakemake still could not activate test-env. How can I tell Snakemake where to find test-env?
",-1,-1,-1.0
75110147,snakemake-workflows / rna-seq-star-deseq2: Could not create conda environment,"I'm trying to run the following snakemake workflow: https://github.com/snakemake-workflows/rna-seq-star-deseq2 . I tried different virtual environments to run this workflow in which I installed the latest snakemake (v7.19.1) and latest mamba (v1.1.0), however I get the error below. I've also tried a different virtual environment using the snakemake version that was present at the last time of this workflow update on August 2021, namely snakemake (v6.7.0) with again the latest mamba (v1.1.0), but I still get the same error below.
I used the following code:
snakemake --cores all --use-conda
and get the following error:
Building DAG of jobs...
Creating conda environment workflow/envs/pandas.yaml...
Downloading and installing remote packages.
Environment for workflow/envs/pandas.yaml created (location: .snakemake/conda/e0a50d7235a9332b573ce8090931af98)
Creating conda environment workflow/envs/rseqc.yaml...
Downloading and installing remote packages.
Environment for workflow/envs/rseqc.yaml created (location: .snakemake/conda/e48233de77aab8c87a948f4b39ef2037)
Creating conda environment workflow/envs/biomart.yaml...
Downloading and installing remote packages.
CreateCondaEnvironmentException:
Could not create conda environment from /DATA/m.venkatesan/rna-seq-star-deseq2/workflow/rules/../envs/biomart.yaml:
Could not solve for environment specs
Encountered problems while solving:
  - package r-tidyverse-1.3.1-r36hc72bb7e_0 requires r-dplyr &gt;=1.0.5, but none of the providers can be installed
  - nothing provides r-base 3.2.0* needed by r-stringr-1.0.0-r3.2.0_0

The environment can't be solved, aborting the operation

The other conda environments in this snakemake workflow initialize without a problem, however it's getting stuck with creating conda environment &quot;workflow/envs/deseq2.yaml&quot;:
channels:
  - conda-forge
  - bioconda
dependencies:
  - bioconductor-deseq2 =1.30.0
  - r-ashr =2.2_47

Does anyone have any advice on how to solve this?
",-1,-1,-1.0
75124316,jupyter nbconvert doesn't work in the environment created by snakemake --use-conda,"I've stuck on this problem for several days.
I created environment envs/jupyter.yaml as followed
channels:
  - conda-forge
  - bioconda
  - defaults
dependencies:
  - libgcc-ng=12.2.0
  - libblas=3.9.0
  - liblapack=3.9.0
  - notebook=6.5.2
  - r-base=4.2.1
  - r-tidyverse=1.3.2
  - bioconductor-complexheatmap=2.14.0
  - bioconductor-biostrings=2.66.0
  - r-irkernel=1.3.1
  - jupyter_contrib_nbextensions=0.7.0
  - biopython=1.74
  - scipy=1.10.0

the snakefile rules looks like
rules stat:
    conda: 'envs/jupyter.yaml'
    notebook: rules.notebook.output.stat

When I run &quot;snakemake --use-conda&quot;, the error
Activating conda environment: .snakemake/conda/dd29012628c49c75d6a5c31f75898e03
Traceback (most recent call last):
  File &quot;/share/home/sibyl/bak/sars2_surveillance/.snakemake/conda/dd29012628c49c75d6a5c31f75898e03/bin/jupyter-nbconvert&quot;, line 6, in &lt;module&gt;
    from nbconvert.nbconvertapp import main
  File &quot;/share/home/sibyl/bak/sars2_surveillance/.snakemake/conda/dd29012628c49c75d6a5c31f75898e03/lib/python3.8/site-packages/nbconvert/nbconvertapp.py&quot;, line 185, in &lt;module&gt;
    class NbConvertApp(JupyterApp):
  File &quot;/share/home/sibyl/bak/sars2_surveillance/.snakemake/conda/dd29012628c49c75d6a5c31f75898e03/lib/python3.8/site-packages/nbconvert/nbconvertapp.py&quot;, line 278, in NbConvertApp
    formats=get_export_names()
  File &quot;/share/home/sibyl/bak/sars2_surveillance/.snakemake/conda/dd29012628c49c75d6a5c31f75898e03/lib/python3.8/site-packages/nbconvert/exporters/base.py&quot;, line 137, in get_export_names
    exporters = sorted(e.name for e in entry_points(group=&quot;nbconvert.exporters&quot;))
TypeError: entry_points() got an unexpected keyword argument 'group'

RuleException:
CalledProcessError in file /share/home/sibyl/bak/sars2_surveillance/Snakefile, line 310:
Command 'source /share/home/sibyl/miniconda3/envs/sars2/bin/activate '/share/home/sibyl/bak/sars2_surveillance/.snakemake/conda/dd29012628c49c75d6a5c31f75898e03'; set -euo pipefail;  jupyter-nbconvert --log-level ERROR --execute --output /share/home/sibyl/bak/sars2_surveillance/test/output/0log/upstream_stat/test.r.ipynb --to notebook --ExecutePreprocessor.timeout=-1 /share/home/sibyl/bak/sars2_surveillance/.snakemake/scripts/tmpcnlnf1wn.upstream_stat.r.ipynb' returned non-zero exit status 1.
  File &quot;/share/home/sibyl/bak/sars2_surveillance/Snakefile&quot;, line 310, in __rule_upstream_stat
  File &quot;/share/home/sibyl/miniconda3/envs/sars2/lib/python3.11/concurrent/futures/thread.py&quot;, line 58, in run


my nbconvert version
  - nbconvert=7.2.7=pyhd8ed1ab_0
  - nbconvert-core=7.2.7=pyhd8ed1ab_0
  - nbconvert-pandoc=7.2.7=pyhd8ed1ab_0

When I went into the virtual environment dd29012628c49c75d6a5c31f75898e03, the command 'jupyter nbconvert' return TypeError like before, even after I reinstalled nbconvert. I also tried to install nbconvert v7.1.0. Same error.
Everything worked perfectly several days ago.Really need help! Thanks a lot.
",1,-1,-1.0
75152401,Snakemake : Failed to set marker file for job started ([WinError 3] The system cannot find the path specified:,"I am new to Snakemake, I just started using it for my project.
I am trying to run a software package that uses Snakemake, however the workflow fails and the problem seems to be caused by windows not being able to locate some files [please see below for the complete error report]. Could someone please give me some ideas of possible solutions? Thank you
snakemake -s U:\Lab\Bereket_public\custom_merfish_rig\Merlin_test\test_data\results\data\snakemake\SnakeFile.Snakefile --cores 4 U:\Lab\Bereket_public\custom_merfish_rig\Merlin_test\test_data\results\data\GenerateMosaic\tasks\GenerateMosaic.done
Building DAG of jobs...
Provided cores: 4
Rules claiming more threads will be scaled down.
Job stats:
job                            count    min threads    max threads
---------------------------  -------  -------------  -------------
FiducialCorrelationWarp            2              1              1
FiducialCorrelationWarpDone        1              1              1
GenerateMosaic                     1              1              1
SimpleGlobalAlignment              1              1              1
total                              5              1              1

Select jobs to execute...

[Tue Jan 17 10:51:28 2023]
Job 1: Running SimpleGlobalAlignment
Reason: Missing output files: U:/Lab/Bereket_public/custom_merfish_rig/Merlin_test/test_data/results/data/SimpleGlobalAlignment/tasks/SimpleGlobalAlignment.done


[Tue Jan 17 10:51:28 2023]
Job 3: Running FiducialCorrelationWarp 0
Reason: Missing output files: U:/Lab/Bereket_public/custom_merfish_rig/Merlin_test/test_data/results/data/FiducialCorrelationWarp/tasks/FiducialCorrelationWarp_0.done

Failed to set marker file for job started ([WinError 3] The system cannot find the path specified: 'U:\\Lab\\Bereket_public\\custom_merfish_rig\\MERlin_epigen_UCSD\\.snakemake\\incomplete\\tmp72nkj4zo.VTovTGFi' -&gt; 'U:\\Lab\\Bereket_public\\custom_merfish_rig\\MERlin_epigen_UCSD\\.snakemake\\incomplete\\VTovTGFiL0JlcmVrZXRfcHVibGljL2N1c3RvbV9tZXJmaXNoX3JpZy9NZXJsaW5fdGVzdC90ZXN0X2RhdGEvcmVzdWx0cy9kYXRhL0ZpZHVjaWFsQ29ycmVsYXRpb25XYXJwL3Rhc2tzL0ZpZHVjaWFsQ29ycmVsYXRpb25XYXJwXzAuZG9uZQ=='). Snakemake will work, but cannot ensure that output files are complete in case of a kill signal or power loss. Please ensure write permissions for the directory U:\Lab\Bereket_public\custom_merfish_rig\MERlin_epigen_UCSD\.snakemake

[Tue Jan 17 10:51:28 2023]
Job 4: Running FiducialCorrelationWarp 1
Reason: Missing output files: U:/Lab/Bereket_public/custom_merfish_rig/Merlin_test/test_data/results/data/FiducialCorrelationWarp/tasks/FiducialCorrelationWarp_1.done

Failed to set marker file for job started ([WinError 3] The system cannot find the path specified: 'U:\\Lab\\Bereket_public\\custom_merfish_rig\\MERlin_epigen_UCSD\\.snakemake\\incomplete\\tmpo2nxcdd7.VTovTGFi' -&gt; 'U:\\Lab\\Bereket_public\\custom_merfish_rig\\MERlin_epigen_UCSD\\.snakemake\\incomplete\\VTovTGFiL0JlcmVrZXRfcHVibGljL2N1c3RvbV9tZXJmaXNoX3JpZy9NZXJsaW5fdGVzdC90ZXN0X2RhdGEvcmVzdWx0cy9kYXRhL0ZpZHVjaWFsQ29ycmVsYXRpb25XYXJwL3Rhc2tzL0ZpZHVjaWFsQ29ycmVsYXRpb25XYXJwXzEuZG9uZQ=='). Snakemake will work, but cannot ensure that output files are complete in case of a kill signal or power loss. Please ensure write permissions for the directory U:\Lab\Bereket_public\custom_merfish_rig\MERlin_epigen_UCSD\.snakemake
MERlin - the MERFISH decoding pipeline
MERlin - the MERFISH decoding pipeline
MERlin - the MERFISH decoding pipeline
Running SimpleGlobalAlignment
[Tue Jan 17 10:51:34 2023]
Finished job 1.
1 of 5 steps (20%) done
Running FiducialCorrelationWarp
Running FiducialCorrelationWarp
U:\Lab\Bereket_public\custom_merfish_rig\MERlin_epigen_UCSD\merlin\analysis\warp.py:143: FutureWarning: The input object of type 'SimilarityTransform' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'SimilarityTransform', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  np.array(transformationList,dtype = object), 'offsets',
U:\Lab\Bereket_public\custom_merfish_rig\MERlin_epigen_UCSD\merlin\analysis\warp.py:143: FutureWarning: The input object of type 'SimilarityTransform' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'SimilarityTransform', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  np.array(transformationList,dtype = object), 'offsets',
WorkflowError:
Error recording metadata for finished job ([WinError 3] The system cannot find the path specified: 'U:\\Lab\\Bereket_public\\custom_merfish_rig\\MERlin_epigen_UCSD\\.snakemake\\metadata\\tmplptve0_x.VTovTGFi' -&gt; 'U:\\Lab\\Bereket_public\\custom_merfish_rig\\MERlin_epigen_UCSD\\.snakemake\\metadata\\VTovTGFiL0JlcmVrZXRfcHVibGljL2N1c3RvbV9tZXJmaXNoX3JpZy9NZXJsaW5fdGVzdC90ZXN0X2RhdGEvcmVzdWx0cy9kYXRhL0ZpZHVjaWFsQ29ycmVsYXRpb25XYXJwL3Rhc2tzL0ZpZHVjaWFsQ29ycmVsYXRpb25XYXJwXzAuZG9uZQ=='). Please ensure write permissions for the directory U:\Lab\Bereket_public\custom_merfish_rig\MERlin_epigen_UCSD\.snakemake
Removing output files of failed job FiducialCorrelationWarp since they might be corrupted:
U:/Lab/Bereket_public/custom_merfish_rig/Merlin_test/test_data/results/data/FiducialCorrelationWarp/tasks/FiducialCorrelationWarp_0.done
WorkflowError:
Error recording metadata for finished job ([WinError 3] The system cannot find the path specified: 'U:\\Lab\\Bereket_public\\custom_merfish_rig\\MERlin_epigen_UCSD\\.snakemake\\metadata\\tmpl6y_8x_q.VTovTGFi' -&gt; 'U:\\Lab\\Bereket_public\\custom_merfish_rig\\MERlin_epigen_UCSD\\.snakemake\\metadata\\VTovTGFiL0JlcmVrZXRfcHVibGljL2N1c3RvbV9tZXJmaXNoX3JpZy9NZXJsaW5fdGVzdC90ZXN0X2RhdGEvcmVzdWx0cy9kYXRhL0ZpZHVjaWFsQ29ycmVsYXRpb25XYXJwL3Rhc2tzL0ZpZHVjaWFsQ29ycmVsYXRpb25XYXJwXzEuZG9uZQ=='). Please ensure write permissions for the directory U:\Lab\Bereket_public\custom_merfish_rig\MERlin_epigen_UCSD\.snakemake
Removing output files of failed job FiducialCorrelationWarp since they might be corrupted:
U:/Lab/Bereket_public/custom_merfish_rig/Merlin_test/test_data/results/data/FiducialCorrelationWarp/tasks/FiducialCorrelationWarp_1.done
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake\log\2023-01-17T105128.126829.snakemake.log

",-1,-1,-1.0
75187641,"Random ""[Errno 13] Permission denied"" during snakemake pipeline execution on cluster","I get an annoying write error during a pipeline execution which shutdown everything and cannot understand the reason. I don't know whether this is a bug or a usage problem, hence posting here before posting an issue on the Snakemake repo.
Snakemake version : 7.19.1
Describe the bug
I randomly get the following error during the execution of a pipeline on a cluster. At the moment, I can't find the reason/context that make it happen.
Looks like, for some reason, Snakemake cannot write a temporary .sh script. Although, it has no problem writing the same script for other wildcards sets.
It might be related to the cluster I'm using and not Snakemake, but I would like to make sure of that.
Logs
    Traceback (most recent call last):
      File &quot;/home/sjuhel/mambaforge/lib/python3.9/site-packages/snakemake/__init__.py&quot;, line 752, in snakemake
        success = workflow.execute(
      File &quot;/home/sjuhel/mambaforge/lib/python3.9/site-packages/snakemake/workflow.py&quot;, line 1089, in execute
        raise e
      File &quot;/home/sjuhel/mambaforge/lib/python3.9/site-packages/snakemake/workflow.py&quot;, line 1085, in execute
        success = self.scheduler.schedule()
      File &quot;/home/sjuhel/mambaforge/lib/python3.9/site-packages/snakemake/scheduler.py&quot;, line 592, in schedule
        self.run(runjobs)
      File &quot;/home/sjuhel/mambaforge/lib/python3.9/site-packages/snakemake/scheduler.py&quot;, line 641, in run
        executor.run_jobs(
      File &quot;/home/sjuhel/mambaforge/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 155, in run_jobs
        self.run(
      File &quot;/home/sjuhel/mambaforge/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 1156, in run
        self.write_jobscript(job, jobscript)
      File &quot;/home/sjuhel/mambaforge/lib/python3.9/site-packages/snakemake/executors/__init__.py&quot;, line 901, in write_jobscript
        with open(jobscript, &quot;w&quot;) as f:
    PermissionError: [Errno 13] Permission denied: '/data/sjuhel/BoARIO-inputs/.snakemake/tmp.7lly73mm/snakejob.run_generic.11469.sh'



Minimal example
The error doesn't happen when running a smaller pipeline.
Additional context
I am running the following pipeline of simulations on a cluster using SLURM.
Profile :
cluster:
  mkdir -p /scratchu/sjuhel/logs/smk/{rule} &amp;&amp;
  sbatch
    --parsable
    --mem={resources.mem_mb}
    --job-name=smk-{rule}-{wildcards}
    --cpus-per-task={threads}
    --output=/scratchu/sjuhel/logs/smk/{rule}/{rule}-{wildcards}-%j.out
    --time={resources.time}
    --partition={resources.partition}
default-resources:
  - mem_mb=2000
  - partition=zen16
  - time=60
  - threads=4
restart-times: 0
max-jobs-per-second: 10
max-status-checks-per-second: 1
local-cores: 1
latency-wait: 60
jobs: 16
keep-going: False
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True
conda-frontend: mamba
cluster-status: status-sacct.sh

The pipeline is runned via :
nohup snakemake generate_csv_from_all_xp --profile simple &gt; &quot;../Runs/snakelog-$(date +&quot;%FT%H%M%z&quot;).log&quot; 2&gt;&amp;1 &amp;

Rules to run (last execution with the error):
Using shell: /usr/bin/bash
Provided cluster nodes: 16
Job stats:
job                         count    min threads    max threads
------------------------  -------  -------------  -------------
generate_csv_from_all_xp        1              1              1
generate_csv_from_xp            3              1              1 -&gt; indicator aggregation
indicators                   4916              1              1 -&gt; symlink to exp folder (multiple exps can share similar results)
indicators_generic           4799              1              1 -&gt; indicators from simulations
init_all_sim_parquet            1              1              1
run_generic                  4771              4              4 -&gt; simulations to run
xp_parquet                      3              1              1 -&gt; regroup simulations by different scenarios
total                       14494              1              4


What I don't understand is that Snakemake is able to write other temporary .sh files, and I don't understand at which point the error happen. And I have no ideas on how to debug this.
Edit [23/01/2023] :
The issue might be related to exiting the ssh session on the cluster. Could it be possible that the nohuped snakemake process cannot write files once I am no longer connected to the server ?
→ I will try with screen instead of nohup.
",-1,-1,-1.0
75451239,How to reference input in params section of snakemake rule?,"I need to process my input file values, turning them into a comma-separated string (instead of white space) in order to pass them to a CLI program. To do this, I want to run the input files through a Python function. How can I reference the input files of a rule in the params section of the same rule?
This is what I've tried, but it doesn't work:
rule a:
    input:
        foo=&quot;a.txt&quot;,
        bar=expand({build}.txt,build=config[&quot;build&quot;]),
    output:
        baz=result.txt,
    params:
        joined_bar=lambda w: &quot;,&quot;.join(input.bar),  # this doesn't work
    shell:
        &quot;&quot;&quot;
        qux --comma-separated-files {params.joined_bar} \
            --foo {input.foo} \
            &gt;{output.baz}
        &quot;&quot;&quot;

It fails with:
InputFunctionException:
   AttributeError: 'builtin_function_or_method' object has no attribute 'bar'

Potentially related but (over-)complicated questions: 
How to define parameters for a snakemake rule with expand input 
Is Snakemake params function evaluated before input file existence?
",-1,-1,-1.0
75475062,Running different snakemake rules in parallel,"I show below a pseudocode version of my snakefile. Snakemake rule A creates the input files for Snakemake rule B2 and I would like to run Snakemake rules B1 and B2 at the same time but am not having success. I can run this snakefile successfully on very small data without a problem (although the Snakemake rules B1 and B2 do not run in parallel) but once I give it larger data it fails to create the output for Snakemake rule B1. The commands between Snakemake rule B1 and B2 use the same program but have different arguments and input files so I didn't think they should be in the same rule.
rule all:
    input: file_A_out, file_B1_out, file_B2_out, file_C_out

rule A:
    input: file_A_in
    output: file_A_out
    log: file_A_log
    shell: 'progA {input} --output {output}'

rule B1: 
    input: file_B1_in
    output: file_B1_out
    group: 'groupB'
    log: file_B1_log
    shell: 'progB {input} -x 100 -o {output}'

rule B2:
    input: file_A_out
    output: file_B2_out
    group: 'groupB'
    log: file_B2_log
    shell: 'progB {input} -x 1 --y -o {output}'

rule C:
    input: file_B1_out, file_B2_out 
    output: file_C_out
    log: file_C_log
    shell: 'progC {input[0]} {input[1]} -o {output}'

I thought using group to group the rules would indicate to Snakemake that the two rules can be ran at once. To execute snakemake I run nohup snakemake --cores 16 &gt; log.txt 2&gt;&amp;1 &amp; however, it only successfully runs rule B2 while the output of rule B1 is deemed corrupted. I have seen solutions on running one rule in parallel but what about running different rules in parallel?
Error in rule B1:
    jobid: 2
    input: 'file_B1_in'
    output: 'file_B1_out'
    log: 'file_B1_log'
 (check log file(s) for error details)
    shell: 'progB {input} -x 100 -o {output}'
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job B1 since they might be corrupted:
file_B1_out
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

",-1,-1,-1.0
75530584,Snakemake overwrites rules' log files when using retries - idiomatic way to make it append?,"I have a Snakemake workflow in which a rule can seemingly randomly fail. I'm using Snakemake 7.7.0 and have set retries for the rule using the retries directive. The command prints to stdout and stderr, and I want to append both to a logfile, keeping output from failed tries, so I can keep track of the failures. The simplified version of what I have is as follows:
rule flaky_rule:
    input:
        infile = &quot;{sample}/foo.txt&quot;
    output:
        outfile = &quot;{sample}/bar.txt&quot;
    retries: 3
    log:
        flaky_rule_log = &quot;{sample}/logs/flaky_rule.log&quot;
    shell:  
        &quot;&quot;&quot;
        flaky_script -i &quot;{input.infile}&quot; -o &quot;{output.outfile}&quot; &gt;&gt; &quot;{log.flaky_rule_log}&quot; 2&gt;&amp;1 
        &quot;&quot;&quot;


However, when I run this and the rule fails and is rerun, the logfile appears to be overwritten. For now, my workaround is to set the logfile in a params directive instead, but this of course will get me told off by the linter since I &quot;don't have a logfile set&quot; and feels a bit hackish to me. Is there a more idiomatic way to do this (either in this version or a higher one)?
",-1,-1,-1.0
75549710,snakemake one to many with checkpoints,"I am building a Snakemake pipeline and I have a rule that creates a folder and fills it with n files. I then want to run the next rule (checkpoint) on each of the n files, and aggregate the output.
Currently my issue is defining the input for the checkpoint rule. The previous rule outputs a directory, and the files in this directory have a constant naming structure range(n).fa (where range(n) signifies 0:(n-1)). I have been having trouble getting the checkpoint to accept these files as input.
Here is my current structure:
...

rule make_ORF_dir:
    input:
       &quot;snake_test/Storage/ORFs.csv&quot;
    output:
        region_split = directory(&quot;snake_test/Storage/split_ORFs/&quot;)
    params:
        chnks=&quot;snake_test/Storage/split_ORFs/{chunk}.fa&quot;
    conda:
        &quot;../enviroment.yml&quot;
    shell:
        &quot;python workflow/scripts/Split_ORFs.py --ORF_csv {input} --out_dir {output} --num_splits 1000&quot;



checkpoint find_novel:
    input:
        expand(&quot;snake_test/Storage/split_ORFs/{chunk}.fa&quot;, chunk = range(1000))
    output:
        directory(&quot;snake_test/Results/{chunk}&quot;)
    conda:
        &quot;../enviroment.yml&quot;
    shell:
        &quot;python scripts/CLI.py --sequences {input} --db_file ../Data/ref.fa --result_dir {output}&quot;


...

This results in the error:
Missing input files for rule find_novel:
    output: snake_test/Results
    affected files:
        snake_test/Storage/split_ORFs/848.fa
        snake_test/Storage/split_ORFs/278.fa
        ...

",-1,-1,-1.0
75630572,What is the proper way to access values in nested dictionaries with Snakemake?,"I often want to process several entities in my analysis. I describe them the following way in my config.yml file:
entities:
    fbr:
        full_name: &quot;FUBAR&quot;
        filepath: &quot;data/junk_1.txt&quot;
    snf:
        full_name: &quot;SNAFU&quot;
        filepath: &quot;data/junk_2.txt&quot;

What is the idiomatic way to pass file paths as an input to a rule? I know that I can do it with a list comprehension, here is a toy Snakefile:
configfile: &quot;config.yml&quot;

rule all:
    input:
        &quot;data/result.txt&quot;

rule first:
    input:
        expand(&quot;{entity}&quot;, entity=[x[&quot;filepath&quot;] for x in config[&quot;entities&quot;].values()])
    output:
        &quot;data/result.txt&quot;
    shell:
        &quot;cat {input} &gt; {output}&quot;


This approach works but feels cumbersome and ugly. Is there a better way to perform expansion with nested dictionaries in Snakemake?
",-1,-1,-1.0
75673029,Bash code with if/else/fi and awk/perl regexps does work outside snakemake but not inside a snakemake rule,"I am constructing a snakemake workflow that is working well for the most part. One of the steps requires a ready.gtf file containing certain fields of information in its last column, specifically gene_name and transcript_name.
I have thought of some bash code that can help the pipeline detect if this information is already present, and decide what to do based on this. In case the field 9 contains that information (i.e. some string in the field 9 matches the regexp (gene|transcript)_name ), then nothing needs to be done to the gtf (except making sure there is no = signs in the file) and it just replaces the = signs for spaces ' ' and outputs this to a ready.gtf file. If this information is not present (i.e. there is no match to the (gene|transcript)_name in the field9), then it parses column 9 using perl regexps and, using paste, it appends this to the input gtf and creates a ready.gtf. For clarity, the expression /.*?(gene|transcript)_id=([^;]+).*?/ aims at identifying and capture anything that falls within the gene_id or trancript_id field(s) (capture group 1), store the corresponding value (which can be any character except the field separator &quot;;&quot;) in a capture group (capture group 2), and later &quot;dump&quot; it using /;\1_name=\2;/ (flanking semicolons are intended). Thus effectively detecting and changing the gene_id and transcript_id for gene_name and transcript name. Appending them at the end is also intended, since the ready.gtf needs BOTH the gene/transcript_id and the gene/transcript_name fields.
The bash code looks like below. I am able to run this well outside snakemake, meaning that it detects gtf files that have and do not have the required info on field9, and it decides accordingly on what to do.
field9=`awk ' $3 == &quot;transcript&quot; ' metadata/parsed.gtf | head -n1 | cut -f9`

pattern='(gene|transcript)_name'

if [[ $field9 =~ $pattern ]]; then
    echo &quot;Detected gene/transcript_name in field9, no need for preparing the gtf&quot;
    perl -pe &quot;s/=/ /g&quot; metadata/parsed.gtf &gt; metadata/ready.gtf

else
    echo &quot;gene_name and/or transcript_name not detected in field9&quot;
    cut -f9 metadata/parsed.gtf | perl -pe 's/.*?(gene|transcript)_id=([^;]+).*?/;\1_name=\2;/g' | perl -pe 's/;;+/;/g' &gt; lane9
    paste -d ' ' metadata/parsed.gtf lane9 | perl -pe &quot;s/=/ /g&quot; &gt; metadata/ready.gtf

And when inside the snakemake rule:
rule preparegtf:
    input:&quot;metadata/{organism}_parsed.gtf&quot;
    output:&quot;metadata/{organism}_ready.gtf&quot;
    params:
        awkexpr = r&quot;&quot;&quot; $3 == &quot;transcript&quot; &quot;&quot;&quot;,
        perlexpr1 = r&quot;&quot;&quot;s/.*?(gene|transcript)_id=([^;]+).*?/;\1_name=\2;/g&quot;&quot;&quot;,
        perlexpr2 = r&quot;&quot;&quot;s/;;+/;/g&quot;&quot;&quot;
    shell:
        '''
        field9=`awk {params.awkexpr:q} {input} | head -n1 | cut -f9`
        pattern='(gene|transcript)_name'
        if [[ $field9 =~ $pattern ]]; then
            echo &quot;Detected gene_name and/or transcript_name in field9, no need for preparing the gtf&quot;
            perl -pe &quot;s/=/ /g&quot; {input} &gt; {output}
        else
            echo &quot;gene_name and/or transcript_name not detected in field9&quot;
            cut -f9 {input} | \
            perl -pe {params.perlexpr1:q} | perl -pe {params.perlexpr2:q} &gt; lane9
            paste -d ' ' {input} lane9 | perl -pe &quot;s/=/ /g&quot; &gt; {output}
        fi
        '''

I have made sure that the workflow works well regardless of this rule. When I  provide the ready.gtf in the expected path, snakemake is able to see it and it does not run the rule above, instead following up with the next ones, and succesfully finishing the pipeline. Likewise the --dry-run works well. Hence I do NOT get any InputNotFound or OutputNotFound errors. Note I also took care, to the best of my knowledge, of having the regexps in the r&quot;&quot;&quot;REGEXP&quot;&quot;&quot; format to help proper quotation. I am not sure the error might be coming from here.
The snakemake run fails with an undisclosed error provided it runs on strict mode.
Below the log of snakemake:
(snakemake_venv) alberto@nostromo:~/projects/dev/pipe/scbe_pipe$ snakemake --cores 8 --use-conda --rerun-incomplete
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job stats:
job                     count    min threads    max threads
--------------------  -------  -------------  -------------
all                         1              1              1
collect_metadata            1              1              1
digitalexpression           1              1              1
finalstep                   1              1              1
makeintervals               1              1              1
makerefflat                 1              1              1
mergebamalignment           1              1              1
preparegtf                  1              1              1
reducegtf                   1              1              1
sortsam                     1              1              1
star                        1              1              1
starindex                   1              1              1
tagbam_genefunction         1              1              1
tagbam_geneintervals        1              1              1
total                      14              1              1

Select jobs to execute...

[Wed Mar  8 11:14:19 2023]
rule preparegtf:
    input: metadata/testorganism_parsed.gtf
    output: metadata/testorganism_ready.gtf
    jobid: 27
    wildcards: organism=testorganism
    resources: tmpdir=~/tmp

[Wed Mar  8 11:14:19 2023]
Error in rule preparegtf:
    jobid: 27
    output: metadata/testorganism_ready.gtf
    shell:
        
        field9=`awk ' $3 == &quot;transcript&quot; ' metadata/testorganism_parsed.gtf | head -n1 | cut -f9`
        pattern='(gene|transcript)_name'
        if [[ $field9 =~ $pattern ]]; then
            echo &quot;Detected gene_name and/or transcript_name in field9, no need for preparing the gtf&quot;
            perl -pe &quot;s/=/ /g&quot; metadata/testorganism_parsed.gtf &gt; metadata/testorganism_ready.gtf
        else
            echo &quot;gene_name and/or transcript_name not detected in field9&quot;
            cut -f9 metadata/testorganism_parsed.gtf |             perl -pe 's/.*?(gene|transcript)_id=([^;]+).*?/;\1_name=\2;/g' | perl -pe 's/;;+/;/g' &gt; lane9
            paste -d ' ' metadata/testorganism_parsed.gtf lane9 | perl -pe &quot;s/=/ /g&quot; &gt; metadata/testorganism_ready.gtf
        fi
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: ~/projects/dev/pipe/scbe_pipe/.snakemake/log/2023-03-08T111403.605948.snakemake.log

To be honest I am a bit at a loss. Since the bash code works outside snakemake and not inside, I am unable to debug as I would normally do. I would appreciate some feedback and the experience of a more trained eye. I can provide a minimal gtf as an example if that helps. Thank you in advance.
Things I have tried:
After asking ChatGPT, it occurred to me to change the perl expressions for awk, since I am more familiar with quotation for awk, to change echo for printf calls, and to change the backticks for $() format. See below. Snakemake however still fails with an identical error.
rule preparegtf:
    input:&quot;metadata/{organism}_parsed.gtf&quot;
    output:&quot;metadata/{organism}_ready.gtf&quot;
    params:
        awkexpr1 = r&quot;&quot;&quot; $3 == &quot;transcript&quot; &quot;&quot;&quot;,
        awkexpr2 = r&quot;&quot;&quot;{{print $9}}&quot;&quot;&quot;,
        awkexpr3 = r&quot;&quot;&quot;{{gsub(/.*?(gene|transcript)_id=([^;]+).*?/;\1_name=\2;&quot;); print $0}}&quot;&quot;&quot;,
        awkexpr4 = r&quot;&quot;&quot;{{gsub(/;;+/,/;/); print $0}}&quot;&quot;&quot;,
        awkexpr5 = r&quot;&quot;&quot;{{gsub(/=/,&quot; &quot;); print $0}}&quot;&quot;&quot;
    shell:
        '''
        field9=$(awk {params.awkexpr1:q} {input} | awk {params.awkexpr2:q} | head -n1)
        pattern='(gene|transcript)_name'
        if [[ $field9 =~ $pattern ]]; then
            printf &quot;Detected gene_name and/or transcript_name in field9, no need for preparing the gtf\n&quot;
            perl -pe &quot;s/=/ /g&quot; {input} &gt; {output}
        else
            printf &quot;gene_name and/or transcript_name not detected in field9\n&quot;
            awk {params.awkexpr3:q} | \
            perl -pe {params.awkexpr4:q} &gt; lane9
            paste -d ' ' temp1.gtf lane9 | awk {params.awkexpr5:q} &gt; {output}
        fi
        '''

",1,1,-1.0
75782120,Filter files in directory by filename pattern for a snakemake pipeline,"I have a directory with files that have the following pattern: &lt;ID&gt;.&lt;date&gt;.json. Some of the IDs are duplicated with different dates. I want to filter the files with the most recent date for each ID.
How can I implement this filtering step in snakemake?
I already have a python function that returns a list of filtered files. This is a minimal example of the snakemake file:

import os
import re

dir_path = &quot;path/to/dir/&quot;
out_dir = &quot;path/to/result_dir&quot;

def get_most_recent_files(wildcards):

  dir = dir_path
  file_pattern = re.compile(r&quot;([A-Za-z0-9]+)\.(\d{4}-\d{2}-\d{2}).json$&quot;)
  id_files = {}

  for filename in os.listdir(dir): 
    match = file_pattern.match(filename)
    if match: 
        id, date = match.groups()
        id_files.setdefault(id, []).append((filename, date))

  most_recent_files = {}
  for id, files in id_files.items(): 
    files.sort(key=lambda x: x[1], reverse=True)
    most_recent_files[id] = files[0][0]
  
  return [dir + file for file in most_recent_files.values()]


rule all: 
    input: 
        out_dir + &quot;{ID}_parsed.csv&quot;

rule parse_jsons: 
    input: 
        get_most_recent_files
    output: 
        out_dir + &quot;{ID}_parsed.csv&quot;
    script: 
        &quot;parser.py&quot;

But like this I get the error:
Wildcards in input files cannot be determined from output files: 'ID'
So I am not sure where I should provide the wildcard &quot;PID&quot;.
I tried different approaches with glob_wildcards and expand, but nothing really worked.
",-1,-1,-1.0
75805587,Snakemake: MissingInputException in rule - Missing input files for rule sam_to_bam:,"I started to learn Snakemake and here is my first code:
SAM_DIR = &quot;/mnt/c/Code/sam_tiny&quot;
BAM_DIR = &quot;/mnt/c/Code/bam&quot;

SAM_FILES = glob_wildcards(os.path.join(SAM_DIR, &quot;{sample}.sam&quot;))
SAMPLES = [wildcards[0] for wildcards in SAM_FILES]

rule all:
    input:
        expand(os.path.join(STATS_DIR, &quot;{sample}.txt&quot;), sample=SAM_FILES)

rule sam_to_bam:
    input:
        os.path.join(SAM_DIR, &quot;{sample}.sam&quot;)
    output:
        os.path.join(BAM_DIR, &quot;{sample}.bam&quot;)
    shell:
        &quot;samtools view -b {input.sam} &gt; {output.bam}&quot;

and, of course, it was not without error.
On running it gives the following error:
MissingInputException in rule sam_to_bam in file /mnt/c/Code/Snakefile.smk, line 16:
Missing input files for rule sam_to_bam:
    output: /mnt/c/Code/bam/['ERR024604_tiny', 'ERR024605_tiny', 'ERR024606_tiny', 'ERR024607_tiny', 'ERR024608_tiny', 'ERR024609_tiny'].bam
    wildcards: sample=['ERR024604_tiny', 'ERR024605_tiny', 'ERR024606_tiny', 'ERR024607_tiny', 'ERR024608_tiny', 'ERR024609_tiny']
    affected files:
        /mnt/c/Code/sam_tiny/['ERR024604_tiny', 'ERR024605_tiny', 'ERR024606_tiny', 'ERR024607_tiny', 'ERR024608_tiny', 'ERR024609_tiny'].sam

I have already checked several times the correct path to the SAM files, the correctness of the files themselves, etc., but I can not understand what the problem is. This might be the easiest problem, but since I just started learning this, I need some support. Thanks!
",-1,-1,-1.0
76005956,Error with write.table in Rscript when used in snakemake workflow,"I'm using a snakemake which call an R script  , but I'm getting this error :
Error in isOpen(file, &quot;w&quot;) : invalid connection
Calls: write.table -&gt; isOpen
Execution halted

my rule in snakemake is :
rule prepare_files:
input: 
       x = &quot;path_to_x/quant.sf&quot;, 
       y = &quot;path_to_Y/quant.sf&quot;
output: 
       &quot;out//Bigquantif.txt&quot;   
script:
   &quot;R/manips.R&quot;

In manips.R i have :
out_file_path &lt;- snakemake@output
x_path &lt;-snakemake@input$x
y_path &lt;-snakemake@input$y
x_fi &lt;- read.table(x_path, head=T, sep=&quot;\t&quot;, row.names=1)
y_fi &lt;- read.table(y_path, head=T, sep=&quot;\t&quot;, row.names=1)
merged_data_final &lt;- merge(x_fi, y_fi, by = &quot;gene_id&quot;)
write.table(x = merged_data_final, file = out_file_path, sep=&quot;\t&quot;)

I tried with write.csv and csv2 but always the same error !!
If any one can help, I will be thankful
I was expecting that was a problem of access rights on the repertory i changed it by chmod +x
but this didn'nt fixe it
",-1,-1,-1.0
76058889,Snakemake wildcards from python wrapper not found,"I am new to using snakemake and I am having difficulty. I have written a python wrapper file to run make snakefile. The wrapper takes a samples.txt with taxon, read1, read2, adapters, and assembly (Y/N). If assembly is needed, the snakefile with assembly is run, if N, a snakefile without assembly is run. It seems like the correct snakefile is being run, but the information for the wildcards isn't being transferred from the wrapper to the snakefile.
samples.txt
Solatube DRR350770_1.fastq.gz DRR350770_2.fastq.gz all.fa Y

run_pipeline.py
import argparse
import os

parser = argparse.ArgumentParser(description=&quot;Run Snakemake pipeline with Trinity assemble or not&quot;)
parser.add_argument(&quot;samples&quot;, help=&quot;path to samples.txt&quot;)
args = parser.parse_args()

with open(args.samples) as f:
    for line in f:
        taxon, read1, read2, adapters, assembly = [x.strip() for x in line.split()]
        if assembly == &quot;Y&quot;:
            os.system(f&quot;snakemake -s SNPwAssembly.smk --cores 16 --config taxon={taxon} read1={read1} read2={read2} adapters={adapters}&quot;)
        elif assembly == &quot;N&quot;:
            os.system(f&quot;snakemake -s SNPwoAssembly≈.smk --cores 16 --config taxon={taxon} read1={read1} read2={read2} adapters={adapters}&quot;)
        else:
            print(f&quot;Invalid value for assembly in line: {line}&quot;)

SNPwAssembly.smk begins:
import os

# define rules                                                                                                                                                                                  
rule all:                                                                                                                                                                                               
    input: expand(&quot;{taxon}.snpden&quot;, taxon = taxon)                                                                                                                                                         
                                                                                                                                                                                                        
rule trinity_assembly:                                                                                                                                                                                  

input:                                                                                                                                                                                              
    read1 = &quot;{taxon}/{read1}&quot;,                                                                                                                                                                      
    read2 = &quot;{taxon}/{read2}&quot;,                                                                                                                                                                      
    adapters = &quot;{adapters}&quot;                                                                                                                                                                         
output:                                                                                                                                                                                             
    &quot;{taxon}/trinity_out_dir/Trinity.fasta&quot;                                                                                                                                                         
threads: 15                                                                                                                                                                                         
shell:                                                                                                                                                                                              
    '''                                                                                                                                                                                             
    singularity exec -e {trinity_path} Trinity --seqType fq --max_memory 450G --left SRR12102895_1.fastq.gz --right SRR12102895_2.fastq.gz --CPU 15 --trimmomatic --quality_trimming_params &quot;all.fa:2:30:10 SLIDINGWINDOW:4:5 LEADING:5 TRAILING:5 MINLEN:25&quot; --output trinity_out_dir                                                                                                                                                                                         
    '''     

when I run python3 run_pipeline.py samples.txt, I get the error:
name 'taxon' is not defined

",-1,-1,-1.0
76100616,split args of my script into multiple lines in snakemake,"as simple as it sounds it is not working for me!
I wrote a workflow with snakemake, in which I'm trying to call my own script with the neccessary options. I would like to split the command into multiple lines as follows (simplified version):
rule stats:
&quot;&quot;&quot;
create summary of fasta file
&quot;&quot;&quot;
input:
   config[&quot;In_dir&quot;] + &quot;Fasta/{sample}.fasta&quot;
params:
   i = config[&quot;i&quot;],
   nseq = 100,
   l = config[&quot;l&quot;], 
   l_mismatch = 3,
   v_dist = 5, 
output:
   config[&quot;Out_dir&quot;] + &quot;/{sample}_stats.csv&quot;,
shell:
   &quot;&quot;&quot;  
   scripts/test_args.py -f {input} \ 
   -l {params.l} -i {params.i} \  
   -ld {params.l_mismatch} \ 
   -vd {params.v_dist} --nseq {params.nseq} \ 
   -o {output}  &amp;&gt; {log}
   &quot;&quot;&quot;  

However, the script is not recognizing the args, instead it prints the usage command with the required options. Since I'm importing argparse to provide the options, the default behaviour is to print the usage of the script if the required options were not given (see example bellow):

I tried other ways as for example in here but still not working. The only format it would work, is to have them all in one line. What am I missing? I already checked my input and the validity of the options, it works only if it is one line?!
Thanks for any tip.
",-1,-1,-1.0
76109079,snakemake threads wildcards not found,"I have a snakemake rule like:
rule get_coverage:
    input: 
        cram=lambda wildcards: expand(config['input']['cram'][wildcards.reference_genome], #access_id = access_id[wildcards.sample_name], 
                                        sample_name='{sample_name}'), 
        bai=lambda wildcards: expand(config['input']['cram'][wildcards.reference_genome] + '.crai', #access_id = access_id[wildcards.sample_name], 
                                        sample_name='{sample_name}'),
        ref=lambda wildcards: config['reference_genome'][wildcards.reference_genome],
        chrom_size=lambda wildcards: config['chrom_size'][wildcards.reference_genome]
    output: 
        coverage=directory(config['output']['median_coverage'])
    conda: 
        src + '/env/acc_mask.yml'
    threads: 19
    script: 
        &quot;&quot;&quot;
        mosdepth \
            -n \
            -t {threads} \
            --use-median \
            -f {input.ref} \
            -b {input.chrom_size} \
            {output.median_coverage} \
            {input.cram}
        &quot;&quot;&quot;

I got a error:
RuleException:
NameError in line xxx of xxx.smk:
The name 'threads' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be ...

In my impression, threads, as a wildcards, can be replaced in the shell. Is there anything wrong with my script?
",-1,-1,-1.0
76137846,Snakemake Cloud Execution & Large Files,"I've read the documentation (https://snakemake.readthedocs.io/en/stable/executing/cloud.html) and run the example project for the google life sciences executor. I am able to run my own workflows, but I'm having trouble running on large datasets.
It appears that snakemake is downloading to my local workstation the files generated by the jobs executed in the cloud. Some of these are much larger than disk space I have available, and I don't see a CLI flag to disable this, which makes me think there is a deeper conceptual issue.
In my mind, I would like my workstation to have light resource requirements and simply orchestrate execution in the cluster. I guess that since rules can conditionally execute based on inputs that I have to run snakemake from an environment that can locally store all of the intermediate results?
For context here's an example of the command I'm executing:
snakemake --google-lifesciences --google-lifesciences-region us-west1 --default-remote-prefix my-bucket --use-conda -j 3

It will populate a local directory my-bucket/ with the output tree. So, if I have a step that blasts against NR, I need 500+GB of local storage which seems very resource inefficient if those files are only needed on job.
",-1,-1,-1.0
76221953,Conflicts between Snakemake and GROMACS?,"I tried to simplify my issue as much as possible and still getting the error.
The whole idea is that I want to execute (inside a much more complex workflow) the command:
gmx mdrun -nt 12 -deffnm emin -cpi on a cluster. For that I have a conda environment with GROMACS and Snakemake.
On a traditional way I have the jobscript (traditional_job.sh) with:
#!/bin/bash
#SBATCH --partition=uds-hub 
#SBATCH --nodes=1
#SBATCH --cpus-per-task=12 
#SBATCH --mem=5000
#SBATCH --time=15:00
#SBATCH --job-name=reproduce_error
#SBATCH --output=reproduce_error.o
#SBATCH --error=reproduce_error.e

gmx mdrun -nt 12 -deffnm emin -cpi

And everything works as expected after sbatch traditional_job.sh. However, if I try to use Snakemake instead, the problems start.
My Snakefile is:
rule gmx:
    input:
        tpr = &quot;emin.tpr&quot;
    output:
        out = 'emin.gro'
    shell:
        '''
           gmx mdrun -nt 12 -deffnm emin -cpi
        '''

And my job.sh:
#!/bin/bash
snakemake \
    --jobs 10000 \
    --verbose \
    --debug-dag \
    --latency-wait 50 \
    --cluster-cancel scancel \
    --rerun-incomplete \
    --keep-going \
    --cluster '
        sbatch \
        --partition=uds-hub \
        --nodes=1 \
        --cpus-per-task=12 \
        --mem=5000 \
        --time=15:00 \
        --job-name=reproduce_error \
        --output=reproduce_error.o \
        --error=reproduce_error.e '

After ./job.sh, the GROMACS's error (written out on reproduce_error.e) is:
Program:     gmx mdrun, version 2022.2-conda_forge
Source file: src/gromacs/taskassignment/resourcedivision.cpp (line 220)

Fatal error:
When using GPUs, setting the number of OpenMP threads without specifying the
number of ranks can lead to conflicting demands. Please specify the number of
thread-MPI ranks as well (option -ntmpi).

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors

I observed that in the output of snakemake is written a different shell (#!/bin/sh). But honestly, I do not know if that could be the problem, neither how to solve it if that is the case:
Jobscript:
#!/bin/sh
# properties = {&quot;type&quot;: &quot;single&quot;, &quot;rule&quot;: &quot;gmx&quot;, &quot;local&quot;: false, &quot;input&quot;: [&quot;emin.tpr&quot;], &quot;output&quot;: [&quot;emin.gro&quot;], &quot;wildcards&quot;: {}, &quot;params&quot;: {}, &quot;log&quot;: [], &quot;threads&quot;: 1, &quot;resources&quot;: {&quot;mem_mb&quot;: 1000, &quot;mem_mib&quot;: 954, &quot;disk_mb&quot;: 1000, &quot;disk_mib&quot;: 954, &quot;tmpdir&quot;: &quot;&lt;TBD&gt;&quot;}, &quot;jobid&quot;: 0, &quot;cluster&quot;: {}}
cd '/home/uds_alma015/GIT/BindFlow/ideas/reproduce error' &amp;&amp; /home/uds_alma015/.conda/envs/abfe/bin/python3.9 -m snakemake --snakefile '/home/uds_alma015/GIT/BindFlow/ideas/reproduce error/Snakefile' --target-jobs 'gmx:' --allowed-rules 'gmx' --cores 'all' --attempt 1 --force-use-threads  --resources 'mem_mb=1000' 'mem_mib=954' 'disk_mb=1000' 'disk_mib=954' --wait-for-files '/home/uds_alma015/GIT/BindFlow/ideas/reproduce error/.snakemake/tmp.lfq5jq4u' 'emin.tpr' --force --keep-target-files --keep-remote --max-inventory-time 0 --nocolor --notemp --no-hooks --nolock --ignore-incomplete --rerun-triggers 'software-env' 'params' 'input' 'mtime' 'code' --skip-script-cleanup  --conda-frontend 'mamba' --wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/' --latency-wait 50 --scheduler 'ilp' --scheduler-solver-path '/home/uds_alma015/.conda/envs/abfe/bin' --default-resources 'mem_mb=max(2*input.size_mb, 1000)' 'disk_mb=max(2*input.size_mb, 1000)' 'tmpdir=system_tmpdir' --mode 2 &amp;&amp; touch '/home/uds_alma015/GIT/BindFlow/ideas/reproduce error/.snakemake/tmp.lfq5jq4u/0.jobfinished' || (touch '/home/uds_alma015/GIT/BindFlow/ideas/reproduce error/.snakemake/tmp.lfq5jq4u/0.jobfailed'; exit 1)

P.s. ChatGPT goes in circle with this question :-)
Update
I even isolate more the error. The following other_job.sh (submitted to the cluster as sbatch other_job.sh) script also gave the same error:
#!/bin/bash
#SBATCH --partition=uds-hub 
#SBATCH --nodes=1
#SBATCH --cpus-per-task=12 
#SBATCH --mem=5000
#SBATCH --time=15:00
#SBATCH --job-name=reproduce_error
#SBATCH --output=reproduce_error.o
#SBATCH --error=reproduce_error.e

/home/uds_alma015/.conda/envs/abfe/bin/python3.9 -m snakemake --snakefile '/home/uds_alma015/GIT/BindFlow/ideas/reproduce_error/Snakefile' --target-jobs 'gmx:' --allowed-rules 'gmx' --cores 'all' --attempt 1 --force-use-threads  --resources 'mem_mb=1000' 'mem_mib=954' 'disk_mb=1000' 'disk_mib=954' --wait-for-files '/home/uds_alma015/GIT/BindFlow/ideas/reproduce_error/.snakemake/tmp.mqan6qbp' 'emin.tpr' --force --keep-target-files --keep-remote --max-inventory-time 0 --nocolor --notemp --no-hooks --nolock --ignore-incomplete --rerun-triggers 'mtime' 'software-env' 'params' 'code' 'input' --skip-script-cleanup  --conda-frontend 'mamba' --wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/' --latency-wait 50 --scheduler 'ilp' --scheduler-solver-path '/home/uds_alma015/.conda/envs/abfe/bin' --default-resources 'mem_mb=max(2*input.size_mb, 1000)' 'disk_mb=max(2*input.size_mb, 1000)' 'tmpdir=system_tmpdir' --mode 2

And this is the command build by snakemake. It looks like that command does not interact (in some how) with the definitions of SBATCH. But still not sure.
",-1,-1,-1.0
76226657,How to create an unknown number of files with snakemake and use them in the next snakemake rule?,"I am doing metagenomics and am running into problems with my snakemake rule below.
My input file is a modified samtools coverage files containing sequence length, accession numbers and their taxon ids per row.
The rule has two goals:

Identify the accession number with the longest nuclotide sequences per taxid and write it to a text file
Identify all accession numbers assigned to the same taxid and write them to a text file

This is my rule:
rule extract_accessions:
    output:
        accession_with_longest_sequence_per_taxid=&quot;xy/relevant_accessions/{sample}_taxid_taxid_max_len.txt&quot;,
        accessions_per_taxid=&quot;xy/relevant_accessions/accessions_per_taxid/{sample}_taxid_taxid.txt&quot;,
    input:
        rules.generate_coverage_table.output.table,
    params:
        max_length_accession_out_path=&quot;xy/relevant_accessions/&quot;,
        acc_list_per_taxid_out_path=&quot;xy/relevant_accessions/accessions_per_taxid&quot;,
    conda: &quot;python39&quot;
    script:
        &quot;~/map_accession_number_to_taxid.py&quot;

rule generate_coverage_table creates a outputs like this: bv00110_S1_R1R2_sorted_filtered.txt where bv00110_S1 corresponds to the sample name {sample}
rule filter_by_mapQ creates bam files like this: bv00110_S1_R1R2_sorted_filtered.sam (again bv00110_S1 is the sample name {sample})
When executing the workflow outputfiles are created as intended but  I get a MissingOutputException:
Waiting at most 5 seconds for missing files.
MissingOutputException in rule extract_accessions in file ~/snakefile, line 440:
Job 11  completed successfully, but some output files are missing. Missing files after 5 seconds. This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait:
xy/relevant_accessions/bv00120_S5_taxid_taxid_max_len.txt
xy/relevant_accessions/accessions_per_taxid/bv00120_S5_taxid_taxid.txt
Shutting down, this might take some time.
Ths error is expected as the python script creates files where the second &quot;taxid&quot; is replaced by the actual taxon IDs found in the input file. However, I do neither know which taxon IDs will be found in the input, nor how many will be found. So, I also do not know how many files will be created previous to executing the rule.
In the next step of my workflow, I need to use the created files to extract alignments from a sam file. For this purpose, I need to know the output names (I guess, I would need to use the {sample} wildcard and somehow defne a new {taxid}wilcard.
rule extract_reads_from_sam:
        output: &quot;z/{sample}/taxid_{taxid}_alignment_subset.bam&quot;,
        input: rules.extract_accessions.output.accessions_per_taxid
        conda samtools
        shell: &quot;&quot;&quot;
               samtools view  -b &lt;path_to_sam&gt; $(cat {input}) &gt; {output}
               &quot;&quot;&quot;

I am not sure which filenames I should put into the output section of the rule extract_accessions. Can anyone help me out?
Thank you so much!
I tried to get rid of the error by replacing the content of output section by &quot;xy/relevant_accessions/job.done&quot;. I let the python script create such a file and expected that the error disappears. However, snakemake threw the following error:
Wildcards in input files cannot be determined from output files:
'sample'
",-1,-1,-1.0
76267968,Snakemake wrappers not working on SLURM cluster compute nodes without internet,"I am trying to use wrappers in my pipeline on a SLURM cluster, where the compute nodes do not have internet access.
I have run the pipeline with --conda-create-envs-only first, and then changed the wrapper: directives to point to local folders containing environment.yaml files.
Jobs fail without specific error. Test rule with the same configuration but without wrappers works. Rules with wrappers work fine on login node if I switch back the wrapper: directive to look online.
I am running:
snakemake --profile myprofile --cores 40 --use-conda

Example rule:
# Run FastQC on the fastq.gz files
rule fastqc_fastq_gz:
    input:
        input_dir + &quot;{sample}_{read}_001.fastq.gz&quot;,
    output:
        html = output_dir + &quot;fastqc/{sample}_{read}_fastqc.html&quot;,
        zip = output_dir + &quot;fastqc/{sample}_{read}_fastqc.zip&quot;,
    params: 
        extra = &quot;--quiet&quot;,
    log:
        output_dir + &quot;logs/fastqc/{sample}_{read}.log&quot;,
    threads: 1,
    resources:
        mem_mb = 1024,   
    wrapper:
        # &quot;file:///path/envs/v1.31.0/bio/fastqc/&quot;   # &lt;- Fails on both login and compute nodes
        &quot;v1.31.0/bio/fastqc&quot;                        # &lt;- Works on login node to download env

I have also tried using more resources, same behavior.
My profile:
cluster:
  mkdir -p logs/{rule} &amp;&amp;
  sbatch
    --partition={resources.partition}
    --qos={resources.qos}
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --job-name=smk-{rule}-{wildcards}
    --output=logs/{rule}/{rule}-{wildcards}-%j.out
    --error=logs/{rule}/{rule}-{wildcards}-.%j.err
    --account=&lt;account&gt;
    --time={resources.time}
    --parsable
default-resources:
  - partition=&lt;partition&gt;
  - qos=sbatch
  - mem_mb=&quot;490G&quot;
  - tmpdir=&quot;/path/to/temp/&quot;
  - time=&quot;0-10:00:00&quot;
max-jobs-per-second: 10
max-status-checks-per-second: 1
latency-wait: 60
jobs: 16
keep-going: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True
cluster-status: status-sacct.sh

The submission log reads:
Error in rule fastqc_fastq_gz:
    jobid: 38
    input: /path/raw/S9_S9_R2_001.fastq.gz
    output: /path/pipeline_out/fastqc/S9_S9_R2_fastqc.html, /path/pipeline_out/fastqc/S9_S9_R2_fastqc.zip
    log: /path/pipeline_out/logs/fastqc/S9_S9_R2.log (check log file(s) for error details)
    conda-env: /path/.snakemake/conda/a116f377bbaddedd93b228a3d4f74b1d_
    cluster_jobid: 1491196

Error executing rule fastqc_fastq_gz on cluster (jobid: 38, external: 1491196, jobscript: /path/.snakemake/tmp.l0raomfw/snakejob.fastqc_fastq_gz.38.sh). For error details see the cluster log and the log files of the involved rule(s).

The tmp files do not exist.
Job logs simply read:
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 80
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=1000, mem_mib=954, disk_mb=7876, disk_mib=7512
Select jobs to execute...

[Date Time]
rule fastqc_fastq_gz:
    input: /path/raw/S9_S9_R2_001.fastq.gz
    output: /path/pipeline_out/fastqc/S9_S9_R2_fastqc.html, /path/pipeline_out/fastqc/S9_S9_R2_fastqc.zip
    log: /path/pipeline_out/logs/fastqc/S9_S9_R2.log
    jobid: 0
    reason: Missing output files: /path/pipeline_out/fastqc/S9_S9_R2_fastqc.zip, /path/pipeline_out/fastqc/S9_S9_R2_fastqc.html
    wildcards: sample=S9_S9, read=R2
    threads: 2
    resources: mem_mb=1000, mem_mib=954, disk_mb=7876, disk_mib=7512, tmpdir=/path/tmp/snakemake, partition=el7taskp, qos=sbatch, time=0-40:00:00

[Date Time]
Error in rule fastqc_fastq_gz:
    jobid: 0
    input: /path/raw/230319/S9_S9_R2_001.fastq.gz
    output: /path/pipeline_out/fastqc/S9_S9_R2_fastqc.html, /path/pipeline_out/fastqc/S9_S9_R2_fastqc.zip
    log: /path/pipeline_out/logs/fastqc/S9_S9_R2.log (check log file(s) for error details)
    conda-env: /path/.snakemake/conda/a116f377bbaddedd93b228a3d4f74b1d_

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

FastQC logs are empty.
If I run on compute nodes with the wrapper directives pointing to github it fails with error:
Building DAG of jobs...
Failed to open source file https://github.com/snakemake/snakemake-wrappers/raw/v1.31.0/bio/fastqc/environment.yaml
ConnectionError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /snakemake/snakemake-wrappers/raw/v1.31.0/bio/fastqc/environment.yaml (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7fd067df0810&gt;: Failed to establish a new connection: [Errno 113] No route to host')), attempt 1/3 failed - retrying in 3 seconds...
Failed to open source file https://github.com/snakemake/snakemake-wrappers/raw/v1.31.0/bio/fastqc/environment.yaml
ConnectionError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /snakemake/snakemake-wrappers/raw/v1.31.0/bio/fastqc/environment.yaml (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7fd067df2150&gt;: Failed to establish a new connection: [Errno 113] No route to host')), attempt 2/3 failed - retrying in 6 seconds...
Failed to open source file https://github.com/snakemake/snakemake-wrappers/raw/v1.31.0/bio/fastqc/environment.yaml
ConnectionError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /snakemake/snakemake-wrappers/raw/v1.31.0/bio/fastqc/environment.yaml (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7fd067dfc250&gt;: Failed to establish a new connection: [Errno 113] No route to host')), attempt 3/3 failed - giving up!
WorkflowError:
Failed to open source file https://github.com/snakemake/snakemake-wrappers/raw/v1.31.0/bio/fastqc/environment.yaml
ConnectionError: HTTPSConnectionPool(host='github.com', port=443): Max retries exceeded with url: /snakemake/snakemake-wrappers/raw/v1.31.0/bio/fastqc/environment.yaml (Caused by NewConnectionError('&lt;urllib3.connection.HTTPSConnection object at 0x7fd067dfc250&gt;: Failed to establish a new connection: [Errno 113] No route to host'))
  File &quot;/path/mambaforge/envs/snakemake/lib/python3.11/site-packages/reretry/api.py&quot;, line 218, in retry_call
  File &quot;/path/mambaforge/envs/snakemake/lib/python3.11/site-packages/reretry/api.py&quot;, line 31, in __retry_internal

",-1,-1,-1.0
76342433,Snakemake rule which combines the output file of different samples obtained from previous rule with no explicit output file,"I'm quite new to Snakemake and I'm facing the following problem:
I have to run a program on several folders (with the same structure) and then combine the results on the different folders in a single file. The command of this program does not require to specify an output file, but rather an output folder, which is saved as a subfolder in each &quot;primary folder&quot;.
I have come up with the code below which, however, it is not running. I guess this is because of two different issues. One is the fact that there is not an explicit output to input relationship between the two rules (one outputs a folder, while the other wants a set of files as input). The second is that I'm not really sure that my way of creating a rule which combines the result of multiple folders is correct.
rule all:
    input: 
        single_folder_results = expand(&quot;test/{folder}/result_folder/quality_report.tsv&quot;, folder=dirs_list),
        results_collated = &quot;result_total.tsv&quot;,

rule program_on_single_folder:
    input: 
        genome = &quot;test/{folder}/{file}&quot;
    output:
        outdir = directory(&quot;{folder}/program_result&quot;),
    threads: n_threads
    shell:
        &quot;&quot;&quot;
        ...
        &quot;&quot;&quot;

rule results_collate:
    input:
        all_results = rules.all.input.single_folder_results 
    output:
        collated_results = rules.all.input.results_collated 
    shell:
        &quot;cat {all_results} | uniq &gt;&gt; {collated_results}&quot;

Snakemake gives the following error:
MissingInputException in rule results_collate in file snakemake.smk, line 50:
Missing input files for rule checkm2_results_collate:
    output: result_total.tsv
    affected files:
    ...

Would anyone know how to solve this?
Thanks for the help!
",1,-1,-1.0
76373661,"Mambaforge cannot collect package metadata for perl-dbi (MacOS M1), to install snakemake","I am really new to anything bioinformatics, so sorry if things are not that clear. I am trying to run a snakemake pipeline for genome analysis. I have installed mambaforge for MacOSM1 by downloading the Mambaforge-MacOSX-arm64.sh file from github.com/conda-forge/miniforge#mambaforge. Then, from the terminal I run:
bash Mambaforge-$(uname)-$(uname -m).sh -u 
Then I tried to install snake-make with:
conda activate base 
From this point on, all commands were triggered from the base environment
mamba create -c bioconda -c conda-forge -n snakemake snakemake-minimal
mamba started running, but I got the following error code, as well as a full error report: 'Multi-download failed. Reason: Transfer finalized, status: 404 [https://conda.anaconda.org/perl-dbi/noarch/repodata.json] 3091 bytes'

To solve this I first tried severel another way to install snakemake:

mamba create --only-deps -n snakemake-main snakemake
but the same error appeared.

then I tried with pip and it seeemed to have worked:

pip install git+https://github.com/snakemake/snakemake
since it looked like the different packages could be downloaded successfully. However, in the end it was not possible to activate snakemake with:
conda activate snakemake
Since it gave the error:
EnvironmentNameNotFound: Could not find conda environment: snakemake
You can list all discoverable environments with: 'conda info --envs'.

Then I realised that the link that manba is tring to use for perl-dbi was indeed not available so I tried to change the address of the channel by editing the .condarc file:

(base) clara@Claras-MacBook-Air ~ % conda config --show-sources        
==&gt; /Users/clara/mambaforge/.condarc &lt;==
channels:
  - bioconda

==&gt; /Users/clara/.condarc &lt;==
auto_activate_base: False
channel_priority: strict
channels:
  - bioconda
  - conda-forge
  - https://anaconda.org/bioconda/perl-dbi/
  - defaults

And again it did not manage to download it:
(base) clara@Claras-MacBook-Air ~ % conda install -c bioconda snakemake
Collecting package metadata (current_repodata.json): failed

UnavailableInvalidChannel: HTTP 404 NOT FOUND for channel bioconda/perl-dbi &lt;https://anaconda.org/bioconda/perl-dbi&gt;

The channel is not accessible or is invalid.

You will need to adjust your conda configuration to proceed.
Use `conda config --show channels` to view your configuration's current state,
and use `conda config --show-sources` to view config file locations.



Thank you so much for any light you might be able to shed on this!
",-1,-1,-1.0
76416431,Could not load lib/graphviz/libgvplugin_pango.so.6 - file not found when trying to make a --rulegraph of a snakemakefile,"I tried to make a pdf file for my snakemake file with:
snakemake --forceall --snakefile snakefile.smk --rulegraph | dot -Tpdf &gt; dag.pdf

But it gives an error:
Warning: Could not load &quot;../lib/graphviz/libgvplugin_pango.so.6&quot; - file not found
Warning: Could not load &quot;../lib/graphviz/libgvplugin_pango.so.6&quot; - file not found
Format: &quot;pdf&quot; not recognized. Use one of: canon cmap cmapx cmapx_np dot dot_json eps fig gv imap imap_np ismap json json0 mp pdf pic plain plain-ext png pov ps ps2 svg svgz tk vml vmlz xdot xdot1.2 xdot1.4 xdot_json
Exception ignored in: &lt;_io.TextIOWrapper name='&lt;stdout&gt;' mode='w' encoding='utf-8'&gt;
BrokenPipeError: [Errno 32] Broken pipe

I Tried reinstalling the graphviz package but still I get the same error. When installing graphviz into a conda environment, the environment cant find the library libgvplugin_pango.so.6
",-1,-1,-1.0
76433097,Snakemake wrappers suddenly stopped working,"I have this wrappers in my snakemake file
rule fastqc:
    input:
        &quot;reads/{sample}_trimmed.fq.gz&quot;
    output:
        html=&quot;qc/fastqc/{sample}.html&quot;,
        zip=&quot;qc/fastqc/{sample}_fastqc.zip&quot; # the suffix _fastqc.zip is necessary for multiqc to find the file
    params:
        extra = &quot;--quiet&quot;
    log:
        &quot;logs/fastqc/{sample}.log&quot;
    threads: config[&quot;resources&quot;][&quot;fastqc&quot;][&quot;cpu&quot;]
    conda:
        &quot;envs/qc.yaml&quot;
    wrapper:
        &quot;v1.31.1/bio/fastqc&quot;


qc.yaml:
name: qc
channels: 
  - bioconda
dependencies:
  - python
  - fastqc
  - multiqc

This normally works, but suddenly it stopped working (I have not changed the code) and I get this error:
[Thu Jun  8 15:34:53 2023]
rule fastqc:
    input: reads/S15_trimmed.fq.gz
    output: qc/fastqc/S15.html, qc/fastqc/S15_fastqc.zip
    log: logs/fastqc/S15.log
    jobid: 2
    reason: Missing output files: qc/fastqc/S15_fastqc.zip
    wildcards: sample=S15
    threads: 4
    resources: tmpdir=/tmp

[Thu Jun  8 15:34:53 2023]
rule fastqc:
    input: reads/L8_trimmed.fq.gz
    output: qc/fastqc/L8.html, qc/fastqc/L8_fastqc.zip
    log: logs/fastqc/L8.log
    jobid: 6
    reason: Missing output files: qc/fastqc/L8_fastqc.zip
    wildcards: sample=L8
    threads: 4
    resources: tmpdir=/tmp

[Thu Jun  8 15:34:53 2023]
rule fastqc:
    input: reads/S8_trimmed.fq.gz
    output: qc/fastqc/S8.html, qc/fastqc/S8_fastqc.zip
    log: logs/fastqc/S8.log
    jobid: 4
    reason: Missing output files: qc/fastqc/S8_fastqc.zip
    wildcards: sample=S8
    threads: 4
    resources: tmpdir=/tmp

python -c &quot;from __future__ import print_function; import sys, json; print(json.dumps([sys.version_info.major, sys.version_info.minor]))&quot;
Activating conda environment: .snakemake/conda/32ae7e363cfd65f035e232e794d5bc2b_
python -c &quot;from __future__ import print_function; import sys, json; print(json.dumps([sys.version_info.major, sys.version_info.minor]))&quot;
Activating conda environment: .snakemake/conda/32ae7e363cfd65f035e232e794d5bc2b_
python -c &quot;from __future__ import print_function; import sys, json; print(json.dumps([sys.version_info.major, sys.version_info.minor]))&quot;
Activating conda environment: .snakemake/conda/32ae7e363cfd65f035e232e794d5bc2b_
Environment defines Python version &lt; 3.7. Using Python of the main process to execute script. Note that this cannot be avoided, because the script uses data structures from Snakemake which are Python &gt;=3.7 only.
Environment defines Python version &lt; 3.7. Using Python of the main process to execute script. Note that this cannot be avoided, because the script uses data structures from Snakemake which are Python &gt;=3.7 only.
/home/user/mambaforge/envs/snakemake/bin/python3.11 /mnt/4TB_SSD/analyses/CRISPR/test/.snakemake/scripts/tmpx1titff7.wrapper.py
/home/user/mambaforge/envs/snakemake/bin/python3.11 /mnt/4TB_SSD/analyses/CRISPR/test/.snakemake/scripts/tmpw1yrfmqi.wrapper.py
Activating conda environment: .snakemake/conda/32ae7e363cfd65f035e232e794d5bc2b_
Activating conda environment: .snakemake/conda/32ae7e363cfd65f035e232e794d5bc2b_
Environment defines Python version &lt; 3.7. Using Python of the main process to execute script. Note that this cannot be avoided, because the script uses data structures from Snakemake which are Python &gt;=3.7 only.
/home/user/mambaforge/envs/snakemake/bin/python3.11 /mnt/4TB_SSD/analyses/CRISPR/test/.snakemake/scripts/tmpnh80ovk0.wrapper.py
Activating conda environment: .snakemake/conda/32ae7e363cfd65f035e232e794d5bc2b_
Traceback (most recent call last):
  File &quot;/mnt/4TB_SSD/analyses/CRISPR/test/.snakemake/scripts/tmpx1titff7.wrapper.py&quot;, line 17, in &lt;module&gt;
    from snakemake_wrapper_utils.snakemake import get_mem
ModuleNotFoundError: No module named 'snakemake_wrapper_utils'
Traceback (most recent call last):
  File &quot;/mnt/4TB_SSD/analyses/CRISPR/test/.snakemake/scripts/tmpw1yrfmqi.wrapper.py&quot;, line 17, in &lt;module&gt;
    from snakemake_wrapper_utils.snakemake import get_mem
ModuleNotFoundError: No module named 'snakemake_wrapper_utils'
Traceback (most recent call last):
  File &quot;/mnt/4TB_SSD/analyses/CRISPR/test/.snakemake/scripts/tmpnh80ovk0.wrapper.py&quot;, line 17, in &lt;module&gt;
    from snakemake_wrapper_utils.snakemake import get_mem
ModuleNotFoundError: No module named 'snakemake_wrapper_utils'
[Thu Jun  8 15:34:54 2023]
Error in rule fastqc:
    jobid: 4
    input: reads/S8_trimmed.fq.gz
    output: qc/fastqc/S8.html, qc/fastqc/S8_fastqc.zip
    log: logs/fastqc/S8.log (check log file(s) for error details)
    conda-env: /mnt/4TB_SSD/analyses/CRISPR/test/.snakemake/conda/32ae7e363cfd65f035e232e794d5bc2b_

[Thu Jun  8 15:34:54 2023]
Error in rule fastqc:
    jobid: 2
    input: reads/S15_trimmed.fq.gz
    output: qc/fastqc/S15.html, qc/fastqc/S15_fastqc.zip
    log: logs/fastqc/S15.log (check log file(s) for error details)
    conda-env: /mnt/4TB_SSD/analyses/CRISPR/test/.snakemake/conda/32ae7e363cfd65f035e232e794d5bc2b_

[Thu Jun  8 15:34:54 2023]
Error in rule fastqc:
    jobid: 6
    input: reads/L8_trimmed.fq.gz
    output: qc/fastqc/L8.html, qc/fastqc/L8_fastqc.zip
    log: logs/fastqc/L8.log (check log file(s) for error details)
    conda-env: /mnt/4TB_SSD/analyses/CRISPR/test/.snakemake/conda/32ae7e363cfd65f035e232e794d5bc2b_

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message


There seems to be a Python version conflict suddenly that cannot be avoided. How can I solve this?
",-1,-1,-1.0
76471741,Snakemake: github module using python module in repo,"I want to use a snakemake workflow available on github as a module in my new workflow. I define it like this:
module distiller:
    snakefile:
        github(&quot;open2c/distiller-sm&quot;, path=&quot;workflow/Snakefile&quot;, branch=&quot;master&quot;)
        # &quot;distiller-sm-master/workflow/Snakefile&quot;
    config:
        config
use rule * from distiller as distiller_*

The distiller module uses local .py files available in the GitHub repo like this:
from .scripts.common import organize_fastqs, needs_downloading

This is where I get a problem: in my new workflow I get this error:
ModuleNotFoundError in file https://raw.githubusercontent.com/open2c/distiller-sm/master/workflow/Snakefile, line 4:
No module named 'snakemake.scripts'

Is there a way to resolve this?
",-1,-1,-1.0
76481004,Snakemake error : MissingInputException in rule bwa_map,"I am trying to create a simple Snakemake workflow and I am having some issues. My file generates the following errors:
MissingInputException in rule bwa_map in file /home/muhammad/snakefile, line 1:
Missing input files for rule bwa_map:
output: mapped_reads_allsamples/barcode01.fastq_runid_3c11097ede35a7f061046232b44e0228595f2f23_0.bam
affected files:
MinION_1stRun_fastq_pass/guppy_demult/fastq_all_barcodes/barcode01.fastq_runid_3c11097ede35a7f061046232b44e0228595f2f23_0.fastq`

My snakefile content is:
rule bwa_map:
    input:
         genome = &quot;miniconda3/data/G6PD_RefSeqGene_FASTA.fa&quot;,
     samples = &quot;MinION_1stRun_fastq_pass/guppy_demult/fastq_all_barcodes/barcode01.fastq_runid_3c11097ede35a7f061046232b44e0228595f2f23_0.fastq&quot;
    output:
        sample_bams = &quot;mapped_reads_allsamples/barcode01.fastq_runid_3c11097ede35a7f061046232b44e0228595f2f23_0.bam&quot;
    shell:
        &quot;bwa mem {input} | samtools view -Sb - &gt; {output}&quot;`

I have these folders/files in my main directory:
/home/muhammad/miniconda3/
/home/muhammad/MinION_1stRun_fastq_pass/
/home/muhammad/ont-guppy/
/home/muhammad/snakefile

Please share your kind comments how can I fix this error.
I tried changing the directories in the input however its not working.
",-1,-1,-1.0
76485096,Why doesn't this snakemake script directive work,"I'm creating a snakemake workflow.  I've never used the run or script directives until yesterday.  I had a run directive that worked just fine, but snakemake --lint complained it was too long and said I should create a separate script and use the script directive:
Lints for rule check_peak_read_len_overlap_params (line 90, /Users/rleach/PROJECT-local/ATACC/REPOS/ATACCompendium/workflow/rules/error_checks.smk):
    * Migrate long run directives into scripts or notebooks:
      Long run directives hamper workflow readability. Use the script or notebook directive instead. Note that the script or notebook directive does not involve boilerplate. Similar to run, you will have direct access to params, input, output, and wildcards.Only use the run directive for a
      handful of lines.
      Also see:
      https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#external-scripts
      https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#jupyter-notebook-integration

So I tried turning this:
rule check_peak_read_len_overlap_params:
    input:
        &quot;results/QC/max_read_length.txt&quot;,
    output:
        &quot;results/QC/parameter_validation.txt&quot;,
    params:
        frac_read_overlap=FRAC_READ_OVERLAP,
        min_peak_width=MAX_ARTIFACT_WIDTH + 1,
        summit_width=SUMMIT_FLANK_SIZE * 2,
    log:
        tail=&quot;results/QC/logs/check_peak_read_len_overlap_params.stderr&quot;,
    run:
        max_read_len = 0
        # Get the max read length from the input file
        infile = open(input[0])
        for line in infile:
            max_read_len = int(line.strip())
            # The file should only be one line
            break
        infile.close()

        max_peak_frac = params.min_peak_width / max_read_len
        max_summit_frac = params.summit_width / max_read_len

        if max_peak_frac &lt; params.frac_read_overlap:
            raise ValueError(
                f&quot;There exist reads (of length {max_read_len}) that, if &quot;
                &quot;mapped to the smallest allowed peak (of length &quot;
                f&quot;{params.min_peak_width}, based on a MAX_ARTIFACT_WIDTH &quot;
                f&quot;of {MAX_ARTIFACT_WIDTH}), would never be counted using a &quot;
                f&quot;FRAC_READ_OVERLAP of {params.frac_read_overlap}.&quot;
            )

        if max_summit_frac &lt; params.frac_read_overlap:
            raise ValueError(
                f&quot;There exist reads (of length {max_read_len}) that, if &quot;
                f&quot;mapped to any summit (of length {params.summit_width}), &quot;
                &quot;would never be counted using a FRAC_READ_OVERLAP of &quot;
                f&quot;{params.frac_read_overlap}.&quot;
            )

        with open(output[0], &quot;w&quot;) as out:
            out.write(&quot;Parameters have validated successfully.&quot;)

into this:
rule check_peak_read_len_overlap_params:
    input:
        &quot;results/QC/max_read_length.txt&quot;,
    output:
        &quot;results/QC/parameter_validation.txt&quot;,
    params:
        frac_read_overlap=FRAC_READ_OVERLAP,
        max_artifact_width=MAX_ARTIFACT_WIDTH,
        summit_flank_size=SUMMIT_FLANK_SIZE,
    log:
        &quot;results/QC/logs/parameter_validation.log&quot;,
    conda:
        &quot;../envs/python3.yml&quot;
    script:
        &quot;scripts/check_peak_read_len_overlap_params.py&quot;

(Note, I changed the parameters at the same time so I could manipulate them in the script instead of under the params directive.)
At first, I simply pasted the run code into a function, and called that function.  Then I tried tweaking the code to get some sort of log output.  I tried adding a shebang at the top.  I tried inporting snakemake (which the docs didn't mention).  I tried a bunch of things, but nothing seemed to work.  Currently, this is what I have in the script:
import sys


def check_peak_read_len_overlap_params(
    infile,
    outfile,
    log,
    frac_read_overlap,
    max_artifact_width,
    summit_flank_size,
):
    log_handle = open(log, &quot;w&quot;)
    sys.stderr = sys.stdout = log_handle

    print(
        &quot;Parameters:\n&quot;
        f&quot;Input: {infile}\n&quot;
        f&quot;Output: {outfile}\n&quot;
        f&quot;Log: {log}\n&quot;
        f&quot;\tFRAC_READ_OVERLAP = {frac_read_overlap}\n&quot;
        f&quot;\tMAX_ARTIFACT_WIDTH = {max_artifact_width}\n&quot;
        f&quot;\tSUMMIT_FLANK_SIZE = {summit_flank_size}&quot;
    )

    min_peak_width = max_artifact_width + 1
    summit_width = summit_flank_size * 2
    max_read_len = 0
    inhandle = open(infile)
    for line in inhandle:
        max_read_len = int(line.strip())
        # The file should only be one line
        break
    inhandle.close()

    max_peak_frac = min_peak_width / max_read_len
    max_summit_frac = summit_width / max_read_len

    if max_peak_frac &lt; frac_read_overlap:
        raise ValueError(
            f&quot;There exist reads (of length {max_read_len}) that, if &quot;
            &quot;mapped to the smallest allowed peak (of length &quot;
            f&quot;{min_peak_width}, based on a MAX_ARTIFACT_WIDTH &quot;
            f&quot;of {max_artifact_width}), would never be counted using a &quot;
            f&quot;FRAC_READ_OVERLAP of {frac_read_overlap}.&quot;
        )

    if max_summit_frac &lt; frac_read_overlap:
        raise ValueError(
            f&quot;There exist reads (of length {max_read_len}) that, if &quot;
            f&quot;mapped to any summit (of length {summit_width}), &quot;
            &quot;would never be counted using a FRAC_READ_OVERLAP of &quot;
            f&quot;{frac_read_overlap}.&quot;
        )

    with open(outfile, &quot;w&quot;) as outhandle:
        outhandle.write(&quot;Parameters have validated successfully.&quot;)

check_peak_read_len_overlap_params(
    snakemake.input[0],
    snakemake.output[0],
    snakemake.log[0],
    snakemake.params.frac_read_overlap,
    snakemake.params.max_artifact_width,
    snakemake.params.summit_flank_size,
)

As I was working on this question, I figured out the issue, but I'll continue to post this and then answer, because nothing in the docs or via google searching, that I could find, answers this question...
",-1,1,-1.0
76554595,Error when using Snakemake variable in R script,"I have got the following Snakemake rule:
rule deseq2:
    input:
        salmon=expand(&quot;salmon/{sample}/quant.sf&quot;, sample=SAMPLES)
    output:
        xlsx=&quot;deseq2/deseq2_diff_trx.xlsx&quot;,
        rdata=&quot;deseq2/dds.Rdata&quot;,
    params:
        map_with,
        genome,
        gtf,
    log:
        &quot;logs/deseq2/deseq2.log&quot;
    conda:
        &quot;envs/deseq2.yml&quot;
    threads: config[&quot;resources&quot;][&quot;deseq2&quot;][&quot;cpu&quot;]
    resources: 
        runtime=config[&quot;resources&quot;][&quot;deseq2&quot;][&quot;time&quot;]
    script:
        &quot;scripts/deseq2.R&quot;

The part of that R script that the rule runs that gives an error is when I save a variable to a file name in the output section of the snakemake variable:
save(dds, snakemake@output[[2]])

Error in save(dds, snakemake@output[[2]]) : 
  object ‘snakemake@output[[2]]’ not found


Earlier in the script I access the params part of the snakemake variable in a similar way, but without any errors:
map.with &lt;- snakemake@params[[1]]
genome &lt;- snakemake@params[[2]]
gtf &lt;- snakemake@params[[3]]

When I print the snakemake variable I get this (only partial,relevant output):
An object of class &quot;Snakemake&quot;
Slot &quot;input&quot;:
[[1]]
[1] &quot;salmon/Control-1/quant.sf&quot;

[[2]]
[1] &quot;salmon/Control-2/quant.sf&quot;

[[3]]
[1] &quot;salmon/Control-Hypoxia-1/quant.sf&quot;

[[4]]
[1] &quot;salmon/Control-Hypoxia-2/quant.sf&quot;

$salmon
[1] &quot;salmon/Control-1/quant.sf&quot;         &quot;salmon/Control-2/quant.sf&quot;        
[3] &quot;salmon/Control-Hypoxia-1/quant.sf&quot; &quot;salmon/Control-Hypoxia-2/quant.sf&quot;


Slot &quot;output&quot;:
[[1]]
[1] &quot;deseq2/deseq2_diff_trx.xlsx&quot;

[[2]]
[1] &quot;deseq2/dds.Rdata&quot;

$xlsx
[1] &quot;deseq2/deseq2_diff_trx.xlsx&quot;

$rdata
[1] &quot;deseq2/dds.Rdata&quot;


Slot &quot;params&quot;:
[[1]]
[1] &quot;salmon&quot;

[[2]]
[1] &quot;hg38&quot;

[[3]]
[1] &quot;/home/user/Documents/references/gtf/hg38/gencode.v43.annotation.gtf&quot;

I have also saved the work space to a file for debugging as suggest by the snakemake web site. When I load this into R I can do the following things:
&gt; snakemake@output[[&quot;rdata&quot;]]
[1] &quot;deseq2/dds.Rdata&quot;

&gt; snakemake@output[[2]]
[1] &quot;deseq2/dds.Rdata&quot;

But when the above code is include in the proper script I get the object not found (see above) error.
What am I doing wrong?
",-1,-1,-1.0
76573531,"When using profile, get error ""snakemake: error: Couldn't parse config file: 'tuple' object has no attribute 'safe_load'""","On snakemake 7.29.0, running snakemake --profile profiles/basel-combined-cluster I get the following error:
snakemake: error: Couldn't parse config file: 'tuple' object has no attribute 'safe_load'

I tried --verbose and --debug, neither of which helped. What can I do to fix this?
",-1,-1,-1.0
76610020,MissingInputExceptions in snakemake,"I'm trying to make a Snakefile that takes all files in a directory then creates outputs in a seperate folder with the same names.
My code is:
strain = glob_wildcards(&quot;resources/bin/{isolate}.fasta&quot;)

rule all:
    input:
        expand(&quot;results/cctyper/{isolate}/&quot;, isolate=strain)

rule cctyper:
    input:
        &quot;resources/bin/{isolate}.fasta&quot;
    output:
        directory(&quot;results/cctyper/{isolate}/&quot;)
    conda:
        &quot;cctyper&quot;
    shell:
        &quot;&quot;&quot;
        mkdir {output}
        touch {output}/cas_operons.tab
        &quot;&quot;&quot;

(The shell output is a stand in because of platform problems).
But I keep getting errors like this:
MissingInputException in rule cctyper in file /Users/cloudy/PRACStest/workflow/Snakefile, line 7:
Missing input files for rule cctyper:
    output: results/cctyper/['SAMEA5061681', 'SAMEA5061682']
    wildcards: isolate=['SAMEA5061681', 'SAMEA5061682']
    affected files:
        resources/bin/['SAMEA5061681', 'SAMEA5061682'].fasta

I've tried making the rule use expand() instead of just using the wildcards (since I specify it anyway) but it doesn't seem to fix it. Why doesn't snakemake think the files are there immediately after listing all of them?
",-1,-1,-1.0
76612391,Snakemake NameError wildcards,"I have been trying to create my first Snakemake pipeline, but I have run into a NameError when running it.
Here is the code:
configfile: &quot;../config/config.yaml&quot;

LONGREADS = config[&quot;LONGREADS&quot;]
LONGREAD_FILE_TYPE = config[&quot;LONGREAD_FILE_TYPE&quot;]
SHORTREADS1 = config[&quot;SHORTREADS1&quot;]
SHORTREADS2 = config[&quot;SHORTREADS2&quot;]
PREFIX = config[&quot;PREFIX&quot;]
REFERENCE = config[&quot;REFERENCE&quot;]
OUTDIR = config[&quot;OUTDIR&quot;]

rule all:
    input:
        expand(&quot;{outdir}/PycoQC/&quot;, outdir=OUTDIR)

if LONGREAD_FILE_TYPE.upper() == &quot;FAST5&quot;:
    rule fast5_conversion:
        message:
            &quot;Rule {rule} processing&quot;
        group:
            &quot;Read processing&quot;
        output:
            directory(&quot;{outdir}/pod5/&quot;)
        threads: 32
        resources:
            mem_mb=30000,
            runtime=75
        conda:
            &quot;envs/pod5.yaml&quot;
        shell:
            &quot;pod5 convert fast5 {LONGREADS}/*.fast5 {output} -t {threads}&quot;

    rule basecalling:
        message:
            &quot;Rule {rule} processing&quot;
        group:
            &quot;Read processing&quot;
        input:
            rules.fast5_conversion.output
        output:
            directory(&quot;{outdir}/guppy_6_5_7_sup/&quot;)
        threads: 3
        resources:
            gpu=1,
            mem_mb=7500,
            runtime=4500
        container:
            &quot;docker://genomicpariscentre/guppy:6.5.7&quot;
        shell:
            &quot;guppy_basecaller -i {input} -s {output} --device auto --compress_fastq --records_per_fastq 0 --config dna_r9.4.1_450bps_sup.cfg&quot;

elif LONGREAD_FILE_TYPE.upper() == &quot;POD5&quot;:
    rule basecalling:
        message:
            &quot;Rule {rule} processing&quot;
        group:
            &quot;Read processing&quot;
        output:
            directory(&quot;{outdir}/guppy_6_5_7_sup/&quot;)
        threads: 3
        resources:
            gpu=1,
            mem_mb=7500,
            runtime=4500
        container:
            &quot;docker://genomicpariscentre/guppy:6.5.7&quot;
        shell:
            &quot;guppy_basecaller -i {LONGREADS} -s {output} --device auto --compress_fastq --records_per_fastq 0 --config dna_r9.4.1_450bps_sup.cfg&quot;

else:
    rule basecalling:
        message:
            &quot;Please provide a valid long read file type.&quot;
        group:
            &quot;Read processing&quot;
        output:
            directory(&quot;{outdir}/guppy_6_5_7_sup/&quot;)
        shell:
            &quot;^C&quot;

rule long_read_trimming:
    message:
        &quot;Rule {rule} processing&quot;
    group:
        &quot;Read processing&quot;
    input:
        rules.basecalling.output
    output:
        &quot;{outdir}/porechop_abi/trimmed_reads.fastq&quot;
    threads: 24
    resources:
        mem_mb=210000,
        runtime=300
    conda:
        &quot;envs/porechop_abi.yaml&quot;
    shell:
        &quot;porechop_abi --ab_initio --threads {threads} -i {input} -o {output}&quot;

rule short_read_trimming:
    message:
        &quot;Rule {rule} processing&quot;
    group:
        &quot;Read processing&quot;
    output:
        directory(&quot;{outdir}/trimmomatic/&quot;)
    threads: 24
    conda:
        &quot;envs/trimmomatic.yaml&quot;
    shell:
        &quot;for f1 in {SHORTREADS1}/*_1.fq.gz do f2=${f1%%_1.fq.gz}_2.fq.gz trimmomatic PE -threads {threads} -phred33 $f1 $f2 {output}/lane1_forwared_paired_${f1##*/} {output}lane1_forward_unpaired_${f1##*/}.fq.gz {output}lane1_reverse_paired_${f2##*/}.fq.gz {output}lane1_reverse_unpaired_${f2##*/}.fq.gz ILLUMINACLIP:/home/WUR/hoger006/lustre_dir/Tools/mambaforge/envs/trimmomatic/share/trimmomatic/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 done&quot;

rule pycoqc:
    message:
        &quot;Rule {rule} processing&quot;
    group:
        &quot;Quality control&quot;
    input:
        rules.basecalling.output
    output:
        directory(&quot;{outdir}/PycoQC/&quot;)
    threads: 5
    resources:
        mem_mb=3500,
        runtime=15
    conda:
        &quot;envs/pycoqc.yaml&quot;
    shell:
        &quot;pycoQC -f {input}/sequencing_summary.txt -o {output}output.html&quot;

And here is the error:
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cluster nodes: 10
Job stats:
job                 count    min threads    max threads
----------------  -------  -------------  -------------
all                     1              1              1
basecalling             1              3              3
fast5_conversion        1             32             32
pycoqc                  1              5              5
total                   4              1             32

Select jobs to execute...
[Tue Jul  4 13:37:46 2023]

group job Read processing (jobs in lexicogr. order):

    [Tue Jul  4 13:37:46 2023]
    Job 2: Rule basecalling processing
    Reason: Missing output files: ../results/guppy_6_5_7_sup; Input files updated by another job: ../results/pod5


    [Tue Jul  4 13:37:46 2023]
    Job 3: Rule fast5_conversion processing
    Reason: Missing output files: ../results/pod5

WorkflowError:
NameError with group job aa08bb21-d8a7-5a54-b420-1d366d760cbe: The name 'outdir' is unknown in this context. Did you mean 'wildcards.outdir'?


I have seen similar errors, such as NameError in Snakemake dryrun mode, but I am not sure how to implement this in my case, like I mentioned I am fairly new at this. Thanks in advance for any help!
",-1,-1,-1.0
76627245,I'm not able to generate Report for my rule in Snakemake,"I'm trying to generate report for my snakemake rule.
rule samtools_index:
    input:
        &quot;sorted_reads/sorted_{sample}.bam&quot;
    output:
        report(&quot;sorted_reads/sorted_{sample}.bam.bai&quot;, caption = &quot;report/{sample}.rst&quot;)
    conda:
        &quot;envs/samtools.yaml&quot; #define isolated software environments per rule
    log:
        &quot;logs/samtools_index/{sample}.log&quot;
    shell:
        &quot;&quot;&quot;
        (samtools index {input} &gt; {output}) 2&gt; {log}
        &quot;&quot;&quot;


but I'm getting a WorkflowError:
Error loading caption file /home/pulkit/snakemake-2/report/{sample}.rst of output marked for report.
WorkflowError:
Failed to open source file /home/pulkit/snakemake-2/report/{sample}.rst
FileNotFoundError: [Errno 2] No such file or directory: '/home/pulkit/snakemake-2/report/{sample}.rst'
rule execution does not generate any report.
",-1,-1,-1.0
76619240,Snakemake process multiple files in one rule,"What is the best way to process a list of files in one rule?
Workflow
The main goal of the workflow is to select the raw data and output the selected data. The directory of the workflow is structured as below.
.
├── data
│   ├── 000_raw
│   │   ├── 15_a.csv
│   │   ├── 15_b.csv
│   │   ├── 15_c.csv
│   │   ├── 16_a.csv
│   │   ├── 16_b.csv
│   │   └── 16_c.csv
│   └── 010_sel
│       ├── 15_a.csv
│       ├── 15_b.csv
│       ├── 15_c.csv
│       ├── 16_a.csv
│       ├── 16_b.csv
│       └── 16_c.csv
├── scripts
│   └── 010_sel.py
└── Snakefile

The selection script 010_sel.py read and produce one file at each time, i.e. the common way to run it is
python scripts/010_sel.py data/000_raw/15_a.csv data/010_sel/15_a.csv

Snakefile
I use expand and run method in the snakemake file.
ls_year_type = [15_a,15_b,15_c,16_a,16_b,16_c]

rule sel_010:
    input:
        expand(&quot;data/000_raw/{year_type}.csv&quot;,year_mag=ls_year_type)
    output:
        expand(&quot;data/010_sel/{year_type}.csv&quot;,year_mag=ls_year_type)
    run: 
        for ifile in range(len(output)):
            os.system(&quot;python scripts/010_sel.py {} {}&quot;.format(input[ifile],output[ifile]))

Problems
There are two problems with this method.

The expand command generate a list. If only one of the files in the list is modified, for example,rm data/010_sel/15_b.csv, snakemake will rerun the scripts on every file in the list. It is time consuming.
If the script 010_sel.py is modified, snakemake will not know it. Need to rerun the snakefile mannually.

Optional method
One optional way is to rewrite the 010_sel.py to include snakemake commands rather than using sys.argv
for i in range(len(snakemake.input)):
    input_file = snakemake.input[i]
    output_file = snakemake.output[i]

In snakemake file change run to script
script:
    &quot;scripts/010_sel.py&quot;

This will solve the second problem but the first one remains.
Thanks in advance for any help.
",1,-1,-1.0
76611464,Workflow not being executed in parallel on batch computing nodes (SLURM cluster grouped execution),"I'm trying to write a profile to run this workflow on NERSC's supercomputer. Batch computing nodes have 128 CPU cores (x2 hyperthreads) and 512 GB of memory. Submission is managed through SLURM and maximum wall time is 12h.
My workflow is mostly composed by a large number of ~1h long, single-threaded jobs. I would like to instruct Snakemake to pack them efficiently and submit a much lower number of jobs to SLURM. Workflows running on a node should profit from all available resources and run in parallel.
This is what I've written so far:
configfile: config.json
keep-going: true
quiet: rules

# profit from Perlmutter's scratch area: https://docs.nersc.gov/filesystems/perlmutter-scratch
# NOTE: should actually set this through the command line, since there is a
# scratch directory for each user and variable expansion does not work here:
#   $ snakemake --shadow-prefix &quot;$PSCRATCH&quot; [...]
# shadow-prefix: &quot;$PSCRATCH&quot;

# NERSC uses the SLURM job scheduler
# - https://snakemake.readthedocs.io/en/stable/executing/cluster.html#executing-on-slurm-clusters
slurm: true

# maximum number of cores requested from the cluster or cloud scheduler
cores: 256
# maximum number of cores used locally, on the interactive node
local-cores: 256
# maximum number of jobs that can exist in the SLURM queue at a time
jobs: 50

# reasonable defaults that do not stress the scheduler
max-jobs-per-second: 20
max-status-checks-per-second: 20

# (LEGEND) NERSC-specific settings
# - https://snakemake.readthedocs.io/en/stable/executing/cluster.html#advanced-resource-specifications
# - https://docs.nersc.gov/jobs
default-resources:
  - slurm_account=&quot;m2676&quot;
  - constraint=&quot;cpu&quot;
  - runtime=120
  - mem_mb=500
  - slurm_extra=&quot;--qos regular --licenses scratch,cfs&quot;

# number of threads used by each rule
set-threads:
  - tier_ver=1
  - tier_raw=1

# memory and runtime requirements for each single rule
# - https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#resources
# - https://docs.nersc.gov/jobs/#available-memory-for-applications-on-compute-nodes
set-resources:
  - tier_ver:mem_mb=500
  - tier_ver:runtime=120
  - tier_raw:mem_mb=500
  - tier_raw:runtime=120

# we define groups in order to let Snakemake group rule instances in the same
# SLURM job. relevant docs:
# - https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#snakefiles-grouping
# - https://snakemake.readthedocs.io/en/stable/executing/grouping.html#job-grouping
groups:
  - tier_ver=sims
  - tier_raw=sims

# disconnected parts of the workflow can run in parallel (at most 256 of them)
# in a group
group-components:
    - sims=256

And this is the relevant part of Snakemake's output (only 40 tasks):
&gt; snakemake --profile workflow/profiles/nersc-batch --verbose
sbatch call: sbatch --job-name 02d1132e-27d6-4d5c-aed4-3a88e1d30e93 -o .snakemake/slurm_logs/group_sims/%j.log --export=ALL -A m2676 -t 120 -C cpu --mem 20000 --cpus-per-task=40 --qos regular --licenses scratch,cfs -D /global/cfs/cdirs/m2676/users/pertoldi/legend-prodenv/sims/benchmark-1 --wrap='/global/cfs/cdirs/m2676/users/pertoldi/legend-prodenv/tools/snakemake-mambaforge3/envs/snakemake/bin/python3.11 -m snakemake --snakefile '&quot;'&quot;'/global/cfs/cdirs/m2676/users/pertoldi/legend-prodenv/sims/benchmark-1/workflow/Snakefile'&quot;'&quot;' --target-jobs [ELIDED] --allowed-rules [tier_raw ... ELIDED ... tier_raw] --local-groupid '&quot;'&quot;'eaff24b4-a825-52a7-9d08-aac77f1f7b10'&quot;'&quot;' --cores '&quot;'&quot;'all'&quot;'&quot;' --attempt 1 --resources '&quot;'&quot;'mem_mb=20000'&quot;'&quot;' '&quot;'&quot;'disk_mib=38160'&quot;'&quot;' '&quot;'&quot;'disk_mb=40000'&quot;'&quot;' '&quot;'&quot;'mem_mib=19080'&quot;'&quot;' --wait-for-files-file '&quot;'&quot;'/global/cfs/cdirs/m2676/users/pertoldi/legend-prodenv/sims/benchmark-1/.snakemake/tmp.gg2t0bhd/snakejob_sims_eaff24b4-a825-52a7-9d08-aac77f1f7b10.waitforfilesfile.txt'&quot;'&quot;' --force --keep-target-files --keep-remote --max-inventory-time 0 --nocolor --notemp --no-hooks --nolock --ignore-incomplete --rerun-triggers '&quot;'&quot;'input'&quot;'&quot;' '&quot;'&quot;'code'&quot;'&quot;' '&quot;'&quot;'mtime'&quot;'&quot;' '&quot;'&quot;'params'&quot;'&quot;' '&quot;'&quot;'software-env'&quot;'&quot;' --skip-script-cleanup  --shadow-prefix '&quot;'&quot;'/pscratch/sd/p/pertoldi'&quot;'&quot;' --conda-frontend '&quot;'&quot;'mamba'&quot;'&quot;' --wrapper-prefix '&quot;'&quot;'https://github.com/snakemake/snakemake-wrappers/raw/'&quot;'&quot;' --configfiles '&quot;'&quot;'/global/cfs/cdirs/m2676/users/pertoldi/legend-prodenv/sims/benchmark-1/config.json'&quot;'&quot;' --latency-wait 5 --scheduler '&quot;'&quot;'greedy'&quot;'&quot;' --scheduler-solver-path '&quot;'&quot;'/global/cfs/cdirs/m2676/users/pertoldi/legend-prodenv/tools/snakemake-mambaforge3/envs/snakemake/bin'&quot;'&quot;' --set-resources '&quot;'&quot;'tier_ver:mem_mb=500'&quot;'&quot;' '&quot;'&quot;'tier_ver:runtime=120'&quot;'&quot;' '&quot;'&quot;'tier_raw:mem_mb=500'&quot;'&quot;' '&quot;'&quot;'tier_raw:runtime=120'&quot;'&quot;' --default-resources '&quot;'&quot;'mem_mb=500'&quot;'&quot;' '&quot;'&quot;'disk_mb=max(2*input.size_mb, 1000)'&quot;'&quot;' '&quot;'&quot;'tmpdir=system_tmpdir'&quot;'&quot;' '&quot;'&quot;'slurm_account=&quot;m2676&quot;'&quot;'&quot;' '&quot;'&quot;'constraint=&quot;cpu&quot;'&quot;'&quot;' '&quot;'&quot;'runtime=120'&quot;'&quot;' '&quot;'&quot;'slurm_extra=&quot;--qos regular --licenses scratch,cfs&quot;'&quot;'&quot;'  --slurm-jobstep --jobs 1 --mode 2'
Job eaff24b4-a825-52a7-9d08-aac77f1f7b10 has been submitted with SLURM jobid 10939730 (log: .snakemake/slurm_logs/group_sims/10939730.log).

And this is the content of that log file:
&gt; cat .snakemake/slurm_logs/group_sims/10939730.log
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 256
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=20000, disk_mib=38160, disk_mb=40000, mem_mib=19080
Select jobs to execute...
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 40
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=500, mem_mib=477, disk_mb=1000, disk_mib=954
Select jobs to execute...

[Sat Jul  1 09:28:58 2023]
Job 0: Producing output file for job 'raw.l200a-wls-reflector-Rn222-to-Po214.0'
Reason: Missing output files: /global/cfs/cdirs/m2676/users/pertoldi/legend-prodenv/sims/benchmark-1/generated/tier/raw/l200a-fibers-Rn222-to-Po214/l200a-fibers-Rn222-to-Po214_0000.root

Changing to shadow directory: /pscratch/sd/p/pertoldi/shadow/tmpb727ncaf
Write-protecting output file /global/cfs/cdirs/m2676/users/pertoldi/legend-prodenv/sims/benchmark-1/generated/tier/raw/l200a-fibers-Rn222-to-Po214/l200a-fibers-Rn222-to-Po214_0000.root.
[Sat Jul  1 09:32:15 2023]
Finished job 0.
1 of 1 steps (100%) done
Write-protecting output file /global/cfs/cdirs/m2676/users/pertoldi/legend-prodenv/sims/benchmark-1/generated/tier/raw/l200a-fibers-Rn222-to-Po214/l200a-fibers-Rn222-to-Po214_0000.root.
[Sat Jul  1 09:32:16 2023]
Finished job 23.
1 of 40 steps (2%) done
Select jobs to execute...
srun: Job 10939730 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for StepId=10939730.1
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 40
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=500, mem_mib=477, disk_mb=1000, disk_mib=954
Select jobs to execute...

[Sat Jul  1 09:33:15 2023]
Job 0: Producing output file for job 'raw.l200a-wls-reflector-Rn222-to-Po214.0'
[...]

As you can see, jobs are serially executed on the node even if they are independent between each other.
The problem seems related to prepending the remote Snakemake invocation with srun (triggered by the --slurm-jobstep option). I had a look at the relevant source code, which looks OK at first sight.
Is there anything clearly wrong in my profile? I found a related bug report here, but I wanted to make sure there's no mistake from my side, before claiming it's a Snakemake bug.
",-1,1,-1.0
76541165,,"Interesting... I'm not sure if this is a bug, or intentional, or me missing something.
You can re-run only up to rule (or file) b with:
snakemake -j 1 -- b

After that, on re-running the all pipeline you get:
snakemake -p -j 1 
Building DAG of jobs...
Nothing to be done (all requested files are present and up to date).
Complete log: .snakemake/log/2023-06-23T153605.565616.snakemake.log

which seems to be what you want. However, this is an odd behavior since you get in state where file b is newer than file c even if file c depends on b, so I would say that requested files are present but they are not up to date.
One a second thought, this may be due to https://github.com/snakemake/snakemake/issues/2011
",-1,-1,-1.0
76520619,"Publish static HTML to GitLab pages (no build, no CI/CD)","I have a collection of static HTML pages that I want to publish to GitLab Pages. These HTML documents were rendered as part of a computationally intensive snakemake workflow running on a HPC. The workflow &quot;knits&quot; HTML reports from Rmarkdown as part of its final steps.
For simplicity, let's assume that I have a single HTML file called index.html and that it contains only &quot;Hello world!!!&quot;. Publishing this to GitHub pages is very easy. See this repo which gets published here.
GitLab, on the other hand, seems to insist that I set up CI/CD, even when the static HTML files are already available. I can see the advantages of using CI/CD but in this particular case this will involve setting up a runner and then getting the runner to run the snakemake workflow on the HPC, which could take a couple of days to complete. So I would prefer to solve the simple case first.
Is there a way to bypass the CI/CD? Or can I set up a kind of &quot;null&quot; CI/CD that simply takes the existing static HTML files and places them in public/, for example? What might such a &quot;null&quot; .gitlab-ci.yml file look like?
I tried to create a simple CI/CD pipeline that just copied index.html into public/. The GitLab repo is here. The pipeline fails for reasons that are completely opaque to me. How can I check error logs for the pipeline?
",-1,-1,-1.0
76435321,Snakemake workflow runs locally but not as github action?,"I have been banging my head against this for a few days now.  I am stumped.  I have tried so many different things, it's hard to keep everything straight.  And running snakemake actually works locally, with pretty much everything I've tried, but this one rule's test always fails as a github action.
The crux of it is, I want to run a tool called bigwigmerge to sum the signals of bigwig files.  However, that tool doesn't work when there's only 1 input file, so I decided that for that case, I would use another tool called bigwigtobedgraph (because the output of bigwigmerge is a bedgraph file).
My initial attempt was this:
rule bigwigs_to_summed_bedgraph:
    input:
        expand(
            &quot;results/bigwig/{dataset}.bw&quot;,
            dataset=DATASETS,
        ),
    output:
        pipe(&quot;results/bigwig/all_summed.bedGraph&quot;),
    params:
        nargs=len(DATASETS),
    log:
        std=&quot;results/bigwig/logs/bigwigs_to_summed_bedgraph.stdout&quot;,
        err=&quot;results/bigwig/logs/bigwigs_to_summed_bedgraph.stderr&quot;,
    conda:
        &quot;../envs/bigwig_tools.yml&quot;
    shell:
        # bigwigmerge requires a minimum of 2 files, but the original R script
        # supports 1, so to support 1 file here, we use bigwigtobedgraph
        &quot;&quot;&quot;
        if [ {params.nargs} -eq 1 ]; then \
            bigwigtobedgraph {input:q} {output:q} 1&gt; {log.std:q} 2&gt; {log.err:q}; \
        else \
            bigwigmerge {input:q} {output:q} 1&gt; {log.std:q} 2&gt; {log.err:q}; \
        fi
        &quot;&quot;&quot;

That works locally when I run:
snakemake --directory .tests/test_6_summits --use-conda --cores 2 --printshellcmds all_atac_summits --forceall --show-failed-logs

But as a github action:
  temporary-summits-delete-after-59-merged:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Codebase
        uses: actions/checkout@v3
      - name: Test summits steps only
        uses: snakemake/snakemake-github-action@v1
        with:
          directory: '.tests/test_6_summits'
          snakefile: 'workflow/Snakefile'
          args: &gt;-
            --use-conda -c 2 --show-failed-logs -p --verbose all_atac_summits

with the all_atac_summits rule:
rule all_atac_summits:
    input:
        &quot;results/all_atac_summits.bed&quot;,
        &quot;results/all_atac_summits.tsv&quot;,

It always issues the error:
Error in group 8c80e530-b72c-4cd2-ba80-00b10c34aa9e:
    jobs:
        rule sort_summed_bedgraph:
            jobid: 5
            output: results/bigwig/all_sorted.bedGraph
            log: results/bigwig/logs/sort_summed_bedgraph.stderr (check log file(s) for error details)
        rule bigwigs_to_summed_bedgraph:
            jobid: 6
            output: results/bigwig/all_summed.bedGraph (pipe)
            log: results/bigwig/logs/bigwigs_to_summed_bedgraph.stdout, results/bigwig/logs/bigwigs_to_summed_bedgraph.stderr (check log file(s) for error details)
Logfile results/bigwig/logs/sort_summed_bedgraph.stderr: empty file

Removing output files of failed job sort_summed_bedgraph since they might be corrupted:
results/bigwig/all_sorted.bedGraph
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message

Things I have tried:

change pipe() to temp() -&gt; works locally but same gh action error
change if [ {params.nargs} -eq 1 ]; then \ to if [ &quot;{params.nargs}&quot; == &quot;1&quot; ]; then \ -&gt; works locally but same gh action error
change if [ {params.nargs} -eq 1 ]; then \ to if [[ {params.nargs} -eq 1 ]]; then \ -&gt; works locally but got a different gh action error. I think it was syntax, but the log is gone, so I can't copy/paste it here
change the shell to:
bigwigmerge {input:q} {output:q} 1&gt; {log.std:q} 2&gt; {log.err:q} || \
bigwigtobedgraph {input:q} {output:q} 1&gt; {log.std:q} 2&gt; {log.err:q}

-&gt; works locally but same gh action error
As a sanity check, change the shell to (I only test the 1 file case RN):
bigwigtobedgraph {input:q} {output:q} 1&gt; {log.std:q} 2&gt; {log.err:q}

-&gt; works locally AND works as gh action (i.e. I am sane)

I considered using run instead of shell, but I learned that you can't use conda with run because it runs the python code in the snakemake environment for some reason.
I've been struggling to break this up into separate rules, to the point where I'm not convinced it can be done.  I simply don't understand what the problem is and the error in the gh action log is completely uninformative.  If I had such a paucity of debug info locally, I would use --printshellcmds so I could copy and paste the command to try it on its own - then I would usually see some shell-level error, like command not found or something.  But I can't do that on github and I can't reproduce the error on my machine.  Why would everything work as expected on my machine, but not as a github action?
UPDATE: So my boss's workstation is an Ubuntu box, and he can produce the same error that github throws.  My machine is macOS.  So it must have to do with differences in the bash shell that snakemake uses?  He has GNU bash, version 5.1.16(1)-release (x86_64-pc-linux-gnu).  I have GNU bash, version 5.0.17(1)-release (x86_64-apple-darwin19.4.0).
",-1,-1,-1.0
76637611,How can I get backwards compatibility on my local machine so it can run an older opensource project locally?,"I am trying to set-up an opensource project called accessmaps to see and understand how it actually works. The problem is that this project was created several years ago and some packages are not supported anymore.
To be more specific, in the documentation of the project they say that in order to make it work, you have to generate some data using another repository. The problem is that when I am trying to use the commands of this repo, I come across this kind of error:
[Thu Jul  6 13:14:03 2023]
Error in rule draw_sidewalks:
    jobid: 0
    output: interim/redrawn/sidewalks.geojson, interim/redrawn/streets.geojson

RuleException:
TypeError in line 496 of /data/cities/seattle/Snakefile:
'MultiPoint' object is not subscriptable
  File &quot;/usr/local/lib/python3.8/site-packages/snakemake/executors/__init__.py&quot;, line 2326, in run_wrapper
  File &quot;/data/cities/seattle/Snakefile&quot;, line 496, in __rule_draw_sidewalks
  File &quot;/usr/local/lib/python3.8/site-packages/sidewalkify/draw/draw.py&quot;, line 39, in draw_sidewalks
  File &quot;/usr/local/lib/python3.8/site-packages/sidewalkify/draw/trim.py&quot;, line 36, in trim
  File &quot;/usr/local/lib/python3.8/site-packages/sidewalkify/draw/trim.py&quot;, line 61, in ixn_and_trim
  File &quot;/usr/local/lib/python3.8/site-packages/snakemake/executors/__init__.py&quot;, line 568, in _callback
  File &quot;/usr/local/lib/python3.8/concurrent/futures/thread.py&quot;, line 57, in run
  File &quot;/usr/local/lib/python3.8/site-packages/snakemake/executors/__init__.py&quot;, line 554, in cached_or_run
  File &quot;/usr/local/lib/python3.8/site-packages/snakemake/executors/__init__.py&quot;, line 2357, in run_wrapper
Exiting because a job execution failed. Look above for error message

The build is split in 24 jobs and when it hits job 11 it crashes showing the error above.
The requirements file of the project contains these packages inside:
-e git://github.com/dezhin/osmread.git@d8d3fe5edd15fdab9526ea7a100ee6c796315663#egg=osmread
crossify==0.1.4
esridump==1.7.0
geopandas==0.4.0
rasterio==1.0a12
requests==2.21
scipy==1.0.1
sidewalkify==0.2.1
snakemake==4.8.0

I am also using
numpy 1.19.5 

as the project does not support numpy versions&gt;1.20.
I tried to rollback a few versions on some packages (like numpy) but it did not actually work. I also tried to install the latest versions of some dependencies that were deprecated but it was also a dead end.
",-1,-1,-1.0
