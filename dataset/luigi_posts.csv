Id,Title,Body,RatingsSentiCR,RatingsGPT35,RatingsGPTFineTuned
31467892,writing output to files from python luigi,"I just tried to run the python luigi example from the documentation:

class TaskA(luigi.Task):
    def output(self):
        return luigi.LocalTarget('xyz')

class FlipLinesBackwards(luigi.Task):
    def requires(self):
        return TaskA()

    def output(self):
        return luigi.LocalTarget('abc')

    def run(self):
        f = self.input().open('r') # this will return a file stream that reads from ""xyz""
        g = self.output().open('w')
        for line in f:
           g.write('%s\n', ''.join(reversed(line.strip().split())))
        g.close() # needed because files are atomic


I ran it using command line:

python Luigi_Test.py FlipLinesBackwards --local-scheduler


I was under the impression that this would create a file in the directory I am running it in but it doesn't?

Am I doing something wrong?
",-1,-1,-1.0
31533967,Using Parameters in python luigi,"I have am triggering Luigi via

luigi.run([""--local-scheduler""], main_task_cls=Test(Server = ActiveServer, Database = DB))   


and in my class I have:

class Test(luigi.Task):

    Database = luigi.Parameter()
    Server = luigi.Parameter()


but the task test can't seem to parse the parameters that I'm feeding it properly?

I am getting:

MissingParameterException: No value for 'Server' (--Server) submitted and no default value has been assigned.

",-1,-1,-1.0
32978429,python luigi died unexpectedly with exit code -11,"I have a data pipeline with luigi that works perfectly fine if I put 1 worker to the task. However, if I put > 1 workers, then it dies (unexpectedly with exit code -11) in a stage with 2 dependencies. The code is rather complex, so a minimum example would be difficult to give. The gist of the matter is that I am doing the following things with gensim:


Building a dictionary from some texts.
Building a corpus from said texts and the dictionary (requires (1)).
Training an LDA model from the corpus and dictionary (requires (1) and (2)).


For some reason, step (3) crashes every time I put more than one worker, even if (1) and (2) are already completed...

Any help would be greatly appreciated!

EDIT: Here is an example of the logging info. TrainLDA is task (3). There are still two tasks after that that require TrainLDA. All earlier tasks finished correctly. I substituted TrainLDA's arguments for ... so that the output would be more readable. The additional info are just print statements we put to help us know what is happening.

DEB

UG: Pending tasks: 3
DEBUG: Asking scheduler for work...
INFO: [pid 28851] Worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825) running   TrainLDA(...)
INFO: Done
INFO: There are no more tasks to run at this time
INFO: TrainLDA(...) is currently run by worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825)
==============================
Corriendo LDA de spanish con nivel de limpieza stopwords
==============================
Número de tópicos: 40
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: TrainLDA(...) is currently run by worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825)
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: TrainLDA(...) is currently run by worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825)
INFO: Worker task TrainLDA(...) died unexpectedly with exit code -11
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: There are 2 pending tasks possibly being run by other workers
INFO: There are 2 pending tasks unique to this worker
INFO: Worker Worker(salt=514562349, workers=4, host=felipe.local, username=Felipe, pid=28825) was stopped. Shutting down Keep-Alive thread

",1,-1,-1.0
33332058,Luigi Pipeline beginning in S3,"My initial files are in AWS S3. Could someone point me how I need to setup this in a Luigi Task?

I reviewed the documentation and found luigi.S3 but is not clear for me what to do with that, then I searched in the web and only get links from mortar-luigi and implementation in top of luigi.

UPDATE

After following the example provided for @matagus (I created the ~/.boto file as suggested too):

# coding: utf-8

import luigi

from luigi.s3 import S3Target, S3Client

class MyS3File(luigi.ExternalTask):
    def output(self):
        return S3Target('s3://my-bucket/19170205.txt')

class ProcessS3File(luigi.Task):

    def requieres(self):
        return MyS3File()

    def output(self):
        return luigi.LocalTarget('/tmp/resultado.txt')

    def run(self):
        result = None

        for input in self.input():
           print(""Doing something ..."")
           with input.open('r') as f:
               for line in f:
                   result = 'This is a line'

        if result:
            out_file = self.output().open('w')
            out_file.write(result)


When I execute it nothing happens

DEBUG: Checking if ProcessS3File() is complete
INFO: Informed scheduler that task   ProcessS3File()   has status   PENDING
INFO: Done scheduling tasks
INFO: Running Worker with 1 processes
DEBUG: Asking scheduler for work...
DEBUG: Pending tasks: 1
INFO: [pid 21171] Worker Worker(salt=226574718, workers=1, host=heliodromus, username=nanounanue, pid=21171) running   ProcessS3File()
INFO: [pid 21171] Worker Worker(salt=226574718, workers=1, host=heliodromus, username=nanounanue, pid=21171) done      ProcessS3File()
DEBUG: 1 running tasks, waiting for next task to finish
INFO: Informed scheduler that task   ProcessS3File()   has status   DONE
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: Worker Worker(salt=226574718, workers=1, host=heliodromus, username=nanounanue, pid=21171) was stopped. Shutting down Keep-Alive thread


As you can see, the message Doing something... never prints. What is wrong?
",-1,-1,-1.0
34147832,output for append job in BigQuery using Luigi Orchestrator,"I have a Bigquery task which only aims to append a daily temp table (Table-xxxx-xx-xx) to an existing table (PersistingTable).

I am not sure how to handle the output(self) method. Indeed, I can not just output PersistingTable as a luigi.contrib.bigquery.BigQueryTarget, since it already exists before the process started. Has anyone asked himself such a question?
",0,-1,-1.0
34341113,Luigi : Step by Step instructions not working,"I'm a newbie to python, I've installed Luigi-2.0.1 on my RHEL linux. Trying to run a sample program 

import luigi

class MyTask(luigi.Task) :
        param = luigi.Parameter(default=42)

        def requires(self):
                return SomeOtherTask(self.param)

        def run(self):
                f = self.output.open('w')
                print &gt;&gt;f, 'hello world'
                f.close()

        def output(self):
                return luigi.LocalTarget('/tmp/foo/bar-%s.txt' % self.param)

if __name__ == '__main__':
        luigi.run()


Executed the following command 

luigi --module maintask.py MyTask


I get the following error. 

Traceback (most recent call last):
  File ""/usr/bin/luigi"", line 5, in &lt;module&gt;
    from pkg_resources import load_entry_point
  File ""/usr/lib/python2.6/site-packages/pkg_resources.py"", line 2655, in &lt;module&gt;
    working_set.require(__requires__)
  File ""/usr/lib/python2.6/site-packages/pkg_resources.py"", line 648, in require
    needed = self.resolve(parse_requirements(requirements))
  File ""/usr/lib/python2.6/site-packages/pkg_resources.py"", line 546, in resolve
    raise DistributionNotFound(req)
pkg_resources.DistributionNotFound: python-daemon&lt;3.0


Upon investigating the web, I could not figure out why this happened. I have python 2.6. Is it that luigi is not compatible with 2.6 and only works for python 3.0 onwards?. 

Appreciate any help 
",1,-1,-1.0
34391996,Python pipeline framework luigi with scikit-learn,"I'm using python pipeline framework luigi and scikit-learn for the machine learning batch jobs especially in MiniBatchDictionaryLearning module. But it doesn't work as I expected when I execute with multiple process. My code is like this.(It's just a example though.)

import luigi
import numpy as np

class First(luigi.Task):

    job_nums = 2

    def requires(self):
        return [Second(job_nums=n) for n in range(self.job_nums)]

    def output(self):
        return luigi.LocalTarget(""end.txt"")

    def run(self):
        with self.output().open(""w"") as out_:
            pass


class Second(luigi.Task):
    data = np.arange(288000)
    job_nums = luigi.IntParameter()

    def output(self):
        return luigi.LocalTarget(""./dict{0}.npy"".format(self.job_nums))

    def run(self):
        from sklearn.decomposition import MiniBatchDictionaryLearning
        dico = MiniBatchDictionaryLearning(n_components=144, n_jobs=1)
        D = dico.fit(np.reshape(self.data, (1000, 288))).components_
        print D
        with self.output().open(""w"") as out_:
            D.dump(out_)


When I execute this code with single process, it works.

$ PYTHONPATH="""" luigi --module this_is_a_test First

DEBUG: Checking if First() is complete
DEBUG: Checking if Secound(job_nums=0) is complete
DEBUG: Checking if Secound(job_nums=1) is complete
~~ snip ~~

===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 3 ran successfully:
    - 1 First()
    - 2 Secound(job_nums=0,1)

This progress looks :) because there were no failed tasks or missing external dependencies

===== Luigi Execution Summary =====


But when I execute with multiple process, it doesn't work and I got this error.

$ PYTHONPATH="""" luigi --module this_is_a_test First --workers 2
DEBUG: Checking if First() is complete
DEBUG: Checking if Secound(job_nums=0) is complete
DEBUG: Checking if Secound(job_nums=1) is complete

~~ snip ~~

DEBUG: 2 running tasks, waiting for next task to finish
INFO: [pid 18109] Worker Worker(salt=166214281, workers=2, host=mhigu.local, username=mhigu, pid=18072) running   Secound(job_nums=0)
DEBUG: 2 running tasks, waiting for next task to finish
DEBUG: 2 running tasks, waiting for next task to finish
INFO: Worker task Secound(job_nums=1) died unexpectedly with exit code -11
INFO: Worker task Secound(job_nums=0) died unexpectedly with exit code -11
INFO: Informed scheduler that task   Secound(job_nums=1)   has status   FAILED
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: Secound(job_nums=0) is currently run by worker Worker(salt=166214281, workers=2, host=mhigu.local, username=mhigu, pid=18072)
INFO: Worker task Secound(job_nums=0) died unexpectedly with exit code -11
INFO: Informed scheduler that task   Secound(job_nums=0)   has status   FAILED
DEBUG: Asking scheduler for work...
INFO: Done
INFO: There are no more tasks to run at this time
INFO: There are 1 pending tasks possibly being run by other workers
INFO: There are 1 pending tasks unique to this worker
INFO: Worker Worker(salt=166214281, workers=2, host=mhigu.local, username=mhigu, pid=18072) was stopped. Shutting down Keep-Alive thread
INFO: 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 2 failed:
    - 2 Secound(job_nums=0,1)
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 First()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====


I checked stack trace and found it out something goes wrong in the scikit-learn fit method but I couldn't find  out exactly what cause is.

Could you tell me how can I fix this problem?
",-1,-1,-1.0
34613296,How to reset luigi task status?,"Currently, I have a bunch of luigi tasks queued together, with a simple dependency chain( a -&gt; b -&gt; c -&gt; d). d gets executed first, and a at the end. a is the task that gets triggered. 

All the targets except a return a luigi.LocalTarget() object and have a single generic luigi.Parameter() which is a string (containing a date and a time). Runs on a luigi central server (which has history enabled).

The problem is that, when I rerun the said task a, luigi checks the history and sees if that particular task has been run before, if it had had a status of DONE, it doesn't run the tasks (d in this case) and I can't have that, changing the string isn't helping (added a random microsecond to it). How do I force run a task ?
",-1,-1,-1.0
34776872,luigi doesn't work with alias,"I'm trying to create an alias for my luigi task. Usually to invoke it I'd type out luigi --module myTask --parameters cats 

However, I want to create an alias to shorten it to myTask --parameters cats, so I went to my .bash_profile and put this alias myTask='luigi --module myTask ""$@""'

However, when I try invoking my task again with the alias, I get this error:
    luigi.task_register.TaskClassNotFoundException: No task myTask. Candidates are: Config,ExternalTask,RangeBase,RangeDaily,RangeDailyBase,RangeHourly,RangeHourlyBase,Task,WrapperTask,core,execution_summary,retcode,scheduler,worker

Am I missing something in my .bash_profile to make this alias work?
",-1,-1,-1.0
36214858,How can I get my Luigi scheduler to utilize multiple cores with the parallel-scheduling flag?,"I have the following line in my luigi.cfg file (on all nodes, scheduler and workers):

[core]
parallel-scheduling: true


However, when I monitor CPU utilization on my luigi scheduler (with a graph of around ~4000 tasks, handling requests from ~100 workers), it is only utilizing a single core on the scheduler, with the single luigid thread often hitting 100% CPU utilization. My understanding is that this configuration variable should parallelize scheduling of tasks. 

The source suggests that this flag should indeed use multiple cores on the scheduler. In https://github.com/spotify/luigi/blob/master/luigi/interface.py#L194, a call is made to https://github.com/spotify/luigi/blob/master/luigi/worker.py#L498 to check the .complete() state of the task in parallel.

What am I missing to get my Luigi scheduler to utilize all of its cores?
",0,-1,-1.0
37918254,Luigid Syntax Error,"I am trying to use luigi in the central-scheduler mode. Whenever i try to run the central scheduler by using the command

$ luigid


It returns a syntax error

Traceback (most recent call last):


 File ""/usr/local/bin/luigid"", line 9, in &lt;module&gt;
    load_entry_point('luigi==2.1.1', 'console_scripts', 'luigid')()
  File ""/usr/local/lib/python2.7/dist-packages/luigi-2.1.1-py2.7.egg/luigi/cmdline.py"", line 15, in luigid
    import luigi.server
  File ""/usr/local/lib/python2.7/dist-packages/luigi-2.1.1-py2.7.egg/luigi/server.py"", line 51, in &lt;module&gt;
    import tornado.httpserver
  File ""/usr/local/lib/python2.7/dist-packages/tornado/httpserver.py"", line 34, in &lt;module&gt;
    from tornado.http1connection import HTTP1ServerConnection, HTTP1ConnectionParameters
  File ""/usr/local/lib/python2.7/dist-packages/tornado/http1connection.py"", line 28, in &lt;module&gt;
    from tornado import gen
  File ""/usr/local/lib/python2.7/dist-packages/tornado/gen.py"", line 1236, in &lt;module&gt;
    import tornado.platform.asyncio
  File ""/usr/local/lib/python2.7/dist-packages/tornado/platform/asyncio.py"", line 33, in &lt;module&gt;
    import asyncio
  File ""/usr/local/lib/python2.7/dist-packages/asyncio/__init__.py"", line 9, in &lt;module&gt;
    from . import selectors
  File ""/usr/local/lib/python2.7/dist-packages/asyncio/selectors.py"", line 39
    ""{!r}"".format(fileobj)) from None
                               ^
SyntaxError: invalid syntax

",-1,-1,-1.0
37819809,"ERROR: Uncaught exception in luigi (TypeError: must be string or buffer, not None)","I am having trouble while calling /triggering Luigi Task from a python code.


Basically i need to trigger a luigi task just like we do on command line, but from a python code
I am using supbrocess.popen to call a luigi task using a shell
command
I have a test code named as test.py and have a test class in module
task_scheduler.py which contains my luigi task (both modules in same location/dir)




        import luigi
        class TestClass(luigi.Task):
            # param = luigi.DictParameter(default=dict())

            def requires(self):
                print ""I am TestClass req""

            def run(self):
                with open('myfile.txt', 'w') as f:
                    f.write(""asasasas"")

                print ""I am TestClass run""






import subprocess

p = subprocess.Popen(""python -m luigi --module task_scheduler TestClass"", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

print p.pid
(output, err) = p.communicate()

print ""-------------O/P-------------""
print output
print ""-------------error-------------""
print err




But I am getting the error as



52688
-------------O/P-------------

-------------error-------------
ERROR: Uncaught exception in luigi
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/luigi/retcodes.py"", line 61, in run_with_retcodes
    worker = luigi.interface._run(argv)['worker']
  File ""/Library/Python/2.7/site-packages/luigi/interface.py"", line 238, in _run
    return _schedule_and_run([cp.get_task_obj()], worker_scheduler_factory)
  File ""/Library/Python/2.7/site-packages/luigi/interface.py"", line 172, in _schedule_and_run
    not(lock.acquire_for(env_params.lock_pid_dir, env_params.lock_size, kill_signal))):
  File ""/Library/Python/2.7/site-packages/luigi/lock.py"", line 82, in acquire_for
    my_pid, my_cmd, pid_file = get_info(pid_dir)
  File ""/Library/Python/2.7/site-packages/luigi/lock.py"", line 67, in get_info
    pid_file = os.path.join(pid_dir, hashlib.md5(cmd_hash).hexdigest()) + '.pid'
TypeError: must be string or buffer, not None




Can anyone please suggest me what I am doing wrong here?
The command ""python -m luigi --module task_scheduler TestClass"" works perfectly if I use shell prompt
",-1,-1,-1.0
39495757,How to handle output with Luigi,"I'm trying to grasp how luigi works, and I get the idea, but actual implementation is a bit harder ;) This is what i have:

class MyTask(luigi.Task):

    x = luigi.IntParameter()

    def requires(self):
        return OtherTask(self.x)

    def run(self):
        print(self.x)

class OtherTask(luigi.Task):

    x = luigi.IntParameter()

    def run(self):
        y = self.x + 1
        print(y)


And this fails with RuntimeError: Unfulfilled dependency at run time: OtherTask_3_5862334ee2. I've figured that I need to produce output using def output(self): to workaround this issue\feature. And I can't comprehend how do I produce reasonable output without writing to a file, say:

def output(self):
    return luigi.LocalTarget('words.txt')

def run(self):

    words = [
            'apple',
            'banana',
            'grapefruit'
            ]

    with self.output().open('w') as f:
        for word in words:
            f.write('{word}\n'.format(word=word))


I've tried reading the documentation, but I can't understand the concept behind output at all. What if I need to output to screen only. What if I need to output an object to another task? Thanks!
",-1,-1,-1.0
40139357,"Luigi task fails on main ""ValueError: No JSON object could be decoded""","I only receive this error when I run the task without --local-scheduler, on OS X El Cap. Daemon created with luigid command is currently running as well.

When I run the task with --local-scheduler, it operates as expected. It also runs fine both with and without --local-scheduler on my Windows 7 vm. Curious as to why it only fails in that one case on OS X.

Code snippet comes from this post I was referencing begin learning Luigi:https://marcobonzanini.com/2015/10/24/building-data-pipelines-with-python-and-luigi/

Code:

import luigi


class PrintNumbers(luigi.Task):
    n = luigi.IntParameter()

    def requires(self):
        return []

    def output(self):
        return luigi.LocalTarget(""numbers_up_to_{}.txt"".format(self.n))

    def run(self):
        with self.output().open('w') as f:
            for i in range(1, self.n+1):
                f.write(""{}\n"".format(i))


if __name__ == '__main__':
    luigi.run()


The trace: 

File ""scrape.py"", line 24, in 
    luigi.run()

File ""/Users/-/.virtualenvs/adwords/lib/python2.7/site-packages/luigi/interface.py"", line 210, in run
    return _run(*args, **kwargs)['success']

File ""/Users/-/.virtualenvs/adwords/lib/python2.7/site-packages/luigi/interface.py"", line 238, in _run
    return _schedule_and_run([cp.get_task_obj()], worker_scheduler_factory)

File ""/Users/-/.virtualenvs/adwords/lib/python2.7/site-packages/luigi/interface.py"", line 194, in _schedule_and_run
    success &amp;= worker.add(t, env_params.parallel_scheduling)

File ""/Users/-/.virtualenvs/adwords/lib/python2.7/site-packages/luigi/worker.py"", line 565, in add
    for next in self._add(item, is_complete):

File ""/Users/-/.virtualenvs/adwords/lib/python2.7/site-packages/luigi/worker.py"", line 682, in _add
    retry_policy_dict=_get_retry_policy_dict(task),

File ""/Users/-/.virtualenvs/adwords/lib/python2.7/site-packages/luigi/worker.py"", line 441, in _add_task
    self._scheduler.add_task(*args, **kwargs)

File ""/Users/-/.virtualenvs/adwords/lib/python2.7/site-packages/luigi/scheduler.py"", line 112, in rpc_func
    return self._request('/api/{}'.format(fn_name), actual_args, **request_args)

File ""/Users/-/.virtualenvs/adwords/lib/python2.7/site-packages/luigi/rpc.py"", line 145, in _request
    response = json.loads(page)[""response""]

File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/init.py"", line 338, in loads
    return _default_decoder.decode(s)

File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.py"", line 366, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())

File 
""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.py"", line 384, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
",-1,-1,-1.0
40140449,Luigi Visualiser to persist jobs for a day,"I have been trying with adding --scheduler-remove-delay parameter to job while invoking a run and also in client.cfg I have added remove-delay property


  [scheduler] 
  record_task_history = True 
  remove-delay = 86400.0


but none these seem to work. Any ideas?

There is an already almost similar problem here - 

Persist Completed Pipeline in Luigi Visualiser
",-1,-1,-1.0
40407936,MySQL Targets in Luigi workflow,"My TaskB requires TaskA, and on completion TaskA writes to a MySQL table, and then TaskB is to take in this output to the table as its input.

I cannot seem to figure out how to do this in Luigi. Can someone point me to an example or give me a quick example here?
",-1,-1,-1.0
40593709,Luigi 'No Instance(s) Available',"On my Windows VM i'm running some small jobs with Luigi. I'm wondering why at the beginning of every run I get a message saying ""No Instance(s) Available"". Every time I rerun the program, one more of these gets printed; so after 10 runs I have 10 ""No Instance(s) Available"" messages. 

It doesn't seem to affect the modules I'm running, so I'm just asking out of curiosity: Why is Luigi/Command Prompt telling me this, and are there any ramifications?

EDIT: getpcmd is the source of the wmic command that produces the message. That trail is:

interface._run -&gt; interface._schedule_and_run -&gt; lock.acquire_for -&gt; lock.get_info -&gt; lock.getpcmd


I think the culprit may be _WorkerScheduleFactory but that's speculation
",-1,-1,-1.0
40707004,Using luigi to update Postgres table,"I've just started using the luigi library. I am regularly scraping a website and inserting any new records into a Postgres database. As I'm trying to rewrite parts of my scripts to use luigi, it's not clear to me how the ""marker table"" is supposed to be used.

Workflow:


Scrape data
Query DB to check if new data differs from old data.
If so, store the new data in the same table.


However, using luigi's postgres.CopyToTable, if the table already exists, no new data will be inserted. I guess I should be using the inserted column in the table_updates table to figure out what new data should be inserted, but it's unclear to me what that process looks like and I can't find any clear examples online.
",-1,-1,-1.0
41794321,Requests in a multiprocess Luigi task,"I have a simple Luigi Elasticsearch indexing task which makes a GET with Requests and push the response to a local ElasticSearch. Also, I have made a second task that calls the first several times, like this:

import luigi
import requests
from luigi.contrib.esindex import CopyToIndex


class RequestTask(CopyToIndex):
    TEST_URL = 'http://www.this-page-intentionally-left-blank.org'
    index = 'example_index'
    iteration = luigi.IntParameter()

    def docs(self):
            res = requests.get(self.TEST_URL).content.decode('utf-8')
            return [{'response': res, 'iteration': self.iteration}]


class ManyRequests(luigi.Task):
    def requires(self):
        return [RequestTask(iteration) for iteration in range(0, 4)]

if __name__ == '__main__':
    luigi.run()


If I run ManyRequests task in a single thread, it works fine. However, If I specify a number of workers (e.g. --workers 4), processes will raise TransportError (index_already_exists_exception) from Elasticsearch, and they won't finish correctly. The number of finished processes is kind of random, so I assume it is due to some collisions writing in the Elasticsearch db. Do I have to implement ManyRequests in a different way?

Any help will be very grateful :)

This is my console when I execute ManyRequests --workers 4:

DEBUG: Checking if RequestTask(iteration=0) is complete
GET http://localhost:9200/update_log/entry/f55cf781cd5b4ff6be1454bc7fc624f874dea7ee [status:404 request:0.082s]
DEBUG: Marker document not found.
DEBUG: Checking if RequestTask(iteration=1) is complete
GET http://localhost:9200/update_log/entry/91af5a96a3e588ae318e996fd64add17465352b3 [status:404 request:0.020s]
DEBUG: Marker document not found.
DEBUG: Checking if RequestTask(iteration=2) is complete
GET http://localhost:9200/update_log/entry/41bb5cbca30df86d0815ec090b4d2fb20f2700d2 [status:404 request:0.051s]
DEBUG: Marker document not found.
DEBUG: Checking if RequestTask(iteration=3) is complete
GET http://localhost:9200/update_log/entry/d2dbeeca292ec62688a993c3b147272af2ba6a92 [status:404 request:0.061s]
DEBUG: Marker document not found.
INFO: Informed scheduler that task   ManyRequests__99914b932b   has status   PENDING
INFO: Informed scheduler that task   RequestTask_3_8a58dae6a3   has status   PENDING
INFO: Informed scheduler that task   RequestTask_2_eee8bd7963   has status   PENDING
INFO: Informed scheduler that task   RequestTask_1_020ce0ec4d   has status   PENDING
INFO: Informed scheduler that task   RequestTask_0_630962ba24   has status   PENDING
INFO: Done scheduling tasks
INFO: Running Worker with 4 processes
DEBUG: Asking scheduler for work...
DEBUG: Pending tasks: 5
DEBUG: Asking scheduler for work...
INFO: [pid 3211] Worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210) running   RequestTask(iteration=3)
DEBUG: Pending tasks: 4
DEBUG: Asking scheduler for work...
INFO: [pid 3212] Worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210) running   RequestTask(iteration=2)
DEBUG: Pending tasks: 3
DEBUG: Asking scheduler for work...
INFO: [pid 3213] Worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210) running   RequestTask(iteration=0)
DEBUG: Pending tasks: 2
DEBUG: 4 running tasks, waiting for next task to finish
INFO: [pid 3214] Worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210) running   RequestTask(iteration=1)
DEBUG: 4 running tasks, waiting for next task to finish
PUT http://localhost:9200/example_index [status:400 request:0.514s]
PUT http://localhost:9200/example_index [status:400 request:0.517s]
PUT http://localhost:9200/example_index [status:400 request:0.520s]
ERROR: [pid 3213] Worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210) failed    RequestTask(iteration=0)
Traceback (most recent call last):
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/worker.py"", line 192, in run
    new_deps = self._run_get_new_deps()
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/worker.py"", line 130, in _run_get_new_deps
    task_gen = self.task.run()
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/contrib/esindex.py"", line 448, in run
    self.create_index()
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/contrib/esindex.py"", line 399, in create_index
    es.indices.create(index=self.index, body=self.settings)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/client/utils.py"", line 71, in _wrapped
    return func(*args, params=params, **kwargs)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/client/indices.py"", line 107, in create
    params=params, body=body)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/transport.py"", line 318, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/connection/http_urllib3.py"", line 127, in perform_request
    self._raise_error(response.status, raw_data)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/connection/base.py"", line 122, in _raise_error
    raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)
elasticsearch.exceptions.RequestError: TransportError(400, 'index_already_exists_exception', 'index [example_index/PpySzpJ-QiSLNupQrmdVjg] already exists')ERROR: [pid 3212] Worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210) failed    RequestTask(iteration=2)
Traceback (most recent call last):
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/worker.py"", line 192, in run
    new_deps = self._run_get_new_deps()
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/worker.py"", line 130, in _run_get_new_deps
    task_gen = self.task.run()
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/contrib/esindex.py"", line 448, in run
    self.create_index()
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/contrib/esindex.py"", line 399, in create_index
    es.indices.create(index=self.index, body=self.settings)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/client/utils.py"", line 71, in _wrapped
    return func(*args, params=params, **kwargs)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/client/indices.py"", line 107, in create
    params=params, body=body)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/transport.py"", line 318, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/connection/http_urllib3.py"", line 127, in perform_request
    self._raise_error(response.status, raw_data)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/connection/base.py"", line 122, in _raise_error
    raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)
elasticsearch.exceptions.RequestError: TransportError(400, 'index_already_exists_exception', 'index [example_index/PpySzpJ-QiSLNupQrmdVjg] already exists')

ERROR: [pid 3214] Worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210) failed    RequestTask(iteration=1)
Traceback (most recent call last):
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/worker.py"", line 192, in run
    new_deps = self._run_get_new_deps()
  File ""/Users/jgc/dev/upm/tfg/TFG-JorgeGarciaCastano/env/lib/python3.5/site-packages/luigi/worker.py"", line 130, in _run_get_new_deps
    task_gen = self.task.run()
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/contrib/esindex.py"", line 448, in run
    self.create_index()
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/luigi/contrib/esindex.py"", line 399, in create_index
    es.indices.create(index=self.index, body=self.settings)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/client/utils.py"", line 71, in _wrapped
    return func(*args, params=params, **kwargs)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/client/indices.py"", line 107, in create
    params=params, body=body)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/transport.py"", line 318, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/connection/http_urllib3.py"", line 127, in perform_request
    self._raise_error(response.status, raw_data)
  File ""/Users/jgc/dev/env/lib/python3.5/site-packages/elasticsearch/connection/base.py"", line 122, in _raise_error
    raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)
elasticsearch.exceptions.RequestError: TransportError(400, 'index_already_exists_exception', 'index [example_index/PpySzpJ-QiSLNupQrmdVjg] already exists')
INFO: Informed scheduler that task   RequestTask_2_eee8bd7963   has status   FAILED
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: RequestTask_3_8a58dae6a3 is currently run by worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210)
DEBUG: RequestTask_1_020ce0ec4d is currently run by worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210)
DEBUG: RequestTask_0_630962ba24 is currently run by worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210)
INFO: Informed scheduler that task   RequestTask_1_020ce0ec4d   has status   FAILED
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: RequestTask_3_8a58dae6a3 is currently run by worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210)
DEBUG: RequestTask_0_630962ba24 is currently run by worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210)
INFO: Informed scheduler that task   RequestTask_0_630962ba24   has status   FAILED
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: RequestTask_3_8a58dae6a3 is currently run by worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210)
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: RequestTask_3_8a58dae6a3 is currently run by worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210)
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: RequestTask_3_8a58dae6a3 is currently run by worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210)
INFO: [pid 3211] Worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210) done      RequestTask(iteration=3)
INFO: Informed scheduler that task   RequestTask_3_8a58dae6a3   has status   DONE
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: There are 4 pending tasks possibly being run by other workers
DEBUG: There are 4 pending tasks unique to this worker
DEBUG: There are 4 pending tasks last scheduled by this worker
INFO: Worker Worker(salt=582258671, workers=4, host=jgc.local, username=jgc, pid=3210) was stopped. Shutting down Keep-Alive thread
INFO: 
===== Luigi Execution Summary =====

Scheduled 5 tasks of which:
* 1 ran successfully:
    - 1 RequestTask(iteration=3)
* 3 failed:
    - 3 RequestTask(iteration=0,1,2)
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 ManyRequests()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

",1,-1,-1.0
42197564,luigi-Failed building wheel while pip install luigi,"error: can't copy 'luigi\static\visualiser\lib\URI.js': doesn't exist or not a regular file

----------------------------------------
Failed building wheel for luigi
Running setup.py clean for luigi
Failed to build luigi
Installing collected packages: luigi
Running setup.py install for luigi ... error

",-1,-1,-1.0
42517538,Event Handling in Python Luigi,"I've been trying to integrate Luigi as our workflow handler. Currently we are using concourse, however many of the things we're trying to do is a hassle to get around in concourse so we made the switch to Luigi as our dependency manager. No problems so far, workflows trigger and execute properly.

The issue comes in when a task fails for whatever reason. This case specifically the requires block of a task, however all cases need to be taken care of. As of right now Luigi gracefully takes care of the error and writes it to STDOUT. It still emits and exit code 0 though, which to concourse means the job passed. A false positive. 

I've been trying to get the event handling to fix this, but I cannot get it to trigger, even with an extremely simple job:

@luigi.Task.event_handler(luigi.Event.FAILURE)
def mourn_failure(task, exception):
    with open('/root/luigi', 'a') as f:
        f.write(""we got the exception!"") #testing in concourse image
    sys.exit(luigi.retcodes.retcode().unhandled_exception)

class Test(luigi.Task):
    def requires(self):
        raise Exception()
        return []

    def run(self):
        pass

    def output(self):
        return []


Then running the command in python shell

luigi.run(main_task_cls=Test, local_scheduler=True)

The exception gets raised, but the even doesn't fire or something.
The file doesn't get written and the exit code is still 0.

Also, if it makes a difference I have my luigi config at /etc/luigi/client.cfg which contains

[retcode]
already_running=10
missing_data=20
not_run=25
task_failed=30
scheduling_error=35
unhandled_exception=40


I'm at a loss as to why the event handler won't trigger, but somehow I need the process to fail on an error.
",-1,-1,-1.0
42563175,How to enable dynamic requirements in Luigi?,"I have built a pipeline of Tasks in Luigi. Because this pipeline is going to be used in different contexts, it was possible that it would require to include more tasks at the beginning of or the end of the pipeline or even totally different dependencies between the tasks.

That's when I thought: ""Hey, why declare the dependencies between the tasks in my config file?"", so I added something like this to my config.py:

PIPELINE_DEPENDENCIES = {
     ""TaskA"": [],
     ""TaskB"": [""TaskA""],
     ""TaskC"": [""TaskA""],
     ""TaskD"": [""TaskB"", ""TaskC""]
}


I was annoyed by having those stacking up parameters throughout the tasks, so at some point I introduced just one parameter, task_config, that every Task has and where every information or data that's necessary for run() is stored. So I put PIPELINE_DEPENDENCIES right in there.

Finally, I would have every Task I defined inherit from both luigi.Task and a custom Mixin class, that would implement the dynamic requires(), which looks something like this:

class TaskRequirementsFromConfigMixin(object):
    task_config = luigi.DictParameter()

    def requires(self):
        required_tasks = self.task_config[""PIPELINE_DEPENDENCIES""]
        requirements = [
            self._get_task_cls_from_str(required_task)(task_config=self.task_config)
            for required_task in required_tasks
        ]
        return requirements

    def _get_task_cls_from_str(self, cls_str):
        ...


Unfortunately, that doesn't work, as running the pipeline gives me the following: 

===== Luigi Execution Summary =====

Scheduled 4 tasks of which:
* 4 were left pending, among these:
    * 4 was not granted run permission by the scheduler:
        - 1 TaskA(...)
        - 1 TaskB(...)
        - 1 TaskC(...)
        - 1 TaskD(...)

Did not run any tasks
This progress looks :| because there were tasks that were not granted run permission by the scheduler

===== Luigi Execution Summary =====


and a lot of

DEBUG: Not all parameter values are hashable so instance isn't coming from the cache


Although I am not sure if that's relevant.

So:
1. What's my mistake? Is it fixable?
2. Is there another way to achieve this?
",-1,1,-1.0
40843575,Architecture for luigi tasks with multiple inputs,"I have number of pickle files, one for each date between 2005 and 2010. Each file contains a dictionary of words with their respective frequencies for that date. I also have a ""master file"" with all unique words for the whole period. There are about 5 million words in total.

I need to take all that data and produce one CSV file per word, which will have one row per date. E.g., for example file some_word.txt:

2005-01-01,0.0003
2005-01-02,0.00034
2005-01-03,0.008


I'm having trouble organizing this process with the luigi framework. My current top-level task takes a word, looks up it's associated frequency for every date and stores the result in a CSV file. I guess I could just loop through every word in my master file and run the task with that word, but I estimate that would take months, if not longer. Here's my top-level AggregateTokenFreqs task in a simplified version.

class AggregateTokenFreqs(luigi.Task):
    word = luigi.Parameter()

    def requires(self):
        pass  # not sure what to require here, master file?

    def output(self):
        return luigi.LocalTarget('data/{}.csv'.format(self.word))

    def run(self):
        results = []
        for date_ in some_list_of_dates:
            with open('pickles/{}.p'.format(date_), 'rb') as f:
                freqs = pickle.load(f)
                results.append((date_, freqs.get(self.word))

        # Write results list to output CSV file

",-1,-1,-1.0
40342183,How to test Luigi with FakeS3?,"I'm trying to test my Luigi pipelines inside a vagrant machine using FakeS3 to simulate my S3 endpoints. For boto to be able to interact with FakeS3 the connection must be setup with the OrdinaryCallingFormat as in:

from boto.s3.connection import S3Connection, OrdinaryCallingFormat
conn = S3Connection('XXX', 'XXX', is_secure=False, 
                    port=4567, host='localhost',
                    calling_format=OrdinaryCallingFormat())


but when using Luigi this connection is buried in the s3 module. I was able to pass most of the options by modifying my luigi.cfg and adding an s3 section as in

[s3]
host=127.0.0.1
port=4567
aws_access_key_id=XXX
aws_secret_access_key=XXXXXX
is_secure=0


but I don't know how to pass the required object for the calling_format. 

Now I'm stuck and don't know how to proceed. Options I can think of:


Figure out how to pass the OrdinaryCallingFormat to S3Connection through luigi.cfg
Figure out how to force boto to always use this calling format in my Vagrant machine, by setting an unknown option to me either in .aws/config or boto.cfg
Make FakeS3 to accept the default calling_format used by boto that happens to be SubdomainCallingFormat (whatever it means).


Any ideas about how to fix this?
",-1,-1,-1.0
39595786,luigi per-task retry policy,"I have an issue configuring luigi per-task retry-policy. I've configured the global luigi.cfg file as follows:

[scheduler]
retry-delay: 1
retry_count: 5

[worker]
keep_alive: true
wait_interval: 3


Furthermore, it states in the luigi configuration manual that writing a task as follows:

class SomeTask(luigi.Task):

   retry_count = 3


will suffice in overriding the luigi retry_count specified in the luigi.cfg. However this setting does not effect the run at all. I've managed to create a task which fails every time just for testing, and logging returns that this task failed 5 times (and not 3). 

I think there's something fundamental i'm missing.
",-1,-1,-1.0
39179592,Luigi LocalTarget binary file,"I am having troubles to write a binary LocalTarget in a Luigi pipeline in my project. I isolated the problem here:

class LuigiTest(luigi.Task):
    def output(self):
        return luigi.LocalTarget('test.npz')

    def run(self):
        with self.output().open('wb') as fout:
            np.savez_compressed(fout, array=np.asarray([1, 2, 3]))


I tried opening as 'w' and 'wb' but I keep getting the following error:

TypeError: write() argument must be str, not bytes


I am using python 3.5.1 and my version of luigi is 2.1.1
",-1,-1,-1.0
42843544,"Running Luigi task from cmd - ""No module named tasks""","I am having problems running a Luigi task through the Windows cmd. Here are the facts:  


Running Anaconda installed in C:\ProgramData\Anaconda2 (Python 2.7)  
Anaconda has added its paths to the PATH variable but there is no PYTHONPATH variable   
The task i am trying to run is located in C:\....\tasks.py
Trying to run it as follows:


  C:\.... luigi --module tasks MyTask --dt 20170316
  ImportError: No module named tasks  



I tried creating a PYTHONPATH variable and adding the exact path to the directory containing my tasks.py file but it didn't work. Another problem I am having, which may be related is when I launch the luigi scheduler through cmd using:

luigid


it works fine but whenever I try to start it using:

luigid --background


I get the following error:

No module named pwd


It seems like there is something wrong with my setup overall, any help would be appreciated.
",-1,-1,-1.0
43107177,Luigi - Unfulfilled %s at run time,"I am trying to learn in a very simple way how luigi works. Just as a newbie I came up with this code
import luigi

class class1(luigi.Task):

  def requires(self):
     return class2()

  def output(self):
    return luigi.LocalTarget('class1.txt')

 def run(self):
    print 'IN class A'


class class2(luigi.Task): 

  def requires(self):
     return []

  def output(self):
     return luigi.LocalTarget('class2.txt')


if __name__ == '__main__':
  luigi.run()

Running this in command prompt gives error saying
raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ',', '.join(missing)))      

which is:
RuntimeError: Unfulfilled dependency at run time: class2__99914b932b  
   

",1,-1,-1.0
43764677,opening luigi.LocalTarget in binary read mode (decoding error),"I'm trying to open a luigi.LocalTarget for reading that points to a zip file (so that I can calculate a hash).  Unfortunately, when I try to read it, I get a UnicodeDecodeError, which I assume means its not getting opened as a binary file.

I can do this (without luigi) and it works fine

file_path  = luigi.LocalTarget('myfile.zip')
with open(file_path, 'rb') as f:
    data = f.read(1048576)


But if I do this

target = luigi.LocalTarget(file_path)
with target.open('rb') as f:
    data = f.read(1048576)


I get this

---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
&lt;ipython-input-28-5240759ed677&gt; in &lt;module&gt;()
      1 target = luigi.LocalTarget(file_path)
      2 with target.open('rb') as f:
----&gt; 3     data = f.read(1048576)

/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/codecs.py in decode(self, input, final)
    319         # decode input (taking the buffer into account)
    320         data = self.buffer + input
--&gt; 321         (result, consumed) = self._buffer_decode(data, self.errors, final)
    322         # keep undecoded input until the next call
    323         self.buffer = data[consumed:]

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc3 in position 10: invalid continuation byte


I'm using Python 3.6 and luigi 2.6.1.  Thanks in advance for any help
",1,-1,-1.0
43903122,luigi upstream task should run once to create input for set of downstream tasks,"I have a nice straight working pipe, where the task I run via luigi on the command line triggers all the required upstream data fetch and processing in it's proper sequence till it trickles out into my database. 

class IMAP_Fetch(luigi.Task):
  """"""fetch a bunch of email messages with data in them""""""
  date = luigi.DateParameter()
  uid = luigi.Parameter()
…
  def output(self):
    loc = os.path.join(self.data_drop, str(self.date))
    # target for requested message
    yield LocalTarget(os.path.join(loc, uid+"".msg""))

  def run(self):
     # code to connect to IMAP server and run FETCH on given UID
     # message gets written to self.output()
…

class RecordData(luigi.contrib.postgres.CopyToTable):
  """"""copy the data in one email message to the database table""""""
  uid = luigi.Parameter()
  date = luigi.DateParameter()
  table = 'msg_data'
  columns = [(id, int), …]

  def requires(self):
    # a task (not shown) that extracts data from one message  
    # which in turn requires the IMAP_Fetch to pull down the message
    return MsgData(self.date, self.uid) 

  def rows(self):
    # code to read self.input() and yield lists of data values 


Great stuff. Unfortunately that first data fetch talks to a remote IMAP server, and every fetch is a new connection and a new query: very slow. I know how to get all the individual message files in one session (task instance). I don't understand how to keep the downstream tasks just as they are, working on one message at a time, since the task that requires one message triggers a fetch of just that one message, not a fetch of all the messages available. I apologize in advance for missing obvious solutions, but it has stumped me so far how to keep my nice simple stupid pipe mostly the way it is but have the funnel at the top suck in all the data in one call. Thanks for your help.
",1,1,-1.0
44818486,Luigi : Rangehourly Examples,"Is there any examples available for the RangeHourly provision (or similar ones like RangeDaily) . I've been trying to use it to have recurring execution of tasks . But I always end up getting an error like below :

DEBUG: Checking if RangeHourly(of=FinalTask, of_params={}, reverse=False, task_limit=50, now=None, param_name=None, start=2017-06-28T15, stop=None, hours_back=0, hours_forward=0) is complete
DEBUG: Empty range. No FinalTask instances expected

Below , is the definition of the task :

class FinalTask (luigi.Task):
    start = luigi.DateHourParameter()
    def requires(self):
            return CleanupTask()
    def run(self):
            cmd='echo ""Workflow Completed""'
            args=shlex.split(cmd)
            exc=subprocess.Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
            stdout,stderr=exc.communicate()
            self.output().open('w').close()
    def output(self):
            return luigi.LocalTarget('/var/flags/FinalTask_success_%s.csv' %start)


Is there anything I'm missing , which is causing this problem ?
",-1,-1,-1.0
44877983,create and insert into database tables using luigi,"I am trying to understand the correct way to drop and recreate a table and insert data into the newly created table using luigi. I have multiple CSV files passed from the prior task to the task which should be inserted into the database.

My current code looks like this.

class CreateTables(sqla.CopyToTable):
connection_string = DatabaseConfig().data_mart_connection_string
table = DatabaseConfig().table_name

def requires(self):
    return CustomerJourneyToCSV()

def output(self):
    return SQLAlchemyTarget(
        connection_string=self.connection_string,
        target_table=""customerJourney_1"",
        update_id=self.update_id(),
        connect_args=self.connect_args,
        echo=self.echo)

def create_table(self, engine):

    base = automap_base()
    Session = sessionmaker(bind=engine)
    session = Session()

    metadata = MetaData(engine)

    base.prepare(engine, reflect=True)

    # Drop existing tables
    for i in range(1, len(self.input())+1):
        for t in base.metadata.sorted_tables:
            if t.name in ""{0}_{1}"".format(self.table, i):
                t.drop(engine)

    # Create new tables and insert data
    i = 1
    for f in self.input():
        df = pd.read_csv(f.path, sep=""|"")
        df.fillna(value="""", inplace=True)

        ts = define_table_schema(df)

        t = Table(""{0}_{1}"".format(self.table, i), metadata, *[Column(*c[0], **c[1]) for c in ts])

        t.create(engine)

        # TODO: Need to remove head and figure out how to stop the connection from timing out
        my_insert = t.insert().values(df.head(500).to_dict(orient=""records""))

        session.execute(my_insert)

        i +=1

    session.commit()


The code works creates the tables and inserts the data but falls over with the following error.

C:\Users\simon\AdviceDataMart\lib\site-packages\luigi\worker.py:191: 
DtypeWarning: Columns (150) have mixed types. Specify dtype option on import 
or set low_memory=False.
  new_deps = self._run_get_new_deps()
File ""C:\Users\simon\AdviceDataMart\lib\site-packages\luigi\worker.py"", line 
191, in run
new_deps = self._run_get_new_deps()
File ""C:\Users\simon\AdviceDataMart\lib\site-packages\luigi\worker.py"", line 
129, in _run_get_new_deps
task_gen = self.task.run()
File ""C:\Users\simon\AdviceDataMart\lib\site-
packages\luigi\contrib\sqla.py"", line 375, in run
for row in itertools.islice(rows, self.chunk_size)]
File ""C:\Users\simon\AdviceDataMart\lib\site-
packages\luigi\contrib\sqla.py"", line 363, in rows
with self.input().open('r') as fobj:
AttributeError: 'list' object has no attribute 'open'


I am not sure what is causing this and am not able to easily debug a luigi pipeline. I am not sure if this has to do with implementation of the run method or the output method?
",-1,-1,-1.0
45001163,Scheduling very large amounts of jobs on python luigi,"I've written a Luigi pipeline to extract 1.2 mio files and then do some sed work on them - see https://gist.github.com/wkerzendorf/395c85a2955002412be302d708329f7f. 

If I run this through Luigi on a few thousand files it schedules fine. But running this on the whole dataset it complains with Failed connecting to remote scheduler. Not sure if I do this the right way. 
",-1,-1,-1.0
45012614,Can Luigi handle parallel runs of the same pipeline?,"Users send us orders. On receipt of an order we run a pipeline which consists of run several tasks. Some tasks take days of computer time.

# Example pipeline
order -&gt; task1 -&gt; task2 -&gt; task3 -&gt; Complete


I want to start processing each order as it comes in, so our compute cluster will be processing multiple orders and their associated run of the pipeline simultaneously

# Diagram showing each order arriving then being processed over time.

|  order1 -&gt; t1 -&gt; t2 -&gt; t3 -&gt; complete
|             order2 -&gt; t1 -&gt; t2 -&gt; t3 -&gt; complete
|                        order3 -&gt; t1 -&gt; t2 -&gt; t3 -&gt; complete
|                        order4 -&gt; t1 -&gt; t2 -&gt; t3 -&gt; complete
|                                                order5 -&gt; t1 -&gt; t
------------------------------------------------------------------
 time 


Can you do this in Luigi? 

I don't think that you can as when I implement a test pipeline, then try to start another instance of the test pipeline, while the first is running I get the following output from the second pipeline instance.

Pid(s) set([11004]) already running

Process finished with exit code 0

",-1,1,-1.0
44417327,python luigi localTarget pickle,"I am running on Windows 7, Python 2.7 via Anaconda 4.3.17, Luigi 2.4.0, Pandas 0.18, sklearn version 0.18.  Per below, I am trying to have a luigi.LocalTarget output be a pickle to store a few different objects (using firstJob) and then read from that pickle in a dependent job (secondJob).  firstJob completes successfully if I run the following from the command line:

""python -m luigi --module luigiPickle firstJob --date 2017-06-07 --local-scheduler""

However, if I try running secondJob i.e., 

""python -m luigi --module luigiPickle secondJob --date 2017-06-07 --local-scheduler""

I get 

Traceback (most recent call last):
  File ""C:\Anaconda2\lib\site-packages\luigi-2.4.0-py2.7.egg\luigi\worker.py"", l
ine 191, in run
    new_deps = self._run_get_new_deps()
  File ""C:\Anaconda2\lib\site-packages\luigi-2.4.0-py2.7.egg\luigi\worker.py"", l
ine 129, in _run_get_new_deps
    task_gen = self.task.run()
  File ""luigiPickle.py"", line 41, in run
    ret2 = pickle.load(inFile)
  File ""C:\Anaconda2\lib\pickle.py"", line 1384, in load
    return Unpickler(file).load()
  File ""C:\Anaconda2\lib\pickle.py"", line 864, in load
    dispatch[key](self)
  File ""C:\Anaconda2\lib\pickle.py"", line 1096, in load_global
    klass = self.find_class(module, name)
  File ""C:\Anaconda2\lib\pickle.py"", line 1130, in find_class
    __import__(module)
ImportError: No module named frame


It appears that luigi is having trouble reading the pickle due to not recognizing the pandas.DataFrame() object (perhaps a scope issue?).

import luigi
import pandas as pd
import pickle
from sklearn.linear_model import LinearRegression

class firstJob(luigi.Task):
    date = luigi.DateParameter()

    def requires(self):
        return None

    def output(self):
        return luigi.LocalTarget('%s_first.pickle' % self.date)

    def run(self):
        ret = {}
        ret['a'] = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
        ret['b'] = pd.DataFrame({'a': [3, 4], 'd': [0, 0]})
        ret['c'] = LinearRegression()
        outFile = self.output().open('wb')
        pickle.dump(ret, outFile, protocol=pickle.HIGHEST_PROTOCOL)
        outFile.close()

class secondJob(luigi.Task):
    date = luigi.DateParameter()

    def requires(self):
        return firstJob(self.date)

    def output(self):
        return luigi.LocalTarget('%s_second.pickle' % self.date)

    def run(self):
        inFile = self.input().open('rb')
        ret2 = pickle.load(inFile)
        inFile.close()

if __name__ == '__main__':
    luigi.run()

",-1,-1,-1.0
44379387,JSON serialization error when creating a Luigi task graph,"I'm trying to batch up the processing of a few Jupyter notebooks using Luigi, and I've run into a problem.

I have two classes. The first, transform.py:

import nbformat
import nbconvert

import luigi
from nbconvert.preprocessors.execute import CellExecutionError


class Transform(luigi.Task):
    """"""Foo.""""""
    notebook = luigi.Parameter()
    requirements = luigi.ListParameter()

    def requires(self):
        return self.requirements

    def run(self):
        nb = nbformat.read(self.notebook, nbformat.current_nbformat)
        # https://nbconvert.readthedocs.io/en/latest/execute_api.html
        ep = nbconvert.preprocessors.ExecutePreprocessor(timeout=600, kernel_name='python3')
        try:
            ep.preprocess(nb, {'metadata': {'path': ""/"".join(self.notebook.split(""/"")[:-1])}})
            with self.output().open('w') as f:
                nbformat.write(nb, f)
        except CellExecutionError:
            pass  # TODO

    def output(self):
        return luigi.LocalTarget(self.notebook)


This defines a Luigi task that takes a notebook as input (along with possible prior requirements to running this task) and ought to run that notebook and report a success or failure as output.

To run Transform tasks I have a tiny Runner class:

import luigi


class Runner(luigi.Task):
    requirements = luigi.ListParameter()

    def requires(self):
        return self.requirements


To run my little job, I do:

from transform Transform
trans = Transform(""../tests/fixtures/empty_valid_errorless_notebook.ipynb"", []) 
from runner import Runner
run_things = Runner([trans])


But this raises TypeError: Object of type 'Transform' is not JSON serializable!

Is my luigi task format correct? If so, is it obvious what component in run is making the entire class unserializable? If not, how should I go about debugging this?
",-1,-1,-1.0
44148630,Luigi child dependency in subclass,"I have a flow A->B->C where C depend on B, B depend on A
Let say I have another flow D->B->C  

I try to reuse the task. How can I easily reuse?    

I can create a subclass that inherit Task B and change the requires to Task D, however to allow D->B->C, I need again to create subclass that inherit Task C and change the requires method to subclass of B.  

This is very troublesome and I wonder is there a easy way to do this> Or this is the correct behavior of luigi?
",1,-1,-1.0
43887207,Luigi server not reachable from python,"I have a fresh Windows 10 installation with a miniconda env (py34) with luigi installed. When I run luigid in command prompt it starts the scheduler, and when I open localhost:8082 in chrome it shows my the web front of the scheduler.

However, when I try to run a task to the scheduler (in python from PyCharm), I seem not to be able to connect. The error I am getting is this:

Traceback (most recent call last):
  File ""C:\MiniConda3\envs\py34\lib\site-packages\luigi\rpc.py"", line 125, in _fetch
    response = self._fetcher.fetch(full_url, body, self._connect_timeout)
  File ""C:\MiniConda3\envs\py34\lib\site-packages\luigi\rpc.py"", line 75, in fetch
    return urlopen(full_url, body, timeout).read().decode('utf-8')
  File ""C:\MiniConda3\envs\py34\lib\urllib\request.py"", line 163, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\MiniConda3\envs\py34\lib\urllib\request.py"", line 472, in open
    response = meth(req, response)
  File ""C:\MiniConda3\envs\py34\lib\urllib\request.py"", line 582, in http_response
    'http', request, response, code, msg, hdrs)
  File ""C:\MiniConda3\envs\py34\lib\urllib\request.py"", line 510, in error
    return self._call_chain(*args)
  File ""C:\MiniConda3\envs\py34\lib\urllib\request.py"", line 444, in _call_chain
    result = func(*args)
  File ""C:\MiniConda3\envs\py34\lib\urllib\request.py"", line 590, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 407: Proxy Authorization Required
10:10:15.217 [   INFO] [luigi-interface] Retrying...
10:10:16.202 [  ERROR] [luigi-interface] Failed connecting to remote scheduler 'http://localhost:8082'


A similar setup on a windows 7 machine works like that. 

Anyone any ideas what might be causing this? Thanks.
",-1,-1,-1.0
43377252,dask + luigi: raise ValueError('url type not understood: %s' % urlpath),"I am trying to merge dask with luigi,
and while business logic works fine by itself, code starts throwing errors when I run a Luigi task:

raise ValueError('url type not understood: %s' % urlpath)
ValueError: url type not understood: &lt;_io.TextIOWrapper name='../data/2017_04_11_oldsource_geocoded.csv-luigi-tmp-1647603946' mode='wb' encoding='UTF-8'&gt;


the code is here (I dropped the business model part to make it shorter):

import pandas as pd
import geopandas as gp
from geopandas.tools import sjoin
from dask import dataframe as dd
from shapely.geometry import Point
from os import path
import luigi

class geocode_tweets(luigi.Task):
    boundaries = _load_geoboundaries()
    nyc = boundaries[0].unary_union

    def requires(self):
        return []

    def output(self):
        self.path = '../data/2017_04_11_oldsource_geocoded.csv'
        return luigi.LocalTarget(self.path)

    def run(self):
        df = dd.read_csv(path.join(data_dir, '2017_03_22_oldsource.csv'))
        df['geometry'] = df.apply(_get_point, axis=1)
        meta = _form_meta(df)

        S = df.map_partitions(
            distributed_sjoin, boundaries=self.boundaries,
            nyc_border=self.nyc, meta=meta).drop('geometry', axis=1)

        f = self.output().open('w')
        S.to_csv(f)
        f.close()


and the problem, it looks like, is in the output part

As far as I understand, problem is that dask does not like Luigi file objects as a substitution to the string.
",-1,-1,-1.0
46044547,luigi running external program can't locate bash file,"I am 'simply' trying to solve how to run an external program from luigi.
Utilising the ExternalProgramTask from luigi.contrib.external_program.

The luigi class is:

from luigi.contrib.external_program import ExternalProgramTask
import luigi

class PoincarreEmbedding(ExternalProgramTask):
    file_path = 'examples/tmp/3'

    def program_args(self):
        return ["". embeddings.sh""]


Luigi runs and fails as it cannot find the file.

INFO: Running command: . embeddings.sh
ERROR: [pid 43886] Worker Worker(salt=404030211, workers=1, 
host=Roberts-MacBook-Pro.local, username=rhsmith, pid=43886) failed    
PoincarreEmbedding()
Traceback (most recent call last):
File ""/Users/rhsmith/luigi/luigi/worker.py"", line 194, in run
new_deps = self._run_get_new_deps()
File ""/Users/rhsmith/luigi/luigi/worker.py"", line 131, in _run_get_new_deps
task_gen = self.task.run()
File ""/Users/rhsmith/luigi/luigi/contrib/external_program.py"", line 98, in run
stderr=tmp_stderr
File ""/Users/rhsmith/anaconda2/lib/python2.7/subprocess.py"", line 390, in __init__
errread, errwrite)
File ""/Users/rhsmith/anaconda2/lib/python2.7/subprocess.py"", line 1024, in _execute_child
raise child_exception
OSError: [Errno 2] No such file or directory


The '. embedding.sh' file is located in the same directory as where the module/task is run from. 

Have been studying this thread but to no avail. I am sure it simple :/
",-1,-1,-1.0
46412322,Luigi task returns unfulfilled dependency at run time when dependency is complete,"I am relatively new to creating flows with Luigi and am trying to understand why my small workflow is resulting in an unfulfilled dependency. I am trying to run the task StageProviders(), which has a single dependency ErrorsLogFile(). The tasks that must be run before StageProviders are simply tasks to create blank files on a shared drive. I receive following message when I try and run the StageProviders task in the following flow as follows:

Code: 

#!/usr/local/bin/python

import luigi
import os
import shutil
import time
import pandas as pd
import time


class DupsExistingLogFile(luigi.Task):
    filename = luigi.Parameter()

    def requires(self):
        return None

    def output(self):
        timestr = time.strftime(""%Y-%m-%d"")
        return luigi.LocalTarget(os.path.join('/root/etc/mnt/Import/LogFiles/' + os.path.splitext(self.filename)[0] + '_' + timestr + ""_DuplicatesExisting.xlsx""))

def run(self):
    timestr = time.strftime(""%Y-%m-%d"")
    src_blank_file_str = os.path.join('/root/etc/mnt/Import/LogFiles/Provider_Blank_DONOTDELETE.xlsx')
    dest_file_str = os.path.join(os.path.join('/root/etc/mnt/Import/LogFiles/' + os.path.splitext(self.filename)[0] + '_' + timestr + ""_DuplicatesExisting.xlsx""))
    shutil.copyfile(src_blank_file_str, dest_file_str)


class DupsLogFile(luigi.Task):
    filename = luigi.Parameter()

    def requires(self):
        return DupsExistingLogFile(self.filename)

    def output(self):
        timestr = time.strftime(""%Y-%m-%d"")
        return luigi.LocalTarget(os.path.join('/root/etc/mnt/Import/LogFiles/' + os.path.splitext(self.filename)[0] + '_' + timestr + ""_Duplicates.xlsx""))

    def run(self):
        timestr = time.strftime(""%Y-%m-%d"")
        src_blank_file_str = os.path.join('/root/etc/mnt/Import/LogFiles/Provider_Blank_DONOTDELETE.xlsx')
        dest_file_str = os.path.join(os.path.join('/root/etc/mnt/Import/LogFiles/' + os.path.splitext(self.filename)[0] + '_' + timestr + ""_Duplicates.xlsx""))
        shutil.copyfile(src_blank_file_str, dest_file_str)


class ErrorsLogFile(luigi.Task):
    filename = luigi.Parameter()

    def requires(self):
        return DupsLogFile(self.filename)

    def output(self):
        timestr = time.strftime(""%Y-%m-%d"")
        return luigi.LocalTarget(os.path.join('/root/etc/mnt/Import/LogFiles/' + os.path.splitext(self.filename)[0] + '_' + timestr + ""_Errprs.xlsx""))

    def run(self):
        timestr = time.strftime(""%Y-%m-%d"")
        src_blank_file_str = os.path.join('/root/etc/mnt/Import/LogFiles/Provider_Blank_DONOTDELETE.xlsx')
        dest_file_str = os.path.join(os.path.join('/root/etc/mnt/Import/LogFiles/' + os.path.splitext(self.filename)[0] + '_' + timestr + ""_Errors.xlsx""))
        shutil.copyfile(src_blank_file_str, dest_file_str)


class StageProviders(luigi.Task):
    filename = luigi.Parameter()

    def requires(self):
    return ErrorsLogFile(self.filename)

    def output(self):
        timestr = time.strftime(""%Y-%m-%d"")
        return luigi.LocalTarget(os.path.join('/root/etc/mnt/Import/LogFiles/_SUCCESS_STG_' + os.path.splitext(self.filename)[0] + '_' + timestr + '.txt'))

def run(self):
    timestr = time.strftime(""%Y-%m-%d"")
    filepath_str = '/root/etc/mnt/Import/' + self.filename
    xls_file = pd.ExcelFile(filepath_str)
    df = xls_file.parse('Sheet1')
    src_blank_file_str = os.path.join('/root/etc/mnt/Import/LogFiles/_SUCCESS.txt')
    dest_file_str = os.path.join('/root/etc/mnt/Import/LogFiles/_SUCCESS_STG_' + os.path.splitext(self.filename)[0] + '_' + timestr + '.txt')
    if not df.empty:
        shutil.copyfile(src_blank_file_str, dest_file_str)
        with self.output().open('w') as out_file:
            for name in df['NP']:
                print(name, end='\n', file=out_file)


Output: 

root@ubuntu:~/pythonfiles/luigi_POC/cpi_luigi_poc/src# python3 -m luigi --module provider_import  StageProviders --filename CCM_provider_sample.xlsx --
local-scheduler
DEBUG: Checking if StageProviders(filename=CCM_provider_sample.xlsx) is complete
DEBUG: Checking if ErrorsLogFile(filename=CCM_provider_sample.xlsx) is complete
INFO: Informed scheduler that task   StageProviders_CCM_provider_sam_ad65b206fd   has status   PENDING
DEBUG: Checking if DupsLogFile(filename=CCM_provider_sample.xlsx) is complete
INFO: Informed scheduler that task   ErrorsLogFile_CCM_provider_sam_ad65b206fd   has status   PENDING
DEBUG: Checking if DupsExistingLogFile(filename=CCM_provider_sample.xlsx) is complete
INFO: Informed scheduler that task   DupsLogFile_CCM_provider_sam_ad65b206fd   has status   PENDING
INFO: Informed scheduler that task   DupsExistingLogFile_CCM_provider_sam_ad65b206fd   has status   PENDING
INFO: Done scheduling tasks
INFO: Running Worker with 1 processes
DEBUG: Asking scheduler for work...
DEBUG: Pending tasks: 4
INFO: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) running   DupsExistingLogFile(filename=CCM_provider_sample.xlsx)
INFO: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) done      DupsExistingLogFile(filename=CCM_provider_sample.xlsx)
DEBUG: 1 running tasks, waiting for next task to finish
INFO: Informed scheduler that task   DupsExistingLogFile_CCM_provider_sam_ad65b206fd   has status   DONE
DEBUG: Asking scheduler for work...
DEBUG: Pending tasks: 3
INFO: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) running   DupsLogFile(filename=CCM_provider_sample.xlsx)
INFO: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) done      DupsLogFile(filename=CCM_provider_sample.xlsx)
DEBUG: 1 running tasks, waiting for next task to finish
INFO: Informed scheduler that task   DupsLogFile_CCM_provider_sam_ad65b206fd   has status   DONE
DEBUG: Asking scheduler for work...
DEBUG: Pending tasks: 2
INFO: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) running   ErrorsLogFile(filename=CCM_provider_sample.xlsx)
INFO: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) done      ErrorsLogFile(filename=CCM_provider_sample.xlsx)
DEBUG: 1 running tasks, waiting for next task to finish
INFO: Informed scheduler that task   ErrorsLogFile_CCM_provider_sam_ad65b206fd   has status   DONE
DEBUG: Asking scheduler for work...
DEBUG: Pending tasks: 1
INFO: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) running   StageProviders(filename=CCM_provider_sample.xlsx)
ERROR: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) failed    StageProviders(filename=CCM_provider_sample.xlsx)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/luigi/worker.py"", line 175, in run
    raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))
RuntimeError: Unfulfilled dependency at run time: ErrorsLogFile_CCM_provider_sam_ad65b206fd
DEBUG: 1 running tasks, waiting for next task to finish
INFO: Informed scheduler that task   StageProviders_CCM_provider_sam_ad65b206fd   has status   FAILED
DEBUG: Checking if StageProviders(filename=CCM_provider_sample.xlsx) is complete
DEBUG: Checking if ErrorsLogFile(filename=CCM_provider_sample.xlsx) is complete
INFO: Informed scheduler that task   StageProviders_CCM_provider_sam_ad65b206fd   has status   PENDING
DEBUG: Checking if DupsLogFile(filename=CCM_provider_sample.xlsx) is complete
INFO: Informed scheduler that task   ErrorsLogFile_CCM_provider_sam_ad65b206fd   has status   PENDING
INFO: Informed scheduler that task   DupsLogFile_CCM_provider_sam_ad65b206fd   has status   DONE
DEBUG: Asking scheduler for work...
DEBUG: Pending tasks: 2
INFO: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) running   ErrorsLogFile(filename=CCM_provider_sample.xlsx)
INFO: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) done      ErrorsLogFile(filename=CCM_provider_sample.xlsx)
DEBUG: 1 running tasks, waiting for next task to finish
INFO: Informed scheduler that task   ErrorsLogFile_CCM_provider_sam_ad65b206fd   has status   DONE
DEBUG: Asking scheduler for work...
DEBUG: Pending tasks: 1
INFO: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) running   StageProviders(filename=CCM_provider_sample.xlsx)
ERROR: [pid 10904] Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) failed    StageProviders(filename=CCM_provider_sample.xlsx)
Traceback (most recent call last):
File ""/usr/local/lib/python3.5/dist-packages/luigi/worker.py"", line 175, in run
    raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))
RuntimeError: Unfulfilled dependency at run time: ErrorsLogFile_CCM_provider_sam_ad65b206fd
DEBUG: 1 running tasks, waiting for next task to finish
INFO: Informed scheduler that task   StageProviders_CCM_provider_sam_ad65b206fd   has status   FAILED
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: There are 1 pending tasks possibly being run by other workers
DEBUG: There are 1 pending tasks unique to this worker
DEBUG: There are 1 pending tasks last scheduled by this worker
INFO: Worker Worker(salt=306235977, workers=1, host=ubuntu, username=root, pid=10904) was stopped. Shutting down Keep-Alive thread
INFO:
===== Luigi Execution Summary =====

Scheduled 4 tasks of which:
* 3 ran successfully:
    - 1 DupsExistingLogFile(filename=CCM_provider_sample.xlsx)
    - 1 DupsLogFile(filename=CCM_provider_sample.xlsx)
    - 1 ErrorsLogFile(filename=CCM_provider_sample.xlsx)
* 1 failed:
- 1 StageProviders(filename=CCM_provider_sample.xlsx)

This progress looks :( because there were failed tasks


It appears this is because of this message: 

RuntimeError: Unfulfilled dependency at run time: ErrorsLogFile_CCM_provider_sam_ad65b206fd

However, reading the output it seems ErrorsLogFile_CCM_provider_sam_ad65b206fd has finished before StageProviders is run?... Why is the scheduler returning unfulfilled dependency? I believe I'm misunderstanding how to ""chain"" tasks together. I simply want the StageProviders task to run following successful completion of the ErrorsLogFile, DupsLogFile, and DupsExistingLogFile tasks. 
",-1,-1,-1.0
46459977,Re-using generic tasks in Luigi,"I'm having trouble understanding how to make re-usable tasks in Luigi, and then use them in a concrete situation.

For example. I have two generic tasks that do something to a file and then output the result:

class GffFilter(luigi.Task):
    ""Filters a GFF file to only one feature""
    feature = luigi.Parameter()
    out_file = luigi.Parameter()
    in_file = luigi.Parameter()
    ...

class BgZip(luigi.Task):
    ""bgZips a file""
    out_file = luigi.Parameter()
    in_file = luigi.Parameter()
    ...


Now, I want a workflow that first filters, then bgzips a specific file using these tasks:

class FilterSomeFile(luigi.WrapperTask):
    def requires(self):
        return GffFilter(in_file='some.gff3', out_file='some.genes.gff3', filter='gene')

    def output(self):
        return self.inputs()

class BgZipSomeFile(luigi.Task):
    def run(self):
        filtered = FilterSomeFile()
        BzZip(filtered)


But this is awkward. In the first task I have no run method, and I'm just using dependencies to use the generic task. Is this correct? Should I be using inheritance here instead?

Then in the second task, I can't use dependencies, because I need the output from FilterSomeFile in order to use BgZip. But using dynamic dependencies seems wrong, because luigi can't build a proper dependency graph.

How should I make a Luigi workflow out of my generic tasks?
",-1,-1,-1.0
46660372,Luigi framework crash,"when I running luigi tasks, sometimes will meet framework crash, cause the following tasks all failed. Here the error log info:

2017-10-05 22:02:02,564 luigi-interface WARNING  Failed pinging scheduler
2017-10-05 22:02:03,129 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (126): localhost
2017-10-05 22:02:03,130 luigi-interface ERROR    Failed connecting to remote scheduler 'http://localhost:8082'
Traceback (most recent call last):
    ...
    File ""/home/develop/data_warehouse/venv/local/lib/python2.7/site-packages/requests/sessions.py"", line 585, in send
    r = adapter.send(request, **kwargs)
    File ""/home/develop/data_warehouse/venv/local/lib/python2.7/site-packages/requests/adapters.py"", line 467, in send
    raise ConnectionError(e, request=request)
    ConnectionError: HTTPConnectionPool(host='localhost', port=8082): Max retries exceeded with url: /api/add_worker (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x7f15128cb3d0&gt;: Failed to establish a new connection: [Errno 111] Connection refused',))
2017-10-05 22:02:03,180 luigi-interface INFO     Worker Worker(salt=150908931, workers=3, host=etl2, username=develop, pid=18019) was stopped. Shutting down Keep-Alive thread
Traceback (most recent call last):
    File ""app_metadata.py"", line 1567, in &lt;module&gt;
    luigi.run()
    File ""/home/develop/data_warehouse/venv/local/lib/python2.7/site-packages/luigi/interface.py"", line 210, in run
    return _run(*args, **kwargs)['success']
    File ""/home/develop/data_warehouse/venv/local/lib/python2.7/site-packages/luigi/interface.py"", line 238, in _run
    return _schedule_and_run([cp.get_task_obj()], worker_scheduler_factory)
    File ""/home/develop/data_warehouse/venv/local/lib/python2.7/site-packages/luigi/interface.py"", line 197, in _schedule_and_run
    success &amp;= worker.run()
    File ""/home/develop/data_warehouse/venv/local/lib/python2.7/site-packages/luigi/worker.py"", line 867, in run
    self._add_worker()
    File ""/home/develop/data_warehouse/venv/local/lib/python2.7/site-packages/luigi/worker.py"", line 652, in _add_worker
    self._scheduler.add_worker(self._id, self._worker_info)
    File ""/home/develop/data_warehouse/venv/local/lib/python2.7/site-packages/luigi/rpc.py"", line 219, in add_worker
    return self._request('/api/add_worker', {'worker': worker, 'info': info})
    File ""/home/develop/data_warehouse/venv/local/lib/python2.7/site-packages/luigi/rpc.py"", line 146, in _request
    page = self._fetch(url, body, log_exceptions, attempts)
    File ""/home/develop/data_warehouse/venv/local/lib/python2.7/site-packages/luigi/rpc.py"", line 138, in _fetch
    last_exception
    luigi.rpc.RPCError: Errors (3 attempts) when connecting to remote scheduler 'http://localhost:8082'


sounds like try to ping central schedule, but be failed, then crashed, later tasks all be blocked, cannot run successfully.

and, some one else also meet the similar error, but his resolution not works.
Github - Failed connecting to remote scheduler #1894
",-1,-1,-1.0
46764974,How to avoid Google-Cloud-Dataflow Import Error when using it inside Luigi,"I have multiple processes which are dependent on each other. I am using Luigi to manage these dependencies. Since there are multiple processes, I have made package of each of the process. I.e All the relevant files of the process are in a folder having an init file. I am using one Luigi task which looks into its dependency and executes which ever process that are required to be completed for that task to run.

In one of those tasks is a dataflow job. When i call that dataflow job it raises an import error,

ImportError: No module named TaskBQ2DS.TaskBQ2DS

 (38c7b56641434bc6): Traceback (most recent call last):
   File ""/usr/local/lib/python2.7/dist-packages/dataflow_worker/batchworker.py"", line 582, in do_work
     work_executor.execute()
   File ""/usr/local/lib/python2.7/dist-packages/dataflow_worker/executor.py"", line 166, in execute
     op.start()
   File ""apache_beam/runners/worker/operations.py"", line 294, in apache_beam.runners.worker.operations.DoOperation.start (apache_beam/runners/worker/operations.c:10607)
     def start(self):
   File ""apache_beam/runners/worker/operations.py"", line 295, in apache_beam.runners.worker.operations.DoOperation.start (apache_beam/runners/worker/operations.c:10501)
     with self.scoped_start_state:
   File ""apache_beam/runners/worker/operations.py"", line 300, in apache_beam.runners.worker.operations.DoOperation.start (apache_beam/runners/worker/operations.c:9702)
     pickler.loads(self.spec.serialized_fn))
   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/internal/pickler.py"", line 225, in loads
     return dill.loads(s)
   File ""/usr/local/lib/python2.7/dist-packages/dill/dill.py"", line 277, in loads
     return load(file)
   File ""/usr/local/lib/python2.7/dist-packages/dill/dill.py"", line 266, in load
     obj = pik.load()
   File ""/usr/lib/python2.7/pickle.py"", line 858, in load
     dispatch[key](self)
   File ""/usr/lib/python2.7/pickle.py"", line 1090, in load_global
     klass = self.find_class(module, name)
   File ""/usr/local/lib/python2.7/dist-packages/dill/dill.py"", line 423, in find_class
     return StockUnpickler.find_class(self, module, name)
   File ""/usr/lib/python2.7/pickle.py"", line 1124, in find_class
     __import__(module)
 ImportError: No module named TaskBQ2DS.TaskBQ2DS


Basically, that file has the dataflow pipes defined in it. I know the error is caused since dataflow is not able to understand The environment around the dataflow job, which is the luigi task, and I think that can be made aware of it using either requirements or setup file. But It is still confusing.

When I use os module to execute that task using command line, it works properly, but when I import that module and run it, it throws that error

My question is

What is the right way of calling that dataflow job?

Should I execute it using the os module by calling it through a bash command ?

or

Should I call it by importing the module in the Luigi task and calling the function which will run the dataflow job.

If I should do it using the latter method, then how do i solve that error?
I know about the requirements file, the setup file that we can provide to dataflow job for our personalised virtual environment.
But I am confused about how to do it right way.

I hope was able to convey my question rightly and not make it an X-Y problem.

I can update the question with more information if needed.
",1,-1,-1.0
47383048,Luigi Central Scheduler significantly slower than --local-scheduler,"I have an extremely simple Luigi luigi.contrib.external_program.ExternalProgramTask. No dependencies on this task. The task simply gets a single input file, passes it to an exe and then a file is written out which completes the task. Each task runs for about 1-4 seconds.

--workers 8 - 100 tasks takes about 3m20s

--workers 1 --local-scheduler - 100 tasks takes about 1m:40s`

This is on my laptop and not ""distributed"" but I don't understand how the --local-scheduler is substantially faster when Luigi should be scheduling multiple tasks to be running in parallel.

When running with the centralized scheduler I see 8+ python processes pop up and running but the overall process is slower.
",-1,-1,-1.0
47808405,Tracking Long-Running Task Status in Luigi's Central Scheduler Web Interface,"In the Luigi framework, I am trying to show a progress bar of a long-running task in the central scheduler's web interface using set_tracking_url, set_progress_bar and set_status in the run() method, like this:

def run(self):
    self.set_tracking_url(""127.0.0.1:8082"")
    for i in range(100):
        self.do_long_calculation(i)
        self.set_status_message(""Analyzing Id %d"" % i)
        self.set_progress_percentage(i)


and I'm running the task using 

PYTHONPATH='.' luigi --module AnalysisTasks LongTask --workers=5


where AnalysisTasks is the python source file and LongTask is the task to which the run() method belongs and luigid running in the background. However I do not see any progress bar or status report. I haven't found any answers or examples to this anywhere. Is it at all possible?
",-1,-1,-1.0
48509083,How to make a Parameter available to all Luigi Tasks?,"In the Luigi docs, the use of a luigi.Config class is recommended for global configuration.

However, I am running into issues when using such a config class in order to pass a commandline argument to various Tasks in the pipeline.
Here's a lightweight example:  

import datetime
import luigi


class HelloWorldTask(luigi.Task):

    def run(self):
        print(""{task} says: Hello world on {date}!"".format(task=self.__class__.__name__,
                                                           date=GlobalParams.date.strftime('%d-%b-%Y')))


class GlobalParams(luigi.Config):
    date = luigi.DateParameter(default=datetime.date.today())


if __name__ == '__main__':
    luigi.run(['HelloWorldTask', '--workers', '1', '--local-scheduler',
               '--GlobalParams-date', '2018-01-01'])


The class GlobalParams defines a DateParameter which I would like to later reference in the run() blocks of pipeline Tasks.  However, this fails with the error,
 AttributeError: 'DateParameter' object has no attribute 'strftime'.  

In the debugger, I can see that a DateParameter object is passed to the HelloWorldTask Task, but any attempts to extract the expected '2018-01-01' value passed at runtime fails.  

Am I misunderstanding how to use these constructs?  How should I be passing a single parameter to (possibly many) Tasks?
",-1,-1,-1.0
48210322,MongoDB in Luigi,"I was trying to build a pipeline with luigi. First by getting data from an API, transform and then save it to a mongo db. I'm still new to luigi, my question is how do I implement the output() function which specifies outputs to a mongo db. And how would I create the require() function for subsequent tasks?

The first one, I was trying to attempt the demo here, but it's using MySql instead of mongodb. So I tried

from luigi.contrib.mongodb import MongoTarget
from pymongo import MongoClient

def output(self):
    # connect to db
    connection = MongoClient(self.host, self.port)
    db_client = connection[self.db_name]
    collection_name = 'myCollection'

    return MongoTarget(db_client, '_id', collection_name)


but it gave me error like this:

TypeError: Can't instantiate abstract class MongoTarget with abstract methods exists


A quick search of the error seems like due to pyMongo, but that solution still doesn't fix it.

For the require part, I'm not sure how to approach it either, I would like to check on if the records existed alraedy so I don't duplicate them. But there is no unique index from my API data, so I guess I have to somehow scan over all the records to make sure there are no duplicates.

There isn't a lot of documentation or examples on using mongo with luigi, any help is appreciated.
",1,-1,-1.0
49029913,How to write output to partitioned table with orc format with luigi?,"suppose we have such job:

class MRjob(JobTask):
  def output(self):
    return ...

  def requires(self):
    return ...

  def mapper(self, line):
    # some line process
    yield key, (...information, stored in hashable type...)

  def reducer(self,key,values):
    # some reduce logic... for example this
    unique = set(values)
    for elem in unique:
      yield key, elem[0], elem[1] 


What should I do inside the output method to insert data to existing table partition (also table is stored in orc format)? I'd like to skip process of converting data to orc, hence I tried to 

return HivePartitionTarget(self.insert_table, database=self.database_name, partition=partition)


but this didn't work. I also found that luigi tries to pass output to some file. With HivePartitionTarget luigi returns error like 'object has no attribute write', so my assumption is that HivePartitionTarget just doesn't contain write method. Thus I think I'm doing something wrong and should use another method but didn't managed to find a single example
",-1,-1,-1.0
50486039,Luigi Pipelining : No module named pwd in Windows,"I am trying to execute the tutorial given in https://marcobonzanini.com/2015/10/24/building-data-pipelines-with-python-and-luigi/.

I am able to run the program on its own using local scheduler, giving me:

Scheduled 2 tasks of which:
* 2 ran successfully:
    - 1 PrintNumbers(n=1000)
    - 1 SquaredNumbers(n=1000)

This progress looks :) because there were no failed tasks or missing external de
pendencies

===== Luigi Execution Summary =====


However, to try the visualization on the server, when I try to run luigid --background, it throws me an error saying I dont have pwd module.
I cannot find a pwd module using pip for windows. 

  File ""c:\users\alex\appdata\local\continuum\anaconda3\lib\site-packages
\luigi\process.py"", line 79, in daemonize
    import daemon
  File ""c:\users\alex\appdata\local\continuum\anaconda3\lib\site-packages
\daemon\__init__.py"", line 42, in &lt;module&gt;
    from .daemon import DaemonContext
  File ""c:\users\alex\appdata\local\continuum\anaconda3\lib\site-packages
\daemon\daemon.py"", line 25, in &lt;module&gt;
    import pwd
ModuleNotFoundError: No module named 'pwd'


I am working in Anaconda Spyder with Python 3.6
",-1,-1,-1.0
51081960,"How to write a pickle file to S3, as a result of a luigi Task?","I want to store a pickle file on S3, as a result of a luigi Task. Below is the class that defines the Task:

class CreateItemVocabulariesTask(luigi.Task):
    def __init__(self):
        self.client = S3Client(AwsConfig().aws_access_key_id,
                               AwsConfig().aws_secret_access_key)
        super().__init__()

    def requires(self):
        return [GetItem2VecDataTask()]

    def run(self):
        filename = 'item2vec_results.tsv'
        data = self.client.get('s3://{}/item2vec_results.tsv'.format(AwsConfig().item2vec_path),
                               filename)
        df = pd.read_csv(filename, sep='\t', encoding='latin1')
        unique_users = df['CustomerId'].unique()
        unique_items = df['ProductNumber'].unique()
        item_to_int, int_to_item = utils.create_lookup_tables(unique_items)
        user_to_int, int_to_user = utils.create_lookup_tables(unique_users)

        with self.output()[0].open('wb') as out_file:
            pickle.dump(item_to_int, out_file)
        with self.output()[1].open('wb') as out_file:
            pickle.dump(int_to_item, out_file)
        with self.output()[2].open('wb') as out_file:
            pickle.dump(user_to_int, out_file)
        with self.output()[3].open('wb') as out_file:
            pickle.dump(int_to_user, out_file)

    def output(self):
        files = [S3Target('s3://{}/item2int.pkl'.format(AwsConfig().item2vec_path), client=self.client),
                 S3Target('s3://{}/int2item.pkl'.format(AwsConfig().item2vec_path), client=self.client),
                 S3Target('s3://{}/user2int.pkl'.format(AwsConfig().item2vec_path), client=self.client),
                 S3Target('s3://{}/int2user.pkl'.format(AwsConfig().item2vec_path), client=self.client),]
        return files


When I run this task I get the error ValueError: Unsupported open mode 'wb'. The items I try to dump into a pickle file are just python dictionaries.

Full traceback:

Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\luigi\worker.py"", line 203, in run
    new_deps = self._run_get_new_deps()
  File ""C:\Anaconda3\lib\site-packages\luigi\worker.py"", line 140, in _run_get_new_deps
    task_gen = self.task.run()
  File ""C:\Users\user\Documents\python workspace\pipeline.py"", line 60, in run
    with self.output()[0].open('wb') as out_file:
  File ""C:\Anaconda3\lib\site-packages\luigi\contrib\s3.py"", line 714, in open
    raise ValueError(""Unsupported open mode '%s'"" % mode)
ValueError: Unsupported open mode 'wb'

",-1,-1,-1.0
51497390,How to run this Luigi Task- TaskClassNotFoundException(cls._missing_task_msg(name)),"I am trying to run 1 luigi task in command prompt named test.py and class named Test like this 

luigi -m test Test --local-scheduler 


How to pass parameter and run this in ubuntu

code sample: 

class Test: 

    def requires(self):
        return class1(param1='path', param2='', param3=123)

    def run(self):
        # some logic with using those 3 params

    def output(self):
        pass

class Class1(luigi.Task):

    param1 = luigi.Parameter()
    fileparam = file
    param2 = luigi.Parameter(default=fileparam)
    param3 = luigi.Parameter(default=123)


but I am getting this error

File ""/home/ubuntu/.local/bin/luigi"", line 11, in &lt;module&gt;
sys.exit(luigi_run())
File ""/home/ubuntu/.local/lib/python3.5/site-packages/luigi/cmdline.py"", line 11, in luigi_run
run_with_retcodes(argv)
File ""/home/ubuntu/.local/lib/python3.5/site-packages/luigi/retcodes.py"", line 69, in run_with_retcodes
with luigi.cmdline_parser.CmdlineParser.global_instance(argv):
File ""/usr/lib/python3.5/contextlib.py"", line 59, in __enter__
return next(self.gen)
File ""/home/ubuntu/.local/lib/python3.5/site-packages/luigi/cmdline_parser.py"", line 52, in global_instance
new_value = CmdlineParser(cmdline_args)
File ""/home/ubuntu/.local/lib/python3.5/site-packages/luigi/cmdline_parser.py"", line 76, in __init__
Register.get_task_cls(root_task)
File ""/home/ubuntu/.local/lib/python3.5/site-packages/luigi/task_register.py"", line 179, in get_task_cls
**raise TaskClassNotFoundException(cls._missing_task_msg(name))
luigi.task_register.TaskClassNotFoundException: No task Test.** Candidates are: Config,ExternalTask,RangeBase,RangeByMinutes,RangeByMinutesBase,RangeDaily,RangeDailyBase,RangeHourly,RangeHourlyBase,Task,TestNotificationsTask,WrapperTask,batch_email,core,email,execution_summary,retcode,scheduler,sendgrid,smtp,worker

",-1,-1,-1.0
51863621,Use a multiprocessing.Queue in luigi,"I have a set of tasks in luigi which all need to access a database. I can have up to 8 tasks accessing my database at the same time provided they are on different ports (I have the list of allowed ports).
How should I best implement this restriction which seems to be similar to the standard restriction of number of workers, ie for my case a task should run when a worker is free AND a database port is free.

I tried creating a multiprocessing.Queue() in __main__ and pass this to the WrapperTask, which receives it as a luigi.Parameter(), but this gives an error and hangs

UserWarning: Parameter ""queue"" with value &lt;multiprocessing.queues.Queue object at 0x00000000149E4518&gt;"" is not of type string.
warnings.warn('Parameter ""{}"" with value ""{}"" is not of type string.'.format(param_name, param_value))


The idea was that a .get() call would hang a Task if the queue is empty and continue once another task .put(port) again.

What is going wrong here? Or am I taking the completely wrong approach to managing the resource in luigi?
",-1,-1,-1.0
52694065,Atomically read from excel (for luigi workflow),"I'm trying to open an excel file in my Luigi workflow using pandas.read_excel() using the built in (atomic) luigi methods.

if self.input() is my luigi target of my excel document, I want to do something like:

with self.input().open('r') as f:
   pandas.read_excel(f)


or more generally:

with open(filename) as f:
   pandas.read_excel(f)

However, this gives me an error:
*** UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd0 in position 10: invalid continuation byte

Disclaimer:

The excel file is from an external task, so I do not have control over what type of computer it is made on or whether or not it contains NAs or blank cells.
",-1,-1,-1.0
53009437,How to properly parallelize similar tasks in Luigi,"I spent a good five or six hours the other day trying to parallelize some work in Luigi, based on the method used here: http://rjbaxley.com/posts/2016/03/13/parallel_jobs_in_luigi.html

The problem I was having was that I kept getting a luigi.task_register.TaskClassAmbigiousException which drove me crazy.  Ultimately I threw luigi.auto_namespace(scope=name) at the top of my package and everything started working, but I don't know why.  Roughly described, I had 3 tasks:

TaskA - required nothing
        provided a txt file with paths

TaskB - requires only input parameters p1 and p2
        provides a .csv file

TaskC - requires output from task A
        yields one TaskB for each path pair from output A
        is completed when all yielded TaskBs are completed.

If anyone can sketch how i should have done this correctly, instead of the hacked together nonsense I have now, I'd be so very grateful
",-1,-1,-1.0
53594580,How to override luigi complete() method so that the underlying task will come to know the previous ones were completed?,"I have the following tasks :

class TaskA(ExternalProgramTask):
    def program_args(self):
        return [""/simpleScripts/shell5.bash""]
    def program_environment(self):
        env = os.environ.copy()
        return env


The shell script is running properly. After this if I try to run any other task , those task will fail by throwing ""unfulfilled dependency error"" in TaskA. 

The reason I understood is usually Luigi will have output() method where a localTarget is created and thus Luigi will come to know that that Task is completed. 

Here while running shell script I don't have any localTarget and thus 
luigi fails to understand whether it is completed or not. How to make this correct ?
",-1,-1,-1.0
54701697,How to check output dynamically with Luigi,"I realize I likely need to use dynamic requirements to accomplish the following task, however I have not been able to wrap my head around what this would look like in practice.

The goal is to use Luigi to generate data and add it to a database, without knowing ahead of time what data will be generated.

Take the following example using mongodb:

import luigi
from uuid import uuid4
from luigi.contrib import mongodb
import pymongo

# Make up IDs, though in practice the IDs may be generated from an API
class MakeID(luigi.Task):
    def run(self):
        with self.output().open('w') as f:
            f.write(','.join([str(uuid4()) for e in range(10)]))

    # Write the data to file
    def output(self):
        return luigi.LocalTarget('data.csv')


class ToDataBase(luigi.Task):
    def requires(self):
        return MakeID()

    def run(self):
        with self.input().open('r') as f:
            ids = f.read().split(',')

        # Add some fake data to simulate generating new data 
        count_data = {key: value for value, key in enumerate(ids)}
        # Add data to the database
        self.output().write(count_data)

    def output(self):
        # Attempt to read non-existent file to get the IDs to check if task is complete
        with self.input().open('r') as f:
            valid_ids = f.read().split(',')
        client = pymongo.MongoClient('localhost',
                                     27017,
                                     ssl=False)

        return mongodb.MongoRangeTarget(client,
                                        'myDB',
                                        'myData',
                                        valid_ids,
                                        'myField')


if __name__ == '__main__':
    luigi.run()


The goal is to obtain data, modify it and then add it to a database. 

The above code fails when run because the output method of ToDataBase runs before the require method so the while the function has access to the input, the input does not yet exist. Regardless I still need to check to be sure that the data was added to the database. 

This github issue is close to what I am looking for, though as I mentioned I have not been able to figure out dynamic requirements for this use case in practice.
",-1,-1,-1.0
55123662,Can Luigi run remote Hadoop jobs?,"If one of the tasks in the Luigi graph need to run on a remote Hadoop cluster, is that possible? The machine on which Luigi runs is different from the Hadoop cluster. Can luigi still check the if the HDFS file in the remote cluster exists?

I tried to find documentation for this but wasn't able to.
",-1,-1,-1.0
55186623,Luigi Tasks going into infinite Loops,"I have a simple luigi Task that on run will yield itself with a different parameter as shown below.

import luigi

class ComputeJob(luigi.Task):

   id_parameter = luigi.parameter.IntParameter()

    #run defination
    def run(self):


        print (""\nrunning task {}"".format(self.id_parameter))
        #Do some work here

        if self.id_parameter &lt; 10: 
            next_val = self.id_parameter + 1
            yield ComputeJob(id_parameter = next_val)


I am expecting it to run for 10 times and then exit the run but after executing the 10th iteration, it starts re-executing the 9 steps from the very start. Due to this the tasks keep looping in step 9 and 10.

so the expected out put should be :

running task 1
running task 2
running task 3
running task 4
running task 5
running task 6
running task 7
running task 8
running task 9
running task 10


but the output that i get is :

running task 1
running task 2
running task 3
running task 4
running task 5
running task 6
running task 7
running task 8
running task 9
running task 10
running task 9
running task 10
running task 9
running task 10
...
...
...


What am I missing here?

Thanks, Oyshik        
",1,-1,-1.0
55308758,How to view luigi error log if task get killed,"I'm trying to run Luigi task which will read few million records from a table and write to a csv. Unfortunately, the job got killed, but I couldn't find any traceback error or any clue to fix this. 


  INFO: Informed scheduler that task   ExportData__99914b932b   has
  status   PENDING INFO: Done scheduling tasks INFO: Running Worker with
  1 processes DEBUG: Asking scheduler for work... DEBUG: Pending tasks:
  1 INFO: [pid 26002] Worker Worker(salt=557665440, workers=1,
  host=unknown, username=root, pid=26002) running   ExportData() Killed

",-1,-1,-1.0
57184032,Using Luigi package to create and run tasks,"I am trying to follow the Luigi-Jupyter tutorial
link: Luigi Tutorial

I have managed to set up my environment but struggling with adding the first task to my pipeline.

I used nano file.py to create a python file and added the following code:

import os
import luigi

output_path = '/Users/mattiaciollaro/Git/luigi_tutorial/output/'

class TellMeMyName(luigi.Task):
    """"""
    An incredibly simple task that writes your name to a text file.
    """"""
    my_name = luigi.Parameter()

    def output(self):
        return luigi.LocalTarget(
            os.path.join(output_path, 'my_name.txt')
        )

    def run(self):
        with open(self.output().path, 'w') as out:
            out.write('Your name is %s' % self.my_name)


I then try and run this on my terminal using:
    python file.py

However I cannot seem to be able to run the task (I'm assuming maybe I am not supposed to write the code in a file?)

This is what I run on terminal:

luigi --module tasks file.py TellMeMyName --my-name Emmanuel


and I get this error message

ModuleNotFoundError: No module named 'tasks'

",-1,-1,-1.0
57242621,Reading url into luigi parameter,"I am trying to read a csv file in my local drive using the Luigi package, specifically luigi.Parameter() as fileName and then read that into a pandas dataframe using pd.read_csv and carry out some data wrangling. 

This is the code I have written for this task:

import luigi
import pandas as pd
class read_blog(luigi.Task):
        fileName = luigi.Parameter()
        def run(self):
                full_file = pd.read_csv(fileName)
                read_blog = full_file[full_file['properties__url'].string.contain$
                        regex=False)]
                blog_readers = read_blog[['anonymous_id','channel',
                        'context__campaign__content','context__campaign__medium',
                        'context__campaign__name','context__campaign_source',
                        'context__campaign__term','timestamp','user_id',
                        'context__page__url','properties__url',
                        'properties__search','context__page__title',
                        'properties__path','context__user_agent',
                        'properties__referrer','rank']]
                blog_readers.to_csv('blog_readers.csv')
if __name__ == '__main__':
        luigi.run()


and then run this on terminal using this:

python cleanup.py read_blog --local-scheduler --fileName '/Users/emmanuels/Desktop/attribute.csv'


This should according to my understanding run the read_blog class in cleanup.py and give the fileName variable a parameter that is a link to my csv file.

My code should then read the csv as pandas dataframe, however this is not happening and this is the full error message I am receiving:

===== Luigi Execution Summary =====

/Users/emmanuels/anaconda3/lib/python3.7/site-packages/luigi/configuration.py:54:UserWarning: LUIGI_CONFIG_PATH points to a file which does not exist. Invalid file: /Users/emmanuels/luigi_tutorial/luigi/luigi.conf
  warnings.warn(""LUIGI_CONFIG_PATH points to a file which does not exist. Invalidfile: {path}"".format(path=config_file))
DEBUG: Checking if read_blog(fileName=/Users/emmanuels/Desktop/attributiondata.csv) is complete
/Users/emmanuels/anaconda3/lib/python3.7/site-packages/luigi/worker.py:328: UserWarning: Task read_blog(fileName=/Users/emmanuels/Desktop/attributiondata.csv) without outputs has no custom complete() method
  is_complete = task.complete()
INFO: Informed scheduler that task   read_blog__Users_emmanuels_23aa7e1a57   has status   PENDING
INFO: Done scheduling tasks
INFO: Running Worker with 1 processes
DEBUG: Asking scheduler for work...
DEBUG: Pending tasks: 1
INFO: [pid 94938] Worker Worker(salt=156803262, workers=1, host=Emmanuels-MacBook-Pro.local, username=emmanuels, pid=94938) running   read_blog(fileName=/Users/emmanuels/Desktop/attributiondata.csv)
ERROR: [pid 94938] Worker Worker(salt=156803262, workers=1, host=Emmanuels-MacBook-Pro.local, username=emmanuels, pid=94938) failed    read_blog(fileName=/Users/emmanuels/Desktop/attributiondata.csv)
Traceback (most recent call last):
  File ""/Users/emmanuels/anaconda3/lib/python3.7/site-packages/luigi/worker.py"", line 191, in run
    new_deps = self._run_get_new_deps()
  File ""/Users/emmanuels/anaconda3/lib/python3.7/site-packages/luigi/worker.py"", line 129, in _run_get_new_deps
    task_gen = self.task.run()
  File ""cleanup.py"", line 8, in run
    full_file = pd.read_csv(fileName)
NameError: name 'fileName' is not defined
DEBUG: 1 running tasks, waiting for next task to finish
INFO: Informed scheduler that task   read_blog__Users_emmanuels_23aa7e1a57   has status   FAILED
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: There are 1 pending tasks possibly being run by other workers
DEBUG: There are 1 pending tasks unique to this worker
DEBUG: There are 1 pending tasks last scheduled by this worker
INFO: Worker Worker(salt=156803262, workers=1, host=Emmanuels-MacBook-Pro.local, username=emmanuels, pid=94938) was stopped. Shutting down Keep-Alive thread
INFO:
===== Luigi Execution Summary =====

Scheduled 1 tasks of which:
* 1 failed:
    - 1 read_blog(fileName=/Users/emmanuels/Desktop/attributiondata.csv)

This progress looks :( because there were failed tasks

",-1,-1,-1.0
57421197,How to assign luigi parameter using list in task wrapper,"I am using luigi to extract different user actions and save each as a csv simultaneously.

The idea to look at my source data, find unique actions and create csv's using the names of each of those actions.

class data_filter(luigi.Task):
        task = luigi.Parameter()
        def run(self):
                data_filter = full_file[full_file['properties_url'].str.contains(task)]
                data_filter.to_csv('/Users/Documents/Data/'+str(task)+'.csv')
        def requires(self):
                return []
        def output(self):
                return luigi.LocalTarget('/Users/Documents/Data/'+str(task)+'.csv')
#chaining tasks with wrapper
class wrapper(luigi.WrapperTask):
        def requires(self):
                file = pd.read_csv('/Users/Desktop/attr.csv')
                actions = file.utm_source.unique()
                task_list = []
                for current_task in actions:
                        task_list.append(data_filter(task=current_task))
                return task_list
        def run(self):
                print ('Wrapper has ended')
                pd.DataFrame().to_csv('/Users/Documents/Data/wrangle.csv')
        def output(self):
                return luigi.LocalTarget('/Users/Documents/Data/dwrangle.csv') 
if __name__ == '__main__':
    luigi.run(wrapper())


The wrapper should tie everything up by, looking at all unique actions, assigning them to task_list and running task_list...while assigning the current task I am iterating through to task = luigi.Paramter in my data_filter class.

However this returns the error message:

  return luigi.LocalTarget('/Users/emmanuels/Documents/GitHub/Springboard-DSC/Springboard-DSC/Capstone 1 - Attribution Model/Data/'+str(task)+'.csv')
NameError: name 'task' is not defined


and

===== Luigi Execution Summary =====

Scheduled 1 tasks of which:
* 1 failed scheduling:
    - 1 wrapper()

Did not run any tasks
This progress looks :( because there were tasks whose scheduling failed


I just want to figure out what I am doing wrong
",-1,-1,-1.0
57685815,"How to fix ""luigi.worker.TaskException: Can not schedule non-task <class '__main__.Task'>"" when trying to execute luigi tasks?","I am new to Luigi and I have created a pipeline where it gets data from a database, transforms data and then loads it back to the database. I created four tasks in it. However, when I am executing the task on cmd or Pycharm, it says that it cannot schedule a non-task. Below is the pseudocode of my pipeline. 
The parameters to each task are not inputs rather being taken from other files. 

    class Task1(luigi.Task): 
          # Some Parameters
         def get_target(): 
         def query():
         def run(): 
    class Task2(luigi.Task):
          # Some Parameters 
         def requires():
           return Task1()
         def func1():
         def func2():
         def run()
    class Task3(luigi.Task): 
         # Some Parameters 
         def requires():
             return Task2()
         def run():
    class Task4(luigi.Task):
         # Some Parameters 
          def requires(): 
              return Task3()
          def run(): 


On Pycharm, I used 

if __name__ == '__main__':
    luigi.build([Task1, Task2, Task3, Task4], workers=5, local_scheduler=True)


and on cmd, I used 

 python .\folder\file.py Task1



but it gave me this error 

INFO: Worker Worker was stopped. Shutting down Keep-Alive thread
Traceback (most recent call last):
  File ""D:/folder/file.py"", line 300, in &lt;module&gt;
    luigi.build([Task1, Task2, Task3, Task4], workers=5, local_scheduler=True)
  File ""C:\Users\Anaconda3\lib\site-packages\luigi\interface.py"", line 237, in build
    luigi_run_result = _schedule_and_run(tasks, worker_scheduler_factory, override_defaults=env_params)
  File ""C:\Users\Anaconda3\lib\site-packages\luigi\interface.py"", line 171, in _schedule_and_run
    success &amp;= worker.add(t, env_params.parallel_scheduling, env_params.parallel_scheduling_processes)
  File ""C:\Users\Anaconda3\lib\site-packages\luigi\worker.py"", line 740, in add
    self._validate_task(task)
  File ""C:\Users\Anaconda3\lib\site-packages\luigi\worker.py"", line 638, in _validate_task
    raise TaskException('Can not schedule non-task %s' % task)
luigi.worker.TaskException: Can not schedule non-task &lt;class '__main__.Task1'&gt;

",-1,-1,-1.0
57705072,Run luigi tasks in parallel,"I have an application with several luigi tasks (I did not write that app). Now I want to introduce another task, in the middle of a process, which will monitor some AWS instances. This task, once started, should run until the end and it must run in parallel with other tasks. You can see picture in the link for better understanding.
Link to the schema
I looked in the documentation but I could not find solution. I am new with luigi and I probably missed something.
",1,-1,-1.0
57946108,How can I run luigid and luigi task within docker?,"I am trying to dockerize my Luigi tasks and want to run Luigid service and tasks inside a docker however I am unable to start luigid within a docker due to which my Luigi tasks do not run since it keeps trying to connect to the central scheduler.   

I have created a docker file, Luigi config file and Luigid.sh file. 

The docker file is as follows:

FROM python:3
COPY . /
WORKDIR /
COPY luigi.cfg luigi.cfg
RUN pip install -r requirements.txt
#RUN chmod 644 ./luigi_app.py
ENTRYPOINT [""python""]
RUN mkdir /usr/local/luigi
EXPOSE 8082
CMD [""luigid""]
CMD [""./luigi_app.py""]


EDIT: 
Here is another version of the docker file. 

ARG PYTHON_VERSION=3.7
FROM python:$PYTHON_VERSION-alpine3.10

ARG LUIGI_VERSION=2.8.9
ENV LUIGI_VERSION ${LUIGI_VERSION}
ENV PYTHON_VERSION ${PYTHON_VERSION}
ENV LUIGI_CONFIG_DIR /etc/luigi/
ENV LUIGI_CONFIG_PATH /etc/luigi/luigi.conf
ENV LUIGI_STATE_DIR /luigi/state

COPY . /pipeline
WORKDIR /pipeline

RUN apk add --no-cache --virtual .build-deps \
      build-base \
      gcc \
      musl-dev \
      libc-dev \
      libffi-dev \
      python3-dev \
      py-mysqldb \
     &amp;&amp; \
      python3 -m pip install \
      luigi==${LUIGI_VERSION} \
      sqlalchemy \
      mysql-connector-python \
      pandas \
      numpy \
      request \
      &amp;&amp; \
    apk --purge del .build-deps &amp;&amp; \
    mkdir -p ${LUIGI_CONFIG_DIR} &amp;&amp; \
    mkdir -p ${LUIGI_STATE_DIR}

COPY logging.cfg ${LUIGI_CONFIG_DIR}
COPY luigi.cfg ${LUIGI_CONFIG_DIR}
VOLUME [""${LUIGI_CONFIG_DIR}"", ""${LUIGI_STATE_DIR}""]

EXPOSE 8082/TCP

COPY luigid.sh /bin/run
RUN chmod +x /bin/run
#ENTRYPOINT [""/bin/run""]
CMD [""./luigi_app.py""]


The luigi.cfg file is 

# 1 to 1 copy - this should be a base image
[core]
default-scheduler-host = luigid-service
default-scheduler-port = 8082
default-scheduler-url  = http://luigid-service:8082/
rpc-connect-timeout    = 5
rpc-retry-attempts     = 3
rpc-retry-wait         = 30

[scheduler]
record_task_history = True
state_path = /usr/local/luigi/luigi-state.pickle
# number of seconds (aka 6 hours) tasks are kept in the UI
remove_delay = 2160

[task_history]
db_connection = sqlite://///usr/local/luigi/luigi-task-hist.db

[retcode]
# The following return codes are the recommended exit codes for Luigi
# They are in increasing level of severity (for most applications)
already_running=0
missing_data=0
not_run=0
task_failed=30
scheduling_error=35
unhandled_exception=40

[execution_summary]
# display all task after a pipeline is run and print their status
summary-length=0


EDIT Here is another version of the luigi.cfg

core]
logging_conf_file: /etc/luigi/logging.cfg

[scheduler]
record_task_history: True
state-path: /luigi/state/luigi-state.pickle

[task_history]
db_connection: sqlite:////luigi/state/luigi-task-history.db


The luigid.sh file is 

#!/bin/sh
cat &lt;&lt; ""EOF""
 _____       __    __    _____      _____     _____
(_   _)      ) )  ( (   (_   _)    / ___ \   (_   _)
  | |       ( (    ) )    | |     / /   \_)    | |
  | |        ) )  ( (     | |    ( (  ____     | |
  | |   __  ( (    ) )    | |    ( ( (__  )    | |
__| |___) )  ) \__/ (    _| |__   \ \__/ /    _| |__
\________/   \______/   /_____(    \____/    /_____(
EOF
echo ""Luigi: $LUIGI_VERSION - Python: $(python --version)""

exec luigid


When I build up my docker image and run it, it gives error that it is unable to connect to the central scheduler which is basically luigid. Can someone please help me correct my files or method?
",-1,-1,-1.0
58158971,Dependency error messages when running luigi pipeline,"Im trying to create a pipeline that starts off with a class that separates one file into multiple csvs based on the state a user is in, then looks at the files created representing the different states and tries to determine whether a user moved from one state to another returning a 1 if the user did and 0 if s/he did not, fits these 'probabilities' using a gaussian kde, saves this as a pickle and then gets samples from the pickle and saves them as csvs.

I am using luigi to build this pipeline but keep encountering error messages when trying to run my code. The pipeline seems to fail when running the state_to_state class.

Here is the code I have written:

separate_csv.py:

import luigi
import pandas as pd
import numpy as np
import os
import state_to_state_transitions2 as sst
class data_filter(luigi.Task):
    file = pd.read_csv('/Users/emmanuels/Desktop/Attribution/finalcleanattributiondata.csv')
    actions = file.state.unique()
    def run(self):
        for current in self.actions:
            filter_file = self.file.loc[self.file.state.str.contains(current,na=False)]
            filter_file.to_csv('/Users/emmanuels/Documents/AttributionData/Data/'+str(current)+'.csv')
    def requires(self):
        return []
    def output(self):
        return luigi.LocalTarget('/Users/emmanuels/Documents/AttributionData/Data/'+str(self.actions)+'.csv')


state_to_state_transitions2;

import luigi
import pandas as pd
import separate_csv
class state_to_state(luigi.Task):
    first_file = luigi.Parameter()
    second_file = luigi.Parameter()
    def run(self):
        #iterate through states and find probability of anonymous id existing in next state
        first = pd.read_csv(self.first_file)
        second = pd.read_csv(self.second_file)
        first['probability'] = first.anonymous_id.isin(second.anonymous_id).astype(int)
        #save anonymous id along with probability (1,0) of whether or not it exists in the next state
        first[['anonymous_id','probability']].to_csv('/Users/emmanuels/Documents/AttributionData/Data/Probabilities/'+str(self.first_file.split('/')[6][:-4]+'to'+self.second_file.split('/')[7][:-4]+'.csv'))
    def requires(self):
        return separate_csv.data_filter()
    def output(self):
        return luigi.LocalTarget('/Users/emmanuels/Documents/AttributionData/Data/Probabilities/'+str(self.first_file.split('/')[6][:-4]+'to'+self.second_file.split('/')[6][:-4]+'.csv'))


gaussian_kdefit;

import pandas as pd
import pickle
from scipy import stats
import luigi
import state_to_state_transitions2 as sst
class save_distributions(luigi.Task):
    file_tag = luigi.Parameter()
    path = '/Users/emmanuels/Documents/AttributionData/Data/Probabilities/'
    def run(self):
        data = pd.read_csv(path+self.file_tag)
        kernel = stats.gaussian_kde(data['probability'])
        #we fit the distribution and save as a pickle
        pickle.dump(kernel,open('/Users/emmanuels/Documents/AttributionData/Data/Probabilities/'+str(self.file_tag)+'probabs'+'.pck','wb'))
    def requires(self):
        files = ['Session.csv','lead.csv','opportunity.csv','complete.csv']
        task_list = []
        for i in range(1,len(files)):
            one = self.path+str(files[i-1])
            two = self.path+str(files[i])
            task_list.append(sst.state_to_state(first_file=one,second_file=two))
        return task_list
    def output(self):
        return luigi.LocalTarget('/Users/emmanuels/Documents/AttributionData/Data/Probabilities/'+str(self.file_tag)+'probabs'+'.pck')


get_samples:

import pandas as pd
import luigi
import gaussian_kdefit as gkde
#takes n samples and saves sample in csv
class sample_output(luigi.Task):
    file_tag = luigi.Parameter()
    size = luigi.Parameter()
    def run(self):
        kernel = pd.read_pickle('/Users/emmanuels/Documents/AttributionData/Data/Probabilities/'+str(self.file_tag)+'probabs'+'.pck')
        kernel = kernel.resample(int(self.size))
        pd.DataFrame(kernel).transpose().to_csv('/Users/emmanuels/Documents/AttributionData/Data/Probabilities/'+str(self.file_tag)+'+sampleprobabs'+'.csv')
    def requires(self):
        files = ['Sessiontolead.csv', 'leadtoopportunity.csv', 'opportunitytocomplete.csv']
        return [gkde.save_distributions(file_tag=file) for file in files]
    def output(self):
        return luigi.LocalTarget('/Users/emmanuels/Documents/AttributionData/Data/Probabilities/'+str(self.file_tag)+'+sampleprobabs'+'.csv')


and my wrapper class:

import get_samples as getsamps
import pandas as pd
import luigi
class wrapper(luigi.WrapperTask):
    def requires(self):
        file_tag = ['Sessiontolead', 'leadtoopportunity', 'opportunitytocomplete']
        task_list = []
        size = 10
        for i in range(0,len(file_tag)):
            for k in range(1,size):
                task_list.append(getsamps.sample_output(file_tag=file_tag[i],size=size))
        return task_list
    def run(self):
        print('Wrapper ran')
        pd.DataFrame().to_csv('/Users/emmanuels/Documents/AttributionData/Data/wrangler1.csv')
    def output(self):
        return luigi.LocalTarget('/Users/emmanuels/Documents/AttributionData/Data/wrangler1.csv')
if __name__ == '__main__':
    luigi.build([wrapper()],workers=8,local_scheduler=True)


Here is a sample of the final clean attribution file:

{'Unnamed: 0': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4},
 'uniques': {0: '2019-06-18 09:00:000000a6a0-00bc-475f-a9e5-9dcbb4309e78https://signup.yoc.com/signup/v1/https://signup.yoc.com/signup/v1/step/businessDetails/',
  1: '2019-06-18 09:00:000000a6a0-00bc-475f-a9e5-9dcbb4309e78https://signup.yoc.com/signup/v1/https://signup.yoc.com/signup/v1/step/businessDetails/',
  2: '2019-06-18 09:00:000000a6a0-00bc-475f-a9e5-9dcbb4309e78https://signup.yoc.com/signup/v1/https://signup.yoc.com/signup/v1/step/userDetails/',
  3: '2018-05-17 20:00:000000c924-5959-4e2d-8757-0d10f96ca462http://m.facebook.com/https://www.yoc.com/signup/',
  4: '2019-02-24 16:00:000002269a-1e39-4cdf-a43e-cecf0a277c1chttps://signup.yoc.com/continue/1551024250465-dfd0e1d5-b76a-4bfa-bc29-9fcf5ef6b91c'},
 'anonymous_id': {0: '0000a6a0-00bc-475f-a9e5-9dcbb4309e78',
  1: '0000a6a0-00bc-475f-a9e5-9dcbb4309e78',
  2: '0000a6a0-00bc-475f-a9e5-9dcbb4309e78',
  3: '0000c924-5959-4e2d-8757-0d10f96ca462',
  4: '0002269a-1e39-4cdf-a43e-cecf0a277c1c'},
 'user_id': {0: '1560849071242-a19cdf50-ceec-41a0-ab51-ba9a45c8cda9',
  1: '1560849071242-a19cdf50-ceec-41a0-ab51-ba9a45c8cda9',
  2: '1560849071242-a19cdf50-ceec-41a0-ab51-ba9a45c8cda9',
  3: nan,
  4: nan},
 'ts': {0: '2019-06-18 09:11:14.409000',
  1: '2019-06-18 09:11:15.028000',
  2: '2019-06-18 09:12:03.118000',
  3: '2018-05-17 20:31:32.203000',
  4: '2019-02-24 16:08:32.661000'},
 'url': {0: 'https://signup.yoc.com/signup/v1/step/businessDetails/',
  1: 'https://signup.yoc.com/signup/v1/step/businessDetails/',
  2: 'https://signup.yoc.com/signup/v1/step/userDetails/',
  3: 'https://www.yoc.com/signup/',
  4: 'https://signup.yoc.com/continue/1551024250465-dfd0e1d5-b76a-4bfa-bc29-9fcf5ef6b91c'},
 'path': {0: '/za/signup/v1/step/businessDetails/',
  1: '/za/signup/v1/step/businessDetails/',
  2: '/za/signup/v1/step/userDetails/',
  3: '/za/signup/',
  4: '/continue/1551024250465-dfd0e1d5-b76a-4bfa-bc29-9fcf5ef6b91c'},
 'referrer_domain': {0: 'signup.yoc.com',
  1: 'signup.yoc.com',
  2: 'signup.yoc.com',
  3: 'm.facebook.com',
  4: nan},
 'utm_campaign': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
 'utm_content': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
 'utm_medium': {0: nan, 1: nan, 2: nan, 3: nan, 4: nan},
 'utm_source': {0: nan, 1: nan, 2: nan, 3: 'facebook', 4: nan},
 'user_agent': {0: 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
  1: 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
  2: 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
  3: 'Mozilla/5.0 (Linux; Android 8.0.0; SM-G965F Build/R16NW; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/66.0.3359.158 Mobile Safari/537.36 [FB_IAB/FB4A;FBAV/172.0.0.66.93;]',
  4: 'Opera/9.80 (Android; Opera Mini/38.1.2254/131.123; U; en) Presto/2.12.423 Version/12.16'},
 'rank': {0: 1, 1: 2, 2: 3, 3: 1, 4: 1},
 'state': {0: 'lead',
  1: 'lead',
  2: 'opportunity',
  3: 'Session',
  4: 'opportunity'}}


Here is the traceback I am getting:

Scheduled 9 tasks of which:
* 1 ran successfully:
    - 1 data_filter(file=/Users/emmanuels/Desktop/Attribution/finalcleanattributiondata.csv)
* 1 failed:
    - 1 state_to_state(first_file=/Users/emmanuels/Documents/AttributionData/Data/Session.csv, second_file=/Users/emmanuels/Documents/AttributionData/Data/lead.csv)
* 7 were left pending, among these:
    * 7 had failed dependencies:
        - 3 sample_output(file_tag=Sessiontolead, size=10) ...
        - 3 save_distributions(file_tag=Sessiontolead.csv,leadtoopportunity.csv,opportunitytocomplete.csv)
        - 1 wrapper()


....

INFO: [pid 45306] Worker Worker(salt=271561701, workers=1, host=Emmanuels-MacBook-Pro.local, username=emmanuels, pid=45306) running   state_to_state(first_file=/Users/emmanuels/Documents/AttributionData/Data/Session.csv, second_file=/Users/emmanuels/Documents/AttributionData/Data/lead.csv)
WARNING: Using wildcards in path /Users/emmanuels/Documents/AttributionData/Data/['lead' 'opportunity' 'Session' 'complete'].csv might lead to processing of an incomplete dataset; override exists() to suppress the warning.
ERROR: [pid 45306] Worker Worker(salt=271561701, workers=1, host=Emmanuels-MacBook-Pro.local, username=emmanuels, pid=45306) failed    state_to_state(first_file=/Users/emmanuels/Documents/AttributionData/Data/Session.csv, second_file=/Users/emmanuels/Documents/AttributionData/Data/lead.csv)
Traceback (most recent call last):
  File ""/Users/emmanuels/anaconda3/lib/python3.7/site-packages/luigi/worker.py"", line 175, in run
    raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))
RuntimeError: Unfulfilled dependency at run time: data_filter__Users_emmanuels_c87d333278
DEBUG: 1 running tasks, waiting for next task to finish
INFO: Informed scheduler that task   state_to_state__Users_emmanuels__Users_emmanuels_c95a12621e   has status   FAILED

",-1,-1,-1.0
58175478,Luigi tasks and wrapper failing,"Have asked a similar question, but decided to breakdown my pipeline into fewer steps to get a deeper understanding of where I am going wrong and to make debugging as easy as possible.

With my first class I am taking a huge csv and separating it to multiple csvs based on the user's current state. I then created another task that then looks at whether or not a given user moved from one state to the next returning 1's and 0's depending on whether or not this happened.

I then have a wrapper class that should dynamically assign parameter values to the previous class. However, my pipeline does not seem to be running and I'm not sure what I am doing wrong

Here is what I have:

separate_csv.py:

import luigi
import pandas as pd
class data_filter(luigi.Task):
    file = luigi.Parameter()
    def run(self):
        for current in actions:
            file_pd = pd.read_csv(self.file)
            actions = file_pd.state.unique()
            filter_file = file_pd.loc[file_pd.state.str.contains(current,na=False)]
            filter_file.to_csv('/Users/emm/Documents/AttributionData/Data/'+str(current)+'.csv')
    def requires(self):
        return []
    def output(self):
        return luigi.LocalTarget('/Users/emm/Documents/AttributionData/Data/complete.csv')


state_to_state_transitions.py:

import luigi
import pandas as pd
import separate_csv as sep
class state_to_state(luigi.Task):
    first_file = luigi.Parameter()
    second_file = luigi.Parameter()
    def run(self):
        #iterate through states and find probability of anonymous id existing in next state
        path = '/Users/emm/Documents/AttributionData/Data/Probabilities/'
        first = pd.read_csv(path+self.first_file)
        second = pd.read_csv(path+self.second_file)
        first['probability'] = first.anonymous_id.isin(second.anonymous_id).astype(int)
        #save anonymous id along with probability (1,0) of whether or not it exists in the next state
        with self.output().open('w') as out_csv:
            out_csv.write(first[['anonymous_id','probability']].to_csv('/Users/emm/Documents/AttributionData/Data/Probabilities/'+str(self.first_file[:-4]+'to'+self.second_file)))
    def requires(self):
        files_two = [sep.data_filter(file='/Users/emm/Desktop/Attribution/finalcleanattributiondata.csv')]
        return files_two
    def output(self):
        return luigi.LocalTarget('/Users/emm/Documents/AttributionData/Data/Probabilities/'+str(self.first_file[:-4]+'to'+self.second_file))


wrapper.py

import state_to_state_transitions2 as sst
import pandas as pd
import luigi
class wrapper(luigi.WrapperTask):
    def requires(self):
        files = ['Session.csv', 'lead.csv', 'opportunity.csv', 'complete.csv']
        task_list = []
        for i in range(1, len(files)):
            task_list.append(sst.state_to_state(first_file=files[i-1],second_file=files[i]))
        return task_list
    def run(self):
        print('Wrapper ran')
        pd.DataFrame().to_csv('/Users/emmanuels/Documents/AttributionData/Data/wrangler1.csv')
    def output(self):
        return luigi.LocalTarget('/Users/emmanuels/Documents/AttributionData/Data/wrangler1.csv')
if __name__ == '__main__':
    luigi.build([wrapper()],workers=8,local_scheduler=True)


Here are parts of my error message:

      File ""/Users/emm/Documents/GitHub/AttributionModel/Capstone/state_to_state_transitions2.py"", line 15, in run
    out_csv.write(first[['anonymous_id','probability']].to_csv('/Users/emm/Documents/AttributionData/Data/Probabilities/'+str(self.first_file[:-4]+'to'+self.second_file)))
TypeError: write() argument must be str, not None
DEBUG: 1 running tasks, waiting for next task to finish
INFO: Informed scheduler that task   state_to_state_lead_csv_opportunity_csv_b31ac9d110   has status   FAILED
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: There are 4 pending tasks possibly being run by other workers
DEBUG: There are 4 pending tasks unique to this worker
DEBUG: There are 4 pending tasks last scheduled by this worker
INFO: Worker Worker(salt=152474850, workers=1, host=Emms-MacBook-Pro.local, username=***, pid=***) was stopped. Shutting down Keep-Alive thread
INFO: 
===== Luigi Execution Summary =====

Scheduled 5 tasks of which:
* 1 present dependencies were encountered:
    - 1 data_filter(file=/Users/emm/Desktop/Attribution/finalcleanattributiondata.csv)
* 3 failed:
    - 3 state_to_state(first_file=Session.csv, second_file=lead.csv) ...
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 wrapper()

This progress looks :( because there were failed tasks

",-1,-1,-1.0
58223646,Problems producing desired luigi output,"I am trying to create a pipeline that takes in 3 files, takes n amount of rows from each file (represented by obs_num)  compares each of the values in the files to a random float between 0 and 1 and either returns the obs_num if it is greater than the random number or false if not. I then append these values to a list (list 1)

I then look at the next second file, check the position of the obs_num, if the num in the same position returned false in the previous file return false, or else check if the num is greater than the random float again, I  then do the same for the third file. I also append these values to lists (list 2 and 3)

I then convert these 3 lists to a dataframe with each list representing a column.

The problem however is that when I run my pipeline the output is a file with one blank column as opposed to a csv with rows equivalent to obs_num.

Here is the code I am using for my wrapper:

import pandas as pd
import luigi
import state_to_state_machine as ssm
class wrapper(luigi.WrapperTask):
    def requires(self):
        file_tag = ['Sessiontolead','leadtoopportunity','opportunitytocomplete']
        size = 10
        for j in range(1,int(size)):
            return[ssm.state_machine(file_tag=i,size=size,obs_nums=j)for i in file_tag]
    def run(self):
        print('The wrapper is complete')
        pd.DataFrame().to_csv('/Users/emm/Documents/AttributionData/Data/datawranglerwrapper3.csv') #never returns anything
    def output(self):
        return luigi.LocalTarget('/Users/emm/Documents/AttributionData/Data/datawranglerwrapper3.csv')
if __name__ == '__main__':
    luigi.build([wrapper()],workers=8,local_scheduler=True)


state machine:

import pandas as pd
import get_samples as gs
import luigi
import random
class state_machine(luigi.Task):
    file_tag = luigi.Parameter()
    obs_nums = luigi.Parameter() #directly get element - don't write to file
    size = luigi.Parameter()
    def run(self):
        path = '/Users/emm/Documents/AttributionData/Data/Probabilities/'
        file = path+self.file_tag+'sampleprobs.csv'
        def generic_state_machine(tag,file=file,obs_nums=self.obs_nums):
            if file.split('/')[7][:4] == tag:
                state_machine = pd.read_csv(file)
                return state_machine.ix[:,1][obs_nums] if s.ix[:,1][obs_nums] &gt; random.uniform(0,1) else False
        session_to_leads = []
        lead_to_opps = []
        opps_to_comp = []
        session_to_leads.append(generic_state_machine(tag='Sessiontoload+sampleprobabs',file=file,obs_nums=self.obs_nums))
        lead_to_opps.append(generic_state_machine(tag='leadtoopportunity+sampleprobabs',file=file,obs_nums=self.obs_nums)) if session_to_leads[self.obs_nums-1] != False else lead_to_opps.append(False)
        opps_to_comp.append(generic_state_machine(tag='opportunitytocomplete+sampleprobabs',file=file,obs_nums=self.obs_nums)) if lead_to_opps[self.obs_nums-1] != False else opps_to_comps.append(False)
        df = pd.DataFrame(zip(session_to_leads,lead_to_opps,opps_to_comp),columns=['session_to_leads','lead_to_opps','oops_to_comp'])
    with self.output().open('w') as out_csv:
        out_csv.write(df.to_csv('/Users/emmanuels/Documents/AttributionData/Data/Probabilities/'+str(self.file_tag)+str(self.size)+'statemachine.csv'))
def output(self):
    return luigi.LocalTarget('/Users/emmanuels/Documents/AttributionData/Data/Probabilities/'+str(self.file_tag)+str(self.size)+'statemachine.csv')


I have asked similar versions of this question, but then has changed each time, I have managed to resolve most of the initial issues- so this is not a repetition on previous questions

So from my understanding in this instance this should produce 3 state machine files each with 10 rows for each observation and the comparison made.

The three files are literally files with 2 columns, the first being the index and the second being probabilities between 0 and 1

I'm not sure if this a problem with the logic of my code or how I am using Luigi
",-1,-1,-1.0
58248151,shared volume with luigi docker server,"I am trying to experiment a bit with docker and luigi https://hub.docker.com/r/spotify/luigi

I created a docker container with spotify/luigi. I am quite new to this and this image does not seem to be accessible through the console.

I created a shared volume and mapped it to /luigi/share/ for the container

import luigi
import time

class HelloWorld(luigi.Task):
    def requires(self):
        return None
    def output(self):
        return luigi.LocalTarget('/luigi/share/helloworld3.txt')
    def run(self):
        time.sleep(1)
        with self.output().open('w') as outfile:
            outfile.write('Hello World!\n')
        time.sleep(1)

class NameSubstituter(luigi.Task):
    name = luigi.Parameter()

    def requires(self):
        return HelloWorld()
    def output(self):
        return luigi.LocalTarget(self.input().path + '.name_' + self.name)
    def run(self):
        time.sleep(1)
        with self.input().open() as infile, self.output().open('w') as outfile:
            text = infile.read()
            text = text.replace('World', self.name)
            outfile.write(text)
        time.sleep(1)

if __name__ == '__main__':
    luigi.run()


This is the example Code I run with python test.py --scheduler-host 192.168.178.48 NameSubstituter

When I now look into another container with the shared volume there is no text file created.

I am a little lost here... 

Thank you in advance
",-1,-1,-1.0
59652494,Luigi: No such file or directory error when submitting a PySpark Task,"Looking for help on this error and I'd really appreciate any advice on how to get past this.  

As the subject says, I'm trying to run a very simple pyspark task but I'm getting ""OSError: [Errno 2] No such file or directory""
.

In my working directory I have...


luigi.cfg
sparkLuigiTest.py


My config file contains...

[spark]
spark-submit:$SPARK_HOME/bin/spark-submit
master:yarn
num-executors:10


The *.py file contains...

import luigi
from luigi.contrib.spark import PySparkTask

class Test(PySparkTask):
    def input(self):
        return None

    def output(self):
        return luigi.LocalTarget(""output.csv"")

    def main(self, sc, *args):
        data = [(1,2,3), (4,5,6), (7,8,9)]
        data = sc.createDataFrame(data, [""A"", ""B"", ""C""])

        df = data.toPandas()
        df.to_csv(""/u/&lt;user name&gt;/tests/luigi_woo.csv"")

if __name__ == ""__main__"":
    luigi.run()


Lastly, I run my script with 

LUIGI_CONFIG_PATH=./luigi.cfg python ./sparkLuigiTest.py --local-scheduler Test


Any ideas on why I'd be getting the following error?

ERROR: [pid 1678418] Worker Worker(salt=778216410, workers=1, host=hpchdp2e, username=&lt;name&gt;, pid=1678418) failed    Test()
Traceback (most recent call last):
  File ""/s/anaconda/users/&lt;name&gt;/miniconda2/lib/python2.7/site-packages/luigi/worker.py"", line 199, in run
    new_deps = self._run_get_new_deps()
  File ""/s/anaconda/users/&lt;name&gt;/miniconda2/lib/python2.7/site-packages/luigi/worker.py"", line 141, in _run_get_new_deps
    task_gen = self.task.run()
  File ""/s/anaconda/users/&lt;name&gt;/miniconda2/lib/python2.7/site-packages/luigi/contrib/spark.py"", line 309, in run
    super(PySparkTask, self).run()
  File ""/s/anaconda/users/&lt;name&gt;/miniconda2/lib/python2.7/site-packages/luigi/contrib/spark.py"", line 66, in run
    super(SparkSubmitTask, self).run()
  File ""/s/anaconda/users/&lt;name&gt;/miniconda2/lib/python2.7/site-packages/luigi/contrib/external_program.py"", line 134, in run
    with self._proc_with_tracking_url_context(proc_args=args, proc_kwargs=kwargs) as proc:
  File ""/s/anaconda/users/&lt;name&gt;/miniconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/s/anaconda/users/&lt;name&gt;/miniconda2/lib/python2.7/site-packages/luigi/contrib/external_program.py"", line 168, in _proc_with_tracking_url_context
    main_proc = subprocess.Popen(proc_args, **proc_kwargs)
  File ""/s/anaconda/users/&lt;name&gt;/miniconda2/lib/python2.7/subprocess.py"", line 394, in __init__
    errread, errwrite)
  File ""/s/anaconda/users/&lt;name&gt;/miniconda2/lib/python2.7/subprocess.py"", line 1047, in _execute_child
    raise child_exception
OSError: [Errno 2] No such file or directory

",1,-1,-1.0
59965610,How to consume output of a Task in luigi,"I have two luigi tasks. 

TaskA runs an external program and the results are stored as a json with luigi.LocalTarget

    class TaskA(ExternalProgramTask):
        def output(self):
           return luigi.LocalTarget(self.outputfile)


In TaskB I want to do some transformation of the json and pickle.dump() it. But I have issues opening the file.

@inherits(TaskA)
class TaskB(luigi.Task):
    def requires(self):
        args = {....}
        return TaskA(**args)

    def run(self):
        try: 
            entries = json.load(self.input().open())
        except json.decoder.JSONDecodeError as e:
            logging.error(f""Decoding error: {e}"")
            return print(e)



But this is not working as I am getting a decoding error:

Decoding error: Expecting value: line 6 column 1

which makes sense, if I try to print self.input().open() I would expect the json. But instead I am getting: &lt;_io.TextIOWrapper name='task1.output.json' mode='rb' encoding='UTF-8'&gt;

I also tried to use yield TaskA() but this also did not work. But according to the documentation, it should work. 

Using Python 3.8.1 and latest luigi version.
",-1,-1,-1.0
59636003,Chaining multiple tasks in Luigi,"I have one Luigi tasks that is set up to fetch different data based on the parameters passed to it and another one that is meant to take those files and send them to our datalake. My issue is that I'm not sure how to schedule multiple versions of the first task in sequence so that they can later be sent to the datalake. Here's my code:

class ListStep(luigi.Task):

    listId = luigi.IntParameter()
    startDate = luigi.Parameter(None)
    endDate = luigi.Parameter(None)
    logPath = luigi.Parameter(None)
    messagePath = luigi.Parameter(None)
    summaryPath = luigi.Parameter(None)
    contactPath = luigi.Parameter(None)
    logFile = luigi.BoolParameter(True)
    endpoint = luigi.Parameter()
    subscribed = luigi.BoolParameter(True)
    fileSuffix = luigi.Parameter(None)

    def output(self):
        today =  datetime.datetime.now()
        todayName = today.strftime(""%m%d%y"")
        pipelineNameLog = ""./pipelinelog/pipelinelog_{}_{}_{}.csv"".format(client.listId, self.endpoint, todayName)
        return luigi.LocalTarget(pipelineNameLog)

    def run(self):
        client = ListkWriter(client_id, client_secret, listId = self.listId, logPath = self.logPath, contactPath = self.contactPath, messagePath = self.messagePath, summaryPath = self.summaryPath)

        if self.endpoint == ""message"":
            filesList = client.getMessages(startDate = self.startDate, endDate = self.endDate, log = self.logFile, fileSuffix = self.fileSuffix)
        elif self.endpoint == ""contacts"":
            filesList = client.getContacts(startDate = self.startDate, endDate = self.endDate,  log = self.logFile, fileSuffix = self.fileSuffix, subscribed = self.subscribed)
        elif self.endpoint == ""summary"":
            filesList = client.getSummary(startDate = self.startDate, endDate = self.endDate, log = self.logFile, fileSuffix = self.fileSuffix)

        with self.output().open('w') as outfile:
            for val in filesList:
                outfile.write("","".join([val, datetime.datetime.now().strftime(""%H:%M:%S"")]))
                outfile.write(""\n"")

class Transfer(luigi.Task):

    step = luigi.TaskParameter()
    uploadPath = luigi.Parameter()

    def requires(self):
        return self.step

    def output(self):
        today =  datetime.datetime.now()
        todayName = today.strftime(""%m%d%y"")
        pipelineNameLog = ""./pipelinelog/pipelinelog_{}_{}_{}.csv"".format(client.listId, self.step.endpoint, todayName)
        return luigi.LocalTarget(pipelineNameLog)

    def run(self):
        containerClient = ContainerClient.from_connection_string(storageCreds, 'datalake')
        with self.input().open('r') as infile:
            fileUpload = infile.readlines()

        for file in fileUpload:
            pathFile = file.split("","")[0]
            fileName = ntpath.basename(pathFile)
            uploadName = self.uploadPath + fileName
            with open(pathFile, 'rb') as f:
                containerClient.upload_blob(uploadName, f)

path = ""/test/""

mMessage = ListStep(endpoint = ""message"", startDate = startDate, listId = listDict['m'], summaryPath = path, logPath = path, messagePath = path, contactPath = path)
gMessage = ListStep(endpoint = ""message"", startDate = startDate, listId = listDict['g'], summaryPath = path, logPath = path, messagePath = path, contactPath = path)
mSummary = ListStep(endpoint = ""summary"", startDate = startDate, listId = listDict['m'], summaryPath = path, logPath = path, messagePath = path, contactPath = path)
gSummary = ListStep(endpoint = ""summary"", startDate = startDate, listId = listDict['g'], summaryPath = path, logPath = path, messagePath = path, contactPath = path)

luigi.run([Transfer(step = mMessage, uploadPath = path), Transfer(step = gMessage, uploadPath = path),
           Transfer(step = mSummary, uploadPath = path), Transfer(step = gSummary, uploadPath = path)],
          local_scheduler = True)



What I want to achieve is that once the data is fetched using the ListStep task (with the specified parameters), they can be sent to the datalake using the Transfer task. After the first set of operations is carried out, I want it to move to the next set of objects. My aim is not to have this all run in parallel, but rather in sequence. When I execute this script, it seems that the scheduler is only counting 5 tasks and not 8. Furthermore it moves on to execute the mMessage task, then the gMessage task without executing the Transfer task. It later chrashes because of an FileExistError and outputs the following summary:

===== Luigi Execution Summary =====

Scheduled 5 tasks of which:
* 2 ran successfully:
    - 2 ListrakStep(...)
* 2 failed:
    - 2 ListrakStep(...)
* 1 were left pending, among these:
    * 1 was not granted run permission by the scheduler:
        - 1 Transfer(step=ListrakStep, destination=datalake, uploadPath=test/)

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

",-1,-1,-1.0
58518036,How to only allow a specific machine to run a task in Luigi,"Machine A has the ability to access a SQL database and Machine B has the ability to access Google Drive. How do I make sure that a task is run on the correct machine if UploadToDrive depends on DownloadSQLData somewhere down the line?

Currently Machine A runs DoSomethingElseWithData and Machine B runs UploadToDrive a few minutes later. This is fine up until the point where one day Machine A might not be working, at which point Machine B will attempt DownloadSQLData as an upstream dependency and fail.

class DownloadSQLData(luigi.Task):

    # ...

    def run(self):
        # Only Machine A can do this
        # ...

class TransformData(luigi.Task):

    # ...

    def requires(self):
        return DownloadSQLData(date=self.date)

class UploadToDrive(luigi.Task):

    # ...

    def requires(self):
        return TransformData(date=self.date)

    def run(self):
        # Only Machine B can do this
        # ...

class DoSomethingElseWithData(luigi.Task):

    #...

    def requires(self):
        return TransformData(date=self.date)


The SQL database from this example is, in reality, not a SQL database but an old system within our company. It does not fail gracefully when unauthorised users try to access it and we'd like to avoid any attempts from Machine B to do so.
",-1,-1,-1.0
57823796,python luigi : requires() can not return Target objects,"I'm really new to Luigi and I would like to set up luigi to execute my API calls.

I'm working with MockFiles since the json object that I retrieve through API are light and I want to avoid to use an external database.

This is my code : 

import luigi 
from luigi import Task, run as runLuigi, mock as LuigiMock
import yaml

class getAllCountries(Task):
    task_complete = False

    def requires(self):
        return LuigiMock.MockFile(""allCountries"")

    def run(self):
        sync = Sync()
        # Get list of all countries
        countries = sync.getAllCountries()
        if(countries is None or len(countries) == 0):
            Logger.error(""Sync terminated. The country array is null"")

        object_to_send = yaml.dump(countries)

        _out = self.output().open('r')
        _out.write(object_to_send)
        _out.close()

        task_complete = True


    def complete(self):
        return self.task_complete


class getActiveCountries(Task):

    task_complete = False 

    def requires(self):
        return getAllCountries()

    def run(self):
        _in = self.input().read('r')
        serialised = _in.read()
        countries = yaml.load(serialised)

        doSync = DoSync()
        activeCountries = doSync.getActiveCountries(countries)  
        if(activeCountries is None or len(activeCountries) == 0):
            Logger.error(""Sync terminated. The active country account array is null"")        

        task_complete = True 

    def complete(self):
        return self.task_complete

if __name__ == ""__main__"":
    runLuigi()


I'm running the project with the following command :

 PYTHONPATH='.' luigi --module app getActiveCountries --workers 2 --local-scheduler


And it fails, and this is the stacktrace that I got :

DEBUG: Checking if getActiveCountries() is complete
DEBUG: Checking if getAllCountries() is complete
INFO: Informed scheduler that task   getActiveCountries__99914b932b   has status   PENDING
ERROR: Luigi unexpected framework error while scheduling getActiveCountries()
Traceback (most recent call last):
    File ""/Users/thibaultlr/anaconda3/envs/testThib/lib/python3.6/site-packages/luigi/worker.py"", line 763, in add
    for next in self._add(item, is_complete):
    File ""/Users/thibaultlr/anaconda3/envs/testThib/lib/python3.6/site-packages/luigi/worker.py"", line 861, in _add
    self._validate_dependency(d)
    File ""/Users/thibaultlr/anaconda3/envs/testThib/lib/python3.6/site-packages/luigi/worker.py"", line 886, in _validate_dependency
    raise Exception('requires() can not return Target objects. Wrap it in an ExternalTask class')
Exception: requires() can not return Target objects. Wrap it in an ExternalTask class
INFO: Worker Worker(salt=797067816, workers=2, host=xxx, pid=85795) was stopped. Shutting down Keep-Alive thread
ERROR: Uncaught exception in luigi
Traceback (most recent call last):
    File ""/Users/thibaultlr/anaconda3/envs/testThib/lib/python3.6/site-packages/luigi/retcodes.py"", line 75, in run_with_retcodes
    worker = luigi.interface._run(argv).worker
    File ""/Users/thibaultlr/anaconda3/envs/testThib/lib/python3.6/site-packages/luigi/interface.py"", line 211, in _run
    return _schedule_and_run([cp.get_task_obj()], worker_scheduler_factory)
    File ""/Users/thibaultlr/anaconda3/envs/testThib/lib/python3.6/site-packages/luigi/interface.py"", line 171, in _schedule_and_run
    success &amp;= worker.add(t, env_params.parallel_scheduling, env_params.parallel_scheduling_processes)
    File ""/Users/thibaultlr/anaconda3/envs/testThib/lib/python3.6/site-packages/luigi/worker.py"", line 763, in add
    for next in self._add(item, is_complete):
    File ""/Users/thibaultlr/anaconda3/envs/testThib/lib/python3.6/site-packages/luigi/worker.py"", line 861, in _add
    self._validate_dependency(d)
    File ""/Users/thibaultlr/anaconda3/envs/testThib/lib/python3.6/site-packages/luigi/worker.py"", line 886, in _validate_dependency
    raise Exception('requires() can not return Target objects. Wrap it in an ExternalTask class')
Exception: requires() can not return Target objects. Wrap it in an ExternalTask class


Also, i'm running the luigid in background and I do not see any tasks that ran on it. Neither if it failed or not 

Any ideas ?
",-1,-1,-1.0
60511075,Luigi dependencies specification issue with a separate task,"I have 3 Luigi tasks: first generates an output file that is written to hadoop, second - uses this output file to load it into Elasticsearch, third one - gets a completely separate file and also loads it into Elasticsearch. Third task is rather disconnected from the first two, but I want it to be run when the first two are finished. There can be multiple files (of the same type) fed into the first task, so in the second one I specify the dependencies like that:

def requires(self):
    return [SeqrVCFToMTTask()]


SeqrVCFToMTTask being the first task. It works fine, so first two tasks run perfectly. Now, when I try to specify dependency in the same way in the third task:

def requires(self):
    return [SeqrMTToESTask()]

(`SeqrMTToESTask` - name of the second task).


It fails with the error:


  raise HTTP_EXCEPTIONS.get(status_code, TransportError)(status_code, error_message, additional_info)
  elasticsearch.exceptions.RequestError: TransportError(400, '')


and the final luigi task output looks like that:

===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 SeqrVCFToMTTask(...)
* 1 failed:
    - 1 SeqrMTToESTask(...)
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 SeqrGenesQCToESTask(...)

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====


I know it has nothing to do with Elasticsearch because the 3rd task - SeqrGenesQCToESTask - runs fine separately when the dependencies are omitted (just uncommenting requires). How should I specify dependencies here correctly? The only thing that I need is for the 3rd task to start running after the first two are finished. 


  Update


More detailed code:

class SeqrVCFToMTTask(HailMatrixTableTask):
    reference_ht_path = luigi.Parameter(...
    ...
    ...

    def run(self):
        self.read_vcf_write_mt()

    def read_vcf_write_mt(self, schema_cls=SeqrVariantsAndGenotypesSchema):
    ...
    ...

class SeqrMTToESTask(HailElasticSearchTask):
    dest_file = luigi.Parameter()
    def __init__(self, *args, **kwargs):
        # TODO: instead of hardcoded index, generate from project_guid, etc.
        super().__init__(*args, **kwargs)

    def requires(self):
        return [SeqrVCFToMTTask()]

    def output(self):
        filename = self.dest_file
        return getTarget(filename)

    def run(self):
        mt = self.import_mt()
        row_table = SeqrVariantsAndGenotypesSchema.elasticsearch_row(mt)
        self.export_table_to_elasticsearch(row_table, self._mt_num_shards(mt))

class SeqrGenesQCToESTask(luigi.Task):
    source_path = luigi.Parameter(...
    ...
    ...

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._es = ElasticsearchClient(host=self.es_host, port=self.es_port)

    def requires(self):
        return [SeqrMTToESTask()]

    def output(self):
        # TODO: Use https://luigi.readthedocs.io/en/stable/api/luigi.contrib.esindex.html.
        filename = self.dest_file
        return getTarget(filename)

    def run(self):
        // Doing some data transformations, then 
        // Exporting data to Elasticsearch


class HailMatrixTableTask(luigi.Task):
     source_paths = luigi.Parameter(...
     ...
     ...

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        try:
            # Locally it should be '_FrozenOrderedDict' but on AWS for an unknown reason only 'FrozenOrderedDict' works
            self.source_paths = list(json.loads(self.source_paths, object_pairs_hook=luigi.parameter._FrozenOrderedDict))
        except json.JSONDecodeError:
            self.source_paths = [self.source_paths]

    def requires(self):
        # We only exclude globs in source path here so luigi does not check if the file exists
        return [VcfFile(filename=s) for s in self.source_paths if '*' not in s]

    def output(self):
        filename = self.dest_path
        return getTarget(filename)

    def complete(self):
        # Complete is called by Luigi to check if the task is done and will skip if it is.
        # By default it checks to see that the output exists, but we want to check for the
        # _SUCCESS file to make sure it was not terminated halfway.
        filename = self.dest_path
        full_path = os.path.join(filename, '_SUCCESS')
        return getTarget(full_path).exists()

    def run(self):
        // Import file, then output in different format

class HailElasticSearchTask(luigi.Task):
    project_guid = luigi.Parameter(...
    ...
    ...

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if not self.es_index:
            self.es_index = compute_index_name(args=public_class_props(self))
        self._es = ElasticsearchClient(host=self.es_host, port=self.es_port)

    def requires(self):
        return [VcfFile(filename=self.source_path)]

    def run(self):
        mt = self.import_mt()
        # TODO: Load into ES

    def import_mt(self):
        return hl.read_matrix_table(self.input()[0].path)

    def export_table_to_elasticsearch(self, table, num_shards):
        // Exports to ES index



  Update


I suspect it happens because the 3rd task - SeqrGenesQCToESTask - opens up connection to Elasticsearch and then the 2nd task - SeqrMTToESTask - can't output to Elasticsearch due to that since the error happens there, in SeqrMTToESTask on the line:

self.export_table_to_elasticsearch(row_table, self._mt_num_shards(mt))

",-1,-1,-1.0
61668424,Luigi not using scheduler host/port from configuration,"Please let me know if my understanding of Luigi is off. 

I currently have luigi running on a Linux instance that I want to use as a central scheduler. 

When trying to run to run Python code from another instance, I cannot get Python to point to the central scheduler. Luigi always tries to connect to http://localhost:8082 instead.

luigi.rpc.RPCError: Errors (3 attempts) when connecting to remote scheduler 'http://localhost:8082'


I have a luigi.cfg file in the project folder that contains: 

[core]
default_scheduler_host=some-ip
default_scheduler_port=some-port
default_scheduler_url=http://some-ip:some-port


I have also set the LUIGI_CONFIG_PATH variable to this file. 

Yet Python keeps attemping to connect to http://localhost:8082. What am I doing wrong here?
",-1,-1,-1.0
62297041,Import luigi throws SyntaxError,"import luigi is the first line of my program. But IDE throws an error:

Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1415, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""task.py"", line 2, in &lt;module&gt;
    import luigi
  File ""/.pyenv/versions/2.7.14/lib/python2.7/site-packages/luigi/__init__.py"", line 23, in &lt;module&gt;
    from luigi import task
SyntaxError: ('invalid syntax', ('/.pyenv/versions/2.7.14/lib/python2.7/site-packages/luigi/task.py', 145, 21, 'class Task(metaclass=Register):\n'))

",-1,-1,-1.0
62394302,AWS EMR luigi pipeline generates $folder$ temp files,"I have a luigi pipeline that I run on AWS EMR. In the luigi.cfg config file I have no such source or destination files, but in S3 bucket I see these {some_name}_$folder$ files for each output folder (in output path) and files themselves. Since I have many pipeline runs with many files these weird _$folder$ accumulate in large quantities and it becomes an issue. How to prevent AWS from outputting them? I could run additional script maybe to delete them but it does not look like how it should be done. 
",-1,-1,-1.0
64287670,Return function to luigi output method,"I am trying to return path of input zip archive. I am stuck on implementing luigi output method:
def get_zip_path() -&gt; str:
    input_zip = ''
    with open(&quot;/input/index.json&quot;, &quot;r&quot;) as input_index:
        json_str = json.load(input_zip)   
        input_zip = json_str[&quot;source&quot;]
    return input_zip

class Input(luigi.Task):
&quot;&quot;&quot;
Unpack dicom zip archive to workdir
&quot;&quot;&quot;
    @property
    def zip_path():
        return get_zip_path()

    def output(self):
        return luigi.LocalTarget(self.zip_path())

I want class Input() return method output the string of zip path &quot;/input/zipfile.zip&quot; but get the error
TypeError: zip_path() takes 0 positional arguments but 1 was given

",-1,-1,-1.0
64837259,Luigi: how to pass arguments to dependencies using luigi.build interface?,"Consider a situation where a task depends on another through a dynamic dependency:
import luigi
from luigi import Task, TaskParameter, IntParameter

class TaskA(Task):
    parent = TaskParameter()
    arg = IntParameter(default=0)
    def requires(self):
        return self.parent()
    def run(self):
        print(f&quot;task A arg = {self.arg}&quot;)

class TaskB(Task):
    arg = IntParameter(default=0)
    def run(self):
        print(f&quot;task B arg = {self.arg}&quot;)

if __name__ == &quot;__main__&quot;:
    luigi.run([&quot;TaskA&quot;, &quot;--parent&quot; , &quot;TaskB&quot;, &quot;--arg&quot;, &quot;1&quot;, &quot;--TaskB-arg&quot;, &quot;2&quot;])

(Notice the default arg=0 Parameter).
Using the luigi.run() interface, this works. As you can see, TaskA is given two arguments: parent=TaskB and arg=1. Furthermore TaskB is also given argument arg=2 by using the syntax --TaskB-arg.

Scheduled 2 tasks of which:
* 1 ran successfully:
    - 1 TaskB(arg=2)
* 1 failed:
    - 1 TaskA(parent=TaskB, arg=1)

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====

(In this example tasks failed because TaskB is not writing its output to a file that TaskA can read. But that's just to keep the example short. The important point is that both TaskA and TaskB are passed the correct arg).
My problem now is: how do I do the exact same thing, but using the luigi.build() interface? There's two reasons why I want to do this: First is that the source code says that luigi.run() shouldn't be used. But second, I can't run more than one luigi.run() per process, but I can do so with luigi.build(). This is important because I want to do something like:
if __name__ == &quot;__main__&quot;:
    for i in range(3):
        luigi.run([&quot;TaskA&quot;, &quot;--parent&quot; , &quot;TaskB&quot;, &quot;--arg&quot;, f&quot;{i}&quot;, &quot;--TaskB-arg&quot;, f&quot;{i}&quot;])

However if you try this you get the error:
Pid(s) {10084} already running
So, in the luigi.build() interface you're supposed to pass it a list of the tasks instantiated with their parameters:
if __name__ == &quot;__main__&quot;:
    for i in range(3):
        luigi.build([TaskA(parent=TaskB, arg=i)])

This does what's expected with regards to TaskA, but TaskB takes the default arg=0.
So question: how to pass arguments to dependencies using luigi.build() interface?
Here's things that I've tried and don't work:
A)
if __name__ == &quot;__main__&quot;:
    for i in range(3):
        luigi.build([TaskA(parent=TaskB, arg=i), TaskB(arg=i)])

Doesn't work because two instances of TaskB are ran: one with the default (wrong) arg, which TaskA depends on, and one with the correct arg, which TaskA doesn't depend on.
B)
if __name__ == &quot;__main__&quot;:
    for i in range(3):
        luigi.build([TaskA(parent=TaskB(arg=i), arg=i)])

TypeError: 'TaskB' object is not callable
C)
if __name__ == &quot;__main__&quot;:
    for i in range(3):
        luigi.build([TaskA(parent=TaskB, arg=i)], &quot;--TaskB-arg&quot;, f&quot;{i}&quot;)

Getting desperate. I tried something like the old interface, but doesn't work:
AttributeError: 'str' object has no attribute 'create_remote_scheduler'
",-1,-1,-1.0
64958830,Luigi: how to pass different arguments to leaf tasks?,"This is my second attempt at understanding how to pass arguments to dependencies in Luigi. The first one was here.
The idea is: I have TaskC which depends on TaskB, which depends on TaskA, which depends on Task0. I want this whole sequence to be exactly the same always, except I want to be able to control what file Task0 reads from, lets call it path. Luigi's philosophy is normally that each task should only know about the Tasks it depends on, and their parameters. The problem with this is that TaskC, TaskB, and TaskA all would have to accept variable path for the sole purpose of then passing it to Task0.
So, the solution that Luigi provides for this is called Configuration Classes
Here's some example code:
from pathlib import Path
import luigi
from luigi import Task, TaskParameter, IntParameter, LocalTarget, Parameter

class config(luigi.Config):
    path = Parameter(default=&quot;defaultpath.txt&quot;)

class Task0(Task):
    path = Parameter(default=config.path)
    arg = IntParameter(default=0)
    def run(self):
        print(f&quot;READING FROM {self.path}&quot;)
        Path(self.output().path).touch()
    def output(self): return LocalTarget(f&quot;task0{self.arg}.txt&quot;)

class TaskA(Task):
    arg = IntParameter(default=0)
    def requires(self): return Task0(arg=self.arg)
    def run(self): Path(self.output().path).touch()
    def output(self): return LocalTarget(f&quot;taskA{self.arg}.txt&quot;)

class TaskB(Task):
    arg = IntParameter(default=0)
    def requires(self): return TaskA(arg=self.arg)
    def run(self): Path(self.output().path).touch()
    def output(self): return LocalTarget(f&quot;taskB{self.arg}.txt&quot;)

class TaskC(Task):
    arg = IntParameter(default=0)
    def requires(self): return TaskB(arg=self.arg)
    def run(self): Path(self.output().path).touch()
    def output(self): return LocalTarget(f&quot;taskC{self.arg}.txt&quot;)

(Ignore all the output and run stuff. They're just there so the example runs successfully.)
The point of the above example is controlling the line print(f&quot;READING FROM {self.path}&quot;) without having tasks A, B, C depend on path.
Indeed, with Configuration Classes I can control the Task0 argument. If Task0 is not passed a path parameter, it takes its default value, which is config().path.
My problem now is that this appears to me to work only at &quot;build time&quot;, when the interpreter first loads the code, but not at run time (the details aren't clear to me).
So neither of these work:
A)
if __name__ == &quot;__main__&quot;:
    for i in range(3):
        config.path = f&quot;newpath_{i}&quot;
        luigi.build([TaskC(arg=i)], log_level=&quot;INFO&quot;)

===== Luigi Execution Summary =====

Scheduled 4 tasks of which:
* 4 ran successfully:
    - 1 Task0(path=defaultpath.txt, arg=2)
    - 1 TaskA(arg=2)
    - 1 TaskB(arg=2)
    - 1 TaskC(arg=2)

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

I'm not sure why this doesn't work.
B)
if __name__ == &quot;__main__&quot;:
    for i in range(3):
        luigi.build([TaskC(arg=i), config(path=f&quot;newpath_{i}&quot;)], log_level=&quot;INFO&quot;)

===== Luigi Execution Summary =====

Scheduled 5 tasks of which:
* 5 ran successfully:
    - 1 Task0(path=defaultpath.txt, arg=2)
    - 1 TaskA(arg=2)
    - 1 TaskB(arg=2)
    - 1 TaskC(arg=2)
    - 1 config(path=newpath_2)

This progress looks :) because there were no failed tasks or missing dependencies

===== Luigi Execution Summary =====

This actually makes sense. There's two config classes, and I only managed to change the path of one of them.
Help?
EDIT: Of course, having path reference a global variable works, but then it's not a Parameter in the usual Luigi sense.
EDIT2: I tried point 1) of the answer below:
config has the same definition
class config(luigi.Config):
    path = Parameter(default=&quot;defaultpath.txt&quot;)

I fixed mistake pointed out, i.e. Task0 is now:
class Task0(Task):
    path = Parameter(default=config().path)
    arg = IntParameter(default=0)
    def run(self):
        print(f&quot;READING FROM {self.path}&quot;)
        Path(self.output().path).touch()
    def output(self): return LocalTarget(f&quot;task0{self.arg}.txt&quot;)

and finally I did:
if __name__ == &quot;__main__&quot;:
    for i in range(3):
        config.path = Parameter(f&quot;file_{i}&quot;)
        luigi.build([TaskC(arg=i)], log_level=&quot;WARNING&quot;)

This doesn't work, Task0 still gets path=&quot;defaultpath.txt&quot;.
",-1,-1,-1.0
65803757,TaskClassAmbigiousException in Luigi when yielding tasks in run(),"I'm struggling with an error in Luigi that I don't understand. I don't know if this is a known issue, a limitation of Luigi or if I am doing something wrong.
I am using Luigi in a real problem with many tasks and many dependences. However, I have made a toy example in which this issue appears clearly.
Let us consider two Tasks, TaskA and TaskB, TaskA requiring the execution of two previous instances of TaskB with differents values of a Luigi Parameter.
If I code the dependences in the requires() method of TaksA, then nothing bad happens. All three tasks execute and I get my exit files written.
But if I code the dependences in the run() method of TaksA, then I get the ugly TaskClassAmbigiousException.
In my real problem, I cannot yield the task in the requires() method, because I need to know the result of a previous task that is also yielded in the requieres() method, so I tried to yield the task in the run() and got the same exception.
Ok, here is the code of the toy example. First, yielding the task in requieres(), and it works:
import luigi

class TaskB(luigi.Task):
    j = luigi.IntParameter(default=1)

    def output(self):
        return luigi.LocalTarget(&quot;data/outputB{j}.txt&quot;.format(j=self.j))

    def requires(self):
        pass

    def run(self):
        print_file = 'TaskB' + str(self.j)

        with self.output().open('w') as out_file:
            out_file.write(print_file)

class TaskA(luigi.Task):
    i = luigi.IntParameter(default=1)

    def output(self):
        return luigi.LocalTarget(&quot;data/outputA{i}.txt&quot;.format(i=self.i))

    def requires(self):
        yield TaskB(j=self.i)
        yield TaskB(j=self.i+1)

    def run(self):
        print_file = &quot;&quot;
        for input_target in self.input():
            with input_target.open('r') as in_file:
                for line in in_file:
                    print_file+=line + 'TaskA' + str(self.i)

        with self.output().open('w') as out_file:
            out_file.write(print_file)
               
if __name__ == '__main__':
   taskA = TaskA(i=2)

And second, yielding the task in run(), and I get this:
 File &quot;/home/ppo0011l/.conda/envs/nudge/lib/python3.6/site-packages/luigi/worker.py&quot;, line 1081, in _handle_next_task
    for module, name, params in new_requirements]

  File &quot;/home/ppo0011l/.conda/envs/nudge/lib/python3.6/site-packages/luigi/worker.py&quot;, line 1081, in &lt;listcomp&gt;
    for module, name, params in new_requirements]

  File &quot;/home/ppo0011l/.conda/envs/nudge/lib/python3.6/site-packages/luigi/task_register.py&quot;, line 251, in load_task
    task_cls = Register.get_task_cls(task_name)

  File &quot;/home/ppo0011l/.conda/envs/nudge/lib/python3.6/site-packages/luigi/task_register.py&quot;, line 181, in get_task_cls
    raise TaskClassAmbigiousException('Task %r is ambiguous' % name)

The code:
import luigi

class TaskB(luigi.Task):
    j = luigi.IntParameter(default=1)

    def output(self):
        return luigi.LocalTarget(&quot;data/outputB{j}.txt&quot;.format(j=self.j))

    def requires(self):
        pass

    def run(self):
        print_file = 'TaskB' + str(self.j)

        with self.output().open('w') as out_file:
            out_file.write(print_file)

class TaskA(luigi.Task):
    i = luigi.IntParameter(default=1)

    def output(self):
        return luigi.LocalTarget(&quot;data/outputA{i}.txt&quot;.format(i=self.i))

    def requires(self):
        pass

    def run(self):
        print_file = &quot;&quot;
        target1 = yield TaskB(j=self.i)
        target2 = yield TaskB(j=self.i+1)
        for input_target in [target1, target2]:
            with input_target.open('r') as in_file:
                for line in in_file:
                    print_file+=line + 'TaskA' + str(self.i)

        with self.output().open('w') as out_file:
            out_file.write(print_file)
               
if __name__ == '__main__':
   taskA = TaskA(i=2)
   luigi.build([taskA], workers=1,local_scheduler=True,log_level='WARNING')

EDIT: I edit to add another related question. As what I want to do is to yield a task with a parameter that depends on an earlier yielded task, if this were possible for me it will be enough:
  def requires(self):
        taskb_target = yield TaskB(j=self.i)
        taskb_target.open('r')
# do something and yield next Task depending on what taskb_target has
        yield TaskB(j=self.i+1)

But unforntunately this does not work. Luigi says &quot;NoneType&quot; object has no attribute &quot;open&quot;.
However, when you yield a task in the run() method, you can access the output in runtime. It seems that there is a big asymmetry...
SECOND EDIT:
I have been doing more trials and I have found a weird conclusion: the second piece of code that I wrote in the original question (when in a .py file) can be executed ad eternum, even if deleting the output files and so forcing luigi to reexecute the tasks. However, the first piece of code can be excecuted only once (adn then, on the first execution, it works!!). But if you delete the files and execute the code again, you will get the ambigous task error.
I think it has something to do with the Register object of luigi. But what is really confusing for me is that this behaviour is different whether I yield the taskB in the requieres or the run method.
What I still don't know if is the problem arises when redefining class Tasks that are already in the Register module of luigi. It could be that... I tried also to place the class definitions in a .py different from the main .py, but when running twice it breaks. The only way to run properly is restarting the kernel, and you have only one chance!
",-1,-1,-1.0
66423898,luigi tasks have conflicting pip dependencies,"I have a luigi pipeline where some luigi.Tasks have conflicting pip dependencies. This causes issues because those tasks are part of the same pipeline (i.e. one task requires the other). I would not want to create separated pipelines as I would not be able to inspect the full pipeline in the scheduler anymore. What are the best practices in this case?
Example: You have two python packages each defining a luigi.Task.
However packageA needs a different version of a library than packageB:
packageA/task1.py requires mypackage==1.0.0
packageB/task2.py requires mypackage==0.9.0
Let's say the pipeline is:
task1 -&gt; task2 -&gt; wrappertask
This is an issue as in task2 I have to import task1 in order to define the requires method:
# packageB/task2.py, needs mypackage==0.9.0

from task1 import Task1 # cannot do this as I would need mypackage==1.0.0

class Task2(luigi.Task):
   id = luigi.Parameter()

   def requires(self):
       task1.Task1(id=id)
   ...  

",-1,-1,-1.0
67133968,Luigi task not writing pandas df to csv,"I have the following code to simply an excel file and return only the required columns. It written as a luigi task containerized on docker and its not returning the csv file while _SUCCESS flag is being created.
Function Code:
def _save_datasets(simplified, outdir: Path, flag):
    out_clean = outdir / 'transformed.csv/'
    flag = outdir / flag
    simplified.to_csv(str(out_clean), index=False)
     # save as csv and create flag file
    flag.touch()

@click.command()
@click.option('--in-csv')
@click.option('--out-dir')
@click.option('--flag')
def transform_data(in_csv,out_dir, flag):
    out_dir = Path(out_dir)
    data=pd.read_csv(in_csv)
    req_dp = data[['description','points']]
 #simplifying the points according to range 
    def transform_points_simplified(points):
        if points &lt; 84:
            return 1
        elif points &gt;= 84 and points &lt; 88:
            return 2 
        elif points &gt;= 88 and points &lt; 92:
            return 3 
        elif points &gt;= 92 and points &lt; 96:
            return 4 
        else:
            return 5
    simplified = req_dp.assign(points_simplified = dp['points'].apply(transform_points_simplified))
    _save_datasets(simplified,out_dir, flag)

Luigi Task code:
#Transform
class TransformData(DockerTask):
    &quot;&quot;&quot;Task to simplify datasets&quot;&quot;&quot;

    in_path = '/usr/share/data/created_csv/'
    in_csv = luigi.Parameter(default= in_path + 'cleaned.csv')
    out_dir = luigi.Parameter(default='/usr/share/data/created_csv/')
    flag = luigi.Parameter('.SUCCESS_TransformData')

    @property
    def image(self):
        return f'code-chal/transform-data:{VERSION}'

    def requires(self):
        return CleanData()

    @property
    def command(self):
        return [
            'python', 'clean_data.py',
            '--in-csv', self.in_csv,
            '--out-dir', self.out_dir,
            '--flag', self.flag
        ]

    def output(self):
        return luigi.LocalTarget(
            path=str(Path(self.out_dir) / self.flag)
        )

The luigi task moves on to the next task due to the creation of _SUCCESS flag, but the next task fails since its dependent on the transformed.csv file which isn't being created.
Thanks
",-1,-1,-1.0
67885792,Conditionnal branch in Luigi Workflows,"I am new to Luigi and trying to have a conditional branch in my flow. The branch task would evaluate a condition, and depending on the result some of its children would be skipped.
For testing this, I just have a dummy task that checks the current hour and returns True if the flow is executed during the morning, False otherwise. This task has two children, one that prints 'Morning' in the console, and one that prints 'Afternoon'. Depending on the result of the branching task, one is activated and the other is skipped. Here is what it looks like in Prefect:

You can see here that the flow was executed during the morning, so the afternoon task was skipped.
After doing some research, I don't know if Luigi is capable of doing something like this or not. What I tried so far is this:
# Third Task
class Branch(luigi.Task):
    def requires(self):
        return Sleep()

    def output(self):
        return luigi.LocalTarget('condition.txt')

    def run(self):
        date = str(datetime.now())
        hour = int (date.split()[1].split(':')[0])
        with self.output().open('w') as out:
            if hour&lt;12:
                out.write('morning')
                open(&quot;afternoon.txt&quot;, &quot;w&quot;).close() # create afternoon Target skips afternoon task
            else:
                out.write('afternoon')
                open(&quot;morning.txt&quot;, &quot;w&quot;).close() # create morning Target skips afternoon task

# Fourth Task depends on result of branch
class Morning(luigi.Task):
    def requires(self):
        return Branch()

    def output(self):
        return luigi.LocalTarget('morning.txt')

    def run(self):
        print (&quot;Morning&quot;)
        with self.output().open('w') as out:
            out.write(&quot;Morning&quot;)


class Afternoon(luigi.Task):
    def requires(self):
        return Branch()

    def output(self):
        return luigi.LocalTarget('afternoon.txt')

    def run(self):
        print (&quot;Afternoon&quot;)
        with self.output().open('w') as out:
            out.write(&quot;Afternoon&quot;)

# Fifth task is a merge after the branch
class Merge(luigi.WrapperTask):
    def requires(self):
        yield Morning()
        yield Afternoon()

    def run(self):
        print (&quot;Merged&quot;)

From what I understood, Luigi will only execute a task if its output does not exist yet. My idea was then to create the output file of the task to skip to prevent it from executing, but it does not work.
",-1,1,-1.0
68875360,How to loop through output in Luigi,"I am trying to use Luigi to build a small scraping pipeline and I'm using Pillow to save the images from the pages I scrape. However, I'm struggling with the output when I try to save each image in loop (e.g. I want to save img_1, img_2, img_3, etc. in the output folder). I tried to pass an &quot;image_id&quot; parameter within the output function but it doesn't work and I can't figure out how to accomplish this.
class DownloadImages(luigi.Task):

    def requires(self):
        pass # taking out dependencies for this example

    def output(self, image_id):
        return luigi.LocalTarget(f&quot;img/img_{image_id}.jpeg&quot;)

    def run(self):
        resp = requests.get(&quot;https://my-site.com&quot;)
        soup = BeautifulSoup(resp.content, &quot;html.parser&quot;)
        images_list = soup.select(&quot;img&quot;)
        for image_id in range(len(images_list)):
            image_url = images_list[image_id][&quot;src&quot;]
            img = Image.open(requests.get(image_url, stream=True).raw)
            img.save(self.output(image_id).path)

",-1,-1,-1.0
70659252,Luigi does not send error codes to concourse ci,"I have a test pipeline on concourse with one job that runs a set of luigi tasks. My problem is: failures in the luigi tasks do not rise up to the concourse job. In other words, if a luigi task fails, concourse will not register that failure and states that the concourse job completed successfully. I will first post the code I am running, then the solutions I have tried.
luigi-tasks.py
class Pipeline1(luigi.WrapperTask):
    def requires(self):
        yield Task1()
        yield Task2()
        yield Task3()

tasks.py
class Task1(luigi.Task):
    def requires(self):
        return None

    def output(self):
        return luigi.LocalTarget('stuff/task1.csv')

    def run(self):
        #uncomment line below to generate task failure
        #assert(True==False)
        print('task 1 complete...')
        t = pd.DataFrame()
        with self.output().open('w') as outtie:
            outtie.write('complete')

# Tasks 2 and 3 are duplicates of this, but with 1s replaced with 2s or 3s.


config file
[retcode]
# codes are in increasing level of severity (for most applications)
already_running=10
missing_data=20
not_run=25
task_failed=30
scheduling_error=35
unhandled_exception=40

begin.sh
#!/bin/sh
set -e
export PYTHONPATH='.' 
luigi --module luigi-tasks Pipeline1 --local-scheduler
echo $?

pipeline.yml
# &lt;resources, resource types, and docker image build job defined here&gt;

#job of interest
- name: run-docker-image
  plan:
  - get: timer
    trigger: true
  - get: docker-image-ecr
    passed: [build-docker-image]
  - get: run-git
  - task: run-script
    image: docker-image-ecr
    config:
      inputs:
      - name: run-git
      platform: linux
      run:
        dir: ./run-git
        path: /bin/bash 
        args: [&quot;begin.sh&quot;]

I've introduced errors in a few ways: assertions/raising an exception (ValueError) within an individual task's run() method and within the wrapper, and sys.exit(luigi.retcodes.retcode().unhandled_exception). I also tried failing all tasks. I did this in case the error needed to be generated in a specific manner/location. Though they all produced a failed task, none of them produced an error in the concourse server.
At first, I thought concourse just gives a success if it can run the file or command tasked to it. I'm not sure it's that simple, though. Interestingly, when I run the pipeline on my local computer (luigi --modules luigi-tasks Pipeline1 --local-scheduler) I get an appropriate return code (e.g. 30), but when I run the pipeline within the concourse server, I get a return code of 0 after the luigi tasks complete (from echo $? in the bash script).
Would appreciate any insight into this problem.
",-1,-1,-1.0
71496861,Luigi: Succesfull task run with Unfulfilled dependency at run time,"I have the following setup
class RootTask(luigi.WrapperTask):

    def requires(self):
        dependencies = [TaskA(), TaskB()]
        yield dependencies
        yield CreatePartials(dependencies=dependencies)


class TaskA(luigi.Task):
    def run(self):
        # write task_a.json

    def output(self):
        return luigi.LocalTarget('task_a.json')


class TaskB(luigi.Task):
    def run(self):
        # write task_a.json

    def output(self):
        return luigi.LocalTarget('task_b.json')


class CreatePartials(luigi.Task):
    dependencies = luigi.TaskParameter()

    def run(self):
        # write root_task_output.json

    def requires(self):
        for dep in self.dependencies:
            yield dep

    def output(self):
        return luigi.LocalTarget('root_task_output.json')

Although the pipeline is executed successfully, I am getting an Unfulfilled dependency at run time exception for each run on the CreatePartials task. There's one peculiarity with this processing flow: TaskA and TaskB have quite different completion times - meaning that TaskA could finish 8 hours later than TaskB. The fact that the data results are correct makes me confident that the code works - somehow the CreatePartials is retried once, it finds both dependencies completed and proceeds with its run method. Nevertheless, I am getting these Unfulfilled dependencies exceptions which

Create logs/alerts noise
I am not sure if they are indicative of a code issue which I am not aware at the moment since the results are correct. And that issue could manifest in some edge I cannot see currently.

So why am I getting these Unfulfilled dependency then?
Thanks a lot.
",-1,-1,-1.0
71539423,Luigi: TypeError: ... missing 1 required positional argument: 'self',"I have the following Luigi setup
import luigi


class RootTask(luigi.WrapperTask):
    def requires(self):
        yield RunAndReport(
            task=TaskA()
        )


class RunAndReport(luigi.WrapperTask):
    task = luigi.TaskParameter()

    def requires(self):
        yield self.task
        yield Report(
            input=self.task.input_file(),
            output=self.task.output_file()
        )


class Report(luigi.Task):
    pass


class TaskA(luigi.Task):

    def input_file(self):
        return 'file://opt/something_in.txt'

    def output_file(self):
        return 'file://opt/something_out.txt'


and I am getting the following error
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.9/dist-packages/luigi-3.0.3-py3.9.egg/luigi/worker.py&quot;, line 401, in check_complete
    is_complete = task.complete()
  File &quot;/usr/local/lib/python3.9/dist-packages/luigi-3.0.3-py3.9.egg/luigi/task.py&quot;, line 822, in complete
    return all(r.complete() for r in flatten(self.requires()))
  File &quot;/usr/local/lib/python3.9/dist-packages/luigi-3.0.3-py3.9.egg/luigi/task.py&quot;, line 883, in flatten
    for result in iterator:
  File &quot;/opt/my_file.py&quot;, line 48, in requires
    input_file=self.task.input_file(),
TypeError: input_file() missing 1 required positional argument: 'self'

As if TaskA is not instantiated and only the class definition is passed. Is this some kind of weird Python lazy initialization thing?
",-1,-1,-1.0
72116390,Luigi DictParameter KeyError,"I have the following task
class Test(luigi.Task):
    foo = luigi.DictParameter(default = {})
    ...

I am using a config file to run this
[Test]
foo = &quot;{'a': 0, 'b': 1}&quot;

I have also tried

'{'a': 0, 'b': 1}'
&quot;{&quot;a&quot;: 0, &quot;b&quot;: 1}&quot;
&quot;{'a': &quot;0&quot;, 'b': &quot;1&quot;}&quot;
&quot;{'a': 0, 'b': 1}&quot;

They all fail with
KeyError: '&quot;a&quot;'

or
KeyError: ''a''

or some a related key error. What am I supposed to configure dictionaries as parameters when I am using a config file?
I tried this was as well but even this does not work. This link is from the PR that fixes the loading of dictionaries from TOML files.
In the official docs of the DictParameter they suggest

luigi --module my_tasks MyTask --tags 

or

luigi --module my_tasks MyTask --tags '{&quot;role&quot;: &quot;web&quot;, &quot;env&quot;: &quot;staging&quot;}'

But there is no mention for a TOML file. Even in their complex example TOML they do not include dictionaries
I have to say that Luigi is one of the most ill-documented projects I've ever worked with unfortunately.
",-1,-1,-1.0
72155889,How to schedule workflow in Luigi?,"I am able to instantaneously execute a pipeline/workflow in luigi using the following:
luigi --module mypipeline mypipeline --local-scheduler

But how can I add a schedule to it, for example executing it every 10 days?
I wasn't able to find scheduler examples or applicable sample code in the documentation
",-1,-1,-1.0
73404016,Variable filenames in Luigi Pipelines,"I'm having some trouble grokking how to chain several small tasks together in a Luigi pipeline when it comes to dynamic file names. How does the filename back propagate?
Let's say that I start with a file called ABC.zip and I ahve several luigi tasks like:

unzip the content of ABC.zip in order to get A.csv, B.csv ... Z.csv
Read each of the CSV files, do some parsing/filtering etc onto them to create A_proc.csv, B_proc.csv etc. Keep in mind I don't have any knowledge of what the filenames will be, how many, what size etc that are contained in the zip file.
Combine A_proc.csv ... Z_proc.csv into A_to_Z.csv

My very limited understanding of Luigi would mean that my code would look something like this:
def Combine(luigi.task):
    def requires(self):
        requiredInputs = []
        files_inside_zip = self.ZipFile.namelist() # get list of files inside zipfile
        for i in files_inside_zip:
            requiredInputs.append(ParseFile(Filename=i))
        return requiredInputs

    def output(self):
        return luigi.LocalTarget(f&quot;data/{zipfile}_A_to_Z.csv&quot;)

    def run(self):
        # Some logic to combine and write

def FilterFile(luigi.task):
    def requires(self):
         return Unzip(FileID=self.FileID) # Is this ABC.zip or a unique instance for each A/B/C.csv file in zip? 

    def output(self):
        return luigi.LocalTarget(f&quot;data/{zipfile}_proc.csv&quot;)

    def run(self):
        # Some logic to filter/parse and write

def UnZip(luigi.task): 
    def output(self):
        return luigi.LocalTarget(f&quot;data/{self.zip_folder}/{self.zip_file}.csv&quot;)

    def run(self):
       self.zip_folder, file_list = unzip(self.zip_name)
       for file in file_list:
           self.zip_file = file
           with self.output().open(&quot;w&quot;) as f:
               #Write to file.
 

Since I won't know how many files to process, until I've done the unzip would it not be better to rather unzip the files, but pass a list of filenames (csv, pickle or whatever) to each task rather than try to spawn a task for each file?
I'm not getting this... I've been looking at this example but I'm not getting how to handle each step when I require the Zip file name, and file name, and I can't tell what they will be before hand...
",-1,-1,-1.0
74294151,Has anyone experienced random file access errors when working with luigi in Windows?,"When working with luigi on Windows 10, the following error is sometimes thrown:
Traceback (most recent call last):
  File &quot;D:\Users\myuser\PycharmProjects\project\venv\lib\site-packages\luigi\worker.py&quot;, line 192, in run
    new_deps = self._run_get_new_deps()
  File &quot;D:\Users\myuser\PycharmProjects\project\venv\lib\site-packages\luigi\worker.py&quot;, line 130, in _run_get_new_deps
    task_gen = self.task.run()
  File &quot;project.py&quot;, line 15000, in run
    data_frame = pd.read_excel(self.input()[&quot;tables&quot;].open(),segment)
  File &quot;D:\Users\myuser\PycharmProjects\project\venv\lib\site-packages\pandas\io\excel.py&quot;, line 191, in read_excel
    io = ExcelFile(io, engine=engine)
  File &quot;D:\Users\myuser\PycharmProjects\project\venv\lib\site-packages\pandas\io\excel.py&quot;, line 247, in __init__
    self.book = xlrd.open_workbook(file_contents=data)
  File &quot;D:\Users\myuser\PycharmProjects\project\venv\lib\site-packages\xlrd\__init__.py&quot;, line 115, in open_workbook
    zf = zipfile.ZipFile(timemachine.BYTES_IO(file_contents))
  File &quot;C:\Python27\lib\zipfile.py&quot;, line 793, in __init__
    self._RealGetContents()
  File &quot;C:\Python27\lib\zipfile.py&quot;, line 862, in _RealGetContents
    raise BadZipfile(&quot;Bad magic number for central directory&quot;)
BadZipfile: Bad magic number for central directory

This error seems to only happen when working on Windows, and happens more frequently when a task requires to access the same file more than once in the same task, or is used by different tasks as a Target even when there are no parallel workers running. Code runs without errors on Linux.
Has anyone else experienced this behavior?
I'm trying to create pandas DataFrames from Excel file sheets but I get a BadZipFile error instead sometimes.
",-1,-1,-1.0
74518031,Luigi: Running Tasks in a Docker Image,"I am testing Luigi's abilities for running tasks in a Docker container, i.e. I would like Luigi to spawn a container from a given Docker image and execute a task therein.
As far as I understood, there is luigi.contrib.docker_runner.DockerTask for this purpose. I tried to modify its command and add an output:
import luigi
from luigi.contrib.docker_runner import DockerTask

class Task(DockerTask):
    def output(self):
        return luigi.LocalTarget(&quot;bla.txt&quot;)
    def command(self):
        return f&quot;touch {self.output()}&quot;

if __name__ == &quot;__main__&quot;:
luigi.build([Task()], workers=2, local_scheduler=True) 

But I am getting

It seems that there is a TypeError in docker. Is my use of DockerTask erroneous? Unfortunately, I cannot find any examples for use cases...
",-1,-1,-1.0
75239530,ı writing data process pipeline with luigi but ı get error,"import os
import luigi
import pandas as pd
import requests as req
from bs4 import BeautifulSoup

class DownloadData(luigi.Task):

    def run(self):
        site = req.get(&quot;http://www.gutenberg.org/browse/scores/top&quot;).text
        with self.output().open(&quot;w&quot;) as f:
            f.write(site)

    def output(self):
        return luigi.LocalTarget(&quot;raw_data.txt&quot;)

    def complete(self):
        return os.path.exists(self.output().path)

class PrePData(luigi.Task):

    def requires(self):
        return DownloadData()

    def run(self):
        data = self.requires()
        bs4ed_data = []
        if data.contains(&quot;&lt;!DOCTYPE html&gt;&quot;):
            bs4ed_data.append()(data,&quot;html.parser&quot;)

        else:
            print(&quot;can not found any problem in this data&quot;)

        return bs4ed_data

    def output(self):
        return luigi.local_target(&quot;data.txt&quot;)

    def complete(self):
        return os.path.exists(self.output().path)

    def on_success(self):
        print(&quot;data preprocessing completed successfully&quot;)

    def on_failure(self):
        print(&quot;data preprocessing failed&quot;)

class RunAllTasks(luigi.WrapperTask):
    def requires(self):
        return [DownloadData(),PrePData()]


ı run this python file with this command in my terminal
python -m luigi --module PipeLineofETL-A RunAllTasks --local-scheduler --workers 4

and error
python -m luigi --module PipeLineofETL-A RunAllTasks --local-scheduler --workers 4
DEBUG: Checking if RunAllTasks() is complete
WARNING: Will not run RunAllTasks() or any dependencies due to error in complete() method:
Traceback (most recent call last):
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py&quot;, line 429, in check_complete
    is_complete = check_complete_cached(task, completion_cache)
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py&quot;, line 414, in check_complete_cached
    is_complete = task.complete()
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/task.py&quot;, line 845, in complete
    return all(r.complete() for r in flatten(self.requires()))
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/task.py&quot;, line 845, in &lt;genexpr&gt;
    return all(r.complete() for r in flatten(self.requires()))
  File &quot;/home/tuna/Belgeler/GitLab/extractdata/ChatGPT's Basic tasks/PipeLineofETL-A.py&quot;, line 40, in complete
    return os.path.exists(self.output().path)
  File &quot;/home/tuna/Belgeler/GitLab/extractdata/ChatGPT's Basic tasks/PipeLineofETL-A.py&quot;, line 37, in output
    return luigi.local_target(&quot;data.txt&quot;)
TypeError: 'module' object is not callable

INFO: Informed scheduler that task   RunAllTasks__99914b932b   has status   UNKNOWN
INFO: Done scheduling tasks
INFO: Running Worker with 4 processes
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
INFO: Worker Worker(salt=1404147006, workers=4, host=tunapc, username=tuna, pid=9077) was stopped. Shutting down Keep-Alive thread
INFO: 
===== Luigi Execution Summary =====

Scheduled 1 tasks of which:
* 1 failed scheduling:
    - 1 RunAllTasks()

Did not run any tasks
This progress looks :( because there were tasks whose scheduling failed

===== Luigi Execution Summary =====


import os
import luigi
import pandas as pd
import requests as req
from bs4 import BeautifulSoup

class DownloadData(luigi.Task):

    def run(self):
        site = req.get(&quot;http://www.gutenberg.org/browse/scores/top&quot;).text
        with self.output().open(&quot;w&quot;) as f:
            f.write(site)

    def output(self):
        return luigi.LocalTarget(&quot;raw_data.txt&quot;)

    def complete(self):
        return os.path.exists(self.output().path)

class PrePData(luigi.Task):

    def requires(self):
        return DownloadData()

    def run(self):
        data = self.requires()
        bs4ed_data = []
        if data.contains(&quot;&lt;!DOCTYPE html&gt;&quot;):
            bs4ed_data.append()(data,&quot;html.parser&quot;)

        else:
            print(&quot;can not found any problem in this data&quot;)

        return bs4ed_data


class RunAllTasks(luigi.WrapperTask):
    def requires(self):
        return [DownloadData(),PrePData()]

ı write same command in terminal and ı get this error
DEBUG: Checking if RunAllTasks() is complete
/home/tuna/.local/lib/python3.10/site-packages/luigi/task.py:845: UserWarning: Task PrePData() without outputs has no custom complete() method
  return all(r.complete() for r in flatten(self.requires()))
DEBUG: Checking if DownloadData() is complete
DEBUG: Checking if PrePData() is complete
/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py:414: UserWarning: Task PrePData() without outputs has no custom complete() method
  is_complete = task.complete()
INFO: Informed scheduler that task   RunAllTasks__99914b932b   has status   PENDING
INFO: Informed scheduler that task   PrePData__99914b932b   has status   PENDING
INFO: Informed scheduler that task   DownloadData__99914b932b   has status   DONE
INFO: Done scheduling tasks
INFO: Running Worker with 4 processes
DEBUG: Asking scheduler for work...
DEBUG: Pending tasks: 2
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: PrePData__99914b932b is currently run by worker Worker(salt=3997262702, workers=4, host=tunapc, username=tuna, pid=10617)
INFO: [pid 10624] Worker Worker(salt=3997262702, workers=4, host=tunapc, username=tuna, pid=10617) running   PrePData()
ERROR: [pid 10624] Worker Worker(salt=3997262702, workers=4, host=tunapc, username=tuna, pid=10617) failed    PrePData()
Traceback (most recent call last):
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py&quot;, line 198, in run
    new_deps = self._run_get_new_deps()
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py&quot;, line 138, in _run_get_new_deps
    task_gen = self.task.run()
  File &quot;/home/tuna/Belgeler/GitLab/extractdata/ChatGPT's Basic tasks/PipeLineofETL-A.py&quot;, line 28, in run
    if data.contains(&quot;&lt;!DOCTYPE html&gt;&quot;):
AttributeError: 'DownloadData' object has no attribute 'contains'
INFO: Informed scheduler that task   PrePData__99914b932b   has status   FAILED
DEBUG: Asking scheduler for work...
DEBUG: Done
DEBUG: There are no more tasks to run at this time
DEBUG: There are 2 pending tasks possibly being run by other workers
DEBUG: There are 2 pending tasks unique to this worker
DEBUG: There are 2 pending tasks last scheduled by this worker
INFO: Worker Worker(salt=3997262702, workers=4, host=tunapc, username=tuna, pid=10617) was stopped. Shutting down Keep-Alive thread
INFO: 
===== Luigi Execution Summary =====

Scheduled 3 tasks of which:
* 1 complete ones were encountered:
    - 1 DownloadData()
* 1 failed:
    - 1 PrePData()
* 1 were left pending, among these:
    * 1 had failed dependencies:
        - 1 RunAllTasks()

This progress looks :( because there were failed tasks

===== Luigi Execution Summary =====


when ı added output() method to DownloadData in requires function, ı get this error
DEBUG: Checking if RunAllTasks() is complete
/home/tuna/.local/lib/python3.10/site-packages/luigi/task.py:845: UserWarning: Task PrePData() without outputs has no custom complete() method
  return all(r.complete() for r in flatten(self.requires()))
DEBUG: Checking if DownloadData() is complete
DEBUG: Checking if PrePData() is complete
/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py:414: UserWarning: Task PrePData() without outputs has no custom complete() method
  is_complete = task.complete()
INFO: Informed scheduler that task   RunAllTasks__99914b932b   has status   PENDING
ERROR: Luigi unexpected framework error while scheduling RunAllTasks()
Traceback (most recent call last):
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py&quot;, line 794, in add
    for next in self._add(item, is_complete):
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py&quot;, line 892, in _add
    self._validate_dependency(d)
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py&quot;, line 917, in _validate_dependency
    raise Exception('requires() can not return Target objects. Wrap it in an ExternalTask class')
Exception: requires() can not return Target objects. Wrap it in an ExternalTask class
INFO: Worker Worker(salt=6506578324, workers=4, host=tunapc, username=tuna, pid=10710) was stopped. Shutting down Keep-Alive thread
ERROR: Uncaught exception in luigi
Traceback (most recent call last):
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/retcodes.py&quot;, line 75, in run_with_retcodes
    worker = luigi.interface._run(argv).worker
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/interface.py&quot;, line 213, in _run
    return _schedule_and_run([cp.get_task_obj()], worker_scheduler_factory)
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/interface.py&quot;, line 171, in _schedule_and_run
    success &amp;= worker.add(t, env_params.parallel_scheduling, env_params.parallel_scheduling_processes)
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py&quot;, line 794, in add
    for next in self._add(item, is_complete):
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py&quot;, line 892, in _add
    self._validate_dependency(d)
  File &quot;/home/tuna/.local/lib/python3.10/site-packages/luigi/worker.py&quot;, line 917, in _validate_dependency
    raise Exception('requires() can not return Target objects. Wrap it in an ExternalTask class')
Exception: requires() can not return Target objects. Wrap it in an ExternalTask class

",-1,-1,-1.0
