,Id,Title,Body,RatingsSentiCR,RatingsGPT35,RatingsGPTFineTuned,merged,topic
0,3559698,how do i filter out non-numeric values in a text field in teradata?,"oI have a teradata table with about 10 million records in it, that stores a numeric id field as a varchar.  i need to transfer the values in this field to a bigint column in another table, but i can't simply say cast(id_field as bigint) because i get an invalid character error.  looking through the values, i find that there could be a character at any position in the string, so let's say the string is varchar(18) i could filter out invalid rows like so :

     where substr(id_field,1,1) not in (/*big,ugly array of non-numeric chars*/)
     and substr(id_field,2,1) not in (/*big,ugly array of non-numeric chars*/)

etc, etc... 


then the cast would work, but this is not feasible in the long run. it's slow and if the string has 18 possible characters, it makes the query unreadable.  how can i filter out rows that have a value in this field that will not cast as a bigint without checking each character individually for an array of non-numeric characters?

example values would be 

   123abc464
   a2.3v65
   a_356087
   ........
   000000000
   BOB KNIGHT
   1235468099


the values follow no specific patterns, I simply need to filter out the ones that contain ANY non-numeric data.
          123456789 is okay but 123.abc_c3865 is not...
",-1,-1,-1.0,"oI have a teradata table with about 10 million records in it, that stores a numeric id field as a varchar.  i need to transfer the values in this field to a bigint column in another table, but i can't simply say cast(id_field as bigint) because i get an invalid character error.  looking through the values, i find that there could be a character at any position in the string, so let's say the string is varchar(18) i could filter out invalid rows like so :

     where substr(id_field,1,1) not in (/*big,ugly array of non-numeric chars*/)
     and substr(id_field,2,1) not in (/*big,ugly array of non-numeric chars*/)

etc, etc... 


then the cast would work, but this is not feasible in the long run. it's slow and if the string has 18 possible characters, it makes the query unreadable.  how can i filter out rows that have a value in this field that will not cast as a bigint without checking each character individually for an array of non-numeric characters?

example values would be 

   123abc464
   a2.3v65
   a_356087
   ........
   000000000
   BOB KNIGHT
   1235468099


the values follow no specific patterns, I simply need to filter out the ones that contain ANY non-numeric data.
          123456789 is okay but 123.abc_c3865 is not...
",3
1,3598327,Problem reading special characters from teradata - JDBC,"I use teradata and the below query outputs ""Altlüd"" when run using a teradata client.

select name as name  from MYTABLE where selector=?


Whereas, I get ""Altl?d"" as the output when I try to execute the query using a java client(jdbc with teradata drivers). I am using ""UTF-8"" charset and I have also tried Latin charset with no luck.

I have also tried this to troubleshoot.

while (rs.next()) {
 System.out.println(rs.getString(1));
 Reader rd = rs.getCharacterStream(1);
 int charr = rd.read();
 while (charr &gt;= 0) {
    System.out.println(charr + "" = "" + ((char) charr));
    charr = rd.read();
 }
}


And the output is 

Altl?dersdorf
65 = A
108 = l
116 = t
108 = l
65533 = ?
100 = d

If you look at the output produced, the int value for the spl character is 65533 which shouldn't be the case. 

Infact it returns 65533 for all the special characters.

Any clues/pointers will be appreciated. Thanks!!!
",-1,-1,-1.0,"I use teradata and the below query outputs ""Altlüd"" when run using a teradata client.

select name as name  from MYTABLE where selector=?


Whereas, I get ""Altl?d"" as the output when I try to execute the query using a java client(jdbc with teradata drivers). I am using ""UTF-8"" charset and I have also tried Latin charset with no luck.

I have also tried this to troubleshoot.

while (rs.next()) {
 System.out.println(rs.getString(1));
 Reader rd = rs.getCharacterStream(1);
 int charr = rd.read();
 while (charr &gt;= 0) {
    System.out.println(charr + "" = "" + ((char) charr));
    charr = rd.read();
 }
}


And the output is 

Altl?dersdorf
65 = A
108 = l
116 = t
108 = l
65533 = ?
100 = d

If you look at the output produced, the int value for the spl character is 65533 which shouldn't be the case. 

Infact it returns 65533 for all the special characters.

Any clues/pointers will be appreciated. Thanks!!!
",3
2,4751146,Oracle XE Database link to Teradata using ODBC,"I installed Oracle Server Express 10g on my computer (WinXP). I want to create a database link to Teradata using ODBC. I've created (non-ODBC) database links to other Oracle databases successfully. However, I can't seem to get the Teradata database link to work.

Here's what I did:

1) Created an ODBC Connection in Windows to Teradata using Teradata's ODBC driver version 13. Tested that it works by connecting to the database using Teradata SQL Assistant. Called the connection LPS_PROD_VIEW. I saved my Login details in the ODBC settings.

2) Edited listener.ora 
In the SID_LIST_LISTENER section: 

(SID_DESC =
  (SID_NAME = LPS_PROD_VIEW)
  (ORACLE_HOME = C:\oraclexe\app\oracle\product\10.2.0\server)
  (PROGRAM = hsodbc)


In the LISTENER section

     (ADDRESS = (PROTOCOL = TCP)(HOST = localhost)(PORT = 1524)


3) In the ...hs\admin\ folder, added initLPS_PROD_VIEW.ora file. Contents:

HS_FDS_CONNECT_INFO = LPS_PROD_VIEW
HS_FDS_TRACE_LEVEL = ON


4) Added an entry in TNSnames.ora (both in the XE server directory and a seperate 10g directory which I had previously before installing Oracle XE).

BMW = 
  (DESCRIPTION = 
  (ADDRESS_LIST = 
  (ADDRESS = (PROTOCOL = TCP)(Host = localhost)(Port = 1524))
  )(CONNECT_DATA = 
  (SID = LPS_PROD_VIEW)(HS=OK)
  )


5) Restarted Oracle listener services through services.msc.

6) Connected to local database to create the database link by doing

Create database link TERADATA connect to &lt;username&gt; identified by &lt;password&gt; using 'LPS_PROD_VIEW' 


7) Attempt to run queries but get an ORA-12154: TNS: Could not resolve the connect identifier specified. 

What am I doing wrong? Does HS support Teradata ODBC ver 13? 

Thanks in advance and appreciate your help!
",1,-1,-1.0,"I installed Oracle Server Express 10g on my computer (WinXP). I want to create a database link to Teradata using ODBC. I've created (non-ODBC) database links to other Oracle databases successfully. However, I can't seem to get the Teradata database link to work.

Here's what I did:

1) Created an ODBC Connection in Windows to Teradata using Teradata's ODBC driver version 13. Tested that it works by connecting to the database using Teradata SQL Assistant. Called the connection LPS_PROD_VIEW. I saved my Login details in the ODBC settings.

2) Edited listener.ora 
In the SID_LIST_LISTENER section: 

(SID_DESC =
  (SID_NAME = LPS_PROD_VIEW)
  (ORACLE_HOME = C:\oraclexe\app\oracle\product\10.2.0\server)
  (PROGRAM = hsodbc)


In the LISTENER section

     (ADDRESS = (PROTOCOL = TCP)(HOST = localhost)(PORT = 1524)


3) In the ...hs\admin\ folder, added initLPS_PROD_VIEW.ora file. Contents:

HS_FDS_CONNECT_INFO = LPS_PROD_VIEW
HS_FDS_TRACE_LEVEL = ON


4) Added an entry in TNSnames.ora (both in the XE server directory and a seperate 10g directory which I had previously before installing Oracle XE).

BMW = 
  (DESCRIPTION = 
  (ADDRESS_LIST = 
  (ADDRESS = (PROTOCOL = TCP)(Host = localhost)(Port = 1524))
  )(CONNECT_DATA = 
  (SID = LPS_PROD_VIEW)(HS=OK)
  )


5) Restarted Oracle listener services through services.msc.

6) Connected to local database to create the database link by doing

Create database link TERADATA connect to &lt;username&gt; identified by &lt;password&gt; using 'LPS_PROD_VIEW' 


7) Attempt to run queries but get an ORA-12154: TNS: Could not resolve the connect identifier specified. 

What am I doing wrong? Does HS support Teradata ODBC ver 13? 

Thanks in advance and appreciate your help!
",1
3,6017422,Threads hanging permanently on JDBC Teradata requests,"I'm using JDBC to query a Teradata server. There are up to 100 simultaneous requests, each one using a fresh connection, and closing it at the end. After some hours of work, some of the threads performing the requests get stuck indefinitely. Eventually a system restart is needed.
From inspecting the call stacks, I see that the threads are in a socket read state, and that it happens when preparing a statement or when closing the connection:

Case 1:

java.lang.Thread.State: RUNNABLE
               at java.net.SocketInputStream.socketRead0(Native Method)
               at java.net.SocketInputStream.read(SocketInputStream.java:129)
               at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:649)
               at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:818)
               at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:125)
               at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:112)
               at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:103)
               at com.teradata.jdbc.jdbc_4.Statement.executeStatement(Statement.java:340)
               at com.teradata.jdbc.jdbc_4.Statement.prepareRequest(Statement.java:507)
               - locked &lt;0x00002aab4f787518&gt; (a com.teradata.jdbc.jdbc_4.PreparedStatement)
               at com.teradata.jdbc.jdbc_4.PreparedStatement.&lt;init&gt;(PreparedStatement.java:66)
               at com.teradata.jdbc.jdbc_4.TDSession.createPreparedStatement(TDSession.java:723)
               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalPreparedStatement.&lt;init&gt;(TeraLocalPreparedStatement.java:89)
               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:333)
               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:152)
...


Case 2:

java.lang.Thread.State: RUNNABLE
               at java.net.SocketInputStream.socketRead0(Native Method)
               at java.net.SocketInputStream.read(SocketInputStream.java:129)
               at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:649)
               at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:818)
               at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:794)
               at com.teradata.jdbc.jdbc.GenericLogOffRspState.action(GenericLogOffRspState.java:66)
               at com.teradata.jdbc.jdbc.GenericLogoffController.run(GenericLogoffController.java:43)
               - locked &lt;..&gt; (a com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF)
               at com.teradata.jdbc.jdbc_4.TDSession.close(TDSession.java:476)
               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.close(TeraLocalConnection.java:259)
...


I'm using the JDBC Teradata driver version 13.10.00.10.

Any idea why it happens?
This issue is very painful for us and any help will be appreciated.

Thanks!
",1,-1,-1.0,"I'm using JDBC to query a Teradata server. There are up to 100 simultaneous requests, each one using a fresh connection, and closing it at the end. After some hours of work, some of the threads performing the requests get stuck indefinitely. Eventually a system restart is needed.
From inspecting the call stacks, I see that the threads are in a socket read state, and that it happens when preparing a statement or when closing the connection:

Case 1:

java.lang.Thread.State: RUNNABLE
               at java.net.SocketInputStream.socketRead0(Native Method)
               at java.net.SocketInputStream.read(SocketInputStream.java:129)
               at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:649)
               at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:818)
               at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:125)
               at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:112)
               at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:103)
               at com.teradata.jdbc.jdbc_4.Statement.executeStatement(Statement.java:340)
               at com.teradata.jdbc.jdbc_4.Statement.prepareRequest(Statement.java:507)
               - locked &lt;0x00002aab4f787518&gt; (a com.teradata.jdbc.jdbc_4.PreparedStatement)
               at com.teradata.jdbc.jdbc_4.PreparedStatement.&lt;init&gt;(PreparedStatement.java:66)
               at com.teradata.jdbc.jdbc_4.TDSession.createPreparedStatement(TDSession.java:723)
               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalPreparedStatement.&lt;init&gt;(TeraLocalPreparedStatement.java:89)
               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:333)
               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:152)
...


Case 2:

java.lang.Thread.State: RUNNABLE
               at java.net.SocketInputStream.socketRead0(Native Method)
               at java.net.SocketInputStream.read(SocketInputStream.java:129)
               at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:649)
               at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:818)
               at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:794)
               at com.teradata.jdbc.jdbc.GenericLogOffRspState.action(GenericLogOffRspState.java:66)
               at com.teradata.jdbc.jdbc.GenericLogoffController.run(GenericLogoffController.java:43)
               - locked &lt;..&gt; (a com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF)
               at com.teradata.jdbc.jdbc_4.TDSession.close(TDSession.java:476)
               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.close(TeraLocalConnection.java:259)
...


I'm using the JDBC Teradata driver version 13.10.00.10.

Any idea why it happens?
This issue is very painful for us and any help will be appreciated.

Thanks!
",0
4,6110652,Executing teradata stored procedure from SAS,"I'm trying to execute Teradata stored procedure from SAS ,but am failed to find the correct syntax,
here is example of what i tried: 

libname tbconn teradata server=""10.11.18.15"" database=""yy"" user=x pw=xx;
execute tbconn.ProcedureName(date '2011-03-31');

and i also tried to use call command instead of excecute but it didnt work also.
any idea people.
",1,-1,-1.0,"I'm trying to execute Teradata stored procedure from SAS ,but am failed to find the correct syntax,
here is example of what i tried: 

libname tbconn teradata server=""10.11.18.15"" database=""yy"" user=x pw=xx;
execute tbconn.ProcedureName(date '2011-03-31');

and i also tried to use call command instead of excecute but it didnt work also.
any idea people.
",3
5,6524789,Connecting to Teradata via Perl,"Has anyone had any success with this? There aren't a great deal of references online and I've exhausted every relevant result on Google. Here's my script:

#!/usr/bin/perl

use DBI;
use DBD::ODBC;

$user = ""user"";
$pw = ""pw"";
$ip = ""192.168.1.0""

#DBI-&gt;trace(DBD::ODBC-&gt;parse_trace_flags('odbconnection'));

#my $connect_attrs = { PrintError =&gt; 0, RaiseError =&gt; 1, AutoCommit =&gt; 1 };

my $dbh = DBI-&gt;connect(""dbi:ODBC:$ip"", $user, $pw);


The error message:

DBI connect('192.168.1.0','user',...) failed: (no error string) at ./teradata.pl line 13


The two lines that are commented out are leftover from my previous fruitless attempts to connect to the DB.

UPDATE: Here are the previous efforts I made with the DBD module.

#!/usr/bin/perl

use DBI;

$user = ""xxxx"";
$pw = ""xxxx"";

my $dbh = DBI-&gt;connect(""dbi:Teradata:tdsn"", $user, $pw);


Error: 

DBI connect('tdsn','xxxx',...) failed: Unable to get host address. at ./teradata.pl line 12




Second Attempt:

#!/usr/bin/perl

use DBI;

$user = ""xxxx"";
$pw = ""xxxx"";

my $dbh = DBI-&gt;connect(""dbi:Teradata:192.168.1.0"", $user, $pw);


Error:

DBI connect('192.168.1.0','xxxx',...) failed: Deprecated logons are not allowed by administrator.  Upgrade client software to latest version. at ./teradata.pl line 12




Third...

#!/usr/bin/perl

use DBI;
use DBD::ODBC;

$user = ""xxxx"";
$pw = ""xxxx"";

my $dbh = DBI-&gt;connect(""dbi:ODBC:tdsn"", $user, $pw);


.odbc.ini

[ODBC]
InstallDir              = /usr/odbc
Trace           = 0
TraceDll                = /usr/odbc/lib/odbctrac.so
TraceFile               = /home/xxxx/odbctrace.log
TraceAutoStop           = 0

[ODBC Data Sources]
default         = tdata.so
testdsn         = tdata.so

[default]
Driver          = /usr/odbc/drivers/tdata.so
Description             = Default DSN is Teradata 5100
DBCName         = **ip_addr**
LastUser                = DLPStats
Username                = xxxx
Password                = xxxx
Database                = MSS_TEMP
DefaultDatabase         = MSS_TEMP

[tdsn]
Driver=/usr/odbc/drivers/tdata.so
Description=Teradata running Teradata V1R5.2
DBCName=**ip_addr**
LastUser=
Username=xxxx
Password=xxxx
Database=
DefaultDatabase=


Error:

DBI connect('tdsn','xxxx',...) failed: (no error string) at ./teradata.pl line 13




odbcinst.ini

[ODBC DRIVERS]
Teradata=Installed

[Teradata]
Driver=/usr/odbc/drivers/tdata.so
APILevel=CORE
ConnectFunctions=YYY
DriverODBCVer=3.51
SQLLevel=1

",-1,-1,-1.0,"Has anyone had any success with this? There aren't a great deal of references online and I've exhausted every relevant result on Google. Here's my script:

#!/usr/bin/perl

use DBI;
use DBD::ODBC;

$user = ""user"";
$pw = ""pw"";
$ip = ""192.168.1.0""

#DBI-&gt;trace(DBD::ODBC-&gt;parse_trace_flags('odbconnection'));

#my $connect_attrs = { PrintError =&gt; 0, RaiseError =&gt; 1, AutoCommit =&gt; 1 };

my $dbh = DBI-&gt;connect(""dbi:ODBC:$ip"", $user, $pw);


The error message:

DBI connect('192.168.1.0','user',...) failed: (no error string) at ./teradata.pl line 13


The two lines that are commented out are leftover from my previous fruitless attempts to connect to the DB.

UPDATE: Here are the previous efforts I made with the DBD module.

#!/usr/bin/perl

use DBI;

$user = ""xxxx"";
$pw = ""xxxx"";

my $dbh = DBI-&gt;connect(""dbi:Teradata:tdsn"", $user, $pw);


Error: 

DBI connect('tdsn','xxxx',...) failed: Unable to get host address. at ./teradata.pl line 12




Second Attempt:

#!/usr/bin/perl

use DBI;

$user = ""xxxx"";
$pw = ""xxxx"";

my $dbh = DBI-&gt;connect(""dbi:Teradata:192.168.1.0"", $user, $pw);


Error:

DBI connect('192.168.1.0','xxxx',...) failed: Deprecated logons are not allowed by administrator.  Upgrade client software to latest version. at ./teradata.pl line 12




Third...

#!/usr/bin/perl

use DBI;
use DBD::ODBC;

$user = ""xxxx"";
$pw = ""xxxx"";

my $dbh = DBI-&gt;connect(""dbi:ODBC:tdsn"", $user, $pw);


.odbc.ini

[ODBC]
InstallDir              = /usr/odbc
Trace           = 0
TraceDll                = /usr/odbc/lib/odbctrac.so
TraceFile               = /home/xxxx/odbctrace.log
TraceAutoStop           = 0

[ODBC Data Sources]
default         = tdata.so
testdsn         = tdata.so

[default]
Driver          = /usr/odbc/drivers/tdata.so
Description             = Default DSN is Teradata 5100
DBCName         = **ip_addr**
LastUser                = DLPStats
Username                = xxxx
Password                = xxxx
Database                = MSS_TEMP
DefaultDatabase         = MSS_TEMP

[tdsn]
Driver=/usr/odbc/drivers/tdata.so
Description=Teradata running Teradata V1R5.2
DBCName=**ip_addr**
LastUser=
Username=xxxx
Password=xxxx
Database=
DefaultDatabase=


Error:

DBI connect('tdsn','xxxx',...) failed: (no error string) at ./teradata.pl line 13




odbcinst.ini

[ODBC DRIVERS]
Teradata=Installed

[Teradata]
Driver=/usr/odbc/drivers/tdata.so
APILevel=CORE
ConnectFunctions=YYY
DriverODBCVer=3.51
SQLLevel=1

",1
6,8619322,Teradata: How can you add an identity column to an existing table?,"I need to add an identity column to an existing table with this SQL:

alter table app.employee 
add ID INTEGER GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1 MINVALUE 0 MAXVALUE 100000000 NO CYCLE)


I can create new tables with an identity column just fine, but the above script gives me the following error: 


  ALTER TABLE Failed. 3706: Syntaxt error: Cannot add new Identity
  Column option


Teradata database is severely lacking in online support and I've only come across one option which is to basically create a copy of the table with the identity column and do a mass insert from the old table to the new one and change all references to the new table.  I find it difficult to believe that this is the only possible way to do this.

What are my options here?
",-1,-1,-1.0,"I need to add an identity column to an existing table with this SQL:

alter table app.employee 
add ID INTEGER GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1 MINVALUE 0 MAXVALUE 100000000 NO CYCLE)


I can create new tables with an identity column just fine, but the above script gives me the following error: 


  ALTER TABLE Failed. 3706: Syntaxt error: Cannot add new Identity
  Column option


Teradata database is severely lacking in online support and I've only come across one option which is to basically create a copy of the table with the identity column and do a mass insert from the old table to the new one and change all references to the new table.  I find it difficult to believe that this is the only possible way to do this.

What are my options here?
",3
7,10909824,Error after upgrading from Teradata 12 to Teradata 13....terasso.dll,"We are currently upgrading our Teradata clients from v12 to v13. For that, the old installations of Teradata 12 were uninstalled from the system and TTU13 was installed. 

After installation when I try to add a ODBC connection using the new Teradata driver, it gives me the following error.


  Unknown error occurred in terasso library


Any help would be highly appreciated.
",-1,-1,-1.0,"We are currently upgrading our Teradata clients from v12 to v13. For that, the old installations of Teradata 12 were uninstalled from the system and TTU13 was installed. 

After installation when I try to add a ODBC connection using the new Teradata driver, it gives me the following error.


  Unknown error occurred in terasso library


Any help would be highly appreciated.
",1
8,10952238,Sample of data within groups - Teradata,"Found a similar question but without an answer that worked succuessfully.
I need to select a sample of 50 of each status type within a single table.
TABLE1
MEMBER  STATUS
1234       A
1324       A
3424       R
3432       S
3232       R
2783       A
2413       S
4144       R
2387       S

I tried:

SEL Member, status
FROM TABLE1 Qualify Row_Number ( ) OVER (PARTITION
BY status ORDER BY random (1,10000)) &lt;=50

As suggested in the previous question/answer but Teradata does not like RANDOM in an Aggregate or Ordered Analytical Function.
",-1,-1,-1.0,"Found a similar question but without an answer that worked succuessfully.
I need to select a sample of 50 of each status type within a single table.
TABLE1
MEMBER  STATUS
1234       A
1324       A
3424       R
3432       S
3232       R
2783       A
2413       S
4144       R
2387       S

I tried:

SEL Member, status
FROM TABLE1 Qualify Row_Number ( ) OVER (PARTITION
BY status ORDER BY random (1,10000)) &lt;=50

As suggested in the previous question/answer but Teradata does not like RANDOM in an Aggregate or Ordered Analytical Function.
",3
9,10987152,Teradata Update Table from Select Statement,"Sorry if the title is unclear. Basically I'm trying to select certain records from multiple tables then update a certain column value for the returned records.

T-SQL Implementation

    UPDATE 
        CUSTOMERS
    SET
        LIKES_US = 'Y'
    FROM
        RESTAURANT REST INNER JOIN CUSTOMERS CUST ON REST.LINK_ID = CUST.LINK_ID
        WHERE
        REST.REST_TYPE = 'Diner' AND CUST.LIKES_US IS NULL


Oracle

    UPDATE 
       (SELECT CUST.LIKES_US
        FROM CUSTOMERS CUST INNER JOIN RESTAURANT REST ON CUST.LINK_ID=REST.LINK_ID
        WHERE REST.REST_TYPE = 'Diner' AND CUST.LIKES_US IS NULL) NEW_CUST
    SET
        NEW_CUST.LIKES_US = 'Y';


I am tried doing the same thing in Teradata as I did in Oracle but I get the following error:

Executed as Single statement.  Failed [3707 : 42000] Syntax error, expected something like a name or a Unicode delimited identifier or an 'UDFCALLNAME' keyword between the 'UPDATE' keyword and '('. 
Elapsed time = 00:00:00.003 

STATEMENT 1: Unknown failed. 


I looked online for the solution but had no luck.
",-1,-1,-1.0,"Sorry if the title is unclear. Basically I'm trying to select certain records from multiple tables then update a certain column value for the returned records.

T-SQL Implementation

    UPDATE 
        CUSTOMERS
    SET
        LIKES_US = 'Y'
    FROM
        RESTAURANT REST INNER JOIN CUSTOMERS CUST ON REST.LINK_ID = CUST.LINK_ID
        WHERE
        REST.REST_TYPE = 'Diner' AND CUST.LIKES_US IS NULL


Oracle

    UPDATE 
       (SELECT CUST.LIKES_US
        FROM CUSTOMERS CUST INNER JOIN RESTAURANT REST ON CUST.LINK_ID=REST.LINK_ID
        WHERE REST.REST_TYPE = 'Diner' AND CUST.LIKES_US IS NULL) NEW_CUST
    SET
        NEW_CUST.LIKES_US = 'Y';


I am tried doing the same thing in Teradata as I did in Oracle but I get the following error:

Executed as Single statement.  Failed [3707 : 42000] Syntax error, expected something like a name or a Unicode delimited identifier or an 'UDFCALLNAME' keyword between the 'UPDATE' keyword and '('. 
Elapsed time = 00:00:00.003 

STATEMENT 1: Unknown failed. 


I looked online for the solution but had no luck.
",3
10,11231313,System.Net.Sockets.SocketException when trying to fill DataTable with TdDataAdapter.Fill() (Teradata DataAdapter),"I've seen other people reporting the System.Net.Sockets.SocketException exception, but they largely involve web services.  We're not calling any web services.  In fact, this ASP.NET app is a single-tier app with the UI and data layer contained in a single assembly.  To retrieve data, we manually open a connection, create a command, and execute a reader or use a data adapter to fill a data table before delivering it to the page.

A few times per month when trying to execute a command, we get System.Net.Sockets.SocketException.  I have no idea what could be causing this.  As stated in the subject, we're accessing Teradata database, so we use TdConnection, TdCommand, TdDataAdapter.

cmdSolutionName = New TdCommand(sSql, Con)
daSolutionName.SelectCommand = cmdSolutionName
daSolutionName.Fill(tmpTable) 'Exception is thrown here


Does anyone know what could cause this exception when working directly with a database connection and not a web service?
",-1,-1,-1.0,"I've seen other people reporting the System.Net.Sockets.SocketException exception, but they largely involve web services.  We're not calling any web services.  In fact, this ASP.NET app is a single-tier app with the UI and data layer contained in a single assembly.  To retrieve data, we manually open a connection, create a command, and execute a reader or use a data adapter to fill a data table before delivering it to the page.

A few times per month when trying to execute a command, we get System.Net.Sockets.SocketException.  I have no idea what could be causing this.  As stated in the subject, we're accessing Teradata database, so we use TdConnection, TdCommand, TdDataAdapter.

cmdSolutionName = New TdCommand(sSql, Con)
daSolutionName.SelectCommand = cmdSolutionName
daSolutionName.Fill(tmpTable) 'Exception is thrown here


Does anyone know what could cause this exception when working directly with a database connection and not a web service?
",0
11,11727972,"Teradata identity column and ""Duplicate unique prime key error in dbname.tablename""","I created a table using the below definition for a Teradata identity column:


      ID INTEGER GENERATED BY DEFAULT AS IDENTITY
           (START WITH 1 
            INCREMENT BY 1 
            MINVALUE 0 
            MAXVALUE 100000000 
            NO CYCLE),
----
      UNIQUE PRIMARY INDEX ( ID )


For several months, the ID column has been working properly, automatically generating a unique value for the column.  Over the past month, however, ELMAH has been intermittently reporting the following exception from our .NET 4.0 ASP.NET app:

Teradata.Client.Provider.TdException: [Teradata Database] [2801] Duplicate unique prime key error in DATABASENAME.TABLENAME.

I was able to replicate it by opening SQL Assistant and inserting a bunch of records into the table with raw SQL.  As expected, most of the time it would insert successfully, but other times it would throw the above exception.

It appears that this error is occuring because Teradata is trying to generate a value for this column that it has previously generated.

Does anyone have any idea how to get to the bottom of what's happening?  At the very least, I'd like some way to debug the issue a bit deeper.
",-1,-1,-1.0,"I created a table using the below definition for a Teradata identity column:


      ID INTEGER GENERATED BY DEFAULT AS IDENTITY
           (START WITH 1 
            INCREMENT BY 1 
            MINVALUE 0 
            MAXVALUE 100000000 
            NO CYCLE),
----
      UNIQUE PRIMARY INDEX ( ID )


For several months, the ID column has been working properly, automatically generating a unique value for the column.  Over the past month, however, ELMAH has been intermittently reporting the following exception from our .NET 4.0 ASP.NET app:

Teradata.Client.Provider.TdException: [Teradata Database] [2801] Duplicate unique prime key error in DATABASENAME.TABLENAME.

I was able to replicate it by opening SQL Assistant and inserting a bunch of records into the table with raw SQL.  As expected, most of the time it would insert successfully, but other times it would throw the above exception.

It appears that this error is occuring because Teradata is trying to generate a value for this column that it has previously generated.

Does anyone have any idea how to get to the bottom of what's happening?  At the very least, I'd like some way to debug the issue a bit deeper.
",3
12,11800136,Teradata FastExport Script - cannot get it to run - The system cannot find the file specified,"I am brand new to teradata. I wrote a fastexport script to take some data from a db and export it to some excel files.
The script is all good. I just can't run it!

I am running the cmd fexp &lt; C:\Documents\ScriptName (that is not the actual path)

Where ScriptName is a .txt file containing the script.

I get the error:


  ""The system cannot find the file specified""


I have tried changing the location of the file and such but always get the same error.

What am I missing here?
",-1,-1,-1.0,"I am brand new to teradata. I wrote a fastexport script to take some data from a db and export it to some excel files.
The script is all good. I just can't run it!

I am running the cmd fexp &lt; C:\Documents\ScriptName (that is not the actual path)

Where ScriptName is a .txt file containing the script.

I get the error:


  ""The system cannot find the file specified""


I have tried changing the location of the file and such but always get the same error.

What am I missing here?
",3
13,10568690,Teradata Rank Over Query (Getting one row to left join),"Hi am new to Teradata and am stuck with a problem

There is an ID table which stores an Unique ID given to each person

CREATE TABLE IDS(
ID VARCHAR(8),
UPDATED_DATE DATE)


Then we have a name and address table which do not have any primary keys that stores demographic information for the IDS

CREATE TABLE NAMES(
ID VARCHAR(8),
NAME VARCHAR(50))
CREATE TABLE ADRRESSES(
ID VARCHAR(8)
ADDRESS VARCHAR(200))


Now each ID can have multiple name and IDS. However for names and address I want to use the ones that are have more counts. If two names have the same COUNT I just want the First row

ID              NAME                COUNT

1234    John Smith  6

1234    Johnnie Smith   6

1234    J Smith     2

In the above example I want the name John Smith. Here is the left Join I am performing since an ID may not have a name or address. Here is what I am trying

SELECT * FROM
(SELECT ID as V_ID from IDS) a
LEFT JOIN
(SELECT ID, NAME, COUNT(*) AS COUNTER,(RANK() OVER(ORDER BY COUNTER DESC)) AS RNK
FROM NAMES 
GROUP BY ID)b
ON a.ID = b.ID
AND b.RNK = 1            -- Should give me only the first row
LEFT JOIN
(SELECT ID, ADDRESS, COUNT(*) AS COUNTER, (RANK() OVER (ORDER BY COUNTER DESC) ) AS RNK
FROM ADDRESSES
GROUP BY ID) c
ON c.ID = a.ID
And c.RNK = 1


However this is not getting me the desired result. I tried using ROW NUMBER instead of RANK also but still no results. How should I write this query in TERDATA?
",-1,-1,-1.0,"Hi am new to Teradata and am stuck with a problem

There is an ID table which stores an Unique ID given to each person

CREATE TABLE IDS(
ID VARCHAR(8),
UPDATED_DATE DATE)


Then we have a name and address table which do not have any primary keys that stores demographic information for the IDS

CREATE TABLE NAMES(
ID VARCHAR(8),
NAME VARCHAR(50))
CREATE TABLE ADRRESSES(
ID VARCHAR(8)
ADDRESS VARCHAR(200))


Now each ID can have multiple name and IDS. However for names and address I want to use the ones that are have more counts. If two names have the same COUNT I just want the First row

ID              NAME                COUNT

1234    John Smith  6

1234    Johnnie Smith   6

1234    J Smith     2

In the above example I want the name John Smith. Here is the left Join I am performing since an ID may not have a name or address. Here is what I am trying

SELECT * FROM
(SELECT ID as V_ID from IDS) a
LEFT JOIN
(SELECT ID, NAME, COUNT(*) AS COUNTER,(RANK() OVER(ORDER BY COUNTER DESC)) AS RNK
FROM NAMES 
GROUP BY ID)b
ON a.ID = b.ID
AND b.RNK = 1            -- Should give me only the first row
LEFT JOIN
(SELECT ID, ADDRESS, COUNT(*) AS COUNTER, (RANK() OVER (ORDER BY COUNTER DESC) ) AS RNK
FROM ADDRESSES
GROUP BY ID) c
ON c.ID = a.ID
And c.RNK = 1


However this is not getting me the desired result. I tried using ROW NUMBER instead of RANK also but still no results. How should I write this query in TERDATA?
",3
14,9908486,Teradata: How can I drop a foreign key constraint from a table?,"I created a table then added a foreign key reference to its primary key in another table.  I need to drop the new table to recreate it with additional columns (I do not want to add the new columns to it).  When I attempt to delete it, it tells me I cannot deleted a referenced table.  So I try to drop the foreign key column from the other table and it tells me that the foreign key column cannot be dropped.  This leaves me with removing the foreign key itself first, but I don't know the name of it.  I came upon this link:

http://forums.teradata.com/forum/database/how-to-drop-a-constraint-without-knowing-its-name

...but it is no help.  I can't seem to locate the name of this foreign key anywhere.  Any help on how to drop this foreign key?
",-1,-1,-1.0,"I created a table then added a foreign key reference to its primary key in another table.  I need to drop the new table to recreate it with additional columns (I do not want to add the new columns to it).  When I attempt to delete it, it tells me I cannot deleted a referenced table.  So I try to drop the foreign key column from the other table and it tells me that the foreign key column cannot be dropped.  This leaves me with removing the foreign key itself first, but I don't know the name of it.  I came upon this link:

http://forums.teradata.com/forum/database/how-to-drop-a-constraint-without-knowing-its-name

...but it is no help.  I can't seem to locate the name of this foreign key anywhere.  Any help on how to drop this foreign key?
",3
15,8237581,SAS connection to Teradata Database using Teradata ODBC,"I'm trying to connect to Teradata in SAS. I set up an teradata ODBC on the machine. The assumption currently for me is that using ODBC is the only way for me to access the database. And here is the syntax of my connection command:

Libname Teradata ODBC dsn = 'dsnname' uid = 'uid' pwd = 'pwd';

results:
Error: The ODBC engine cannot be found.
Error: Error in the LIBNAME statement.

It keeps saying that the ODBC engine cannot be found. I'm really confused now. Is there anything wrong with the command? Or I have to do something else outside SAS?

I check the licence
Proc Setinit;

result:
SAS/ACCESS Interface to Teradata                  ** the date shows not expired.

Could anyone give me some idea. Thank you very much!
",-1,-1,-1.0,"I'm trying to connect to Teradata in SAS. I set up an teradata ODBC on the machine. The assumption currently for me is that using ODBC is the only way for me to access the database. And here is the syntax of my connection command:

Libname Teradata ODBC dsn = 'dsnname' uid = 'uid' pwd = 'pwd';

results:
Error: The ODBC engine cannot be found.
Error: Error in the LIBNAME statement.

It keeps saying that the ODBC engine cannot be found. I'm really confused now. Is there anything wrong with the command? Or I have to do something else outside SAS?

I check the licence
Proc Setinit;

result:
SAS/ACCESS Interface to Teradata                  ** the date shows not expired.

Could anyone give me some idea. Thank you very much!
",1
16,8179291,How to write batch (*.bat) script to execute Teradata query using BTEQ?,"Below is the content of my script.bat :

@echo off

cd C:\Program Files\Teradata\Client\13.0\bin

bteq .LOGON server/username,password;

select date;

.LOGOFF

@echo off goto end

:end @echo exit


I have no problem with the logon, but it seems that bteq can't read my query statement:


  select date;


It keeps prompting for input. Can anyone help me to get bteq to read and execute the query statement?

I've tried the solutions online about input and output file:

bteq &lt;myscript.txt&gt; mylog.log


but it didn't work either.
",-1,-1,-1.0,"Below is the content of my script.bat :

@echo off

cd C:\Program Files\Teradata\Client\13.0\bin

bteq .LOGON server/username,password;

select date;

.LOGOFF

@echo off goto end

:end @echo exit


I have no problem with the logon, but it seems that bteq can't read my query statement:


  select date;


It keeps prompting for input. Can anyone help me to get bteq to read and execute the query statement?

I've tried the solutions online about input and output file:

bteq &lt;myscript.txt&gt; mylog.log


but it didn't work either.
",3
17,7568172,Tuning in Teradata (group by),"I am trying to tune a query in Teradata. It's pretty huge, so I am giving below the outline:

SEL column_1, column_2......column_20, sum(column_21), sum(column_22),.....sum(column_30)
from table_a a
inner join table_b b
on conditions...
group by column_1, ...,column_20;


I am trying to tune this. It's hitting a performance roadblock in the group by. The tables A and B are huge (more than 2 billion records).

I tried the following options, but none of them improved the performance:

1) Collected all necessary stats

2) Created a JI on the columns from table A and B

3) Created an AJI on the columns and the summations from table A and B

4) Created a SI on each of the tables for the columns involved in group by.

Can someone suggest how to proceed further?
",0,1,-1.0,"I am trying to tune a query in Teradata. It's pretty huge, so I am giving below the outline:

SEL column_1, column_2......column_20, sum(column_21), sum(column_22),.....sum(column_30)
from table_a a
inner join table_b b
on conditions...
group by column_1, ...,column_20;


I am trying to tune this. It's hitting a performance roadblock in the group by. The tables A and B are huge (more than 2 billion records).

I tried the following options, but none of them improved the performance:

1) Collected all necessary stats

2) Created a JI on the columns from table A and B

3) Created an AJI on the columns and the summations from table A and B

4) Created a SI on each of the tables for the columns involved in group by.

Can someone suggest how to proceed further?
",3
18,4926057,Teradata macro parameter as time interval,"This is almost the same problem as Informix defining an INTERVAL with a parameter but is in Teradata.

I'm creating a macro that accepts a string in the form hh:mm:ss to be used as an interval.

The macro wants to do something with a timestamp in the past hh:mm:ss.

Here's the basic sql

CREATE MACRO TEST_MACRO (
    HHMMSS CHAR(8)
)
AS
(
SELECT 
    CAST(CURRENT_TIME AS TIMESTAMP(0)),
    CAST(CURRENT_TIME - INTERVAL :HHMMSS HOUR TO SECOND AS TIMESTAMP(0))
)


I get the error Failed 3707: Syntax error, expected something like a string or a Unicode character literal between the 'INTERVAL' keyword and ':'.

Is there a way around this?
",-1,-1,-1.0,"This is almost the same problem as Informix defining an INTERVAL with a parameter but is in Teradata.

I'm creating a macro that accepts a string in the form hh:mm:ss to be used as an interval.

The macro wants to do something with a timestamp in the past hh:mm:ss.

Here's the basic sql

CREATE MACRO TEST_MACRO (
    HHMMSS CHAR(8)
)
AS
(
SELECT 
    CAST(CURRENT_TIME AS TIMESTAMP(0)),
    CAST(CURRENT_TIME - INTERVAL :HHMMSS HOUR TO SECOND AS TIMESTAMP(0))
)


I get the error Failed 3707: Syntax error, expected something like a string or a Unicode character literal between the 'INTERVAL' keyword and ':'.

Is there a way around this?
",3
19,4321432,How to delete rows in a Teradata table that are not in another table?,"What makes my situation tricky is that I don't have a single column key, with a simple list of primary keys to delete (for instance, ""delete from table where key in ([list])"").  I have multiple columns together as the primary key, and would need to join on all of them.

Using what I know of other databases, I thought this might be done as:

DELETE FROM
    table1 t1
  LEFT OUTER JOIN
      table2 t2
    ON
      t2.key1 = t1.key1 AND
      t2.key2 = t1.key2
  WHERE
    t2.key1 IS NULL;


But Teradata (v12) responds with error number 3706, saying ""Syntax error: Joined Tables are not allowed in FROM clause.""
",-1,-1,-1.0,"What makes my situation tricky is that I don't have a single column key, with a simple list of primary keys to delete (for instance, ""delete from table where key in ([list])"").  I have multiple columns together as the primary key, and would need to join on all of them.

Using what I know of other databases, I thought this might be done as:

DELETE FROM
    table1 t1
  LEFT OUTER JOIN
      table2 t2
    ON
      t2.key1 = t1.key1 AND
      t2.key2 = t1.key2
  WHERE
    t2.key1 IS NULL;


But Teradata (v12) responds with error number 3706, saying ""Syntax error: Joined Tables are not allowed in FROM clause.""
",3
20,12668132,Drop table in teradata,"I tried to drop a table in Teradata database with C# if the table exist. 

cmd.CommandText = string.Format(""IF EXISTS
(SELECT * FROM sysobjects WHERE type = 'U' AND name = '{0}')  
     BEGIN DROP TABLE '{0}' END"", Customer.TableName);  
cmd.ExecuteNonQuery();


But the above always failed with :   


  {""[Teradata Database] [3706] Syntax error: expected something between the beginning of the request and the 'IF' keyword.""}


Second code i tried, the code below works !!! 

cmd.CommandText = ""select count (*) from Customer.TableName"";
                    reader = cmd.ExecuteReader();

                    if (reader.FieldCount &gt; 0)
                    {
                        reader.Close();
                        cmd.CommandText = ""Drop table Customer.TableName"";
                        reader = cmd.ExecuteReader();
                    }


However, it works only when got table exist. If the table Customer.TableName does not exist, then it will failed when undergo this 

""select count (*) from Customer.TableName"";
                        reader = cmd.ExecuteReader();

",-1,-1,-1.0,"I tried to drop a table in Teradata database with C# if the table exist. 

cmd.CommandText = string.Format(""IF EXISTS
(SELECT * FROM sysobjects WHERE type = 'U' AND name = '{0}')  
     BEGIN DROP TABLE '{0}' END"", Customer.TableName);  
cmd.ExecuteNonQuery();


But the above always failed with :   


  {""[Teradata Database] [3706] Syntax error: expected something between the beginning of the request and the 'IF' keyword.""}


Second code i tried, the code below works !!! 

cmd.CommandText = ""select count (*) from Customer.TableName"";
                    reader = cmd.ExecuteReader();

                    if (reader.FieldCount &gt; 0)
                    {
                        reader.Close();
                        cmd.CommandText = ""Drop table Customer.TableName"";
                        reader = cmd.ExecuteReader();
                    }


However, it works only when got table exist. If the table Customer.TableName does not exist, then it will failed when undergo this 

""select count (*) from Customer.TableName"";
                        reader = cmd.ExecuteReader();

",3
21,13092033,Teradata and JDBC driver - classnotfoundexception ...but its there?,"Trying to work with a JDBC connection to Teradata.  I've loaded the tdgssconfig.jar and terajdbc4.jar file and adding them to the classpath with javac when I compile in Linux.  But I still get a ClassnotFoundException when trying to compile. 

I've not worked with java in a while, but I've scoured the net and it looks like it should work.

Simple Code:

import java.sql.*;
class TDtest {
    public static void main(String[] args) {    
        System.out.println(classpath);

        Class.forName(""com.teradata.jdbc.TeraDriver"");
    }
}


*.jars are definitely there:

[user1@box java]# ls -l /home/user1/test/java/libs/*
-rwxrwxrwx 1 user1 user1 2405 Oct 26 12:00 /home/user1/test/java/libs/tdgssconfig.jar
-rwxrwxrwx 1 user1 user1 873860 Oct 26 12:00 /home/user1/test/java/libs/terajdbc4.jar


verbose error log - it looks like the classpath is right to me:

javac -verbose -cp "".:/home/user1/test/java/libs/tdgssconfig.jar:/home/user1/test/java/libs/terajdbc4.jar"" TDtest.java
[parsing started TDtest.java]
[parsing completed 21ms]
[search path for source files: .,/home/user1/test/java/libs/tdgssconfig.jar,/home/user1/test/java/libs/terajdbc4.jar]
[search path for class files: /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/resources.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/rt.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/sunrsasign.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/jsse.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/jce.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/charsets.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/classes,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/dnsns.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/sunpkcs11.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/localedata.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/gnome-java-bridge.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/sunjce_provider.jar,.,/home/user1/test/java/libs/tdgssconfig.jar,/home/user1/test/java/libs/terajdbc4.jar]
[loading java/lang/Object.class(java/lang:Object.class)]
[loading java/lang/String.class(java/lang:String.class)]
[checking TDtest]
[loading java/lang/Class.class(java/lang:Class.class)]
[loading java/lang/Error.class(java/lang:Error.class)]
[loading java/lang/ClassNotFoundException.class(java/lang:ClassNotFoundException.class)]
[loading java/lang/Exception.class(java/lang:Exception.class)]
[loading java/lang/Throwable.class(java/lang:Throwable.class)]
[loading java/lang/RuntimeException.class(java/lang:RuntimeException.class)]
TDtest.java:4: unreported exception java.lang.ClassNotFoundException; must be caught or declared to be thrown
        Class.forName(""com.teradata.jdbc.TeraDriver"");


I've tried un-jar'ing the jdbc jar's and they definitely have com/teradata/jdbc/TeraDriver.class in them.

I'm at a loss.  Any idea what I'm doing wrong?
",-1,-1,-1.0,"Trying to work with a JDBC connection to Teradata.  I've loaded the tdgssconfig.jar and terajdbc4.jar file and adding them to the classpath with javac when I compile in Linux.  But I still get a ClassnotFoundException when trying to compile. 

I've not worked with java in a while, but I've scoured the net and it looks like it should work.

Simple Code:

import java.sql.*;
class TDtest {
    public static void main(String[] args) {    
        System.out.println(classpath);

        Class.forName(""com.teradata.jdbc.TeraDriver"");
    }
}


*.jars are definitely there:

[user1@box java]# ls -l /home/user1/test/java/libs/*
-rwxrwxrwx 1 user1 user1 2405 Oct 26 12:00 /home/user1/test/java/libs/tdgssconfig.jar
-rwxrwxrwx 1 user1 user1 873860 Oct 26 12:00 /home/user1/test/java/libs/terajdbc4.jar


verbose error log - it looks like the classpath is right to me:

javac -verbose -cp "".:/home/user1/test/java/libs/tdgssconfig.jar:/home/user1/test/java/libs/terajdbc4.jar"" TDtest.java
[parsing started TDtest.java]
[parsing completed 21ms]
[search path for source files: .,/home/user1/test/java/libs/tdgssconfig.jar,/home/user1/test/java/libs/terajdbc4.jar]
[search path for class files: /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/resources.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/rt.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/sunrsasign.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/jsse.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/jce.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/charsets.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/classes,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/dnsns.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/sunpkcs11.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/localedata.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/gnome-java-bridge.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/sunjce_provider.jar,.,/home/user1/test/java/libs/tdgssconfig.jar,/home/user1/test/java/libs/terajdbc4.jar]
[loading java/lang/Object.class(java/lang:Object.class)]
[loading java/lang/String.class(java/lang:String.class)]
[checking TDtest]
[loading java/lang/Class.class(java/lang:Class.class)]
[loading java/lang/Error.class(java/lang:Error.class)]
[loading java/lang/ClassNotFoundException.class(java/lang:ClassNotFoundException.class)]
[loading java/lang/Exception.class(java/lang:Exception.class)]
[loading java/lang/Throwable.class(java/lang:Throwable.class)]
[loading java/lang/RuntimeException.class(java/lang:RuntimeException.class)]
TDtest.java:4: unreported exception java.lang.ClassNotFoundException; must be caught or declared to be thrown
        Class.forName(""com.teradata.jdbc.TeraDriver"");


I've tried un-jar'ing the jdbc jar's and they definitely have com/teradata/jdbc/TeraDriver.class in them.

I'm at a loss.  Any idea what I'm doing wrong?
",0
22,13819710,How to calculate moving sum with reset based on condition in teradata SQL?,"I have this data and I want to sum the field USAGE_FLAG but reset when it drops to 0 or moves to a new ID keeping the dataset ordered by SU_ID and WEEK:

SU_ID   WEEK    USAGE_FLAG
100        1    0
100        2    7
100        3    7
100        4    0
101        1    0
101        2    7
101        3    0
101        4    7
102        1    7
102        2    7
102        3    7
102        4    0


So I want to create this table:

SU_ID   WEEK    USAGE_FLAG    SUM
100        1    0             0
100        2    7             7
100        3    7             14
100        4    0             0
101        1    0             0
101        2    7             7
101        3    0             0
101        4    7             7
102        1    7             7
102        2    7             14
102        3    7             21
102        4    0             0


I have tried the MSUM() function using GROUP BY but it won't keep the order I want above. It groups the 7's and the week numbers together which I don't want.

Anyone know if this is possible to do? I'm using teradata
",-1,-1,-1.0,"I have this data and I want to sum the field USAGE_FLAG but reset when it drops to 0 or moves to a new ID keeping the dataset ordered by SU_ID and WEEK:

SU_ID   WEEK    USAGE_FLAG
100        1    0
100        2    7
100        3    7
100        4    0
101        1    0
101        2    7
101        3    0
101        4    7
102        1    7
102        2    7
102        3    7
102        4    0


So I want to create this table:

SU_ID   WEEK    USAGE_FLAG    SUM
100        1    0             0
100        2    7             7
100        3    7             14
100        4    0             0
101        1    0             0
101        2    7             7
101        3    0             0
101        4    7             7
102        1    7             7
102        2    7             14
102        3    7             21
102        4    0             0


I have tried the MSUM() function using GROUP BY but it won't keep the order I want above. It groups the 7's and the week numbers together which I don't want.

Anyone know if this is possible to do? I'm using teradata
",3
23,13971587,How to insert multiple records using single insert statement in Teradata?,"In teradata, can we insert multiple records using single insert statement in query. If yes, how ?

Say I am trying to do something like:

insert test_rank (storeid,prodid,sales) values (1,'A',1000) ( 2,'B',2000) ,(3,'C',3000); 


but this is not working in teradata to insert all 3 records in one statement.
",-1,-1,-1.0,"In teradata, can we insert multiple records using single insert statement in query. If yes, how ?

Say I am trying to do something like:

insert test_rank (storeid,prodid,sales) values (1,'A',1000) ( 2,'B',2000) ,(3,'C',3000); 


but this is not working in teradata to insert all 3 records in one statement.
",3
24,14270745,Issue with querying Teradata in Python/Pyodbc,"I'm trying to query a Teradata database in Python with PyODBC.
The connection to database is established alright; however, when I try to fetch result, I ran into this error ""Invalid literal for Decimal: u''"". Help please.

I am on RHEL6, with Python 2.7.3

Here is the code and result:

import pyodbc

sql = ""select * from table""

pyodbc.pooling = False
cnx = pyodbc.connect(""DRIVER={Teradata};DBCNAME=host;DATABASE=database;   AUTHENTICATION=LDAP;UID=user;PWD=password"", autocommit=True, ANSI=True)
cursor = cnx.cursor()
rows = cursor.execute(sql).fetchone()




InvalidOperation                          Traceback (most recent call last)
&lt;ipython-input-25-f2a0c81ca0e4&gt; in &lt;module&gt;()
----&gt; 1 test.fetchone()

/usr/local/lib/python2.7/decimal.pyc in __new__(cls, value, context)
    546                     context = getcontext()
    547                 return context._raise_error(ConversionSyntax,
--&gt; 548                                 ""Invalid literal for Decimal: %r"" % value)
    549 
    550             if m.group('sign') == ""-"":

/usr/local/lib/python2.7/decimal.pyc in _raise_error(self, condition, explanation, *args)
   3864         # Errors should only be risked on copies of the context
   3865         # self._ignored_flags = []
-&gt; 3866         raise error(explanation)
   3867 
   3868     def _ignore_all_flags(self):

InvalidOperation: Invalid literal for Decimal: u''

",-1,-1,-1.0,"I'm trying to query a Teradata database in Python with PyODBC.
The connection to database is established alright; however, when I try to fetch result, I ran into this error ""Invalid literal for Decimal: u''"". Help please.

I am on RHEL6, with Python 2.7.3

Here is the code and result:

import pyodbc

sql = ""select * from table""

pyodbc.pooling = False
cnx = pyodbc.connect(""DRIVER={Teradata};DBCNAME=host;DATABASE=database;   AUTHENTICATION=LDAP;UID=user;PWD=password"", autocommit=True, ANSI=True)
cursor = cnx.cursor()
rows = cursor.execute(sql).fetchone()




InvalidOperation                          Traceback (most recent call last)
&lt;ipython-input-25-f2a0c81ca0e4&gt; in &lt;module&gt;()
----&gt; 1 test.fetchone()

/usr/local/lib/python2.7/decimal.pyc in __new__(cls, value, context)
    546                     context = getcontext()
    547                 return context._raise_error(ConversionSyntax,
--&gt; 548                                 ""Invalid literal for Decimal: %r"" % value)
    549 
    550             if m.group('sign') == ""-"":

/usr/local/lib/python2.7/decimal.pyc in _raise_error(self, condition, explanation, *args)
   3864         # Errors should only be risked on copies of the context
   3865         # self._ignored_flags = []
-&gt; 3866         raise error(explanation)
   3867 
   3868     def _ignore_all_flags(self):

InvalidOperation: Invalid literal for Decimal: u''

",1
25,14339475,Teradata Assistant Database Explorer,"I am having problems with Teradata Assistant (verion Teradata.Net 13.11.0.1)
The Problem is with Database Explorer window becuse it is fixed length and width and I can´t adjust it as answer window and history window.
Can you please help me with this because the width of the Assistant is very small and when I expand a database I can't see the the column names.
",-1,-1,-1.0,"I am having problems with Teradata Assistant (verion Teradata.Net 13.11.0.1)
The Problem is with Database Explorer window becuse it is fixed length and width and I can´t adjust it as answer window and history window.
Can you please help me with this because the width of the Assistant is very small and when I expand a database I can't see the the column names.
",3
26,14449253,Date range comparison using varchar columns in Teradata,"I've been tasked to take a calendar date range value from a form front-end and use it to, among other things, feed a query in a Teradata table that does not have a datetime column.  Instead the date is aggregated from two varchar columns:  one for year (CY = current year, LY = last year, LY-1, etc), and one for the date with format MonDD (like Jan13, Dec08, etc).

I'm using Coldfusion for the form and result page, so I have the ability to dynamically create the query, but I can't think of a good way to do it for all possible cases.  Any ideas?  Even year differences aside, I can't think of anything outside of a direct comparison on each day in the range with a potential ton of separate OR statements in the query.  I'm light on SQL knowledge - maybe there's a better way to script it in the SQL itself using some sort of conversion on the two varchar columns to form an actual date range where date comparisons could then be made?
",1,-1,-1.0,"I've been tasked to take a calendar date range value from a form front-end and use it to, among other things, feed a query in a Teradata table that does not have a datetime column.  Instead the date is aggregated from two varchar columns:  one for year (CY = current year, LY = last year, LY-1, etc), and one for the date with format MonDD (like Jan13, Dec08, etc).

I'm using Coldfusion for the form and result page, so I have the ability to dynamically create the query, but I can't think of a good way to do it for all possible cases.  Any ideas?  Even year differences aside, I can't think of anything outside of a direct comparison on each day in the range with a potential ton of separate OR statements in the query.  I'm light on SQL knowledge - maybe there's a better way to script it in the SQL itself using some sort of conversion on the two varchar columns to form an actual date range where date comparisons could then be made?
",3
27,14477486,RowID in Teradata,"I need to pull a row ID with a select statement. Something that is similar to the row ID of oracle. How would I do that in Teradata? I am trying the following query but it is throwing error.

select rowid,emp_id,e_name from test;

Error msg : Syntax error: ROWID not allowed.


Thanks in advance.
",-1,-1,-1.0,"I need to pull a row ID with a select statement. Something that is similar to the row ID of oracle. How would I do that in Teradata? I am trying the following query but it is throwing error.

select rowid,emp_id,e_name from test;

Error msg : Syntax error: ROWID not allowed.


Thanks in advance.
",3
28,15633186,Teradata BTEQ Export Script Resulting Invalid Character in O/P File,"Here is my simple BTEQ Script

.LOGON 127.0.0.1/tduser,tduser
.EXPORT DATA FILE=C:\Documents and Settings\Owner\Desktop\Study\Google Drive\TD\BTEQ\Exported_File.txt
    SELECT      account_number
    FROM    samples.accounts
    WHERE   balance_current  &lt; 500 ;
.EXPORT RESET
.QUIT


=====================================================================================

The output of the script:

BTEQ 12.00.00.01 Tue Mar 26 19:58:59 2013

+---------+---------+---------+---------+---------+---------+---------+----
.LOGON 127.0.0.1/tduser,

 *** Logon successfully completed.
 *** Teradata Database Release is 12.00.00.10
 *** Teradata Database Version is 12.00.00.10
 *** Transaction Semantics are BTET.
 *** Character Set Name is 'ASCII'.

 *** Total elapsed time was 1 second.

+---------+---------+---------+---------+---------+---------+---------+----
.EXPORT DATA FILE=C:\Documents and Settings\Owner\Desktop\Study\Google Driv
e\TD\BTEQ\Exported_File.txt
 *** To reset export, type .EXPORT RESET
+---------+---------+---------+---------+---------+---------+---------+----
 SELECT   account_number
 FROM  samples.accounts
 WHERE  balance_current  &lt; 500 ;

 *** Success, Stmt# 1 ActivityCount = 4
 *** Query completed. 4 rows found. One column returned.
 *** Total elapsed time was 1 second.


+---------+---------+---------+---------+---------+---------+---------+----
.EXPORT RESET
 *** Output returned to console.
+---------+---------+---------+---------+---------+---------+---------+----
.QUIT
 *** You are now logged off from the DBC.
 *** Exiting BTEQ...
 *** RC (return code) = 0


=====================================================================================

Once, I check the file it shows invalid character's in the text file. Any possible workaround? Unicode problem? I am not sure.I even tried pasting the file content below, but its empty.
 
 
 
    

Thanks in advance.
",-1,-1,-1.0,"Here is my simple BTEQ Script

.LOGON 127.0.0.1/tduser,tduser
.EXPORT DATA FILE=C:\Documents and Settings\Owner\Desktop\Study\Google Drive\TD\BTEQ\Exported_File.txt
    SELECT      account_number
    FROM    samples.accounts
    WHERE   balance_current  &lt; 500 ;
.EXPORT RESET
.QUIT


=====================================================================================

The output of the script:

BTEQ 12.00.00.01 Tue Mar 26 19:58:59 2013

+---------+---------+---------+---------+---------+---------+---------+----
.LOGON 127.0.0.1/tduser,

 *** Logon successfully completed.
 *** Teradata Database Release is 12.00.00.10
 *** Teradata Database Version is 12.00.00.10
 *** Transaction Semantics are BTET.
 *** Character Set Name is 'ASCII'.

 *** Total elapsed time was 1 second.

+---------+---------+---------+---------+---------+---------+---------+----
.EXPORT DATA FILE=C:\Documents and Settings\Owner\Desktop\Study\Google Driv
e\TD\BTEQ\Exported_File.txt
 *** To reset export, type .EXPORT RESET
+---------+---------+---------+---------+---------+---------+---------+----
 SELECT   account_number
 FROM  samples.accounts
 WHERE  balance_current  &lt; 500 ;

 *** Success, Stmt# 1 ActivityCount = 4
 *** Query completed. 4 rows found. One column returned.
 *** Total elapsed time was 1 second.


+---------+---------+---------+---------+---------+---------+---------+----
.EXPORT RESET
 *** Output returned to console.
+---------+---------+---------+---------+---------+---------+---------+----
.QUIT
 *** You are now logged off from the DBC.
 *** Exiting BTEQ...
 *** RC (return code) = 0


=====================================================================================

Once, I check the file it shows invalid character's in the text file. Any possible workaround? Unicode problem? I am not sure.I even tried pasting the file content below, but its empty.
 
 
 
    

Thanks in advance.
",3
29,15661416,Teradata SQL Syntax - Common Table Expressions,"When using multiple CTE's in MSSQL 2008, I normally separate them with a comma.

But when I try this in a Teradata environment, I get an error with the syntax.

Works in MS SQL:

WITH CTE1 AS 
(SELECT TOP 2 Name FROM Sales.Store)
,CTE2 AS 
(SELECT TOP 2 ProductNumber, Name FROM Production.Product)
,CTE3 AS 
(SELECT TOP 2 Name FROM Person.ContactType)
SELECT * FROM CTE1,CTE2,CTE3


Now, attempting to put into Teradata syntax:

WITH RECURSIVE CTE1 (Name) AS 
(SELECT TOP 2 Name FROM Sales.Store)
,RECURSIVE CTE2 (ProductNumber, Name) AS 
(SELECT TOP 2 ProductNumber, Name FROM Production.Product)
,RECURSIVE CTE3 (Name) AS 
(SELECT TOP 2 Name FROM Person.ContactType)
SELECT * 
FROM CTE1,CTE2,CTE3



  Syntax error, expected something like a name or a Unicode delimited
  identifier between ',' and the 'RECURSIVE' keyword.


2nd attempt (without using RECURSIVE multiple times)

WITH RECURSIVE CTE1 (Name) AS 
(SELECT TOP 2 Name FROM Sales.Store)
,CTE2 (ProductNumber, Name) AS 
(SELECT TOP 2 ProductNumber, Name FROM Production.Product)
,CTE3 (Name) AS 
(SELECT TOP 2 Name FROM Person.ContactType)
SELECT * 
FROM CTE1,CTE2,CTE3



  Multiple WITH definitions are not supported.

",-1,-1,-1.0,"When using multiple CTE's in MSSQL 2008, I normally separate them with a comma.

But when I try this in a Teradata environment, I get an error with the syntax.

Works in MS SQL:

WITH CTE1 AS 
(SELECT TOP 2 Name FROM Sales.Store)
,CTE2 AS 
(SELECT TOP 2 ProductNumber, Name FROM Production.Product)
,CTE3 AS 
(SELECT TOP 2 Name FROM Person.ContactType)
SELECT * FROM CTE1,CTE2,CTE3


Now, attempting to put into Teradata syntax:

WITH RECURSIVE CTE1 (Name) AS 
(SELECT TOP 2 Name FROM Sales.Store)
,RECURSIVE CTE2 (ProductNumber, Name) AS 
(SELECT TOP 2 ProductNumber, Name FROM Production.Product)
,RECURSIVE CTE3 (Name) AS 
(SELECT TOP 2 Name FROM Person.ContactType)
SELECT * 
FROM CTE1,CTE2,CTE3



  Syntax error, expected something like a name or a Unicode delimited
  identifier between ',' and the 'RECURSIVE' keyword.


2nd attempt (without using RECURSIVE multiple times)

WITH RECURSIVE CTE1 (Name) AS 
(SELECT TOP 2 Name FROM Sales.Store)
,CTE2 (ProductNumber, Name) AS 
(SELECT TOP 2 ProductNumber, Name FROM Production.Product)
,CTE3 (Name) AS 
(SELECT TOP 2 Name FROM Person.ContactType)
SELECT * 
FROM CTE1,CTE2,CTE3



  Multiple WITH definitions are not supported.

",3
30,15842107,"Teradata Database 3710 Insufficient memory to parse this request, during QueryRewrite phase","I am working with visual studio's 2010 and using .Net Data Provider for Teradata 14.0 and receiving the following error.  [Teradata Database] [3710] Insufficient memory to parse this request, during QueryRewrite phase.  I am trying to bring in a simple small table using ADO.net entity model.

Anyone know how to fix this error?
",1,-1,-1.0,"I am working with visual studio's 2010 and using .Net Data Provider for Teradata 14.0 and receiving the following error.  [Teradata Database] [3710] Insufficient memory to parse this request, during QueryRewrite phase.  I am trying to bring in a simple small table using ADO.net entity model.

Anyone know how to fix this error?
",3
31,15873903,Teradata setLoginTimeout not working,"I am building an app in extjs connecting to teradata at the back-end.  It works fine locally but when deployed its giving 


  [Error 1277] [SQLState 08S01] Login time-out for Connection to server after 12seconds. 


I am trying to increase the timeout now. 

Class.forName(""com.teradata.jdbc.TeraDriver"");
DriverManager.setLoginTimeout(100);
Connection conn = DriverManager.getConnection(connectionString, ""user"", ""pass"");


Still I'm getting same time-out error after 12 seconds. It seems like setLoginTimeout didn't work. Where am I going wrong? Is there any other solution other than increasing time-out ? 

P.S: For one server it worked fine now I changed only the server name to point to another server and I'm getting timeout. 
",-1,-1,-1.0,"I am building an app in extjs connecting to teradata at the back-end.  It works fine locally but when deployed its giving 


  [Error 1277] [SQLState 08S01] Login time-out for Connection to server after 12seconds. 


I am trying to increase the timeout now. 

Class.forName(""com.teradata.jdbc.TeraDriver"");
DriverManager.setLoginTimeout(100);
Connection conn = DriverManager.getConnection(connectionString, ""user"", ""pass"");


Still I'm getting same time-out error after 12 seconds. It seems like setLoginTimeout didn't work. Where am I going wrong? Is there any other solution other than increasing time-out ? 

P.S: For one server it worked fine now I changed only the server name to point to another server and I'm getting timeout. 
",3
32,17112104,Teradata Volatile Table Statement is not creating any rows,"I want to create table in Teradata. Therefore I am using this syntax:

    CREATE VOLATILE TABLE a AS
    (
        Select * FROM ...
    ) WITH DATA PRIMARY INDEX ( ACCOUNT_ID )
;


The inner SELECT statement results in 4 rows. However, when I run the entire query, the resulting data set does not have any rows. Strange, I know - that`s why I'm writing. Please help. Thanks.
",1,-1,-1.0,"I want to create table in Teradata. Therefore I am using this syntax:

    CREATE VOLATILE TABLE a AS
    (
        Select * FROM ...
    ) WITH DATA PRIMARY INDEX ( ACCOUNT_ID )
;


The inner SELECT statement results in 4 rows. However, when I run the entire query, the resulting data set does not have any rows. Strange, I know - that`s why I'm writing. Please help. Thanks.
",3
33,17143215,Teradata CASE WHEN attribute IN SELECT,"I have 2 tables in Teradata - ""a"" and ""ch"". Table ch contains 2 colums ""amt"" and ""code"". Between tables ""a"" and ""ch"" is LEFT JOIN. Join is made, and in the SELECT part I am trying to SUM amt values. But when a ""code"" attribute has a speific values it hast to take only 70% of the ""amt"" value.

This is what I have tried:

SELECT SUM 
       (
       CASE WHEN ch.code IN (SELECT code from ...)
          then 0.7*ch.amt
       )   
       else ch.amt
       END
FROM a LEFT JOIN ch


I get an error:


  Illegal expression in WHEN Clause of CASE expression.


Google says, that it is because CASE does not allow SELECT statements.

Any suggestion how can I achieve the above described functionality?
",-1,-1,-1.0,"I have 2 tables in Teradata - ""a"" and ""ch"". Table ch contains 2 colums ""amt"" and ""code"". Between tables ""a"" and ""ch"" is LEFT JOIN. Join is made, and in the SELECT part I am trying to SUM amt values. But when a ""code"" attribute has a speific values it hast to take only 70% of the ""amt"" value.

This is what I have tried:

SELECT SUM 
       (
       CASE WHEN ch.code IN (SELECT code from ...)
          then 0.7*ch.amt
       )   
       else ch.amt
       END
FROM a LEFT JOIN ch


I get an error:


  Illegal expression in WHEN Clause of CASE expression.


Google says, that it is because CASE does not allow SELECT statements.

Any suggestion how can I achieve the above described functionality?
",3
34,17196710,Invoking UDF in Teradata,"I have some troubles invoking SQL User Defined Functions in Teradata.
I've created the following function

*REPLACE FUNCTION ""twm_source"".""TD_FN_CALC"" (
        ""func"" CHARACTER(1) CHARACTER SET LATIN,
        ""a"" INTEGER,
        ""b"" INTEGER)
    RETURNS INTEGER
    SPECIFIC ""td_fn_calc""
    LANGUAGE SQL
    CONTAINS SQL
    DETERMINISTIC
    CALLED ON NULL INPUT
    SQL SECURITY DEFINER
    COLLATION INVOKER
    INLINE TYPE 1
    RETURN CASE
    WHEN func = 'A'
    THEN A + B
    WHEN func = 'S'
    THEN A - B
    WHEN func = 'M'
    THEN A * B
    ELSE A / B
END;*


But when I execute the following query against Teradata 14.0 Server 

select ""twm_source"".""TD_FN_CALC""('M',3,8);


it gives error


  Failed [5589 : HY000] Function 'TD_FN_CALC' does not exist. 


Could anyone please help me to find out what is wrong.
Any help is deeply appreciated.
",-1,-1,-1.0,"I have some troubles invoking SQL User Defined Functions in Teradata.
I've created the following function

*REPLACE FUNCTION ""twm_source"".""TD_FN_CALC"" (
        ""func"" CHARACTER(1) CHARACTER SET LATIN,
        ""a"" INTEGER,
        ""b"" INTEGER)
    RETURNS INTEGER
    SPECIFIC ""td_fn_calc""
    LANGUAGE SQL
    CONTAINS SQL
    DETERMINISTIC
    CALLED ON NULL INPUT
    SQL SECURITY DEFINER
    COLLATION INVOKER
    INLINE TYPE 1
    RETURN CASE
    WHEN func = 'A'
    THEN A + B
    WHEN func = 'S'
    THEN A - B
    WHEN func = 'M'
    THEN A * B
    ELSE A / B
END;*


But when I execute the following query against Teradata 14.0 Server 

select ""twm_source"".""TD_FN_CALC""('M',3,8);


it gives error


  Failed [5589 : HY000] Function 'TD_FN_CALC' does not exist. 


Could anyone please help me to find out what is wrong.
Any help is deeply appreciated.
",3
35,17240695,Access slow when joining on Teradata SQL connections?,"I have a simple Access database I use to create a number of reports. I've linked to a Teradata database server in our organization to pull some additional employee-level details. There is a simple left join on employee number, and all I pull is the name and the role. 

The query without the connect takes maybe a minute or so to run and is very quick once loaded. Left joining on the Teradata connection slows everything down to a crawl. It can take 10 minutes or so to run the query through Excel. When the query is loaded in Access, scrolling through it is very slow.

I should note there's no performance issues with the Teradata server. I pull unrelated reports from the same and different tables, with complex joins and the speed is very quick.

I tried creating an even simpler query that does almost noting, and the performance issues are still there. Here is the code:

SELECT EMPL_DETAILS_CURR.NM_PREFX, EMPL_DETAILS_CURR.NM_GIVEN, 
MC.DT_APP_ENTRY, MC.CHANNEL_IND

FROM MC LEFT JOIN EMPL_DETAILS_CURR ON MC.EMP_ID = EMPL_DETAILS_CURR.EMP_ID;


There are only 7000 records in MC. 
",1,-1,-1.0,"I have a simple Access database I use to create a number of reports. I've linked to a Teradata database server in our organization to pull some additional employee-level details. There is a simple left join on employee number, and all I pull is the name and the role. 

The query without the connect takes maybe a minute or so to run and is very quick once loaded. Left joining on the Teradata connection slows everything down to a crawl. It can take 10 minutes or so to run the query through Excel. When the query is loaded in Access, scrolling through it is very slow.

I should note there's no performance issues with the Teradata server. I pull unrelated reports from the same and different tables, with complex joins and the speed is very quick.

I tried creating an even simpler query that does almost noting, and the performance issues are still there. Here is the code:

SELECT EMPL_DETAILS_CURR.NM_PREFX, EMPL_DETAILS_CURR.NM_GIVEN, 
MC.DT_APP_ENTRY, MC.CHANNEL_IND

FROM MC LEFT JOIN EMPL_DETAILS_CURR ON MC.EMP_ID = EMPL_DETAILS_CURR.EMP_ID;


There are only 7000 records in MC. 
",3
36,17297596,can't connect teradata from sql clr code,"I try to create SQLCLR proc that access teradata server and I get this error:  


  System.IO.FileLoadException: LoadFrom(), LoadFile(), Load(byte[]) and LoadModule() have been disabled by the host.


I google about this error and I found that I need to create &lt;dllname&gt;.XmlSerializers.dll and add this to the DB. I did that and still I get same error.

The code work stably with Oracle and SQL Server, but I can't add Teradata.

I'm using Teradata.Client.Provider namespace from the assembly Teradata.Client.Provider.

The server version is 2012.
",-1,-1,-1.0,"I try to create SQLCLR proc that access teradata server and I get this error:  


  System.IO.FileLoadException: LoadFrom(), LoadFile(), Load(byte[]) and LoadModule() have been disabled by the host.


I google about this error and I found that I need to create &lt;dllname&gt;.XmlSerializers.dll and add this to the DB. I did that and still I get same error.

The code work stably with Oracle and SQL Server, but I can't add Teradata.

I'm using Teradata.Client.Provider namespace from the assembly Teradata.Client.Provider.

The server version is 2012.
",1
37,17809540,SQOOP Import from Teradata : Create table Ok but without data,"I use sqoop to dial with my TD database.
When i try this, everything is OK (my table is create in default hive database)

sqoop import \
 -libjars $LIB_JARS \
 -Dteradata.db.input.job.type=hive \
 -Dteradata.db.input.target.table=hive_table \
 -Dteradata.db.input.target.table.schema=""c1 bigint"" \
 -m 1 \
 --connect jdbc:teradata://PRD/Database=database \
 --connection-manager org.apache.sqoop.teradata.TeradataConnManager \
 --username userTD \
 --password passTD \
 --table tableTD


But when i try to specify another hive database with :

 -Dteradata.db.input.target.database=hive_database \


The script return OK, the table is create but without any data inside...

Need somme help...

Thanks
",1,-1,-1.0,"I use sqoop to dial with my TD database.
When i try this, everything is OK (my table is create in default hive database)

sqoop import \
 -libjars $LIB_JARS \
 -Dteradata.db.input.job.type=hive \
 -Dteradata.db.input.target.table=hive_table \
 -Dteradata.db.input.target.table.schema=""c1 bigint"" \
 -m 1 \
 --connect jdbc:teradata://PRD/Database=database \
 --connection-manager org.apache.sqoop.teradata.TeradataConnManager \
 --username userTD \
 --password passTD \
 --table tableTD


But when i try to specify another hive database with :

 -Dteradata.db.input.target.database=hive_database \


The script return OK, the table is create but without any data inside...

Need somme help...

Thanks
",0
38,18706656,How to specify user_constraints and other tables in Teradata,"I am new to Teradata. I have been using Oracle for quite a bit. However, when I tried to run the following queries in Teradata, they simply did not work. How can I translate the following queries into Teradata:

select Table_name, constraint_name, constraint_type,
   r_constraint_name, Delete_rule, search_condition
 from user_constraints
  order by table_name, constraint_name;

select object_name, object_id, Object_type,
   Created, last_DDL_time, status
 from user_Objects
  order by object_name;

select table_name, column_name, data_type, data_length,
   data_precision, nullable, column_id, data_default
 from user_tab_columns
  order by table_name, column_name;

",-1,-1,-1.0,"I am new to Teradata. I have been using Oracle for quite a bit. However, when I tried to run the following queries in Teradata, they simply did not work. How can I translate the following queries into Teradata:

select Table_name, constraint_name, constraint_type,
   r_constraint_name, Delete_rule, search_condition
 from user_constraints
  order by table_name, constraint_name;

select object_name, object_id, Object_type,
   Created, last_DDL_time, status
 from user_Objects
  order by object_name;

select table_name, column_name, data_type, data_length,
   data_precision, nullable, column_id, data_default
 from user_tab_columns
  order by table_name, column_name;

",3
39,18395546,".NET Data Provider for Teradata 1150009 Message truncation error, not all data was received","I get this error message whenever i try to establish a connection to Teradata server using Teradata client provider 13.10.

[.NET Data Provider for Teradata[1150009] Message truncation error, not all data was received.

Please advise me to get rid of this error.
",-1,-1,-1.0,"I get this error message whenever i try to establish a connection to Teradata server using Teradata client provider 13.10.

[.NET Data Provider for Teradata[1150009] Message truncation error, not all data was received.

Please advise me to get rid of this error.
",1
40,18332385,Fast Export script of Teradata,"i am trying to pull data from Teradata and put it into hadoop.

i have written a script to do so. 

Well this is not a direct process.

It is staged to Hadoop's local and then put into Hadoop.

While running the script i am getting the following error:


0002 .LOGTABLE  log_1;
**** 16:06:28 UTY1006 CLI error: 235, MTDP: EM_GSSINITFAIL(235): call to
     gss_init failed.
**** 16:06:28 UTY2410 Total processor time used = '0 Seconds'
.       Start : 16:06:28 - TUE AUG 20, 2013
.       End   : 16:06:28 - TUE AUG 20, 2013
.       Highest return code encountered = '12'.


Can anyone help me and tell what is the mistake here? What does that error mean?
",-1,-1,-1.0,"i am trying to pull data from Teradata and put it into hadoop.

i have written a script to do so. 

Well this is not a direct process.

It is staged to Hadoop's local and then put into Hadoop.

While running the script i am getting the following error:


0002 .LOGTABLE  log_1;
**** 16:06:28 UTY1006 CLI error: 235, MTDP: EM_GSSINITFAIL(235): call to
     gss_init failed.
**** 16:06:28 UTY2410 Total processor time used = '0 Seconds'
.       Start : 16:06:28 - TUE AUG 20, 2013
.       End   : 16:06:28 - TUE AUG 20, 2013
.       Highest return code encountered = '12'.


Can anyone help me and tell what is the mistake here? What does that error mean?
",3
41,18261427,How does hibernate resolve schema for initially validating to a Teradata database?,"Environment:

Java/Spring application that uses JPA/Hibernate for persistence and connects to a Teradata datasource configured in the app container (Tomcat) which is accessed through JNDI.

Versions that I am using:

java: 6
spring: 3.2.4.RELEASE
hibernate.core: 4.2.4.Final
hibernate.entitymanager: 4.2.4.Final
hibernate.validator: 5.0.1.Final
springdata: 1.3.4.RELEASE
javax.validation: 1.1.0.Final


Problem:

There are two Teradata databases in the same server that have a same named table but with different columns:

DatDe001.SFITEM
Columns: [iipcst, iidesc, iivend, updated_at, iisku#, created_at, item_expdt, item_effdt]

DEV_DIG_UMT.SFITEM
Columns: [iipcst, iidesc, iivend, row_updt_tms, iisku#, row_insrt_tms, item_expdt, item_effdt]


As you can see the columns that differ are updated_at -> row_updt_tms and created_at -> row_insrt_tms

I am using a JNDI datasource which is configured using this jdbc url:

jdbc:teradata://&lt;server_ip&gt;/DATABASE=DEV_DIG_UMT,DBS_PORT=1025,COP=OFF,CHARSET=UTF8,TMODE=ANSI  


It is supposed that the jdbc connection will resolve the location of the table using the DATABASE value in that jdbc url. However Hibernate seems to be taking the wrong one: DatDe001.SFITEM when performing the initial schema validation, that is at the moment of context initialization when Spring tries to create the EntityManagerFactory bean:

2013-08-15 13:32:03,635 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000261: Table found: DatDe001.SFITEM
2013-08-15 13:32:03,635 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000037: Columns: [iipcst, iidesc, iivend, updated_at, iisku#, created_at, item_expdt, item_effdt]


So as my JPA entity (see the entity below in the post) does not have those columns, the hibernate validation throws an exception (see the summarized stack trace):

org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.dao.annotation.PersistenceExceptionTranslationPostProcessor#0': Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in file [C:\APP\springsource\vfabric-tc-server-developer-2.9.2.RELEASE\base-instance\wtpwebapps\profile-items\WEB-INF\classes\META-INF\spring\applicationContext.xml]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: persistenceUnit] Unable to build EntityManagerFactory
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:529)
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:458)
...
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in file [C:\APP\springsource\vfabric-tc-server-developer-2.9.2.RELEASE\base-instance\wtpwebapps\profile-items\WEB-INF\classes\META-INF\spring\applicationContext.xml]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: persistenceUnit] Unable to build EntityManagerFactory
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1482)
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:521)
...
Caused by: javax.persistence.PersistenceException: [PersistenceUnit: persistenceUnit] Unable to build EntityManagerFactory
            at org.hibernate.ejb.Ejb3Configuration.buildEntityManagerFactory(Ejb3Configuration.java:924)
            at org.hibernate.ejb.Ejb3Configuration.buildEntityManagerFactory(Ejb3Configuration.java:899)
...
Caused by: org.hibernate.HibernateException: Missing column: row_updt_tms in DatDe001.SFITEM
at org.hibernate.mapping.Table.validateColumns(Table.java:366)
at org.hibernate.cfg.Configuration.validateSchema(Configuration.java:1305)
at org.hibernate.tool.hbm2ddl.SchemaValidator.validate(SchemaValidator.java:155)
at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:508)
at org.hibernate.cfg.Configuration.buildSessionFactory(Configuration.java:1790)
at org.hibernate.ejb.EntityManagerFactoryImpl.&lt;init&gt;(EntityManagerFactoryImpl.java:96)
at org.hibernate.ejb.Ejb3Configuration.buildEntityManagerFactory(Ejb3Configuration.java:914)


After I saw that, I was wondering if this behavior will persist when executing a query statement to the db through JPA/hibernate, or if it will point to the right table in that case.

Then just for investigation purposes I changed my JPA entity to have the same columns that DatDe001.SFITEM table:

@Entity
public class Sfitem implements Serializable {
    private static final long serialVersionUID = 1L;

    @EmbeddedId
    private SfitemPK id;

    @Column(name=""\""iidesc\"""")
    private String iidesc;

    @Column(name=""\""iipcst\"""")
    private BigDecimal iipcst;

    @Column(name=""\""iivend\"""")
    private BigDecimal iivend;

    @Temporal
    @Column(name=""\""item_expdt\"""")
    private Date itemExpdt;

    @Temporal
    @Column(name=""\""created_at\"""")
    private Date createdAt;

    @Temporal
    @Column(name=""\""updated_at\"""")
    private Date updatedAt;

    ...
}


I started the application and it got loaded successfully. Instead of showing the exception now the log looked good:

...
2013-08-15 14:42:52,056 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000261: Table found: DatDe001.SFITEM
2013-08-15 14:42:52,056 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000037: Columns: [iipcst, iidesc, iivend, updated_at, iisku#, created_at, item_expdt, item_effdt]
2013-08-15 14:42:52,061 DEBUG localhost-startStop-1 org.hibernate.internal.SessionFactoryImpl - Checking 0 named HQL queries
2013-08-15 14:42:52,061 DEBUG localhost-startStop-1 org.hibernate.internal.SessionFactoryImpl - Checking 0 named SQL queries
2013-08-15 14:42:52,063 TRACE localhost-startStop-1 org.hibernate.service.internal.AbstractServiceRegistryImpl - Initializing service [role=org.hibernate.service.config.spi.ConfigurationService]
2013-08-15 14:42:52,113 TRACE localhost-startStop-1 org.hibernate.service.internal.AbstractServiceRegistryImpl - Initializing service [role=org.hibernate.stat.spi.StatisticsImplementor]
...


I tried to execute a query to the table and surprisingly found that this time Hibernate was pointing to the right database/schema: DEV_DIG_UMT, the query failed because now the entity had the columns for the other database: DatDe001, see the log:

2013-08-15 14:50:05,731 TRACE tomcat-http--4 org.hibernate.engine.query.spi.QueryPlanCache - Located HQL query plan in cache (SELECT o FROM Sfitem o WHERE o.id.iisku = :iisku AND o.id.itemEffdt &lt;= :date AND coalesce(o.itemExpdt, cast('9999-12-31' as date)) &gt;= :date)
2013-08-15 14:50:05,766 TRACE tomcat-http--4 org.hibernate.engine.query.spi.QueryPlanCache - Located HQL query plan in cache (SELECT o FROM Sfitem o WHERE o.id.iisku = :iisku AND o.id.itemEffdt &lt;= :date AND coalesce(o.itemExpdt, cast('9999-12-31' as date)) &gt;= :date)
2013-08-15 14:50:05,768 TRACE tomcat-http--4 org.hibernate.engine.query.spi.HQLQueryPlan - Find: SELECT o FROM Sfitem o WHERE o.id.iisku = :iisku AND o.id.itemEffdt &lt;= :date AND coalesce(o.itemExpdt, cast('9999-12-31' as date)) &gt;= :date
2013-08-15 14:50:05,772 TRACE tomcat-http--4 org.hibernate.engine.spi.QueryParameters - Named parameters: {iisku=387671, date=2013-08-08}
2013-08-15 14:50:05,810 DEBUG tomcat-http--4 org.hibernate.SQL - select sfitem0_.""iisku#"" as iisku1_0_, sfitem0_.""item_effdt"" as item_eff2_0_, sfitem0_.""created_at"" as created_3_0_, sfitem0_.""iidesc"" as iidesc4_0_, sfitem0_.""iipcst"" as iipcst5_0_, sfitem0_.""iivend"" as iivend6_0_, sfitem0_.""item_expdt"" as item_exp7_0_ from sfitem sfitem0_ where sfitem0_.""iisku#""=? and sfitem0_.""item_effdt""&lt;=? and coalesce(sfitem0_.""item_expdt"", cast('9999-12-31' as DATE))&gt;=?
2013-08-15 14:50:05,832 DEBUG tomcat-http--4 org.hibernate.engine.jdbc.spi.SqlExceptionHelper - could not prepare statement [select sfitem0_.""iisku#"" as iisku1_0_, sfitem0_.""item_effdt"" as item_eff2_0_, sfitem0_.""created_at"" as created_3_0_, sfitem0_.""iidesc"" as iidesc4_0_, sfitem0_.""iipcst"" as iipcst5_0_, sfitem0_.""iivend"" as iivend6_0_, sfitem0_.""item_expdt"" as item_exp7_0_ from sfitem sfitem0_ where sfitem0_.""iisku#""=? and sfitem0_.""item_effdt""&lt;=? and coalesce(sfitem0_.""item_expdt"", cast('9999-12-31' as DATE))&gt;=?]
com.teradata.jdbc.jdbc_4.util.JDBCException: [Teradata Database] [TeraJDBC 14.00.00.21] [Error 3810] [SQLState 42S22] Column/Parameter 'DEV_DIG_UMT.sfitem0_.created_at' does not exist.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:307)
    at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:102)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:320)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:201)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:121)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:112)
...
    at org.hibernate.engine.jdbc.internal.StatementPreparerImpl$5.doPrepare(StatementPreparerImpl.java:161)
    at org.hibernate.engine.jdbc.internal.StatementPreparerImpl$StatementPreparationTemplate.prepareStatement(StatementPreparerImpl.java:182)
    at org.hibernate.engine.jdbc.internal.StatementPreparerImpl.prepareQueryStatement(StatementPreparerImpl.java:159)
    at org.hibernate.loader.Loader.prepareQueryStatement(Loader.java:1859)
        at org.hibernate.loader.Loader.executeQueryStatement(Loader.java:1836)
        at org.hibernate.loader.Loader.executeQueryStatement(Loader.java:1816)
        at org.hibernate.loader.Loader.doQuery(Loader.java:900)
        at org.hibernate.loader.Loader.doQueryAndInitializeNonLazyCollections(Loader.java:342)
        at org.hibernate.loader.Loader.doList(Loader.java:2526)
        at org.hibernate.loader.Loader.doList(Loader.java:2512)
        at org.hibernate.loader.Loader.listIgnoreQueryCache(Loader.java:2342)
        at org.hibernate.loader.Loader.list(Loader.java:2337)
        at org.hibernate.loader.hql.QueryLoader.list(QueryLoader.java:495)


This means that hibernate validation and the query executor routines are behaving differently

The entity with the correct fields:

@Entity
public class Sfitem implements Serializable {
    private static final long serialVersionUID = 1L;

    @EmbeddedId
    private SfitemPK id;

    @Column(name=""\""iidesc\"""")
    private String iidesc;

    @Column(name=""\""iipcst\"""")
    private BigDecimal iipcst;

    @Column(name=""\""iivend\"""")
    private BigDecimal iivend;

    @Column(name=""\""item_expdt\"""")
    private Date itemExpdt;

    @Column(name=""\""row_insrt_tms\"""")
    private Timestamp rowInsrtTms;

    @Column(name=""\""row_updt_tms\"""")
    private Timestamp rowUpdtTms;

    ...
}


Persistence.xml

&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;persistence xmlns=""http://java.sun.com/xml/ns/persistence"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" version=""2.0"" xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd""&gt;
&lt;persistence-unit name=""persistenceUnit"" transaction-type=""RESOURCE_LOCAL""&gt;
        &lt;provider&gt;org.hibernate.ejb.HibernatePersistence&lt;/provider&gt;
        &lt;properties&gt;
            &lt;property name=""hibernate.dialect"" value=""org.hibernate.dialect.TeradataDialect""/&gt;
            &lt;!-- value=""create"" to build a new database on each run; value=""update"" to modify an existing database; value=""create-drop"" means the same as ""create"" but also drops tables when Hibernate closes; value=""validate"" makes no changes to the database --&gt;
            &lt;property name=""hibernate.hbm2ddl.auto"" value=""validate""/&gt;
            &lt;property name=""hibernate.ejb.naming_strategy"" value=""org.hibernate.cfg.ImprovedNamingStrategy""/&gt;
            &lt;property name=""hibernate.connection.charSet"" value=""UTF-8""/&gt;
            &lt;!-- Uncomment the following two properties for JBoss only --&gt;
            &lt;!-- property name=""hibernate.validator.apply_to_ddl"" value=""false"" /--&gt;
            &lt;!-- property name=""hibernate.validator.autoregister_listeners"" value=""false"" /--&gt;
        &lt;/properties&gt;
    &lt;/persistence-unit&gt;
&lt;/persistence&gt;


Datasource and entity manager beans:

&lt;bean id=""dataSource"" class=""org.springframework.jndi.JndiObjectFactoryBean""&gt;
  &lt;property name=""jndiName"" value=""${datasource.jndiName}""/&gt;
  &lt;property name=""lookupOnStartup"" value=""true""/&gt;
  &lt;property name=""resourceRef"" value=""true"" /&gt;
&lt;/bean&gt;

&lt;bean class=""org.springframework.orm.jpa.JpaTransactionManager"" id=""transactionManager""&gt;
    &lt;property name=""entityManagerFactory"" ref=""entityManagerFactory""/&gt;
&lt;/bean&gt;

&lt;bean class=""org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean"" id=""entityManagerFactory""&gt;
    &lt;property name=""persistenceUnitName"" value=""persistenceUnit""/&gt;
    &lt;property name=""dataSource"" ref=""dataSource""/&gt;
&lt;/bean&gt;


Is that a bug or a configuration issue? Has anyone faced this same issue? 

I don't want to configure a default schema in the persistence unit nor in the entities, because the approach we are following is to keep the datasource configuration outside the application and in a single place by using the JNDI datasource defined in the container context. That way we don't need to worry when deploying to different environments (Dev, QA, Prod, etc)
",-1,-1,-1.0,"Environment:

Java/Spring application that uses JPA/Hibernate for persistence and connects to a Teradata datasource configured in the app container (Tomcat) which is accessed through JNDI.

Versions that I am using:

java: 6
spring: 3.2.4.RELEASE
hibernate.core: 4.2.4.Final
hibernate.entitymanager: 4.2.4.Final
hibernate.validator: 5.0.1.Final
springdata: 1.3.4.RELEASE
javax.validation: 1.1.0.Final


Problem:

There are two Teradata databases in the same server that have a same named table but with different columns:

DatDe001.SFITEM
Columns: [iipcst, iidesc, iivend, updated_at, iisku#, created_at, item_expdt, item_effdt]

DEV_DIG_UMT.SFITEM
Columns: [iipcst, iidesc, iivend, row_updt_tms, iisku#, row_insrt_tms, item_expdt, item_effdt]


As you can see the columns that differ are updated_at -> row_updt_tms and created_at -> row_insrt_tms

I am using a JNDI datasource which is configured using this jdbc url:

jdbc:teradata://&lt;server_ip&gt;/DATABASE=DEV_DIG_UMT,DBS_PORT=1025,COP=OFF,CHARSET=UTF8,TMODE=ANSI  


It is supposed that the jdbc connection will resolve the location of the table using the DATABASE value in that jdbc url. However Hibernate seems to be taking the wrong one: DatDe001.SFITEM when performing the initial schema validation, that is at the moment of context initialization when Spring tries to create the EntityManagerFactory bean:

2013-08-15 13:32:03,635 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000261: Table found: DatDe001.SFITEM
2013-08-15 13:32:03,635 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000037: Columns: [iipcst, iidesc, iivend, updated_at, iisku#, created_at, item_expdt, item_effdt]


So as my JPA entity (see the entity below in the post) does not have those columns, the hibernate validation throws an exception (see the summarized stack trace):

org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.dao.annotation.PersistenceExceptionTranslationPostProcessor#0': Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in file [C:\APP\springsource\vfabric-tc-server-developer-2.9.2.RELEASE\base-instance\wtpwebapps\profile-items\WEB-INF\classes\META-INF\spring\applicationContext.xml]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: persistenceUnit] Unable to build EntityManagerFactory
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:529)
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:458)
...
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'entityManagerFactory' defined in file [C:\APP\springsource\vfabric-tc-server-developer-2.9.2.RELEASE\base-instance\wtpwebapps\profile-items\WEB-INF\classes\META-INF\spring\applicationContext.xml]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: persistenceUnit] Unable to build EntityManagerFactory
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1482)
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:521)
...
Caused by: javax.persistence.PersistenceException: [PersistenceUnit: persistenceUnit] Unable to build EntityManagerFactory
            at org.hibernate.ejb.Ejb3Configuration.buildEntityManagerFactory(Ejb3Configuration.java:924)
            at org.hibernate.ejb.Ejb3Configuration.buildEntityManagerFactory(Ejb3Configuration.java:899)
...
Caused by: org.hibernate.HibernateException: Missing column: row_updt_tms in DatDe001.SFITEM
at org.hibernate.mapping.Table.validateColumns(Table.java:366)
at org.hibernate.cfg.Configuration.validateSchema(Configuration.java:1305)
at org.hibernate.tool.hbm2ddl.SchemaValidator.validate(SchemaValidator.java:155)
at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:508)
at org.hibernate.cfg.Configuration.buildSessionFactory(Configuration.java:1790)
at org.hibernate.ejb.EntityManagerFactoryImpl.&lt;init&gt;(EntityManagerFactoryImpl.java:96)
at org.hibernate.ejb.Ejb3Configuration.buildEntityManagerFactory(Ejb3Configuration.java:914)


After I saw that, I was wondering if this behavior will persist when executing a query statement to the db through JPA/hibernate, or if it will point to the right table in that case.

Then just for investigation purposes I changed my JPA entity to have the same columns that DatDe001.SFITEM table:

@Entity
public class Sfitem implements Serializable {
    private static final long serialVersionUID = 1L;

    @EmbeddedId
    private SfitemPK id;

    @Column(name=""\""iidesc\"""")
    private String iidesc;

    @Column(name=""\""iipcst\"""")
    private BigDecimal iipcst;

    @Column(name=""\""iivend\"""")
    private BigDecimal iivend;

    @Temporal
    @Column(name=""\""item_expdt\"""")
    private Date itemExpdt;

    @Temporal
    @Column(name=""\""created_at\"""")
    private Date createdAt;

    @Temporal
    @Column(name=""\""updated_at\"""")
    private Date updatedAt;

    ...
}


I started the application and it got loaded successfully. Instead of showing the exception now the log looked good:

...
2013-08-15 14:42:52,056 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000261: Table found: DatDe001.SFITEM
2013-08-15 14:42:52,056 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000037: Columns: [iipcst, iidesc, iivend, updated_at, iisku#, created_at, item_expdt, item_effdt]
2013-08-15 14:42:52,061 DEBUG localhost-startStop-1 org.hibernate.internal.SessionFactoryImpl - Checking 0 named HQL queries
2013-08-15 14:42:52,061 DEBUG localhost-startStop-1 org.hibernate.internal.SessionFactoryImpl - Checking 0 named SQL queries
2013-08-15 14:42:52,063 TRACE localhost-startStop-1 org.hibernate.service.internal.AbstractServiceRegistryImpl - Initializing service [role=org.hibernate.service.config.spi.ConfigurationService]
2013-08-15 14:42:52,113 TRACE localhost-startStop-1 org.hibernate.service.internal.AbstractServiceRegistryImpl - Initializing service [role=org.hibernate.stat.spi.StatisticsImplementor]
...


I tried to execute a query to the table and surprisingly found that this time Hibernate was pointing to the right database/schema: DEV_DIG_UMT, the query failed because now the entity had the columns for the other database: DatDe001, see the log:

2013-08-15 14:50:05,731 TRACE tomcat-http--4 org.hibernate.engine.query.spi.QueryPlanCache - Located HQL query plan in cache (SELECT o FROM Sfitem o WHERE o.id.iisku = :iisku AND o.id.itemEffdt &lt;= :date AND coalesce(o.itemExpdt, cast('9999-12-31' as date)) &gt;= :date)
2013-08-15 14:50:05,766 TRACE tomcat-http--4 org.hibernate.engine.query.spi.QueryPlanCache - Located HQL query plan in cache (SELECT o FROM Sfitem o WHERE o.id.iisku = :iisku AND o.id.itemEffdt &lt;= :date AND coalesce(o.itemExpdt, cast('9999-12-31' as date)) &gt;= :date)
2013-08-15 14:50:05,768 TRACE tomcat-http--4 org.hibernate.engine.query.spi.HQLQueryPlan - Find: SELECT o FROM Sfitem o WHERE o.id.iisku = :iisku AND o.id.itemEffdt &lt;= :date AND coalesce(o.itemExpdt, cast('9999-12-31' as date)) &gt;= :date
2013-08-15 14:50:05,772 TRACE tomcat-http--4 org.hibernate.engine.spi.QueryParameters - Named parameters: {iisku=387671, date=2013-08-08}
2013-08-15 14:50:05,810 DEBUG tomcat-http--4 org.hibernate.SQL - select sfitem0_.""iisku#"" as iisku1_0_, sfitem0_.""item_effdt"" as item_eff2_0_, sfitem0_.""created_at"" as created_3_0_, sfitem0_.""iidesc"" as iidesc4_0_, sfitem0_.""iipcst"" as iipcst5_0_, sfitem0_.""iivend"" as iivend6_0_, sfitem0_.""item_expdt"" as item_exp7_0_ from sfitem sfitem0_ where sfitem0_.""iisku#""=? and sfitem0_.""item_effdt""&lt;=? and coalesce(sfitem0_.""item_expdt"", cast('9999-12-31' as DATE))&gt;=?
2013-08-15 14:50:05,832 DEBUG tomcat-http--4 org.hibernate.engine.jdbc.spi.SqlExceptionHelper - could not prepare statement [select sfitem0_.""iisku#"" as iisku1_0_, sfitem0_.""item_effdt"" as item_eff2_0_, sfitem0_.""created_at"" as created_3_0_, sfitem0_.""iidesc"" as iidesc4_0_, sfitem0_.""iipcst"" as iipcst5_0_, sfitem0_.""iivend"" as iivend6_0_, sfitem0_.""item_expdt"" as item_exp7_0_ from sfitem sfitem0_ where sfitem0_.""iisku#""=? and sfitem0_.""item_effdt""&lt;=? and coalesce(sfitem0_.""item_expdt"", cast('9999-12-31' as DATE))&gt;=?]
com.teradata.jdbc.jdbc_4.util.JDBCException: [Teradata Database] [TeraJDBC 14.00.00.21] [Error 3810] [SQLState 42S22] Column/Parameter 'DEV_DIG_UMT.sfitem0_.created_at' does not exist.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:307)
    at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:102)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:320)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:201)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:121)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:112)
...
    at org.hibernate.engine.jdbc.internal.StatementPreparerImpl$5.doPrepare(StatementPreparerImpl.java:161)
    at org.hibernate.engine.jdbc.internal.StatementPreparerImpl$StatementPreparationTemplate.prepareStatement(StatementPreparerImpl.java:182)
    at org.hibernate.engine.jdbc.internal.StatementPreparerImpl.prepareQueryStatement(StatementPreparerImpl.java:159)
    at org.hibernate.loader.Loader.prepareQueryStatement(Loader.java:1859)
        at org.hibernate.loader.Loader.executeQueryStatement(Loader.java:1836)
        at org.hibernate.loader.Loader.executeQueryStatement(Loader.java:1816)
        at org.hibernate.loader.Loader.doQuery(Loader.java:900)
        at org.hibernate.loader.Loader.doQueryAndInitializeNonLazyCollections(Loader.java:342)
        at org.hibernate.loader.Loader.doList(Loader.java:2526)
        at org.hibernate.loader.Loader.doList(Loader.java:2512)
        at org.hibernate.loader.Loader.listIgnoreQueryCache(Loader.java:2342)
        at org.hibernate.loader.Loader.list(Loader.java:2337)
        at org.hibernate.loader.hql.QueryLoader.list(QueryLoader.java:495)


This means that hibernate validation and the query executor routines are behaving differently

The entity with the correct fields:

@Entity
public class Sfitem implements Serializable {
    private static final long serialVersionUID = 1L;

    @EmbeddedId
    private SfitemPK id;

    @Column(name=""\""iidesc\"""")
    private String iidesc;

    @Column(name=""\""iipcst\"""")
    private BigDecimal iipcst;

    @Column(name=""\""iivend\"""")
    private BigDecimal iivend;

    @Column(name=""\""item_expdt\"""")
    private Date itemExpdt;

    @Column(name=""\""row_insrt_tms\"""")
    private Timestamp rowInsrtTms;

    @Column(name=""\""row_updt_tms\"""")
    private Timestamp rowUpdtTms;

    ...
}


Persistence.xml

&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;persistence xmlns=""http://java.sun.com/xml/ns/persistence"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" version=""2.0"" xsi:schemaLocation=""http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd""&gt;
&lt;persistence-unit name=""persistenceUnit"" transaction-type=""RESOURCE_LOCAL""&gt;
        &lt;provider&gt;org.hibernate.ejb.HibernatePersistence&lt;/provider&gt;
        &lt;properties&gt;
            &lt;property name=""hibernate.dialect"" value=""org.hibernate.dialect.TeradataDialect""/&gt;
            &lt;!-- value=""create"" to build a new database on each run; value=""update"" to modify an existing database; value=""create-drop"" means the same as ""create"" but also drops tables when Hibernate closes; value=""validate"" makes no changes to the database --&gt;
            &lt;property name=""hibernate.hbm2ddl.auto"" value=""validate""/&gt;
            &lt;property name=""hibernate.ejb.naming_strategy"" value=""org.hibernate.cfg.ImprovedNamingStrategy""/&gt;
            &lt;property name=""hibernate.connection.charSet"" value=""UTF-8""/&gt;
            &lt;!-- Uncomment the following two properties for JBoss only --&gt;
            &lt;!-- property name=""hibernate.validator.apply_to_ddl"" value=""false"" /--&gt;
            &lt;!-- property name=""hibernate.validator.autoregister_listeners"" value=""false"" /--&gt;
        &lt;/properties&gt;
    &lt;/persistence-unit&gt;
&lt;/persistence&gt;


Datasource and entity manager beans:

&lt;bean id=""dataSource"" class=""org.springframework.jndi.JndiObjectFactoryBean""&gt;
  &lt;property name=""jndiName"" value=""${datasource.jndiName}""/&gt;
  &lt;property name=""lookupOnStartup"" value=""true""/&gt;
  &lt;property name=""resourceRef"" value=""true"" /&gt;
&lt;/bean&gt;

&lt;bean class=""org.springframework.orm.jpa.JpaTransactionManager"" id=""transactionManager""&gt;
    &lt;property name=""entityManagerFactory"" ref=""entityManagerFactory""/&gt;
&lt;/bean&gt;

&lt;bean class=""org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean"" id=""entityManagerFactory""&gt;
    &lt;property name=""persistenceUnitName"" value=""persistenceUnit""/&gt;
    &lt;property name=""dataSource"" ref=""dataSource""/&gt;
&lt;/bean&gt;


Is that a bug or a configuration issue? Has anyone faced this same issue? 

I don't want to configure a default schema in the persistence unit nor in the entities, because the approach we are following is to keep the datasource configuration outside the application and in a single place by using the JNDI datasource defined in the container context. That way we don't need to worry when deploying to different environments (Dev, QA, Prod, etc)
",0
42,18053609,Numeric Overflow in Recursive Query : Teradata,"I'm new to teradata. I want to insert numbers 1 to 1000 into the table test_seq, which is created as below.

create table test_seq(
    seq_id integer
);


After searching on this site, I came up with recusrive query to insert the numbers.

insert into test_seq(seq_id)
with recursive cte(id) as (
    select 1 from test_dual
    union all
    select id + 1 from cte
    where id + 1 &lt;= 1000
    )
select id from cte;


test_dual is created as follows and it contains just a single value. (something like DUAL in Oracle)

create table test_dual(
    test_dummy varchar(1)
);

insert into test_dual values ('X');


But, when I run the insert statement, I get the error, Failure 2616 Numeric overflow occurred during computation.

What did I do wrong here? Isn't the integer datatype enough to hold numeric value 1000?
Also, is there a way to write the query so that i can do away with test_dual table?
",-1,-1,-1.0,"I'm new to teradata. I want to insert numbers 1 to 1000 into the table test_seq, which is created as below.

create table test_seq(
    seq_id integer
);


After searching on this site, I came up with recusrive query to insert the numbers.

insert into test_seq(seq_id)
with recursive cte(id) as (
    select 1 from test_dual
    union all
    select id + 1 from cte
    where id + 1 &lt;= 1000
    )
select id from cte;


test_dual is created as follows and it contains just a single value. (something like DUAL in Oracle)

create table test_dual(
    test_dummy varchar(1)
);

insert into test_dual values ('X');


But, when I run the insert statement, I get the error, Failure 2616 Numeric overflow occurred during computation.

What did I do wrong here? Isn't the integer datatype enough to hold numeric value 1000?
Also, is there a way to write the query so that i can do away with test_dual table?
",3
43,17648372,How can I get table creation scripts on teradata with jdbc?,"I want to get table creation script on teradata with jdbc.
I used this code which I found it on stackoverflow :

   StringBuilder sb = new StringBuilder( 1024 );
                if ( columnCount &gt; 0 ) { 
                    sb.append( ""Create table "").append( rsmd.getTableName( 1 )  ).append( "" ( "" );
                }
                for ( int i = 1; i &lt;= columnCount; i ++ ) {
                    if ( i &gt; 1 ) sb.append( "", "" );
                    String columnName = rsmd.getColumnLabel( i );
                    String columnType = rsmd.getColumnTypeName( i );

                    sb.append( columnName ).append( "" "" ).append( columnType );

                    int precision = rsmd_ddl.getPrecision( i );
                    if ( precision != 0 ) {
                        sb.append( ""( "" ).append( precision  ).append( "" )"" );
                    }
                } // for columns
                sb.append( "" ) "" );


But the problem is : when the type is VARCHAR the precision is 0 but in teradata the column is VARCHAR(100) but how can I find 100 ? 

Thanks.
",-1,-1,-1.0,"I want to get table creation script on teradata with jdbc.
I used this code which I found it on stackoverflow :

   StringBuilder sb = new StringBuilder( 1024 );
                if ( columnCount &gt; 0 ) { 
                    sb.append( ""Create table "").append( rsmd.getTableName( 1 )  ).append( "" ( "" );
                }
                for ( int i = 1; i &lt;= columnCount; i ++ ) {
                    if ( i &gt; 1 ) sb.append( "", "" );
                    String columnName = rsmd.getColumnLabel( i );
                    String columnType = rsmd.getColumnTypeName( i );

                    sb.append( columnName ).append( "" "" ).append( columnType );

                    int precision = rsmd_ddl.getPrecision( i );
                    if ( precision != 0 ) {
                        sb.append( ""( "" ).append( precision  ).append( "" )"" );
                    }
                } // for columns
                sb.append( "" ) "" );


But the problem is : when the type is VARCHAR the precision is 0 but in teradata the column is VARCHAR(100) but how can I find 100 ? 

Thanks.
",3
44,19523643,query to return specific date from teradata timestamp(6),"How can i search for a particular date for eg: '2013-10-22' from teradata timestamp(6) field?

sel * from table A
where date = '2013-10-22';


I tried the above query which is throwing error. Please help!
",-1,-1,-1.0,"How can i search for a particular date for eg: '2013-10-22' from teradata timestamp(6) field?

sel * from table A
where date = '2013-10-22';


I tried the above query which is throwing error. Please help!
",3
45,19776230,"Connecting Java and Teradata: The UserId, Password or Account is invalid","I have been trying to connect to Teradata  

Class.forName(""com.teradata.jdbc.TeraDriver"");
        String connectionString = ""jdbc:teradata://xxx.xxxxxx.com/database=xxxxxx,  tmode=ANSI,  charset=UTF8"";
        String user = ""Rocket512"";
        String password = ""aui8mn5"";
        Connection conn = DriverManager.getConnection(connectionString, user, password);


Got the following 

  Exception in thread ""main"" com.teradata.jdbc.jdbc_4.util.JDBCException: [Teradata Database] 
[TeraJDBC 14.10.00.17] [Error 8017] [SQLState 28000] The UserId, Password or Account is invalid.
        at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:300)
        at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:666)
        at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:216)


I know that the host is specified correctly since i did not get UnknownHost Exception.
Also I have double checked my userid and password are correct.



I ran query suggested by @beni23 (thank you)

select * 
from dbc.logonoff 
where logdate &gt;= date '2013-10-31'


Here is the result that I got



What is Bad Password? I used SQL Assistant with this very password and it works great. Why cannot i connect with Java?
",1,-1,-1.0,"I have been trying to connect to Teradata  

Class.forName(""com.teradata.jdbc.TeraDriver"");
        String connectionString = ""jdbc:teradata://xxx.xxxxxx.com/database=xxxxxx,  tmode=ANSI,  charset=UTF8"";
        String user = ""Rocket512"";
        String password = ""aui8mn5"";
        Connection conn = DriverManager.getConnection(connectionString, user, password);


Got the following 

  Exception in thread ""main"" com.teradata.jdbc.jdbc_4.util.JDBCException: [Teradata Database] 
[TeraJDBC 14.10.00.17] [Error 8017] [SQLState 28000] The UserId, Password or Account is invalid.
        at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:300)
        at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:666)
        at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:216)


I know that the host is specified correctly since i did not get UnknownHost Exception.
Also I have double checked my userid and password are correct.



I ran query suggested by @beni23 (thank you)

select * 
from dbc.logonoff 
where logdate &gt;= date '2013-10-31'


Here is the result that I got



What is Bad Password? I used SQL Assistant with this very password and it works great. Why cannot i connect with Java?
",0
46,20007939,Teradata JDBC Create Volatile Table error 3585,"I have never used the Teradata JDBC driver before, so I may be running into something very obvious...

Basically, I am trying to create a volatile table from which I can query.

Questions


Maybe I just can't run CREATE VOLATILE TABLE statements like this...  What am I doing wrong?
Maybe the driver version is too old? (Currently 12.00.00.110)


Java code

PreparedStatement  pstmtCreateVT = dbConn.prepareStatement(util.getQuery(""CREATE_VOLATILE_TABLE_ABC""));
   pstmtCreateVT.setInt(1, 7);
   pstmtCreateVT.setInt(2, 11);
   pstmtCreateVT.executeQuery();


Query.properties

CREATE_VOLATILE_TABLE_ABC = \
CREATE VOLATILE TABLE ABC \
AS ( SELECT DISTINCT t1.NAME \
  , lookup.price \
FROM SUPERMARKET.FRUITS t1 \
  INNER JOIN SUPERMARKET.PRICING lookup \
    ON 
      lookup.price BETWEEN ? AND ? \
) \
WITH DATA NO PRIMARY INDEX \
ON COMMIT PRESERVE ROWS ;


Error

2013-11-15 09:55:19,220 DEBUG [CreatePoolingConnection.createConnection:102] url: jdbc:teradata://&lt;&lt;some_url_here&gt;&gt;/DATABASE=SUPERMARKET,ENCRYPTDATA=ON,TMODE=ANSI
2013-11-15 09:55:19,673 ERROR [BOMPartsApplicability.fetch:245] ERROR:
com.teradata.jdbc.jdbc_4.util.JDBCException: [Teradata Database] [TeraJDBC 12.00.00.110] [Error 3585] [SQLState HY000] USING modifier NOT allowed with DDL.
   at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:277)
   at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:102)
   at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:285)
   at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:176)
   at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:108)
   at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:99)
   at com.teradata.jdbc.jdbc_4.Statement.executeStatement(Statement.java:309)
   at com.teradata.jdbc.jdbc_4.Statement.prepareRequest(Statement.java:467)
   at com.teradata.jdbc.jdbc_4.PreparedStatement.&lt;init&gt;(PreparedStatement.java:53)
   at com.teradata.jdbc.jdbc_4.TDSession.createPreparedStatement(TDSession.java:506)
   at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalPreparedStatement.&lt;init&gt;(TeraLocalPreparedStatement.java:84)
   at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:328)
   at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:149)
   at GroceryStore.SetupFruits.fetch(BOMPartsApplicability.java:147)
   at GroceryStore.SetupFruits.main(BOMPartsApplicability.java:359)

",-1,-1,-1.0,"I have never used the Teradata JDBC driver before, so I may be running into something very obvious...

Basically, I am trying to create a volatile table from which I can query.

Questions


Maybe I just can't run CREATE VOLATILE TABLE statements like this...  What am I doing wrong?
Maybe the driver version is too old? (Currently 12.00.00.110)


Java code

PreparedStatement  pstmtCreateVT = dbConn.prepareStatement(util.getQuery(""CREATE_VOLATILE_TABLE_ABC""));
   pstmtCreateVT.setInt(1, 7);
   pstmtCreateVT.setInt(2, 11);
   pstmtCreateVT.executeQuery();


Query.properties

CREATE_VOLATILE_TABLE_ABC = \
CREATE VOLATILE TABLE ABC \
AS ( SELECT DISTINCT t1.NAME \
  , lookup.price \
FROM SUPERMARKET.FRUITS t1 \
  INNER JOIN SUPERMARKET.PRICING lookup \
    ON 
      lookup.price BETWEEN ? AND ? \
) \
WITH DATA NO PRIMARY INDEX \
ON COMMIT PRESERVE ROWS ;


Error

2013-11-15 09:55:19,220 DEBUG [CreatePoolingConnection.createConnection:102] url: jdbc:teradata://&lt;&lt;some_url_here&gt;&gt;/DATABASE=SUPERMARKET,ENCRYPTDATA=ON,TMODE=ANSI
2013-11-15 09:55:19,673 ERROR [BOMPartsApplicability.fetch:245] ERROR:
com.teradata.jdbc.jdbc_4.util.JDBCException: [Teradata Database] [TeraJDBC 12.00.00.110] [Error 3585] [SQLState HY000] USING modifier NOT allowed with DDL.
   at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:277)
   at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:102)
   at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:285)
   at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:176)
   at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:108)
   at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:99)
   at com.teradata.jdbc.jdbc_4.Statement.executeStatement(Statement.java:309)
   at com.teradata.jdbc.jdbc_4.Statement.prepareRequest(Statement.java:467)
   at com.teradata.jdbc.jdbc_4.PreparedStatement.&lt;init&gt;(PreparedStatement.java:53)
   at com.teradata.jdbc.jdbc_4.TDSession.createPreparedStatement(TDSession.java:506)
   at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalPreparedStatement.&lt;init&gt;(TeraLocalPreparedStatement.java:84)
   at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:328)
   at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:149)
   at GroceryStore.SetupFruits.fetch(BOMPartsApplicability.java:147)
   at GroceryStore.SetupFruits.main(BOMPartsApplicability.java:359)

",0
47,20132142,Teradata JDBC prepared statement error,"When using Teradata 14 over JDBC I get the following SQL error for this SQL query bindings pair

query

""select regexp_instr('abc', 'a' || ?) s"" 


bindings

""bc""


error

 com.teradata.jdbc.jdbc_4.util.JDBCException : [Teradata Database] [TeraJDBC 14.10.00.17] [Error 3536] [SQLState HY000] UPPERCASE or CASESPECIFIC specified for non-CHAR data.


When I execute the query directly with inline literals it works correctly.

Any ideas what does wrong here?
",-1,-1,-1.0,"When using Teradata 14 over JDBC I get the following SQL error for this SQL query bindings pair

query

""select regexp_instr('abc', 'a' || ?) s"" 


bindings

""bc""


error

 com.teradata.jdbc.jdbc_4.util.JDBCException : [Teradata Database] [TeraJDBC 14.10.00.17] [Error 3536] [SQLState HY000] UPPERCASE or CASESPECIFIC specified for non-CHAR data.


When I execute the query directly with inline literals it works correctly.

Any ideas what does wrong here?
",3
48,22219912,Error in JDBC-Teradata Connectivity,"I have tried connecting to teredata DB through JDBC. I am using the jars 'terajdbc4.jar' and 'tdgssconfig.jar', but I am getting a ClassNotFoundException and NoClassDefFoundError for something kind of com.ncr.teradata.jtdgss.TdgssManager is not found.   

Please help me to find a solution. I have provided my code snippet followed by the error log.
The user name and passwords are correct as connect to the teradata DB through UNIX using that ID.

public class HelloTeradataJDBC {


  public static void main(String[] args) throws Exception {

    String url=""jdbc:teradata://10.10.***.**/DBS_PORT= 1025/DATABASE= ******/TMODE=ANSI,CHARSET=UTF8"";

    try{
    Class.forName(""com.ncr.teradata.TeraDriver"");
    Connection conn=DriverManager.getConnection(url, ""*****"", ""******"");
    //Connection conn=DriverManager.getConnection(connurl, ""javauser1"", ""password1"");

    String query=""select * from xi.san_emp"";


    PreparedStatement stmt=conn.prepareStatement(query);
    ResultSet rs=stmt.executeQuery();
    while(rs.next()) {
        String col1=rs.getString(1);
        System.out.println(""col1=""+col1);
    }
    }catch(ClassNotFoundException e){
        e.printStackTrace();
    }
  }
}


Error:

Exception in thread ""main"" java.lang.NoClassDefFoundError: com/ncr/teradata/jtdgss/TdgssManager
    at com.ncr.teradata.TeraEncrypt.getTDgssVersion(TeraEncrypt.java:548)
    at com.ncr.teradata.jdbc_4.parcel.ConfigFeatureTdgss.&lt;init&gt;(ConfigFeatureTdgss.java:44)
    at com.ncr.teradata.jdbc_4.statemachine.InitDBConfigState.action(InitDBConfigState.java:68)
    at com.ncr.teradata.jdbc_4.statemachine.LogonController.run(LogonController.java:50)
    at com.ncr.teradata.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:150)
    at com.ncr.teradata.jdbc_3.ifjdbc_4.TeraLocalConnection.&lt;init&gt;(TeraLocalConnection.java:89)
    at com.ncr.teradata.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:50)
    at com.ncr.teradata.TeraDriver.connect(TeraDriver.java:214)
    at java.sql.DriverManager.getConnection(DriverManager.java:582)
    at java.sql.DriverManager.getConnection(DriverManager.java:185)
    at HelloTeradataJDBC.main(HelloTeradataJDBC.java:15)
Caused by: java.lang.ClassNotFoundException: com.ncr.teradata.jtdgss.TdgssManager
    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    ... 11 more

",1,-1,-1.0,"I have tried connecting to teredata DB through JDBC. I am using the jars 'terajdbc4.jar' and 'tdgssconfig.jar', but I am getting a ClassNotFoundException and NoClassDefFoundError for something kind of com.ncr.teradata.jtdgss.TdgssManager is not found.   

Please help me to find a solution. I have provided my code snippet followed by the error log.
The user name and passwords are correct as connect to the teradata DB through UNIX using that ID.

public class HelloTeradataJDBC {


  public static void main(String[] args) throws Exception {

    String url=""jdbc:teradata://10.10.***.**/DBS_PORT= 1025/DATABASE= ******/TMODE=ANSI,CHARSET=UTF8"";

    try{
    Class.forName(""com.ncr.teradata.TeraDriver"");
    Connection conn=DriverManager.getConnection(url, ""*****"", ""******"");
    //Connection conn=DriverManager.getConnection(connurl, ""javauser1"", ""password1"");

    String query=""select * from xi.san_emp"";


    PreparedStatement stmt=conn.prepareStatement(query);
    ResultSet rs=stmt.executeQuery();
    while(rs.next()) {
        String col1=rs.getString(1);
        System.out.println(""col1=""+col1);
    }
    }catch(ClassNotFoundException e){
        e.printStackTrace();
    }
  }
}


Error:

Exception in thread ""main"" java.lang.NoClassDefFoundError: com/ncr/teradata/jtdgss/TdgssManager
    at com.ncr.teradata.TeraEncrypt.getTDgssVersion(TeraEncrypt.java:548)
    at com.ncr.teradata.jdbc_4.parcel.ConfigFeatureTdgss.&lt;init&gt;(ConfigFeatureTdgss.java:44)
    at com.ncr.teradata.jdbc_4.statemachine.InitDBConfigState.action(InitDBConfigState.java:68)
    at com.ncr.teradata.jdbc_4.statemachine.LogonController.run(LogonController.java:50)
    at com.ncr.teradata.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:150)
    at com.ncr.teradata.jdbc_3.ifjdbc_4.TeraLocalConnection.&lt;init&gt;(TeraLocalConnection.java:89)
    at com.ncr.teradata.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:50)
    at com.ncr.teradata.TeraDriver.connect(TeraDriver.java:214)
    at java.sql.DriverManager.getConnection(DriverManager.java:582)
    at java.sql.DriverManager.getConnection(DriverManager.java:185)
    at HelloTeradataJDBC.main(HelloTeradataJDBC.java:15)
Caused by: java.lang.ClassNotFoundException: com.ncr.teradata.jtdgss.TdgssManager
    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    ... 11 more

",0
49,22500587,Subtraction of Timestamps teradata,"I am trying to find difference between 2 timestamps in teradata. I am using the following code:

(date1-date2)day(4) to second  as time_diff


This is giving the error: Interval Field Overflow. What could be reason for that? Is there some other way to calculate the difference between 2 timestamps?

And when I am using this:

case when(((date2+ INTERVAL '72' hour )-date1) day(4) to second)&gt;0 then '&lt;72 hrs'


then the error i am getting is Invalid operation for DateTime or Interval.
Please help
",-1,-1,-1.0,"I am trying to find difference between 2 timestamps in teradata. I am using the following code:

(date1-date2)day(4) to second  as time_diff


This is giving the error: Interval Field Overflow. What could be reason for that? Is there some other way to calculate the difference between 2 timestamps?

And when I am using this:

case when(((date2+ INTERVAL '72' hour )-date1) day(4) to second)&gt;0 then '&lt;72 hrs'


then the error i am getting is Invalid operation for DateTime or Interval.
Please help
",3
50,22511692,teradata error while creating sp with cursor,"I am new to Teradata and created some stored procedure using cursor and getting this error. Please help

An owner referenced by user does not have SELECT access to(some column in table)
Syntax error, expected something like an 'END' keyword between ';' and the 'DECLARE' keyword.'.
 Referring to undefined cursor 'abc'.
An owner referenced by user does not have SELECT access to (some column in table)
An owner referenced by user does not have SELECT access to (some column in table)
 Referring to undefined cursor 'abc'.
 Referring to undefined cursor 'abc'.
",1,-1,-1.0,"I am new to Teradata and created some stored procedure using cursor and getting this error. Please help

An owner referenced by user does not have SELECT access to(some column in table)
Syntax error, expected something like an 'END' keyword between ';' and the 'DECLARE' keyword.'.
 Referring to undefined cursor 'abc'.
An owner referenced by user does not have SELECT access to (some column in table)
An owner referenced by user does not have SELECT access to (some column in table)
 Referring to undefined cursor 'abc'.
 Referring to undefined cursor 'abc'.
",3
51,22681732,SUBSTR BETWEEN in Teradata,"Output of the following SQL statement is '279A'

    SELECT
    SUBSTR('H0279A',3)


I am confused why this query returns 1:

        SELECT 
        CASE 
            WHEN SUBSTR('H0279A',3) BETWEEN '0000' AND '9999'   
            THEN 1
        ELSE 0
        END


How can '279A' be between '0000' AND '9999' ?
I am using Teradata.
",0,-1,-1.0,"Output of the following SQL statement is '279A'

    SELECT
    SUBSTR('H0279A',3)


I am confused why this query returns 1:

        SELECT 
        CASE 
            WHEN SUBSTR('H0279A',3) BETWEEN '0000' AND '9999'   
            THEN 1
        ELSE 0
        END


How can '279A' be between '0000' AND '9999' ?
I am using Teradata.
",3
52,22956763,Teradata-replacing self join,"I have table in Teradata and have trillion of record.
 Temp- with cat_nbr as PI

Cat_nbr | brand_Nbr |card_nbr
1       |  10       | 100
1       |   10      |101
1       |20         | 100
1       | 20        | 102
2       |10         | 100
2       | 10        |103
2       |30         |100
2       |30         |105
3       |40         |106
3       | 30        |107


I need to find out categories total no of customer for a particular brand.
Just an ex. for brand no:10
First we need to check which cat have brand no 10, in this cat 1,2 have it.
Then for all cutomer in cat 1,2 ; we need count(distinct card_no).

result shoul be like 

brand_nbr|total_cust
10       | 5


I have written the below query to achive this:-

select k.brand_nbr,count(distinct l.card_nbr) 
from temp k join temp l on k.cat_nbr=l.cat_nbr
group by 1;


It give me proper result but the thing , we have trillion of records in table and when I do run the query it goes on processing like more than 2 hrs.

I need a solution to improve the performance so that it can max in 30 min.
I have checked the amps , there are 16 amps for my database.

Please masters help me out if you have any solution for this.

Thanks in advance.
",1,-1,-1.0,"I have table in Teradata and have trillion of record.
 Temp- with cat_nbr as PI

Cat_nbr | brand_Nbr |card_nbr
1       |  10       | 100
1       |   10      |101
1       |20         | 100
1       | 20        | 102
2       |10         | 100
2       | 10        |103
2       |30         |100
2       |30         |105
3       |40         |106
3       | 30        |107


I need to find out categories total no of customer for a particular brand.
Just an ex. for brand no:10
First we need to check which cat have brand no 10, in this cat 1,2 have it.
Then for all cutomer in cat 1,2 ; we need count(distinct card_no).

result shoul be like 

brand_nbr|total_cust
10       | 5


I have written the below query to achive this:-

select k.brand_nbr,count(distinct l.card_nbr) 
from temp k join temp l on k.cat_nbr=l.cat_nbr
group by 1;


It give me proper result but the thing , we have trillion of records in table and when I do run the query it goes on processing like more than 2 hrs.

I need a solution to improve the performance so that it can max in 30 min.
I have checked the amps , there are 16 amps for my database.

Please masters help me out if you have any solution for this.

Thanks in advance.
",3
53,23117412,Query run in Teradata Assistant vs Pyodbc returns different results,"I am running this exact query (hard coded values and all) in both Teradata SQL Assistant and via pyodbc in Python 2.7. I receive different results from the two methods, and it appears the pyodbc one is incorrect.

SELECT serial_num, event_desc,event_level, count(*) as ""Count"", MAX(event_ts) as ""Latest Event Timestamp""
    FROM mytable
    WHERE
    serial_num in('serial1','serial2')
    AND event_ts &gt;= '2014-01-01 00:00:00.000'
    AND event_ts &lt;= '2014-03-31 23:59:59.999'
    and event_desc = 'My Test Event'
    group by serial_num, event_desc,event_level
    order by serial_num, ""Latest Event Timestamp""


Teradata Assistant results look like this

serial_num  event_desc      event_level     Count   Latest Event Timestamp
serial1     My Test Event   1               5       3/4/2014 10:03:28.000000
serial2     My Test Event   1               12      3/27/2014 13:01:25.000000
serial2     My Test Event   2               4       3/27/2014 13:32:59.000000


However, when run in Python using pyodbc, I get different results:

serial_num  event_desc      event_level     Count   Latest Event Timestamp
serial1     'My Test Event' '1'             2       datetime.datetime(2014, 1, 3, 10, 5, 15
serial2     'My Test Event' '2'             1       datetime.datetime(2014, 3, 14, 2, 22, 47
serial2     'My Test Event' '1'             4       datetime.datetime(2014, 3, 22, 6, 36, 40


The differences are in the count and timestamp columns. I am not sure what, exactly, the query is doing in python. It returns valid time stamps, but they aren't the max time same for the group by and the count is incorrect. Why?

This is the ungroupped data:

serial_num  event_desc      event_level     Latest Event Timestamp
serial1     My Test Event   1               1/2/2014 07:11:22.000000
serial1     My Test Event   1               1/3/2014 10:05:15.000000
serial1     My Test Event   1               1/19/2014 13:32:17.000000
serial1     My Test Event   1               3/4/2014 09:16:15.000000
serial1     My Test Event   1               3/4/2014 10:03:28.000000
serial2     My Test Event   1               1/19/2014 14:04:47.000000
serial2     My Test Event   1               1/28/2014 13:27:00.000000
serial2     My Test Event   2               1/28/2014 13:57:12.000000
serial2     My Test Event   1               2/5/2014 01:36:47.000000
serial2     My Test Event   2               2/5/2014 02:56:53.000000
serial2     My Test Event   1               2/23/2014 01:57:19.000000
serial2     My Test Event   1               2/27/2014 13:50:08.000000
serial2     My Test Event   1               2/28/2014 13:55:51.000000
serial2     My Test Event   1               3/9/2014 15:31:00.000000
serial2     My Test Event   1               3/14/2014 01:31:36.000000
serial2     My Test Event   2               3/14/2014 02:22:47.000000
serial2     My Test Event   1               3/16/2014 03:29:04.000000
serial2     My Test Event   1               3/22/2014 02:07:04.000000
serial2     My Test Event   1               3/22/2014 06:36:40.000000
serial2     My Test Event   1               3/27/2014 13:01:25.000000
serial2     My Test Event   2               3/27/2014 13:32:59.000000


The python code that is running is this

qry = &lt;&lt;Above SQL&gt;&gt;
conn=pyodbc.connect('DRIVER={Teradata};DBCNAME=server;UID=user;PWD=password;QUIETMODE=YES;', ANSI=True, autocommit=True)
teracurs=self.conn.cursor()
res = teracurs.execute(qry).fetchall()        


Why do the MAX() and count statements work as expected if I run in Teradata SQL Assistant, but not if I run the query via pyodbc? 
",-1,-1,-1.0,"I am running this exact query (hard coded values and all) in both Teradata SQL Assistant and via pyodbc in Python 2.7. I receive different results from the two methods, and it appears the pyodbc one is incorrect.

SELECT serial_num, event_desc,event_level, count(*) as ""Count"", MAX(event_ts) as ""Latest Event Timestamp""
    FROM mytable
    WHERE
    serial_num in('serial1','serial2')
    AND event_ts &gt;= '2014-01-01 00:00:00.000'
    AND event_ts &lt;= '2014-03-31 23:59:59.999'
    and event_desc = 'My Test Event'
    group by serial_num, event_desc,event_level
    order by serial_num, ""Latest Event Timestamp""


Teradata Assistant results look like this

serial_num  event_desc      event_level     Count   Latest Event Timestamp
serial1     My Test Event   1               5       3/4/2014 10:03:28.000000
serial2     My Test Event   1               12      3/27/2014 13:01:25.000000
serial2     My Test Event   2               4       3/27/2014 13:32:59.000000


However, when run in Python using pyodbc, I get different results:

serial_num  event_desc      event_level     Count   Latest Event Timestamp
serial1     'My Test Event' '1'             2       datetime.datetime(2014, 1, 3, 10, 5, 15
serial2     'My Test Event' '2'             1       datetime.datetime(2014, 3, 14, 2, 22, 47
serial2     'My Test Event' '1'             4       datetime.datetime(2014, 3, 22, 6, 36, 40


The differences are in the count and timestamp columns. I am not sure what, exactly, the query is doing in python. It returns valid time stamps, but they aren't the max time same for the group by and the count is incorrect. Why?

This is the ungroupped data:

serial_num  event_desc      event_level     Latest Event Timestamp
serial1     My Test Event   1               1/2/2014 07:11:22.000000
serial1     My Test Event   1               1/3/2014 10:05:15.000000
serial1     My Test Event   1               1/19/2014 13:32:17.000000
serial1     My Test Event   1               3/4/2014 09:16:15.000000
serial1     My Test Event   1               3/4/2014 10:03:28.000000
serial2     My Test Event   1               1/19/2014 14:04:47.000000
serial2     My Test Event   1               1/28/2014 13:27:00.000000
serial2     My Test Event   2               1/28/2014 13:57:12.000000
serial2     My Test Event   1               2/5/2014 01:36:47.000000
serial2     My Test Event   2               2/5/2014 02:56:53.000000
serial2     My Test Event   1               2/23/2014 01:57:19.000000
serial2     My Test Event   1               2/27/2014 13:50:08.000000
serial2     My Test Event   1               2/28/2014 13:55:51.000000
serial2     My Test Event   1               3/9/2014 15:31:00.000000
serial2     My Test Event   1               3/14/2014 01:31:36.000000
serial2     My Test Event   2               3/14/2014 02:22:47.000000
serial2     My Test Event   1               3/16/2014 03:29:04.000000
serial2     My Test Event   1               3/22/2014 02:07:04.000000
serial2     My Test Event   1               3/22/2014 06:36:40.000000
serial2     My Test Event   1               3/27/2014 13:01:25.000000
serial2     My Test Event   2               3/27/2014 13:32:59.000000


The python code that is running is this

qry = &lt;&lt;Above SQL&gt;&gt;
conn=pyodbc.connect('DRIVER={Teradata};DBCNAME=server;UID=user;PWD=password;QUIETMODE=YES;', ANSI=True, autocommit=True)
teracurs=self.conn.cursor()
res = teracurs.execute(qry).fetchall()        


Why do the MAX() and count statements work as expected if I run in Teradata SQL Assistant, but not if I run the query via pyodbc? 
",1
54,23349799,Make Teradata Sql Assistant default to vertical tabs,"Is there a way to make Teradata Sql Assistant default to vertical columns?  I can manually set my results in a vertical tab, but when I execute the query again it puts them right back at the bottom in a horizontal tab.

I tried untabbed, and placed the windows where I wanted them, and when I execute a query it sticks the results behind the query window and shrinks the results window!

This is driving me absolutely insane... Is there any way to just make the results return in a default vertical window so I can see them and my query side by side?
",-1,-1,-1.0,"Is there a way to make Teradata Sql Assistant default to vertical columns?  I can manually set my results in a vertical tab, but when I execute the query again it puts them right back at the bottom in a horizontal tab.

I tried untabbed, and placed the windows where I wanted them, and when I execute a query it sticks the results behind the query window and shrinks the results window!

This is driving me absolutely insane... Is there any way to just make the results return in a default vertical window so I can see them and my query side by side?
",3
55,23528583,Cannot nest aggregate operations - Teradata Case When,"With help I recently received on SO, I managed to get data using the following snippet within my SELECT query:

COUNT(CASE WHEN CAST(EVENT_TIMESTAMP AS DATE) - CONTRACT_EFFECTIVE_DATE = 2 THEN 1 END) AS ""2 day"",


This produced the count of instances of an event of a particular condition being met.

But I would now like to calculate the number of people (Unique email addresses) associated with these events.

I tried this:

COUNT(CASE WHEN CAST(EVENT_TIMESTAMP AS DATE) - CONTRACT_EFFECTIVE_DATE = 2 THEN COUNT(DISTINCT TmpNIMSalesForceDB.EMAIL) END) AS ""2 day""


The logic seems right in my mind - if the result of the case is 2 then count the distinct email addresses for that case.

But it seems SQL (Or Teradata) does not like nesting in this way. The resulting error message was ""Cannot next aggregate function"".

Can I use COUNT(DISTINCT in this way?
",-1,-1,-1.0,"With help I recently received on SO, I managed to get data using the following snippet within my SELECT query:

COUNT(CASE WHEN CAST(EVENT_TIMESTAMP AS DATE) - CONTRACT_EFFECTIVE_DATE = 2 THEN 1 END) AS ""2 day"",


This produced the count of instances of an event of a particular condition being met.

But I would now like to calculate the number of people (Unique email addresses) associated with these events.

I tried this:

COUNT(CASE WHEN CAST(EVENT_TIMESTAMP AS DATE) - CONTRACT_EFFECTIVE_DATE = 2 THEN COUNT(DISTINCT TmpNIMSalesForceDB.EMAIL) END) AS ""2 day""


The logic seems right in my mind - if the result of the case is 2 then count the distinct email addresses for that case.

But it seems SQL (Or Teradata) does not like nesting in this way. The resulting error message was ""Cannot next aggregate function"".

Can I use COUNT(DISTINCT in this way?
",3
56,23639344,Split String Teradata SQL,"I'm looking to split a string in Teradata.

The table might look something like this.

column1
hello:goodbye:afternoon


I'm trying to use SUBSTRING and INSTR to extract specific words.  So, say I want to select ""goodbye"". I'm trying the following query. 

SELECT SUBSTRING(a.column1 from index(a.column1,':')+1 for INSTR(a.column1,':',0,2))
FROM db.table as a


I get the following error.

SELECT Failed. [3707] Syntax error, expected something like ')' between the word 'INSTR' and '('


I'm not sure why I'm getting that error.  It lets me use INDEX to deduce a number in place of INSTR, so I'm not sure why it is acting this way when I use INSTR.
",-1,-1,-1.0,"I'm looking to split a string in Teradata.

The table might look something like this.

column1
hello:goodbye:afternoon


I'm trying to use SUBSTRING and INSTR to extract specific words.  So, say I want to select ""goodbye"". I'm trying the following query. 

SELECT SUBSTRING(a.column1 from index(a.column1,':')+1 for INSTR(a.column1,':',0,2))
FROM db.table as a


I get the following error.

SELECT Failed. [3707] Syntax error, expected something like ')' between the word 'INSTR' and '('


I'm not sure why I'm getting that error.  It lets me use INDEX to deduce a number in place of INSTR, so I'm not sure why it is acting this way when I use INSTR.
",3
57,23111540,Null values in IN clause - Teradata,"Here is my Table1

personid
1
?
2
3
4
?
6




Here is my query

select * 
from table2
where personid not in 
(
select personid
from table1
)


The result is nothing



Here is my second query

select * 
from table2
where personid not in 
(
select personid
from table1         
where personid is not null
)


The result is ok



Question : why the first query did not work ? I can't see any logical problem . Do nulls skrew up teradata ?
",-1,-1,-1.0,"Here is my Table1

personid
1
?
2
3
4
?
6




Here is my query

select * 
from table2
where personid not in 
(
select personid
from table1
)


The result is nothing



Here is my second query

select * 
from table2
where personid not in 
(
select personid
from table1         
where personid is not null
)


The result is ok



Question : why the first query did not work ? I can't see any logical problem . Do nulls skrew up teradata ?
",3
58,23021930,How to group substrings in Teradata 14?,"I have the following table in Teradata 14 ,  I am not allowed to write procedures and functions myself, but i can use strtok, strtok_split_to_table etc

id  property
1   1234X (Yel), 2225Y (Red), 1234X (Gre),
2
3   1222Y (Pin), 
4   1134E (Yel), 4565Y (Whi), 1134E (Red), 2222Y (Red), 


How can I group the above table so that each object would have all attributes listed in one brackets

id  property
1   1234X (Yel Gre), 2225Y (Red), 
2   
3   1222Y (Pin ),
4   1134E (Yel Red), 4565Y (Whi), 2222Y (Red), 


The property code is always a 5 character string, e.g. 1222Y . The color code is always 3 character , e.g. Pin 



I tried using this solution but got an error A column or character expression is larger than max size 

In addition I tried strtok_split_to_table and was able to create a modified table, but do not how to proceed from that
",-1,1,-1.0,"I have the following table in Teradata 14 ,  I am not allowed to write procedures and functions myself, but i can use strtok, strtok_split_to_table etc

id  property
1   1234X (Yel), 2225Y (Red), 1234X (Gre),
2
3   1222Y (Pin), 
4   1134E (Yel), 4565Y (Whi), 1134E (Red), 2222Y (Red), 


How can I group the above table so that each object would have all attributes listed in one brackets

id  property
1   1234X (Yel Gre), 2225Y (Red), 
2   
3   1222Y (Pin ),
4   1134E (Yel Red), 4565Y (Whi), 2222Y (Red), 


The property code is always a 5 character string, e.g. 1222Y . The color code is always 3 character , e.g. Pin 



I tried using this solution but got an error A column or character expression is larger than max size 

In addition I tried strtok_split_to_table and was able to create a modified table, but do not how to proceed from that
",3
59,22822878,copy table from teradata to mysql,"i have searched for hours with no real solution.

I want to setup an ongoing task (everynight). I have a table in a Teradata database on server 1. Everynight i need to copy an entire table from this teradata instance to my development server (server 2) that has MySQL 5.6.

How do i copy an entire table form server 1 to server 2?

Things i have tried:
1)Select all data from table x into ResultSet from teradata server 1. Insert into mysql via preparedStatement. But this is crazy slow. Also i am not sure how to Drop the table and recreate it each night with the schema from the teradata server.

Any help please?
",1,-1,-1.0,"i have searched for hours with no real solution.

I want to setup an ongoing task (everynight). I have a table in a Teradata database on server 1. Everynight i need to copy an entire table from this teradata instance to my development server (server 2) that has MySQL 5.6.

How do i copy an entire table form server 1 to server 2?

Things i have tried:
1)Select all data from table x into ResultSet from teradata server 1. Insert into mysql via preparedStatement. But this is crazy slow. Also i am not sure how to Drop the table and recreate it each night with the schema from the teradata server.

Any help please?
",3
60,22793620,Teradata 'Unable to get catalog string' error when using ODBC to connect,"I am trying to reach a remote host that is running Teradata services via ODBC.
The host that I am trying to connect from is 64-bit RHEL 6.x with the following Teradata software installed:


bteq
fastexp
fastld 
jmsaxsmod
mload    
mqaxsmod 
npaxsmod 
sqlpp 
tdodbc
tdwallet
tptbase 
tptstream
tpump    


When I try to connect to the remote host via Python (interactive session), I receive a 'Unable to get catalog string' error:

[@myhost:/path/to/scripts] -&gt;python

Python 2.6.6 (r266:84292, Nov 21 2013, 10:50:32)

[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2

Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

&gt;&gt;&gt; import pyodbc

&gt;&gt;&gt; pyodbc.pooling = False

&gt;&gt;&gt; cn = pyodbc.connect(""DRIVER={Teradata}; SERVER=12.245.67.255:1025;UID=usr;PWD=pwd"", ANSI = True)

Traceback (most recent call last):

  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;

pyodbc.Error: ('28000', '[28000] [Teradata][ODBC Teradata Driver] Unable to get catalog string. (0) (SQLDriverConnect)')


Furthermore, when I try to use isql (from the unixODBC yum package), I receive the same error

[@my_host:/path/to/scripts] -&gt;isql -v proddsn

[28000][Teradata][ODBC Teradata Driver] Unable to get catalog string.

[ISQL]ERROR: Could not SQLConnect

",-1,-1,-1.0,"I am trying to reach a remote host that is running Teradata services via ODBC.
The host that I am trying to connect from is 64-bit RHEL 6.x with the following Teradata software installed:


bteq
fastexp
fastld 
jmsaxsmod
mload    
mqaxsmod 
npaxsmod 
sqlpp 
tdodbc
tdwallet
tptbase 
tptstream
tpump    


When I try to connect to the remote host via Python (interactive session), I receive a 'Unable to get catalog string' error:

[@myhost:/path/to/scripts] -&gt;python

Python 2.6.6 (r266:84292, Nov 21 2013, 10:50:32)

[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2

Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

&gt;&gt;&gt; import pyodbc

&gt;&gt;&gt; pyodbc.pooling = False

&gt;&gt;&gt; cn = pyodbc.connect(""DRIVER={Teradata}; SERVER=12.245.67.255:1025;UID=usr;PWD=pwd"", ANSI = True)

Traceback (most recent call last):

  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;

pyodbc.Error: ('28000', '[28000] [Teradata][ODBC Teradata Driver] Unable to get catalog string. (0) (SQLDriverConnect)')


Furthermore, when I try to use isql (from the unixODBC yum package), I receive the same error

[@my_host:/path/to/scripts] -&gt;isql -v proddsn

[28000][Teradata][ODBC Teradata Driver] Unable to get catalog string.

[ISQL]ERROR: Could not SQLConnect

",1
61,21592783,Nesting qualify statements in Teradata,"Is it possible to ""nest"" qualify statements in Teradata?

I have some data that looks like this:

event_id = 1:

 user_id        action_timestamp
 971,134,265   17mar2010 20:16:56
 739,071,748   17mar2010 22:19:59
 919,853,934   18mar2010 15:47:49
 919,853,934   18mar2010 15:55:21
 919,853,934   18mar2010 16:01:20
 919,853,934   18mar2010 16:01:48
 919,853,934   18mar2010 16:04:52
 472,665,603   20mar2010 18:23:58
 472,665,603   20mar2010 18:24:07
 472,665,603   20mar2010 18:24:26
  ....
 event_id = 2:     
 971,134,265   17mar2069 20:16:56
 739,071,748   17mar2069 22:19:59
 919,853,934   18mar2069 15:47:49
 919,853,934   18mar2069 15:55:21
 919,853,934   18mar2069 16:01:20
 919,853,934   18mar2069 16:01:48
 919,853,934   18mar2069 16:04:52
 472,665,603   20mar2069 18:23:58
 472,665,603   20mar2069 18:24:07
 472,665,603   20mar2069 18:24:26


For user 919,853,934, I would like to grab ""18mar2010 16:04:52"" action (the last one in the first cluster of events).

I tried this, which does not grab the right date:

SELECT action_timestamp
       ,user_id
       ,event_id
FROM table
WHERE ...
QUALIFY (
    MAX(action_timestampt) OVER (PARTITION BY user_id, event_id) = action_timestamp
    AND MIN(action_timestamp) OVER (PARTITION BY user_id) = action_timestamp
) 


This actually makes sense since the MAX and MIN apply to the whole data, rather than sequentially.

I also tried 2 separate qualify statements to get the MIN() part to apply to the subset of the data created by the MAX() part, but that errors.
",-1,-1,-1.0,"Is it possible to ""nest"" qualify statements in Teradata?

I have some data that looks like this:

event_id = 1:

 user_id        action_timestamp
 971,134,265   17mar2010 20:16:56
 739,071,748   17mar2010 22:19:59
 919,853,934   18mar2010 15:47:49
 919,853,934   18mar2010 15:55:21
 919,853,934   18mar2010 16:01:20
 919,853,934   18mar2010 16:01:48
 919,853,934   18mar2010 16:04:52
 472,665,603   20mar2010 18:23:58
 472,665,603   20mar2010 18:24:07
 472,665,603   20mar2010 18:24:26
  ....
 event_id = 2:     
 971,134,265   17mar2069 20:16:56
 739,071,748   17mar2069 22:19:59
 919,853,934   18mar2069 15:47:49
 919,853,934   18mar2069 15:55:21
 919,853,934   18mar2069 16:01:20
 919,853,934   18mar2069 16:01:48
 919,853,934   18mar2069 16:04:52
 472,665,603   20mar2069 18:23:58
 472,665,603   20mar2069 18:24:07
 472,665,603   20mar2069 18:24:26


For user 919,853,934, I would like to grab ""18mar2010 16:04:52"" action (the last one in the first cluster of events).

I tried this, which does not grab the right date:

SELECT action_timestamp
       ,user_id
       ,event_id
FROM table
WHERE ...
QUALIFY (
    MAX(action_timestampt) OVER (PARTITION BY user_id, event_id) = action_timestamp
    AND MIN(action_timestamp) OVER (PARTITION BY user_id) = action_timestamp
) 


This actually makes sense since the MAX and MIN apply to the whole data, rather than sequentially.

I also tried 2 separate qualify statements to get the MIN() part to apply to the subset of the data created by the MAX() part, but that errors.
",3
62,21592383,How to execute multiple queries in teradata?,"For example a query : create table ; select xxx  ; delete  ;

How to execute it in one session ? 

I saw one answer to a similar question about mysql. The trick is to turn on allow multiple queries 

String dbUrl = ""jdbc:mysql:///test?allowMultiQueries=true"";




For teradata specifically,
what is the solution ?

I tried 

String dbUrl = ""jdbc:odbc:dsn?allowMultiQueries=true"";


It is not properly working ?
",0,-1,-1.0,"For example a query : create table ; select xxx  ; delete  ;

How to execute it in one session ? 

I saw one answer to a similar question about mysql. The trick is to turn on allow multiple queries 

String dbUrl = ""jdbc:mysql:///test?allowMultiQueries=true"";




For teradata specifically,
what is the solution ?

I tried 

String dbUrl = ""jdbc:odbc:dsn?allowMultiQueries=true"";


It is not properly working ?
",3
63,21093020,"subquery that starts with ""with"" in teradata is not working","I have a query (that I cannot modify) that starts like this

with CodeSet (
   code_context_c
 , bom_index_c
 , src_qs_c
 , src_code_set_c 
 , src_code_set_x 
 , src_code_value_c
 , src_code_value_x
 , tgt_code_set_c
 , tgt_code_value_c
  ) as (
SELECT ...


and then goes on. Now I need to use it as a subquery and do something like

select * from (with CodeSet (
       code_context_c
     , bom_index_c
     , src_qs_c
     , src_code_set_c 
     , src_code_set_x 
     , src_code_value_c
     , src_code_value_x
     , tgt_code_set_c
     , tgt_code_value_c
      ) as (
    SELECT ...


but Teradata does not like it... Anyone has seen this before? Changing the query would require some time and I would prefer not to. Anyone can help me out here? 

Error message is:

SELECT Failed.  [3707] Syntax error, expected something like a name or a Unicode delimited identifier or '(' between the 'from' keyword and the 'as' keyword.

Thanks in advance,
Umberto
",-1,-1,-1.0,"I have a query (that I cannot modify) that starts like this

with CodeSet (
   code_context_c
 , bom_index_c
 , src_qs_c
 , src_code_set_c 
 , src_code_set_x 
 , src_code_value_c
 , src_code_value_x
 , tgt_code_set_c
 , tgt_code_value_c
  ) as (
SELECT ...


and then goes on. Now I need to use it as a subquery and do something like

select * from (with CodeSet (
       code_context_c
     , bom_index_c
     , src_qs_c
     , src_code_set_c 
     , src_code_set_x 
     , src_code_value_c
     , src_code_value_x
     , tgt_code_set_c
     , tgt_code_value_c
      ) as (
    SELECT ...


but Teradata does not like it... Anyone has seen this before? Changing the query would require some time and I would prefer not to. Anyone can help me out here? 

Error message is:

SELECT Failed.  [3707] Syntax error, expected something like a name or a Unicode delimited identifier or '(' between the 'from' keyword and the 'as' keyword.

Thanks in advance,
Umberto
",3
64,20757887,Ambiguous error in Lookup Override for teradata query,"I am using below self join query as Lookup override in informatica. This is running        fine in teradata.

SELECT A.region_cd AS REGION_CODE, 
       A.enp_no    AS ENP_NBR, 
       B.sla_cd    AS SLA_CODE 
FROM   edb_man_work.emp A, 
       edb_man_work.emp B 
WHERE  A.company_no = Trim(Cast(B.enp_no AS INTEGER)) 
       AND A.region_cd = B.region_cd 


This is running fine in teradata but while running in mapping it is giving error

as Column SLA_CD is ambiguous.

I am not sure why this is giving this type of error.
",-1,-1,-1.0,"I am using below self join query as Lookup override in informatica. This is running        fine in teradata.

SELECT A.region_cd AS REGION_CODE, 
       A.enp_no    AS ENP_NBR, 
       B.sla_cd    AS SLA_CODE 
FROM   edb_man_work.emp A, 
       edb_man_work.emp B 
WHERE  A.company_no = Trim(Cast(B.enp_no AS INTEGER)) 
       AND A.region_cd = B.region_cd 


This is running fine in teradata but while running in mapping it is giving error

as Column SLA_CD is ambiguous.

I am not sure why this is giving this type of error.
",3
65,20084432,Why subquery does not work in teradata?,"I tried this

SELECT * 
FROM
( SELECT *
FROM mytable;
);


and this

SELECT * 
FROM
( SELECT *
FROM mytable
);


Why these simple queries do not execute in Teradata? 
",-1,-1,-1.0,"I tried this

SELECT * 
FROM
( SELECT *
FROM mytable;
);


and this

SELECT * 
FROM
( SELECT *
FROM mytable
);


Why these simple queries do not execute in Teradata? 
",3
66,23776604,How to import data into teradata tables from delimited file using BTEQ import?,"I am trying to execute following bteq command on linux environment but couldn't load data properly into Teradata DB server. Can someone please advise me to resolve the below issue that I am facing while loading.

BTEQ Command used :

.SET width 64000;
.SET session transaction btet;
.logmech ldap
.logon XXXXXXX/XXXXXXXX,********;

DATABASE corecm;

.PACK 1000
.IMPORT VARTEXT '~' FILE=/v/global/user/application_event_bus_evt
.REPEAT *
USING(APPLICATION_EVENT_ID CHAR(24),BUS_EVT_ID CHAR(24),BUS_EVT_VID BIGINT,BUS_EVT_RESTATE_IN SMALLINT)

insert into corecm.application_event_bus_evt (APPLICATION_EVENT_ID
, BUS_EVT_ID
, BUS_EVT_VID
, BUS_EVT_RESTATE_IN
)
values
( COALESCE(:APPLICATION_EVENT_ID,1)
, COALESCE(:BUS_EVT_ID,1)
, COALESCE(:BUS_EVT_VID,1)
, COALESCE(:BUS_EVT_RESTATE_IN,1)
) ;
.LOGOFF;
.EXIT;


SAMPLE INPUT FILE DELIMITTER ""~"" [ /v/global/user/application_event_bus_evt ] :

Ckn3gMxLEeOgIQBQVgErYA==~g+GDDtlaY3n7BdUrYshDFA==~1~1
CL1kEcxLEeOgIQBQVgErYA==~qoKoiuGDbClpcGt/z6RKGw==~1~1
oYIVcMxKEeOgIQBQVgErYA==~mfmQiwl7yAteevzJfilMvA==~1~1
5N7ME5bM4xGhM7exj3ykUw==~yFM2FZbM4xGhM7exj3ykUw==~1~0
JLBH4JfM4xGDH9s5+Ds/8w==~doZ/7pfM4xGDH9s5+Ds/8w==~1~0
fGvpoMxKEeOgIQBQVgErYA==~mQUQIK2mY6WIPcszfp5BTQ==~1~1


Table Definition :

CREATE MULTISET TABLE CORECM.APPLICATION_EVENT_BUS_EVT ,NO FALLBACK ,
     NO BEFORE JOURNAL,
     NO AFTER JOURNAL,
     CHECKSUM = DEFAULT,
     DEFAULT MERGEBLOCKRATIO
     (
      APPLICATION_EVENT_ID CHAR(26) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL,
      BUS_EVT_ID CHAR(26) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL,
      BUS_EVT_VID BIGINT NOT NULL,
      BUS_EVT_RESTATE_IN SMALLINT)
UNIQUE PRIMARY INDEX ( APPLICATION_EVENT_ID ,BUS_EVT_ID ,BUS_EVT_VID )
INDEX APPLICATION_EVENT_BUS_EVT_IDX1 ( APPLICATION_EVENT_ID )
INDEX APPLICATION_EVENT_BUS_EVT_IDX2 ( BUS_EVT_ID ,BUS_EVT_VID );


Results set in DB server as,

    APPLICATION_EVENT_ID        BUS_EVT_ID                  BUS_EVT_VID             BUS_EVT_RESTATE_IN 

1    Ckn3gMxLEeOgIQBQVgErYA     == g+GDDtlaY3n7BdUrYshD     85,849,873,219,141,958  12,544
2    CL1kEcxLEeOgIQBQVgErYA     == qoKoiuGDbClpcGt/z6RK     85,849,873,219,155,783  12,544
3    oYIVcMxKEeOgIQBQVgErYA     == mfmQiwl7yAteevzJfilM     85,849,873,219,142,006  12,544
4    5N7ME5bM4xGhM7exj3ykUw     == JAf0GpbM4xGhM7exj3yk     85,849,873,219,155,797  12,288
5    JLBH4JfM4xGDH9s5+Ds/8w     == Du6T7pfM4xGDH9s5+Ds/     85,849,873,219,155,768  12,288
6    fGvpoMxKEeOgIQBQVgErYA     == mQUQIK2mY6WIPcszfp5B     85,849,873,219,146,068  12,544


If we look at the Data, we can see two issues as,


First two column data length is 24 CHARACTERS ( as per input file ), but the issue is that it been shifted two characters in next column.
Column BUS_EVT_VID and BUS_EVT_RESTATE_IN has wrong data 85,849,873,219,141,958 and 12,544 instead of 1 and 1 respectively (this may be because first two column data got shifted)


I tried following options to resolve the above issue but couldn't resolve the issue,


Modified the Table Definition, i.e. changed datatype to
CHAR(28),CHAR(24),CHAR(26) 
Modified the Table Definition column
    datatypes to VARCHAR(24), VARCHAR(26) 
Modified BTEQ command, i.e. altered datatype in below line,
         USING(APPLICATION_EVENT_ID CHAR(24),BUS_EVT_ID CHAR(24),BUS_EVT_VID BIGINT,BUS_EVT_RESTATE_IN SMALLINT)


Thanks in advance.
",-1,-1,-1.0,"I am trying to execute following bteq command on linux environment but couldn't load data properly into Teradata DB server. Can someone please advise me to resolve the below issue that I am facing while loading.

BTEQ Command used :

.SET width 64000;
.SET session transaction btet;
.logmech ldap
.logon XXXXXXX/XXXXXXXX,********;

DATABASE corecm;

.PACK 1000
.IMPORT VARTEXT '~' FILE=/v/global/user/application_event_bus_evt
.REPEAT *
USING(APPLICATION_EVENT_ID CHAR(24),BUS_EVT_ID CHAR(24),BUS_EVT_VID BIGINT,BUS_EVT_RESTATE_IN SMALLINT)

insert into corecm.application_event_bus_evt (APPLICATION_EVENT_ID
, BUS_EVT_ID
, BUS_EVT_VID
, BUS_EVT_RESTATE_IN
)
values
( COALESCE(:APPLICATION_EVENT_ID,1)
, COALESCE(:BUS_EVT_ID,1)
, COALESCE(:BUS_EVT_VID,1)
, COALESCE(:BUS_EVT_RESTATE_IN,1)
) ;
.LOGOFF;
.EXIT;


SAMPLE INPUT FILE DELIMITTER ""~"" [ /v/global/user/application_event_bus_evt ] :

Ckn3gMxLEeOgIQBQVgErYA==~g+GDDtlaY3n7BdUrYshDFA==~1~1
CL1kEcxLEeOgIQBQVgErYA==~qoKoiuGDbClpcGt/z6RKGw==~1~1
oYIVcMxKEeOgIQBQVgErYA==~mfmQiwl7yAteevzJfilMvA==~1~1
5N7ME5bM4xGhM7exj3ykUw==~yFM2FZbM4xGhM7exj3ykUw==~1~0
JLBH4JfM4xGDH9s5+Ds/8w==~doZ/7pfM4xGDH9s5+Ds/8w==~1~0
fGvpoMxKEeOgIQBQVgErYA==~mQUQIK2mY6WIPcszfp5BTQ==~1~1


Table Definition :

CREATE MULTISET TABLE CORECM.APPLICATION_EVENT_BUS_EVT ,NO FALLBACK ,
     NO BEFORE JOURNAL,
     NO AFTER JOURNAL,
     CHECKSUM = DEFAULT,
     DEFAULT MERGEBLOCKRATIO
     (
      APPLICATION_EVENT_ID CHAR(26) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL,
      BUS_EVT_ID CHAR(26) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL,
      BUS_EVT_VID BIGINT NOT NULL,
      BUS_EVT_RESTATE_IN SMALLINT)
UNIQUE PRIMARY INDEX ( APPLICATION_EVENT_ID ,BUS_EVT_ID ,BUS_EVT_VID )
INDEX APPLICATION_EVENT_BUS_EVT_IDX1 ( APPLICATION_EVENT_ID )
INDEX APPLICATION_EVENT_BUS_EVT_IDX2 ( BUS_EVT_ID ,BUS_EVT_VID );


Results set in DB server as,

    APPLICATION_EVENT_ID        BUS_EVT_ID                  BUS_EVT_VID             BUS_EVT_RESTATE_IN 

1    Ckn3gMxLEeOgIQBQVgErYA     == g+GDDtlaY3n7BdUrYshD     85,849,873,219,141,958  12,544
2    CL1kEcxLEeOgIQBQVgErYA     == qoKoiuGDbClpcGt/z6RK     85,849,873,219,155,783  12,544
3    oYIVcMxKEeOgIQBQVgErYA     == mfmQiwl7yAteevzJfilM     85,849,873,219,142,006  12,544
4    5N7ME5bM4xGhM7exj3ykUw     == JAf0GpbM4xGhM7exj3yk     85,849,873,219,155,797  12,288
5    JLBH4JfM4xGDH9s5+Ds/8w     == Du6T7pfM4xGDH9s5+Ds/     85,849,873,219,155,768  12,288
6    fGvpoMxKEeOgIQBQVgErYA     == mQUQIK2mY6WIPcszfp5B     85,849,873,219,146,068  12,544


If we look at the Data, we can see two issues as,


First two column data length is 24 CHARACTERS ( as per input file ), but the issue is that it been shifted two characters in next column.
Column BUS_EVT_VID and BUS_EVT_RESTATE_IN has wrong data 85,849,873,219,141,958 and 12,544 instead of 1 and 1 respectively (this may be because first two column data got shifted)


I tried following options to resolve the above issue but couldn't resolve the issue,


Modified the Table Definition, i.e. changed datatype to
CHAR(28),CHAR(24),CHAR(26) 
Modified the Table Definition column
    datatypes to VARCHAR(24), VARCHAR(26) 
Modified BTEQ command, i.e. altered datatype in below line,
         USING(APPLICATION_EVENT_ID CHAR(24),BUS_EVT_ID CHAR(24),BUS_EVT_VID BIGINT,BUS_EVT_RESTATE_IN SMALLINT)


Thanks in advance.
",3
67,23788054,Cannot flush cache error when uploading data with scriptella to Teradata using FASTLOAD,"I tried to upload data in a CSV file to an empty table I created. But got the following error:

May 21, 2014 10:18:18 AM &lt;INFO&gt; Execution Progress.Initializing properties: 1%
May 21, 2014 10:18:18 AM &lt;INFO&gt; Execution Progress.Initialized connection id=in_file, CsvConnection, Dialect{CSV 1.0}, properties {}: 3%
May 21, 2014 10:18:19 AM &lt;INFO&gt; Execution Progress.Initialized connection id=db, JdbcConnection{com.teradata.jdbc.jdk6.JDK6_FastLoadManager_Connection}, Dialect{Teradata Teradata Database 13.10.07.24}, properties {statement.batchSize=100000}: 5%
May 21, 2014 10:18:19 AM &lt;INFO&gt; Execution Progress./etl/query[1] prepared: 10%
May 21, 2014 10:18:19 AM &lt;INFO&gt; Registered JMX mbean: scriptella:type=etl,url=""file:/xxx/fastload.xml""
May 21, 2014 10:18:44 AM &lt;INFO&gt; Execution Progress./etl/query[1] executed: 95%
May 21, 2014 10:18:44 AM &lt;INFO&gt; Execution Progress.Complete
May 21, 2014 10:18:55 AM &lt;WARNING&gt; Unable to rollback transaction for connection CsvConnection: Transactions are not supported by CsvConnection
May 21, 2014 10:18:55 AM &lt;SEVERE&gt; Script /xxx/fastload.xml execution failed.
Unable to commit transaction - cannot flush cache
JDBC provider exception: Unable to commit transaction - cannot flush cache
Error codes: [HY000, 1154]
Driver exception: java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 14.10.00.17] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""mydatabase"".""myemptytable"". Details of the failure can be found in the exception chain that is accessible with getNextException.


This is my fastload.xml file:

&lt;!DOCTYPE etl SYSTEM ""http://scriptella.javaforge.com/dtd/etl.dtd""&gt;
&lt;etl&gt;
&lt;description&gt;Scriptella  script&lt;/description&gt;
&lt;properties&gt;
    &lt;include href=""$config""/&gt; &lt;!--Load from external properties file--&gt;
&lt;/properties&gt;
&lt;connection id=""in_file"" driver=""csv"" url=""$inputfile""&gt;
&lt;/connection&gt;
&lt;connection id=""db"" driver=""$driver"" url=""$url"" user=""$user"" password=""$password"" classpath=""lib/terajdbc4.jar""&gt;
statement.batchSize=100000
&lt;/connection&gt;

&lt;query connection-id=""in_file""&gt;
&lt;!-- Empty query means select all columns --&gt;
&lt;script connection-id=""db""&gt;
INSERT INTO  mydatabase.myemptytable VALUES (?1,?2,?3);
&lt;/script&gt;
&lt;/query&gt;

&lt;/etl&gt;


This is my connection URL:

url=jdbc:teradata://mypath/TMODE=ANSI,CHARSET=UTF8,TYPE=FASTLOAD


Uploading without FASTLOAD works for the same file. 
I tried to google the error message but did not find anything. Anyone know what's the problem here? Thanks.
",-1,-1,-1.0,"I tried to upload data in a CSV file to an empty table I created. But got the following error:

May 21, 2014 10:18:18 AM &lt;INFO&gt; Execution Progress.Initializing properties: 1%
May 21, 2014 10:18:18 AM &lt;INFO&gt; Execution Progress.Initialized connection id=in_file, CsvConnection, Dialect{CSV 1.0}, properties {}: 3%
May 21, 2014 10:18:19 AM &lt;INFO&gt; Execution Progress.Initialized connection id=db, JdbcConnection{com.teradata.jdbc.jdk6.JDK6_FastLoadManager_Connection}, Dialect{Teradata Teradata Database 13.10.07.24}, properties {statement.batchSize=100000}: 5%
May 21, 2014 10:18:19 AM &lt;INFO&gt; Execution Progress./etl/query[1] prepared: 10%
May 21, 2014 10:18:19 AM &lt;INFO&gt; Registered JMX mbean: scriptella:type=etl,url=""file:/xxx/fastload.xml""
May 21, 2014 10:18:44 AM &lt;INFO&gt; Execution Progress./etl/query[1] executed: 95%
May 21, 2014 10:18:44 AM &lt;INFO&gt; Execution Progress.Complete
May 21, 2014 10:18:55 AM &lt;WARNING&gt; Unable to rollback transaction for connection CsvConnection: Transactions are not supported by CsvConnection
May 21, 2014 10:18:55 AM &lt;SEVERE&gt; Script /xxx/fastload.xml execution failed.
Unable to commit transaction - cannot flush cache
JDBC provider exception: Unable to commit transaction - cannot flush cache
Error codes: [HY000, 1154]
Driver exception: java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 14.10.00.17] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""mydatabase"".""myemptytable"". Details of the failure can be found in the exception chain that is accessible with getNextException.


This is my fastload.xml file:

&lt;!DOCTYPE etl SYSTEM ""http://scriptella.javaforge.com/dtd/etl.dtd""&gt;
&lt;etl&gt;
&lt;description&gt;Scriptella  script&lt;/description&gt;
&lt;properties&gt;
    &lt;include href=""$config""/&gt; &lt;!--Load from external properties file--&gt;
&lt;/properties&gt;
&lt;connection id=""in_file"" driver=""csv"" url=""$inputfile""&gt;
&lt;/connection&gt;
&lt;connection id=""db"" driver=""$driver"" url=""$url"" user=""$user"" password=""$password"" classpath=""lib/terajdbc4.jar""&gt;
statement.batchSize=100000
&lt;/connection&gt;

&lt;query connection-id=""in_file""&gt;
&lt;!-- Empty query means select all columns --&gt;
&lt;script connection-id=""db""&gt;
INSERT INTO  mydatabase.myemptytable VALUES (?1,?2,?3);
&lt;/script&gt;
&lt;/query&gt;

&lt;/etl&gt;


This is my connection URL:

url=jdbc:teradata://mypath/TMODE=ANSI,CHARSET=UTF8,TYPE=FASTLOAD


Uploading without FASTLOAD works for the same file. 
I tried to google the error message but did not find anything. Anyone know what's the problem here? Thanks.
",0
68,24747452,Where are Teradata header files downloaded from? (DBD::Teradata build),"I am attempting to build the Perl DBD::Teradata DBI driver on 64 bit linux.  However, I do not have the header files necessary to do so.  According to the documentation (http://www.presicient.com/tdatdbd/), the following files are required:

parcel.h
dbcarea.h
coperr.h
coptypes.h 


I've spent hours scouring Teradata's site and the internet at large with no success.  I saw mention of a CLIv2 developer's kit, but could not locate this either.

Could anyone point me to where I can get these files?  I would sincerely appreciate the help.
",1,-1,-1.0,"I am attempting to build the Perl DBD::Teradata DBI driver on 64 bit linux.  However, I do not have the header files necessary to do so.  According to the documentation (http://www.presicient.com/tdatdbd/), the following files are required:

parcel.h
dbcarea.h
coperr.h
coptypes.h 


I've spent hours scouring Teradata's site and the internet at large with no success.  I saw mention of a CLIv2 developer's kit, but could not locate this either.

Could anyone point me to where I can get these files?  I would sincerely appreciate the help.
",1
69,24747659,Teradata Row size Error,"When I query Teradata for a specific table in bteq , I get the following error. 
How to query this table in BTEQ or any other TD SQL Client using JDBC or ODBC.

 *** Failure 9804 Response Row size or Constant Row size overflow.
                Statement# 1, Info =0 
 *** Total elapsed time was 1 second.

",-1,-1,-1.0,"When I query Teradata for a specific table in bteq , I get the following error. 
How to query this table in BTEQ or any other TD SQL Client using JDBC or ODBC.

 *** Failure 9804 Response Row size or Constant Row size overflow.
                Statement# 1, Info =0 
 *** Total elapsed time was 1 second.

",3
70,24765597,Unable to ODBC connect to Teradata via PHP,"Windows' (7) ODBC administrator is configured with my username and password. I am having trouble connecting to my Teradata DB using ODBC and php. I have no trouble connecting with Python by simply using the connection string ""DSN=Teradata""

import pypyodbc
conn = pypyodbc.connect('DSN=Teradata')


or R, 

require(RODBC)
odbcConnect('Teradata')


But the following code 

&lt;?php
  $conn = odbc_connect('DSN=Teradata');
  $query = odbc_exec($conn, ""select top 10 * from pretendTable"");
  while(odbc_fetch_row($query)) {
    odbc_result($query, 1);
  }
  echo ""END"";
?&gt;


only outputs ""END"". Any ideas?

EDIT

and also, VBA

Dim cn As New ADODB.Connection
Dim rs As New ADODB.Recordset

cn.Open ""Teradata""
rs.Open ""Select top 10 * from pretendTable"", cn


php is the only one that is not cooperating

EDIT 2

Somewhat repaired. 

I have two installs of php on my computer. One was installed by wamp, the other using the php.net binary executable. It is the latter that is not working. This leads me to believe that the ODBC package/functionality does not come installed by default. However, I'll defer to the hive... 

The former works with $conn = odbc_connect(""Teradata"",""pretendUsername"",""pretendPassword"");
",-1,-1,-1.0,"Windows' (7) ODBC administrator is configured with my username and password. I am having trouble connecting to my Teradata DB using ODBC and php. I have no trouble connecting with Python by simply using the connection string ""DSN=Teradata""

import pypyodbc
conn = pypyodbc.connect('DSN=Teradata')


or R, 

require(RODBC)
odbcConnect('Teradata')


But the following code 

&lt;?php
  $conn = odbc_connect('DSN=Teradata');
  $query = odbc_exec($conn, ""select top 10 * from pretendTable"");
  while(odbc_fetch_row($query)) {
    odbc_result($query, 1);
  }
  echo ""END"";
?&gt;


only outputs ""END"". Any ideas?

EDIT

and also, VBA

Dim cn As New ADODB.Connection
Dim rs As New ADODB.Recordset

cn.Open ""Teradata""
rs.Open ""Select top 10 * from pretendTable"", cn


php is the only one that is not cooperating

EDIT 2

Somewhat repaired. 

I have two installs of php on my computer. One was installed by wamp, the other using the php.net binary executable. It is the latter that is not working. This leads me to believe that the ODBC package/functionality does not come installed by default. However, I'll defer to the hive... 

The former works with $conn = odbc_connect(""Teradata"",""pretendUsername"",""pretendPassword"");
",1
71,24797427,Fetching data from teradata using teradata bteq utility,"I am using Teradata bteq utility to run teradata commands form unix server.

I am able to connect to teradata, but while fetching data it gives only 7 columns and a dot(.) at end of decimal field.
I am using query,
select * from databasename.tablename

output
column1(decimal)   column2 column3(decimal)
   74664.              S        67469.

Don't know why it giving dot(.)

Can anybody help??
",1,-1,-1.0,"I am using Teradata bteq utility to run teradata commands form unix server.

I am able to connect to teradata, but while fetching data it gives only 7 columns and a dot(.) at end of decimal field.
I am using query,
select * from databasename.tablename

output
column1(decimal)   column2 column3(decimal)
   74664.              S        67469.

Don't know why it giving dot(.)

Can anybody help??
",3
72,24803578,Run a .vbs script that connects to Teradata with no timeout,"Is there ANY way to run a .vbs file with a connection with a larger timeout? 

Background:

I'm using Excel VBA to upload data to Teradata.  It's relatively easy to create an insert statement and then execute it with rs.Execute().  

However, for around 80k rows, this takes about an hour to run.  I thought of two possible speedups.  

1) Send multiple rows (say, 500) to the rs.Execute() at a time, separated by semicolons, and let Teradata insert them in parallel.  

2) Send multiple insert requests at a time without waiting for the 1st one to finish.  This is difficult in VBA, since there's no support for multithreading and rs.Execute is blocking.  So to get around this, I created .vbs files which just contain this:

Dim rs
Set rs = CreateObject(""ADODB.Recordset"")
constr = ""Driver={Teradata};commandTimeout=0;DBCNAME=xx;UID=xx;PWD=xx;""
rs.Open ""insert statement1;insert statement2;"", constr, adOpenForwardOnly


And I spin off a bunch of those.  however, if i send too much data at once, it times out (Teradata has a 30 second timeout for queries run this way.  ""commandTimeout"" doesn't appear to have any effect whether I set it to 0 or 1200 or whatever.  I can't find an appropriate size packet to insert since the load on Teradata keeps changing with use throughout the day.  Sometimes 500 at a time works fine and sometimes only 15 stays under the 30s timeout.
",1,-1,-1.0,"Is there ANY way to run a .vbs file with a connection with a larger timeout? 

Background:

I'm using Excel VBA to upload data to Teradata.  It's relatively easy to create an insert statement and then execute it with rs.Execute().  

However, for around 80k rows, this takes about an hour to run.  I thought of two possible speedups.  

1) Send multiple rows (say, 500) to the rs.Execute() at a time, separated by semicolons, and let Teradata insert them in parallel.  

2) Send multiple insert requests at a time without waiting for the 1st one to finish.  This is difficult in VBA, since there's no support for multithreading and rs.Execute is blocking.  So to get around this, I created .vbs files which just contain this:

Dim rs
Set rs = CreateObject(""ADODB.Recordset"")
constr = ""Driver={Teradata};commandTimeout=0;DBCNAME=xx;UID=xx;PWD=xx;""
rs.Open ""insert statement1;insert statement2;"", constr, adOpenForwardOnly


And I spin off a bunch of those.  however, if i send too much data at once, it times out (Teradata has a 30 second timeout for queries run this way.  ""commandTimeout"" doesn't appear to have any effect whether I set it to 0 or 1200 or whatever.  I can't find an appropriate size packet to insert since the load on Teradata keeps changing with use throughout the day.  Sometimes 500 at a time works fine and sometimes only 15 stays under the 30s timeout.
",3
73,25165544,Limiting the number of rows in subqueries with Teradata,"I'm new to Teradata and I'm facing a problem I didn't have with the previous database I used.
Basically, I'm trying to reduce the number of rows returned in subqueries inside a where clause. I had no problem doing this previously with the ROWNUM function.

My previous query was something like:

SELECT * FROM myTable
WHERE field1 = 'foo' AND field2 in(
    SELECT field2 FROM anotherTable
    WHERE field3 = 'bar' AND ROWNUM&lt;100);


Since I can't use ROWNUM in TD, I've looked for equivalent functions or at least functions that would get me where I wanted even if they were'nt exactly equivalent. 
I found and tried : ROW_NUMBER, TOP and SAMPLE.

I tried ROW_NUMBER() but Teradata doesn't allow analytic functions in WHERE clauses.
I tried TOP N but this option is not supported in a subquery.
I tried SAMPLE N but it is not supported in subqueries either.

So... I have to admit I'm a bit stuck right now and was wondering if there was any solution that would allow me to limit the number of rows returned in a subquery using Teradata and that would be pretty similar to what I did up to now?
Also, if there aren't any, how would it be possible to build the query differently to use it appropriately with Teradata?

Thanks!
",-1,-1,-1.0,"I'm new to Teradata and I'm facing a problem I didn't have with the previous database I used.
Basically, I'm trying to reduce the number of rows returned in subqueries inside a where clause. I had no problem doing this previously with the ROWNUM function.

My previous query was something like:

SELECT * FROM myTable
WHERE field1 = 'foo' AND field2 in(
    SELECT field2 FROM anotherTable
    WHERE field3 = 'bar' AND ROWNUM&lt;100);


Since I can't use ROWNUM in TD, I've looked for equivalent functions or at least functions that would get me where I wanted even if they were'nt exactly equivalent. 
I found and tried : ROW_NUMBER, TOP and SAMPLE.

I tried ROW_NUMBER() but Teradata doesn't allow analytic functions in WHERE clauses.
I tried TOP N but this option is not supported in a subquery.
I tried SAMPLE N but it is not supported in subqueries either.

So... I have to admit I'm a bit stuck right now and was wondering if there was any solution that would allow me to limit the number of rows returned in a subquery using Teradata and that would be pretty similar to what I did up to now?
Also, if there aren't any, how would it be possible to build the query differently to use it appropriately with Teradata?

Thanks!
",3
74,25170991,"Dealing with duplicate names in Teradata ""create table with data""","Is there a way to automate dealing with/ignoring duplicate names when creating a new table in Teradata.

Assume I'm joining a table with itself. The table has columns 

id, email, address1, address2, city, state, zip.


,

create table mytable2 as (
  select a.*, b.* from mytable a
  left join mytable b on a.email = b.email
  where a.id &lt;&gt; b.id
) with data


requires me to write out 

a.address1 as aaddress1, b.address1 as baddress1, a.address2 as aaddress2...


Is there an easier way of dealing with this? When I'm joining many tables, this hurts my productivity. Often these tables are for research and the potential ambiguity is not a problem. 
",-1,1,-1.0,"Is there a way to automate dealing with/ignoring duplicate names when creating a new table in Teradata.

Assume I'm joining a table with itself. The table has columns 

id, email, address1, address2, city, state, zip.


,

create table mytable2 as (
  select a.*, b.* from mytable a
  left join mytable b on a.email = b.email
  where a.id &lt;&gt; b.id
) with data


requires me to write out 

a.address1 as aaddress1, b.address1 as baddress1, a.address2 as aaddress2...


Is there an easier way of dealing with this? When I'm joining many tables, this hurts my productivity. Often these tables are for research and the potential ambiguity is not a problem. 
",3
75,25490938,ADO.NET Execution Task - Parameter Mapping Failure (Teradata),"Recently the server that houses all of my SSIS packages was upgraded. This has caused a need for all of our existing packages that utalize an OLE DB connection to be migrated to an ADO.NET connection. One of the issues I am running into is that passing paramaters into the SQL Execution Task are no longer working. Even after following the instruction provided by micrtosfot here.

Before I start let me share my setup.







The SQL I am testing with is extremly simple. Even with such a simple statement I am receiving the following error message.



In the past if I ran into issues like this I would just set the SQL as a variable and through an expression update the parts of the statement that needs to be updated. However, the statement is over 4k characters long. Has anyone had this issue using an ADO.NET connection to teradata? If so, any suggestions on how to solve it. I have searched high/low on google w/o any luck. The most I have found is people asking the same question without any answers.

Thanks
",1,-1,-1.0,"Recently the server that houses all of my SSIS packages was upgraded. This has caused a need for all of our existing packages that utalize an OLE DB connection to be migrated to an ADO.NET connection. One of the issues I am running into is that passing paramaters into the SQL Execution Task are no longer working. Even after following the instruction provided by micrtosfot here.

Before I start let me share my setup.







The SQL I am testing with is extremly simple. Even with such a simple statement I am receiving the following error message.



In the past if I ran into issues like this I would just set the SQL as a variable and through an expression update the parts of the statement that needs to be updated. However, the statement is over 4k characters long. Has anyone had this issue using an ADO.NET connection to teradata? If so, any suggestions on how to solve it. I have searched high/low on google w/o any luck. The most I have found is people asking the same question without any answers.

Thanks
",3
76,25546298,Adding multiple columns with default value (Teradata),"I tried to search but I have not found anything. I am trying to add two columns to a table withe a default value (Teradata). I am trying with this statement

ALTER TABLE TEST 
add (DWH_Change_dt date  default CURRENT_DATE, dwh_create_dt date  default current_date);


This does not work with default clause. I get this error

Syntax error, expected something like a 'BETWEEN' keyword or an 'IN' keyword or a 'LIKE' keyword or a 'CONTAINS' keyword between the word 'DWH_Change_dt' and the 'date'

If I add one column at a time it works (without parenthesis). Anyone has any idea? What is wrong?

Thanks,
Umberto
",-1,-1,-1.0,"I tried to search but I have not found anything. I am trying to add two columns to a table withe a default value (Teradata). I am trying with this statement

ALTER TABLE TEST 
add (DWH_Change_dt date  default CURRENT_DATE, dwh_create_dt date  default current_date);


This does not work with default clause. I get this error

Syntax error, expected something like a 'BETWEEN' keyword or an 'IN' keyword or a 'LIKE' keyword or a 'CONTAINS' keyword between the word 'DWH_Change_dt' and the 'date'

If I add one column at a time it works (without parenthesis). Anyone has any idea? What is wrong?

Thanks,
Umberto
",3
77,25772560,Storing Japanese characters in Teradata,"I have a SSIS package that pushes data from a SQL database to a Teradata database.  In my SQL database, I have a particular table that stores Japanese characters, which are read in from a file (encoded in UTF 8 format).  The column that holds this data is of type ""nvarchar"".  Currently, I have no issues viewing the characters in SQL.  However, when I run my package these characters are displaying as junk in teradata.  Even if I do a simple insert with Japanese characters, I cannot view the data.  The column in teradata is of type varchar (CHARACTER SET UNICODE NOT CASESPECIFIC).  I know there is no nvarchar datatype in teradta. Any thoughts on how to store these characters?
",-1,-1,-1.0,"I have a SSIS package that pushes data from a SQL database to a Teradata database.  In my SQL database, I have a particular table that stores Japanese characters, which are read in from a file (encoded in UTF 8 format).  The column that holds this data is of type ""nvarchar"".  Currently, I have no issues viewing the characters in SQL.  However, when I run my package these characters are displaying as junk in teradata.  Even if I do a simple insert with Japanese characters, I cannot view the data.  The column in teradata is of type varchar (CHARACTER SET UNICODE NOT CASESPECIFIC).  I know there is no nvarchar datatype in teradta. Any thoughts on how to store these characters?
",3
78,26116911,Java to Teradata connectivity,"Iam trying to connect to Teradata using Java JDBC. I have giving the connection string, userid,password. 

But iam ending up with the error as 

""No Class definition error for Java/Sql/ParameterMetaData ""

I have tdgssconfig.jar, rt.jar and terajdbc4.jar in my classpath. Tried google/bing for the solutions, but in vain.

Any inputs/help would be highly appreciated.
",-1,-1,-1.0,"Iam trying to connect to Teradata using Java JDBC. I have giving the connection string, userid,password. 

But iam ending up with the error as 

""No Class definition error for Java/Sql/ParameterMetaData ""

I have tdgssconfig.jar, rt.jar and terajdbc4.jar in my classpath. Tried google/bing for the solutions, but in vain.

Any inputs/help would be highly appreciated.
",0
79,26363681,improve complex query in Teradata,"I would like to know how to get all the rows from table1 that have a matching row in table3.

Teh structure of the tables is:

table1:
k1 k2

table2:
k1 k2 t1 t2 date type

table3:
t1 t2 date status


The conditions are:


k1 and k2 have to match with the corresponding columns in table2.
In table2 I will only chek those rows where date='today' and type='a'.
That can return 0, 1 or many rows in table2.
Looking at t1 and t2 from table 2, I get the rows that match in table3.
If in table3 date='today' and status='ok', I will return the original row from table1, this is, k1 and k2.


How can I do this query (inner joins, exists, whatever) having into account that the three tables have millions of rows, so it must be as optimal as possible?

I have the query, which is right for sure, but they are too many conditions for Teradata to come with the answer. Too many joins, I think.
",0,-1,-1.0,"I would like to know how to get all the rows from table1 that have a matching row in table3.

Teh structure of the tables is:

table1:
k1 k2

table2:
k1 k2 t1 t2 date type

table3:
t1 t2 date status


The conditions are:


k1 and k2 have to match with the corresponding columns in table2.
In table2 I will only chek those rows where date='today' and type='a'.
That can return 0, 1 or many rows in table2.
Looking at t1 and t2 from table 2, I get the rows that match in table3.
If in table3 date='today' and status='ok', I will return the original row from table1, this is, k1 and k2.


How can I do this query (inner joins, exists, whatever) having into account that the three tables have millions of rows, so it must be as optimal as possible?

I have the query, which is right for sure, but they are too many conditions for Teradata to come with the answer. Too many joins, I think.
",3
80,26723342,Unable to connect to teradata from Linux,"I have installed teradata in Linux box and made odbc.ini file settings 

but when I tried to connect to the Teradata I am getting below error 

/opt/teradata/client/14.10/odbc_64/bin/tdxodbc: /usr/lib64/libodbc.so: no versio
n information available (required by /opt/teradata/client/14.10/odbc_64/bin/tdxo
dbc)

Enter Data Source Name: TD_ODBC

Enter UserID: XXXXXXXXX

Enter Password:


Connecting with SQLConnect(DSN=TD_ODBC,UID=****,PWD=*)...

adhoc: SQLError() couldn't find text, RC=100


ODBC connection closed.

can some body help me out why I am getting this error 
",-1,-1,-1.0,"I have installed teradata in Linux box and made odbc.ini file settings 

but when I tried to connect to the Teradata I am getting below error 

/opt/teradata/client/14.10/odbc_64/bin/tdxodbc: /usr/lib64/libodbc.so: no versio
n information available (required by /opt/teradata/client/14.10/odbc_64/bin/tdxo
dbc)

Enter Data Source Name: TD_ODBC

Enter UserID: XXXXXXXXX

Enter Password:


Connecting with SQLConnect(DSN=TD_ODBC,UID=****,PWD=*)...

adhoc: SQLError() couldn't find text, RC=100


ODBC connection closed.

can some body help me out why I am getting this error 
",1
81,27042276,Optimize query Teradata,"I would appreciate it if you can help me with a problem that i have.
I have this join condition :

SELECT *
FROM    
T1_STAGING.(first_table) AS STG 
JOIN T1_STAGING.(second_table) AS B
    ON 
    (
            STG.DLOF_ID_NO=B.DLOF_ID_NO_RU
    )


This simple join is taking too long to finish, more than 20 minutes. The data of each table is less than 600,000K data. i tried the following things :

I took statistics on each table. 
I changed the columns to be PRIMARY INDEX. 
I created JOIN INDEX for the second table but still nothing! 
The query never ends it takes 20 mins ++. This seems to be data distribution problem in the second table, but i can't do anything with the data. 
Please bear in mind that if i join my first_table with any other it takes only seconds. 

Can you give me a suggestion to try? I need to optimize it for better performance.

Here is the explain of TERADATA:
Explain SEL *
FROM
    T1_STAGING.DLS_DLO_OWS_STAGE_STG AS STG 
    JOIN T1_STAGING.DLS_ACQUISITION_STG AS B
        ON 
        (
                STG.DLOF_ID_NO=B.DLOF_ID_NO_RU
        )

1) First, we lock a distinct T1_STAGING.""pseudo table"" for read on a
     RowHash to prevent global deadlock for T1_STAGING.STG.
  2) Next, we lock a distinct T1_STAGING.""pseudo table"" for read on a
     RowHash to prevent global deadlock for T1_STAGING.B.
  3) We lock T1_STAGING.STG for read, and we lock T1_STAGING.B for read.
  4) We execute the following steps in parallel.
       1) We do an all-AMPs RETRIEVE step from T1_STAGING.B by way of
          an all-rows scan with no residual conditions split into Spool
          2 (all_amps) with a condition of (""DLOF_ID_NO_RU IN (:)"") to
          qualify skewed rows and Spool 3 (all_amps) with a condition
          of (""DLOF_ID_NO_RU IN (:)"") to qualify rows matching skewed
          rows of the skewed relation and Spool 4 (all_amps) with
          remaining rows fanned out into 2 hash join partitions.  Spool
          2 is built locally on the AMPs.  Then we do a SORT to order
          Spool 2 by row hash.  The size of Spool 2 is estimated with
          high confidence to be 303 rows.  Spool 3 is built locally on
          the AMPs.  The size of Spool 3 is estimated with high
          confidence to be 4,710 rows.  Spool 4 is redistributed by
          hash code to all AMPs.  The size of Spool 4 is estimated with
          high confidence to be 97,742 rows.  The estimated time for
          this step is 1.27 seconds.
       2) We do an all-AMPs RETRIEVE step from T1_STAGING.STG by way of
          an all-rows scan with no residual conditions split into Spool
          6 (all_amps) with a condition of (""DLOF_ID_NO IN (:)"") to
          qualify skewed rows and Spool 5 (all_amps) with a condition
          of (""DLOF_ID_NO IN (:)"") to qualify rows matching skewed
          rows of the skewed relation and Spool 7 (all_amps) with
          remaining rows fanned out into 2 hash join partitions.  Spool
          6 is built locally on the AMPs.  The size of Spool 6 is
          estimated with high confidence to be 21,587 rows.  Spool 5 is
          built locally on the AMPs.  The size of Spool 5 is estimated
          with high confidence to be 7 rows.  Spool 7 is redistributed
          by hash code to all AMPs.  The size of Spool 7 is estimated
          with high confidence to be 301,682 rows.  The estimated time
          for this step is 4.20 seconds.
  5) We execute the following steps in parallel.
       1) We do an all-AMPs RETRIEVE step from Spool 5 (Last Use) by
          way of an all-rows scan into Spool 8 (all_amps), which is
          duplicated on all AMPs.  Then we do a SORT to order Spool 8
          by the hash code of (T1_STAGING.STG.DLOF_ID_NO).  The size of
          Spool 8 is estimated with high confidence to be 336 rows (
          640,080 bytes).  The estimated time for this step is 0.01
          seconds.
       2) We do an all-AMPs RETRIEVE step from Spool 3 (Last Use) by
          way of an all-rows scan into Spool 9 (all_amps), which is
          duplicated on all AMPs.  The result spool file will not be
          cached in memory.  The size of Spool 9 is estimated with high
          confidence to be 226,080 rows (391,796,640 bytes).  The
          estimated time for this step is 1.05 seconds.
  6) We do an all-AMPs JOIN step from Spool 8 (Last Use) by way of a
     RowHash match scan, which is joined to Spool 2 (Last Use) by way
     of a RowHash match scan.  Spool 8 and Spool 2 are joined using a
     merge join, with a join condition of (""DLOF_ID_NO = DLOF_ID_NO_RU"").
     The result goes into Spool 1 (group_amps), which is built locally
     on the AMPs.  The result spool file will not be cached in memory.
     The size of Spool 1 is estimated with low confidence to be 2,121
     rows (11,491,578 bytes).  The estimated time for this step is 0.03
     seconds.
  7) We do an all-AMPs JOIN step from Spool 6 (Last Use) by way of an
     all-rows scan, which is joined to Spool 9 (Last Use) by way of an
     all-rows scan.  Spool 6 and Spool 9 are joined using a single
     partition hash join, with a join condition of (""DLOF_ID_NO =
     DLOF_ID_NO_RU"").  The result goes into Spool 1 (group_amps), which
     is built locally on the AMPs.  The result spool file will not be
     cached in memory.  The size of Spool 1 is estimated with low
     confidence to be 9,243,161 rows (50,079,446,298 bytes).  The
     estimated time for this step is 0.60 seconds.
  8) We do an all-AMPs JOIN step from Spool 4 (Last Use) by way of an
     all-rows scan, which is joined to Spool 7 (Last Use) by way of an
     all-rows scan.  Spool 4 and Spool 7 are joined using a hash join
     of 2 partitions, with a join condition of (""DLOF_ID_NO =
     DLOF_ID_NO_RU"").  The result goes into Spool 1 (group_amps), which
     is built locally on the AMPs.  The result spool file will not be
     cached in memory.  The size of Spool 1 is estimated with low
     confidence to be 731,525 rows (3,963,402,450 bytes).  The
     estimated time for this step is 0.96 seconds.
  9) Finally, we send out an END TRANSACTION step to all AMPs involved
     in processing the request.
  -> The contents of Spool 1 are sent back to the user as the result of
     statement 1.  The total estimated time is 6.84 seconds.
",1,-1,-1.0,"I would appreciate it if you can help me with a problem that i have.
I have this join condition :

SELECT *
FROM    
T1_STAGING.(first_table) AS STG 
JOIN T1_STAGING.(second_table) AS B
    ON 
    (
            STG.DLOF_ID_NO=B.DLOF_ID_NO_RU
    )


This simple join is taking too long to finish, more than 20 minutes. The data of each table is less than 600,000K data. i tried the following things :

I took statistics on each table. 
I changed the columns to be PRIMARY INDEX. 
I created JOIN INDEX for the second table but still nothing! 
The query never ends it takes 20 mins ++. This seems to be data distribution problem in the second table, but i can't do anything with the data. 
Please bear in mind that if i join my first_table with any other it takes only seconds. 

Can you give me a suggestion to try? I need to optimize it for better performance.

Here is the explain of TERADATA:
Explain SEL *
FROM
    T1_STAGING.DLS_DLO_OWS_STAGE_STG AS STG 
    JOIN T1_STAGING.DLS_ACQUISITION_STG AS B
        ON 
        (
                STG.DLOF_ID_NO=B.DLOF_ID_NO_RU
        )

1) First, we lock a distinct T1_STAGING.""pseudo table"" for read on a
     RowHash to prevent global deadlock for T1_STAGING.STG.
  2) Next, we lock a distinct T1_STAGING.""pseudo table"" for read on a
     RowHash to prevent global deadlock for T1_STAGING.B.
  3) We lock T1_STAGING.STG for read, and we lock T1_STAGING.B for read.
  4) We execute the following steps in parallel.
       1) We do an all-AMPs RETRIEVE step from T1_STAGING.B by way of
          an all-rows scan with no residual conditions split into Spool
          2 (all_amps) with a condition of (""DLOF_ID_NO_RU IN (:)"") to
          qualify skewed rows and Spool 3 (all_amps) with a condition
          of (""DLOF_ID_NO_RU IN (:)"") to qualify rows matching skewed
          rows of the skewed relation and Spool 4 (all_amps) with
          remaining rows fanned out into 2 hash join partitions.  Spool
          2 is built locally on the AMPs.  Then we do a SORT to order
          Spool 2 by row hash.  The size of Spool 2 is estimated with
          high confidence to be 303 rows.  Spool 3 is built locally on
          the AMPs.  The size of Spool 3 is estimated with high
          confidence to be 4,710 rows.  Spool 4 is redistributed by
          hash code to all AMPs.  The size of Spool 4 is estimated with
          high confidence to be 97,742 rows.  The estimated time for
          this step is 1.27 seconds.
       2) We do an all-AMPs RETRIEVE step from T1_STAGING.STG by way of
          an all-rows scan with no residual conditions split into Spool
          6 (all_amps) with a condition of (""DLOF_ID_NO IN (:)"") to
          qualify skewed rows and Spool 5 (all_amps) with a condition
          of (""DLOF_ID_NO IN (:)"") to qualify rows matching skewed
          rows of the skewed relation and Spool 7 (all_amps) with
          remaining rows fanned out into 2 hash join partitions.  Spool
          6 is built locally on the AMPs.  The size of Spool 6 is
          estimated with high confidence to be 21,587 rows.  Spool 5 is
          built locally on the AMPs.  The size of Spool 5 is estimated
          with high confidence to be 7 rows.  Spool 7 is redistributed
          by hash code to all AMPs.  The size of Spool 7 is estimated
          with high confidence to be 301,682 rows.  The estimated time
          for this step is 4.20 seconds.
  5) We execute the following steps in parallel.
       1) We do an all-AMPs RETRIEVE step from Spool 5 (Last Use) by
          way of an all-rows scan into Spool 8 (all_amps), which is
          duplicated on all AMPs.  Then we do a SORT to order Spool 8
          by the hash code of (T1_STAGING.STG.DLOF_ID_NO).  The size of
          Spool 8 is estimated with high confidence to be 336 rows (
          640,080 bytes).  The estimated time for this step is 0.01
          seconds.
       2) We do an all-AMPs RETRIEVE step from Spool 3 (Last Use) by
          way of an all-rows scan into Spool 9 (all_amps), which is
          duplicated on all AMPs.  The result spool file will not be
          cached in memory.  The size of Spool 9 is estimated with high
          confidence to be 226,080 rows (391,796,640 bytes).  The
          estimated time for this step is 1.05 seconds.
  6) We do an all-AMPs JOIN step from Spool 8 (Last Use) by way of a
     RowHash match scan, which is joined to Spool 2 (Last Use) by way
     of a RowHash match scan.  Spool 8 and Spool 2 are joined using a
     merge join, with a join condition of (""DLOF_ID_NO = DLOF_ID_NO_RU"").
     The result goes into Spool 1 (group_amps), which is built locally
     on the AMPs.  The result spool file will not be cached in memory.
     The size of Spool 1 is estimated with low confidence to be 2,121
     rows (11,491,578 bytes).  The estimated time for this step is 0.03
     seconds.
  7) We do an all-AMPs JOIN step from Spool 6 (Last Use) by way of an
     all-rows scan, which is joined to Spool 9 (Last Use) by way of an
     all-rows scan.  Spool 6 and Spool 9 are joined using a single
     partition hash join, with a join condition of (""DLOF_ID_NO =
     DLOF_ID_NO_RU"").  The result goes into Spool 1 (group_amps), which
     is built locally on the AMPs.  The result spool file will not be
     cached in memory.  The size of Spool 1 is estimated with low
     confidence to be 9,243,161 rows (50,079,446,298 bytes).  The
     estimated time for this step is 0.60 seconds.
  8) We do an all-AMPs JOIN step from Spool 4 (Last Use) by way of an
     all-rows scan, which is joined to Spool 7 (Last Use) by way of an
     all-rows scan.  Spool 4 and Spool 7 are joined using a hash join
     of 2 partitions, with a join condition of (""DLOF_ID_NO =
     DLOF_ID_NO_RU"").  The result goes into Spool 1 (group_amps), which
     is built locally on the AMPs.  The result spool file will not be
     cached in memory.  The size of Spool 1 is estimated with low
     confidence to be 731,525 rows (3,963,402,450 bytes).  The
     estimated time for this step is 0.96 seconds.
  9) Finally, we send out an END TRANSACTION step to all AMPs involved
     in processing the request.
  -> The contents of Spool 1 are sent back to the user as the result of
     statement 1.  The total estimated time is 6.84 seconds.
",4
82,27049104,select update in teradata Syntax Error,"I am attempting to update one table with values from another in Teradata using an update from clause:

update p84
set p84.ACCOUNT_NME = p92.GL_ACCOUNT_NUM
from D_FAR_SBXD.T_FSM_ACCOUNT_DIMENSION p84
full outer join D_FAR_SBXD.T_FSM_ACCOUNT p92 on p84.ACCOUNT_NME = p92.ACCOUNT_NME
where p92.ACCOUNT_TREE_NME='ACCT_OLAP_GAAP' 
and p84.ACCOUNT_NME is not null
and p92.GL_ACCOUNT_NUM &lt;&gt; '999999'
and p92.GL_ACCOUNT_NUM &lt;&gt; 'M99999'


I have an error: [Teradata][ODBC Teradata Driver][Teradata Database] Syntax error: expected something between the word 'GL_ACCOUNT_NUM' and the 'from' keyword. (42000,-3706)

What am I missing?
",-1,-1,-1.0,"I am attempting to update one table with values from another in Teradata using an update from clause:

update p84
set p84.ACCOUNT_NME = p92.GL_ACCOUNT_NUM
from D_FAR_SBXD.T_FSM_ACCOUNT_DIMENSION p84
full outer join D_FAR_SBXD.T_FSM_ACCOUNT p92 on p84.ACCOUNT_NME = p92.ACCOUNT_NME
where p92.ACCOUNT_TREE_NME='ACCT_OLAP_GAAP' 
and p84.ACCOUNT_NME is not null
and p92.GL_ACCOUNT_NUM &lt;&gt; '999999'
and p92.GL_ACCOUNT_NUM &lt;&gt; 'M99999'


I have an error: [Teradata][ODBC Teradata Driver][Teradata Database] Syntax error: expected something between the word 'GL_ACCOUNT_NUM' and the 'from' keyword. (42000,-3706)

What am I missing?
",3
83,27054613,Usage of REGEXP_SIMILAR in Teradata,"I'm working on Teradata and trying to use REGEXP_SIMILAR function.

 *** Teradata Database Release is 14.10.03.10                   
 *** Teradata Database Version is 14.10.03.06  


Here's my sample data.

create table test_table(
    test_col varchar(20)
);

insert into test_table values('lorem');
insert into test_table values('984kd');
insert into test_table values('ier7j');
insert into test_table values('34535');
insert into test_table values('lore9');
insert into test_table values(' 09sd');


I want to see the records which start with a number.

select test_col, regexp_similar(test_col, '^\d+','i')
from test_table;

test_col              regexp_similar(test_col,'^\d+','i')
--------------------  -----------------------------------
lore9                                                   0
lorem                                                   0
 09sd                                                   0
ier7j                                                   0
984kd                                                   0
34535                                                   1


But, the above query shows a match only for '34535' row and not for '984kd'. Seems like ^ character(also $) don't have the desired effect.

Isn't REGEXP_SIMILAR similar to REGEXP_LIKE of Oracle?

Can someone explain why is this happening and how to solve this. 
",1,-1,-1.0,"I'm working on Teradata and trying to use REGEXP_SIMILAR function.

 *** Teradata Database Release is 14.10.03.10                   
 *** Teradata Database Version is 14.10.03.06  


Here's my sample data.

create table test_table(
    test_col varchar(20)
);

insert into test_table values('lorem');
insert into test_table values('984kd');
insert into test_table values('ier7j');
insert into test_table values('34535');
insert into test_table values('lore9');
insert into test_table values(' 09sd');


I want to see the records which start with a number.

select test_col, regexp_similar(test_col, '^\d+','i')
from test_table;

test_col              regexp_similar(test_col,'^\d+','i')
--------------------  -----------------------------------
lore9                                                   0
lorem                                                   0
 09sd                                                   0
ier7j                                                   0
984kd                                                   0
34535                                                   1


But, the above query shows a match only for '34535' row and not for '984kd'. Seems like ^ character(also $) don't have the desired effect.

Isn't REGEXP_SIMILAR similar to REGEXP_LIKE of Oracle?

Can someone explain why is this happening and how to solve this. 
",3
84,27237389,Connecting to Teradata with Perl and undefined symbol: DBCHINI,"i am having issues trying to get perl to connect to Teradata. I have installed the Teradata DBD from CPAN as well as the teradata utilities required. however when i attempt to run a perl script i get the following error:

/usr/local/bin/perl: symbol lookup error: /usr/local/lib64/perl5/auto/DBD/Teradata     /Cli/Cli.so: undefined symbol: DBCHINI


thinking that this was an issue with paths and what appears to be an issue with Teradata drivers looking in the 32 bit directories i updated the following paths:

export ODBCINI=/opt/teradata/client/14.00/odbc_64/odbc.ini 
export NLSPATH=/opt/teradata/client/14.00/odbc_64/msg/%N.cat 
export LD_LIBRARY_PATH=/opt/teradata/client/14.00/odbc_64/lib
export ODBC_HOME=/opt/teradata/client/14.00/odbc_64


however it did not resolve the issue. This is installed on a 64bit RHEL and i am using Perl v5.10.1 
",-1,-1,-1.0,"i am having issues trying to get perl to connect to Teradata. I have installed the Teradata DBD from CPAN as well as the teradata utilities required. however when i attempt to run a perl script i get the following error:

/usr/local/bin/perl: symbol lookup error: /usr/local/lib64/perl5/auto/DBD/Teradata     /Cli/Cli.so: undefined symbol: DBCHINI


thinking that this was an issue with paths and what appears to be an issue with Teradata drivers looking in the 32 bit directories i updated the following paths:

export ODBCINI=/opt/teradata/client/14.00/odbc_64/odbc.ini 
export NLSPATH=/opt/teradata/client/14.00/odbc_64/msg/%N.cat 
export LD_LIBRARY_PATH=/opt/teradata/client/14.00/odbc_64/lib
export ODBC_HOME=/opt/teradata/client/14.00/odbc_64


however it did not resolve the issue. This is installed on a 64bit RHEL and i am using Perl v5.10.1 
",1
85,27605820,Teradata - SQL SECURITY CREATOR,"I am using the SQL SECURITY CREATOR stored proc hint to create a stored proc (logged in as DBC), but when I execute the proc as another user it appears that it does not have sufficient priveleges.

The idea (long term) is that the database names will be dynamic parameters passed into the proc - so tell me to just add the needed permissions.  This has to work at runtime.  I just hardcoded everything to demonstrate the crux of the problem.

Teradata 13.10 (VMWare test machine).

--Logged in as DBC
CREATE DATABASE dbAlpha FROM SYSDBA AS PERMANENT = 100;
CREATE DATABASE dbBravo FROM SYSDBA AS PERMANENT = 0;
CREATE DATABASE dbCharlie_Proc FROM SYSDBA AS PERMANENT = 200000;

GRANT CREATE PROCEDURE ON dbCharlie_Proc TO DBC;

Replace Procedure dbCharlie_Proc.GiveSpace (
    )
SQL SECURITY CREATOR --The stored proc will run under the security context of the     user     that created it - DBC in this case
--When you specify SQL SECURITY CREATOR, Teradata Database verifies and applies the privileges of the user who created the procedure.
BEGIN 
    DECLARE strSQL VARCHAR(8000);
    SET strSQL = 'CREATE DATABASE dbDelta_temp from SYSDBA as PERMANENT = 5';
    EXECUTE IMMEDIATE strSQL;
    SET strSQL = 'Give dbDelta_temp to dbBravo';
    EXECUTE IMMEDIATE strSQL;
    SET strSQL = 'drop database dbDelta_temp';
    EXECUTE IMMEDIATE strSQL;
End;

CALL dbCharlie_Proc.GiveSpace();


Gives me this error:

GIVESPACE:The user does not have DROP DATABASE access to database dbDelta_temp. (3524)
",-1,-1,-1.0,"I am using the SQL SECURITY CREATOR stored proc hint to create a stored proc (logged in as DBC), but when I execute the proc as another user it appears that it does not have sufficient priveleges.

The idea (long term) is that the database names will be dynamic parameters passed into the proc - so tell me to just add the needed permissions.  This has to work at runtime.  I just hardcoded everything to demonstrate the crux of the problem.

Teradata 13.10 (VMWare test machine).

--Logged in as DBC
CREATE DATABASE dbAlpha FROM SYSDBA AS PERMANENT = 100;
CREATE DATABASE dbBravo FROM SYSDBA AS PERMANENT = 0;
CREATE DATABASE dbCharlie_Proc FROM SYSDBA AS PERMANENT = 200000;

GRANT CREATE PROCEDURE ON dbCharlie_Proc TO DBC;

Replace Procedure dbCharlie_Proc.GiveSpace (
    )
SQL SECURITY CREATOR --The stored proc will run under the security context of the     user     that created it - DBC in this case
--When you specify SQL SECURITY CREATOR, Teradata Database verifies and applies the privileges of the user who created the procedure.
BEGIN 
    DECLARE strSQL VARCHAR(8000);
    SET strSQL = 'CREATE DATABASE dbDelta_temp from SYSDBA as PERMANENT = 5';
    EXECUTE IMMEDIATE strSQL;
    SET strSQL = 'Give dbDelta_temp to dbBravo';
    EXECUTE IMMEDIATE strSQL;
    SET strSQL = 'drop database dbDelta_temp';
    EXECUTE IMMEDIATE strSQL;
End;

CALL dbCharlie_Proc.GiveSpace();


Gives me this error:

GIVESPACE:The user does not have DROP DATABASE access to database dbDelta_temp. (3524)
",3
86,27699296,Teradata INSTR in UNIX SHELL,"INSTR and SUBSTRING are not working in SHELL SCRIPT
Hi
Iam Using INSTR and SUBSTRING in UNIX shell script. They are working in Teradata sql assistant but they both are not working in UNIX SHELL SCRIPT.
I changed SUBSTRING to SUBSTR and it worked. But i still have problem with INSTR. Can any one help me out.
Example :
select
case when  SubRegion like '%REGION%' then SubRegion  else SubRegion || ' '
|| 'REGION' end REGION_NAME,
SUBSTR(nodes  FROM instr(nodes,'-',1,1)+1 for instr(nodes,'-',1,1)-1) AS node,
SgSpeed,
SgUtil,
PortCount,
CAST(WeekEndingDate as DATE) WEEKENDINGDATE
FROM RNL_VIEWS.WT_CmtsSgUtil
WHERE instr(nodes,'-') &gt; 0
and WeekEndingDate =  '2014-12-06'
ERROR:
SUBSTR(nodes  FROM instr(nodes,'-',1,1)+1 for instr(nodes,'-',1,1)-1) AS n
ode,
               $

*** Failure 3706 Syntax error: expected something between the word 'nodes'
and the 'FROM' keyword.
            Statement# 1, Info =582 

*** Total elapsed time was 1 second.
Thanks
Naveen
",-1,-1,-1.0,"INSTR and SUBSTRING are not working in SHELL SCRIPT
Hi
Iam Using INSTR and SUBSTRING in UNIX shell script. They are working in Teradata sql assistant but they both are not working in UNIX SHELL SCRIPT.
I changed SUBSTRING to SUBSTR and it worked. But i still have problem with INSTR. Can any one help me out.
Example :
select
case when  SubRegion like '%REGION%' then SubRegion  else SubRegion || ' '
|| 'REGION' end REGION_NAME,
SUBSTR(nodes  FROM instr(nodes,'-',1,1)+1 for instr(nodes,'-',1,1)-1) AS node,
SgSpeed,
SgUtil,
PortCount,
CAST(WeekEndingDate as DATE) WEEKENDINGDATE
FROM RNL_VIEWS.WT_CmtsSgUtil
WHERE instr(nodes,'-') &gt; 0
and WeekEndingDate =  '2014-12-06'
ERROR:
SUBSTR(nodes  FROM instr(nodes,'-',1,1)+1 for instr(nodes,'-',1,1)-1) AS n
ode,
               $

*** Failure 3706 Syntax error: expected something between the word 'nodes'
and the 'FROM' keyword.
            Statement# 1, Info =582 

*** Total elapsed time was 1 second.
Thanks
Naveen
",3
87,28326804,How to format a date as part of a string in Teradata?,"I'm trying to derive a filename in Teradata.

The format should be like this:


  X_&lt;YYYYMMDDHHMI&gt;_Y.dat


This is how I'm trying to achieve it: 


  'X_' || CAST(CURRENT_TIMESTAMP(FORMAT 'YYYYMMDDHHMI') (CHAR (12)) AS VARCHAR(50))   || '_Y.dat'


But I keep getting a bunch of syntax errors.

Any idea on how to achieve this?
",-1,-1,-1.0,"I'm trying to derive a filename in Teradata.

The format should be like this:


  X_&lt;YYYYMMDDHHMI&gt;_Y.dat


This is how I'm trying to achieve it: 


  'X_' || CAST(CURRENT_TIMESTAMP(FORMAT 'YYYYMMDDHHMI') (CHAR (12)) AS VARCHAR(50))   || '_Y.dat'


But I keep getting a bunch of syntax errors.

Any idea on how to achieve this?
",3
88,28499959,Connecting R To Teradata VOLATILE TABLE,"I am using R to try and connect to a teradata database and am running into difficulties
The steps in the process are below

1)  Create Connection
2)  Create a VOLATILE TABLE
3)  Load information from a data frame into the Volatile table

Here is where it fails, giving me an error message

Error in sqlSave(conn, mydata, tablename = ""TEMP"", rownames = FALSE,  : 
  first argument is not an open RODBC channel

The code is below

# Import Data From Text File and remove duplicates
mydata = read.table(""Keys.txt"")
mydata.unique = unique(mydata)

strSQL.TempTable = ""CREATE VOLATILE TABLE TEMP………[Table Details]""
    ""UNIQUE PRIMARY INDEX(index)""
    ""ON COMMIT PRESERVE ROWS;""

 # Connect To Database
   conn &lt;- tdConnect('Teradata')

 # Execute Temp Table
   tdQuery(strSQL.TempTable)
   sqlSave(conn, mydata, tablename = ""TEMP "",rownames = FALSE, append = TRUE)


Can anyone help, Is it closing off the connection before I can upload the information into the Table?
",-1,-1,-1.0,"I am using R to try and connect to a teradata database and am running into difficulties
The steps in the process are below

1)  Create Connection
2)  Create a VOLATILE TABLE
3)  Load information from a data frame into the Volatile table

Here is where it fails, giving me an error message

Error in sqlSave(conn, mydata, tablename = ""TEMP"", rownames = FALSE,  : 
  first argument is not an open RODBC channel

The code is below

# Import Data From Text File and remove duplicates
mydata = read.table(""Keys.txt"")
mydata.unique = unique(mydata)

strSQL.TempTable = ""CREATE VOLATILE TABLE TEMP………[Table Details]""
    ""UNIQUE PRIMARY INDEX(index)""
    ""ON COMMIT PRESERVE ROWS;""

 # Connect To Database
   conn &lt;- tdConnect('Teradata')

 # Execute Temp Table
   tdQuery(strSQL.TempTable)
   sqlSave(conn, mydata, tablename = ""TEMP "",rownames = FALSE, append = TRUE)


Can anyone help, Is it closing off the connection before I can upload the information into the Table?
",3
89,28536136,Teradata / Aster: How do i setup tab delimited output when using ACT tool of Aster DB?,"I want to export aster tables into tab delimited files, each file containing just 100 records. Hence i'm using ACT Tool as i cannot specify an SQL (select * from table_name limit 100) with ncluster_export tool.
One issue i'm facing is when specifying the -F option with ACT tool, i am not able to specify TAB as the delimiter.
I have tried '\t' and 'TAB', which did not resolve to correct TAB.
I have checked the guide and act --help, couldn't find the correct option. 

My commands have been like: 

act -h xx.xx.xx.xx -U &lt;&lt;username&gt;&gt; -w &lt;&lt;password&gt;&gt; -p &lt;&lt;port&gt; -d mintdw  -t -F ""\t"" -A -c ""select * from &lt;&lt;tablename&gt;&gt; limit 100;"" -o /tmp/filename

Please help me out.
Closest post on SE i found was :  Teradata / Aster : Fast Export / ncluster_export using query
",-1,-1,-1.0,"I want to export aster tables into tab delimited files, each file containing just 100 records. Hence i'm using ACT Tool as i cannot specify an SQL (select * from table_name limit 100) with ncluster_export tool.
One issue i'm facing is when specifying the -F option with ACT tool, i am not able to specify TAB as the delimiter.
I have tried '\t' and 'TAB', which did not resolve to correct TAB.
I have checked the guide and act --help, couldn't find the correct option. 

My commands have been like: 

act -h xx.xx.xx.xx -U &lt;&lt;username&gt;&gt; -w &lt;&lt;password&gt;&gt; -p &lt;&lt;port&gt; -d mintdw  -t -F ""\t"" -A -c ""select * from &lt;&lt;tablename&gt;&gt; limit 100;"" -o /tmp/filename

Please help me out.
Closest post on SE i found was :  Teradata / Aster : Fast Export / ncluster_export using query
",3
90,28582838,Uninstall teradata ulitilies issue,"I am trying to install teradata utilities 15.x, 

I earlier installed teradata ODBC drive and .NET Data Provider for teradata.

Now if I start the installation, I get an error saying:

""The installation failed for the following reason:
A TTU 14.0 suite is installed. Please uninstall all TTU 14.10 suites before installing TTU 15.00""

I tried to uninstall the suite using uninstall_TTU.vbs, but I get below error:

TTUSuiteSilent.exe was not found in .\TTU directory.
This script needs TTUSuiteSilent.exe to remove suite packages.
Aborting uninstallation....

Any advise on how to get this uninstalled?
",-1,-1,-1.0,"I am trying to install teradata utilities 15.x, 

I earlier installed teradata ODBC drive and .NET Data Provider for teradata.

Now if I start the installation, I get an error saying:

""The installation failed for the following reason:
A TTU 14.0 suite is installed. Please uninstall all TTU 14.10 suites before installing TTU 15.00""

I tried to uninstall the suite using uninstall_TTU.vbs, but I get below error:

TTUSuiteSilent.exe was not found in .\TTU directory.
This script needs TTUSuiteSilent.exe to remove suite packages.
Aborting uninstallation....

Any advise on how to get this uninstalled?
",1
91,28272946,SAS teradata connection,"I am using following query to connect SAS with Teradata:

proc sql;
    connect to Teradata (server = ‘WML’ user = ‘******’  password = ‘*******’ mode = Teradata );
quit;


But I am getting following error:


  ERROR : Teradata connection: TheUserId, Passowrd or Account is invalid


I have am able to work in Teradata with same username and password. Why am I getting this error
",-1,-1,-1.0,"I am using following query to connect SAS with Teradata:

proc sql;
    connect to Teradata (server = ‘WML’ user = ‘******’  password = ‘*******’ mode = Teradata );
quit;


But I am getting following error:


  ERROR : Teradata connection: TheUserId, Passowrd or Account is invalid


I have am able to work in Teradata with same username and password. Why am I getting this error
",1
92,27848855,Teradata REGEXP_SPLIT_TO_TABLE Inserting Spaces between Numbers,"I am having trouble with Teradata's REGEXP_SPLIT_TO_TABLE.  It is separating the results correctly, but it is inserting spaces between each individual number.  I used CHAR2HEXINT and found the Hex of the space is 00.  Here is the statement I am using:

SELECT *  
FROM TABLE(
       REGEXP_SPLIT_TO_TABLE('2625 1410', '2625 1410', '[ \t\r\n\v\f]' , 'i') 
         RETURNS (outkey VARCHAR(250), token_ndx INTEGER, token VARCHAR(220) )
   ) AS t1;


It returns 2 6 2 5 as a row and 1 4 1 0 as a row.  I want the results to be 2625 as a row and 1410 as a row.  Is there something I am doing wrong that is adding the spaces?  Is there a way to get rid of the spaces?  Thanks in advance.
",-1,-1,-1.0,"I am having trouble with Teradata's REGEXP_SPLIT_TO_TABLE.  It is separating the results correctly, but it is inserting spaces between each individual number.  I used CHAR2HEXINT and found the Hex of the space is 00.  Here is the statement I am using:

SELECT *  
FROM TABLE(
       REGEXP_SPLIT_TO_TABLE('2625 1410', '2625 1410', '[ \t\r\n\v\f]' , 'i') 
         RETURNS (outkey VARCHAR(250), token_ndx INTEGER, token VARCHAR(220) )
   ) AS t1;


It returns 2 6 2 5 as a row and 1 4 1 0 as a row.  I want the results to be 2625 as a row and 1410 as a row.  Is there something I am doing wrong that is adding the spaces?  Is there a way to get rid of the spaces?  Thanks in advance.
",3
93,26517922,export result into excel sheet from teradata sql assistant,"I want to export the results into excel sheet by running the query in Teradata SQL Assistant.
I used copy paste but it didnt work
Thanks in advance.
",1,-1,-1.0,"I want to export the results into excel sheet by running the query in Teradata SQL Assistant.
I used copy paste but it didnt work
Thanks in advance.
",3
94,26003090,Trimming a string in Teradata,"I have a table in Teradata with a particular column ""location"" like

location
Rockville County, TX 
Green River County, IL
Joliet County, CA
Jones County, FL
.
.
.


What I need to do is strip off everything after the county's name and turn the column into something like

location
Rockville
Green River 
Joliet 
Jones


I've been trying to use the function trim like

trim(trailing ' County' from location)


but it's not working. Any ideas?
",-1,-1,-1.0,"I have a table in Teradata with a particular column ""location"" like

location
Rockville County, TX 
Green River County, IL
Joliet County, CA
Jones County, FL
.
.
.


What I need to do is strip off everything after the county's name and turn the column into something like

location
Rockville
Green River 
Joliet 
Jones


I've been trying to use the function trim like

trim(trailing ' County' from location)


but it's not working. Any ideas?
",3
95,25941702,Read data from Teradata in Execute SQL task in SSIS 2008,"I am having a requirement to read a table from Teradata. Pull the records and send the email based on some condition. I have achieved this using Execute SQL task (which pulls me the data from Teradata) and For Loop Container (which send the email for every record). 

I am currently using ADO.NET connection to connect to Teradata in Execute SQL task. The issue here is, I am not able to deploy this on SQL server. This package runs perfectly fine and the moment I run this from SQL Server agent, it fails with below error:

""Executed as user: HNETNT\SQLDWDEV. Microsoft (R) SQL Server Execute Package Utility Version 10.50.4000.0 for 32-bit Copyright (C) Microsoft Corporation 2010. All rights reserved. Started: 10:41:45 AM Error: 2014-09-19 10:41:46.27 Code: 0xC00291EC Source: Execute Query Execute SQL Task Description: Failed to acquire connection ""Teradata Connection"". Connection may not be configured correctly or you may not have the right permissions on this connection. End Error DTExec: The package execution returned DTSER_FAILURE (1).""

It looks like, I am using wrong connection manager, i.e. ADO.NET. Can someone please help me to make this work. I tried Microsoft Annuity driver, but this cannot be used in Execute SQL task. Also I tried using ODBC connection. This is also failing.
",-1,-1,-1.0,"I am having a requirement to read a table from Teradata. Pull the records and send the email based on some condition. I have achieved this using Execute SQL task (which pulls me the data from Teradata) and For Loop Container (which send the email for every record). 

I am currently using ADO.NET connection to connect to Teradata in Execute SQL task. The issue here is, I am not able to deploy this on SQL server. This package runs perfectly fine and the moment I run this from SQL Server agent, it fails with below error:

""Executed as user: HNETNT\SQLDWDEV. Microsoft (R) SQL Server Execute Package Utility Version 10.50.4000.0 for 32-bit Copyright (C) Microsoft Corporation 2010. All rights reserved. Started: 10:41:45 AM Error: 2014-09-19 10:41:46.27 Code: 0xC00291EC Source: Execute Query Execute SQL Task Description: Failed to acquire connection ""Teradata Connection"". Connection may not be configured correctly or you may not have the right permissions on this connection. End Error DTExec: The package execution returned DTSER_FAILURE (1).""

It looks like, I am using wrong connection manager, i.e. ADO.NET. Can someone please help me to make this work. I tried Microsoft Annuity driver, but this cannot be used in Execute SQL task. Also I tried using ODBC connection. This is also failing.
",1
96,25344128,How to convert a SAS retain in Teradata sql?,"I'm trying to convert SAS code into Teradata and I'm struggling how to convert the code below. The issue is how to convert the retain statement into sql.

** Datastep code

data test2;
  set test1;
  by name_id;
  format counter 8.0;
  retain counter year_counter;
  if first.name_id then do;
    counter = month_start;
    year_counter=year_start;
  end;
  else counter=counter+1;
  if(counter=13) then do;
    counter=1;
    year_counter=year_counter+1;
  end;
  mmyy_counter=compress(counter|""/""|year_counter);
  mmyy=compress(month|""/""|year);
run;


**Data

name_id  month_start  month_year
12131as  07           2010
12132ab  02           2003


**My thoughts

I was thinking of putting this in a procedure, although I'm not sure how.
Another possibility is to use row_number, but again I am not sure how to start.
",-1,-1,-1.0,"I'm trying to convert SAS code into Teradata and I'm struggling how to convert the code below. The issue is how to convert the retain statement into sql.

** Datastep code

data test2;
  set test1;
  by name_id;
  format counter 8.0;
  retain counter year_counter;
  if first.name_id then do;
    counter = month_start;
    year_counter=year_start;
  end;
  else counter=counter+1;
  if(counter=13) then do;
    counter=1;
    year_counter=year_counter+1;
  end;
  mmyy_counter=compress(counter|""/""|year_counter);
  mmyy=compress(month|""/""|year);
run;


**Data

name_id  month_start  month_year
12131as  07           2010
12132ab  02           2003


**My thoughts

I was thinking of putting this in a procedure, although I'm not sure how.
Another possibility is to use row_number, but again I am not sure how to start.
",3
97,28655162,Connect Python to Teradata in mac with pyodbc,"I successfully installed pyodbc module for python 2.7. However, when input the following code to connect to teradata,

import pyodbc 
conn = pyodbc.connect('DRIVER={Teradata};DBCNAME=&lt;tdwc&gt;;UID=&lt;UID&gt;;PWD=&lt;UID&gt;;QUIETMODE=YES;')

I got the following error;

Traceback (most recent call last):
  File """", line 1, in 
    pyodbc.connect('DRIVER={Teradata};DBCNAME=;UID=;PWD=;QUIETMODE=YES;')
Error: ('00000', '[00000] [iODBC][Driver Manager]dlopen(/Library/Application Support/teradata/client/ODBC/lib/tdata.dylib, 6): Library not loaded: libtdparse.dylib\n  Referenced from: /Library/Application Support/teradata/client/ODBC/lib/tdata.dylib\n  Reason: image not found (0) (SQLDriverConnect)')

What should I do to have this fixed? Any ideas?
",-1,-1,-1.0,"I successfully installed pyodbc module for python 2.7. However, when input the following code to connect to teradata,

import pyodbc 
conn = pyodbc.connect('DRIVER={Teradata};DBCNAME=&lt;tdwc&gt;;UID=&lt;UID&gt;;PWD=&lt;UID&gt;;QUIETMODE=YES;')

I got the following error;

Traceback (most recent call last):
  File """", line 1, in 
    pyodbc.connect('DRIVER={Teradata};DBCNAME=;UID=;PWD=;QUIETMODE=YES;')
Error: ('00000', '[00000] [iODBC][Driver Manager]dlopen(/Library/Application Support/teradata/client/ODBC/lib/tdata.dylib, 6): Library not loaded: libtdparse.dylib\n  Referenced from: /Library/Application Support/teradata/client/ODBC/lib/tdata.dylib\n  Reason: image not found (0) (SQLDriverConnect)')

What should I do to have this fixed? Any ideas?
",1
98,28686914,Teradata create global temporary table,"In Teradata I want to create a global temporary table and then reference this global table in other queries. I don't want to create a volatile table as I am using Teradata through another front-end and that front-end has to be able to tell whether the temporary table exists, hence volatile tables are not an option as it does not exist in the data dictionary.

Here's my code to create the temporary table hihihi.

create set global temporary table hihihi as 
(select 
* 
from 
db.a_permanent_table) 
with no data 
on commit preserve rows;

select * from hihihi;


According to the research I have done I can't use the


  with data


option for temporary tables (eg. see this link). So I have to use the 


  no data


option. I think it also says that when I reference this temporary table the data will ""materialize"". 

However when I do the select as below

select * from hihihi;


nothing is returned? What am I missing in my understanding of global temporary tables?
",-1,-1,-1.0,"In Teradata I want to create a global temporary table and then reference this global table in other queries. I don't want to create a volatile table as I am using Teradata through another front-end and that front-end has to be able to tell whether the temporary table exists, hence volatile tables are not an option as it does not exist in the data dictionary.

Here's my code to create the temporary table hihihi.

create set global temporary table hihihi as 
(select 
* 
from 
db.a_permanent_table) 
with no data 
on commit preserve rows;

select * from hihihi;


According to the research I have done I can't use the


  with data


option for temporary tables (eg. see this link). So I have to use the 


  no data


option. I think it also says that when I reference this temporary table the data will ""materialize"". 

However when I do the select as below

select * from hihihi;


nothing is returned? What am I missing in my understanding of global temporary tables?
",3
99,28745083,how to get the return codes from SAS pass-through SQL to Teradata ?,"In SAS 9.2, how do I get the return codes / error messages from explicit pass-through sql to teradata? Printed in log or output or something.

I already got a small query to work fine, but having some trouble with a more complex one. Debugging would be much easier with the error messages. 
Tried the sqlxmsg and sqlxrc that are used when querying db2, but of course those don't work... haven't found any documentation on this. (I'm quite new to Teradata)
",-1,-1,-1.0,"In SAS 9.2, how do I get the return codes / error messages from explicit pass-through sql to teradata? Printed in log or output or something.

I already got a small query to work fine, but having some trouble with a more complex one. Debugging would be much easier with the error messages. 
Tried the sqlxmsg and sqlxrc that are used when querying db2, but of course those don't work... haven't found any documentation on this. (I'm quite new to Teradata)
",3
100,28832535,Child user can't select view of parent in teradata,"In Teradata SQLAssistant, I have created a db user USER1 which has 17 views. This user can do a SELECT on any view, thanks to a profile/role I granted and then associated to him. 
His child USER2 is a db user with the same profile/role as USER1, but my problem is he can't select any of the views of USER1 :


  Table/View **** not found, or you have no access rights


When I check in Teradata Administrator, USER2 has the same right (READ) on the datawarehouse tables used by the 17 views as USER1....

Can anyone help me ?
I can provide more details.

Thanks in advance.



EDIT1 : I have the possibility to do that select, but i need to do a
 Grant SELECT on table_used_by_view to USER1 WITH GRANT OPTION; on each table...  I really need to stick with the ROLE/PROFILE method, so I can't accept this solution (as you can't do a WITH GRANT OPTION on a role).
",1,-1,-1.0,"In Teradata SQLAssistant, I have created a db user USER1 which has 17 views. This user can do a SELECT on any view, thanks to a profile/role I granted and then associated to him. 
His child USER2 is a db user with the same profile/role as USER1, but my problem is he can't select any of the views of USER1 :


  Table/View **** not found, or you have no access rights


When I check in Teradata Administrator, USER2 has the same right (READ) on the datawarehouse tables used by the 17 views as USER1....

Can anyone help me ?
I can provide more details.

Thanks in advance.



EDIT1 : I have the possibility to do that select, but i need to do a
 Grant SELECT on table_used_by_view to USER1 WITH GRANT OPTION; on each table...  I really need to stick with the ROLE/PROFILE method, so I can't accept this solution (as you can't do a WITH GRANT OPTION on a role).
",3
101,28847453,Teradata Connection with Microsoft Excel Macro,"I am getting problem while trying to run Marcos in Microsoft Excel for Teradata Connection. My code for Connecting Teradata through marcos

Dim hostName As String: hostName = ""locahost""
Dim dbName As String: dbName = ""STG_CNV""
Dim username As String: username = ""username""
Dim password As String: password = ""password""
Dim Row As Integer: Row = 1

Dim Query As String, lastColumn As Integer, lastRow As Long

'Connect to database
Conn.Open ""Driver={Teradata};"" &amp; _
          ""DBCName="" &amp; hostName &amp; "";"" &amp; _
          ""Database="" &amp; dbName &amp; "";"" &amp; _
          ""Uid="" &amp; username &amp; "";"" &amp; _
          ""Pwd="" &amp; password &amp; "";"" &amp; _
          ""Extended Properties=""""EXTENDLOBSUPPORT=Yes""""""
          '""Extended Properties=""""USENATIVELOBSUPPORT=Yes""""""


I am Error in as

[Teradata][ODBC Teradata Drive] Major Status=0x04bd Minor Status=0xe1000095-[terasso] Cannot load TDGSS library.


As I know this is problem with ODBC drive. But my teradata client is running wiht ODBC drive but in Microsoft Excel, its not. How can we solve it ? 
",-1,-1,-1.0,"I am getting problem while trying to run Marcos in Microsoft Excel for Teradata Connection. My code for Connecting Teradata through marcos

Dim hostName As String: hostName = ""locahost""
Dim dbName As String: dbName = ""STG_CNV""
Dim username As String: username = ""username""
Dim password As String: password = ""password""
Dim Row As Integer: Row = 1

Dim Query As String, lastColumn As Integer, lastRow As Long

'Connect to database
Conn.Open ""Driver={Teradata};"" &amp; _
          ""DBCName="" &amp; hostName &amp; "";"" &amp; _
          ""Database="" &amp; dbName &amp; "";"" &amp; _
          ""Uid="" &amp; username &amp; "";"" &amp; _
          ""Pwd="" &amp; password &amp; "";"" &amp; _
          ""Extended Properties=""""EXTENDLOBSUPPORT=Yes""""""
          '""Extended Properties=""""USENATIVELOBSUPPORT=Yes""""""


I am Error in as

[Teradata][ODBC Teradata Drive] Major Status=0x04bd Minor Status=0xe1000095-[terasso] Cannot load TDGSS library.


As I know this is problem with ODBC drive. But my teradata client is running wiht ODBC drive but in Microsoft Excel, its not. How can we solve it ? 
",1
102,29105836,Turning a Comma Separated string into individual rows in Teradata,"I read the post:
Turning a Comma Separated string into individual rows

And really like the solution:

SELECT A.OtherID,  
     Split.a.value('.', 'VARCHAR(100)') AS Data  
 FROM  
 (     SELECT OtherID,  
         CAST ('&lt;M&gt;' + REPLACE(Data, ',', '&lt;/M&gt;&lt;M&gt;') + '&lt;/M&gt;' AS XML) AS Data  
     FROM  Table1
 ) AS A CROSS APPLY Data.nodes ('/M') AS Split(a); 


But it did not work when I tried to apply the method in Teradata for a similar question.  Here is the summarized error code: 
select failed 3707: expected something between '.' and the 'value' keyword. So is the code only valid in SQL Server?  Would anyone help me to make it work in Teradata or SAS SQL?  Your help will be really appreciated! 
",-1,-1,-1.0,"I read the post:
Turning a Comma Separated string into individual rows

And really like the solution:

SELECT A.OtherID,  
     Split.a.value('.', 'VARCHAR(100)') AS Data  
 FROM  
 (     SELECT OtherID,  
         CAST ('&lt;M&gt;' + REPLACE(Data, ',', '&lt;/M&gt;&lt;M&gt;') + '&lt;/M&gt;' AS XML) AS Data  
     FROM  Table1
 ) AS A CROSS APPLY Data.nodes ('/M') AS Split(a); 


But it did not work when I tried to apply the method in Teradata for a similar question.  Here is the summarized error code: 
select failed 3707: expected something between '.' and the 'value' keyword. So is the code only valid in SQL Server?  Would anyone help me to make it work in Teradata or SAS SQL?  Your help will be really appreciated! 
",3
103,29639133,How to join a large table (1M+) with a reference table in Teradata?,"I have the following 2 tables-
tableA (3 million rows; this is the ""data table"")
tableB (2300 rows; this is the ""reference table"")  

Schemas -
tableA - id, field1, field2, field3, num_of_actual_items, num_of_possible_items
tableB - field1, field2, field3, num_of_possible_items  

Background
There are 14000 unique ids in tableA.
So a [select count(distinct id) from tableA] gives 14000 as the answer.  

There are 2300 possible combinations of field1, field2 and field3 and all of them with their corresponding num_of_possible_items are listed in tableB.  

tableA does not contain entries for an id when num_of_actual_items for a given ""field1-field2-field3"" combination is 0. This is what I am trying to fix.  

Ideally tableA should have 32,200,000 rows (14000 ids X 2300 combinations). The query I have is as follows:  

select A1.id, A1.field1, A1.field2, A1.field3, A1.num_of_actual_items, 
A1.num_of_possible_items
from tableA A1
union
select distinct A2.id, B.field1, B.field2, B.field3, 0 as 
num_of_actual_items, B.num_of_possible_items
from tableA A2, tableB B
where A2.field1 || A2.field2 || A2.field3 &lt;&gt; B.field1 || B.field2 || 
B.field3


The above query will give 2 rows for each id and corresponding field1-field2-field3 combination (one for the real num_of_actual_items and one for the artificially added 0 entry).
In the next step, the duplicate rows can be removed by doing an aggregate (max of num_of_actual_items) and the problem is solved.  

However, this solution only works when tableA has 5000-10000 rows. When I try it with the full 3 million rows, I get an ""out of spool space"" error.  

The above query actually populates a volatile table and I have created an indices on field1,field2 &amp; field3 and also collected stats on all 3.  

Any ideas what I can do to optimize/change the query for such large volumes?
I am using Teradata.  

Edit: I have added the suggestions by Andrew (limiting rows that don't match) and dnoeth (returning only distinct rows as opposed to all) but still no avail.
I keep running out of spool space.
",1,-1,-1.0,"I have the following 2 tables-
tableA (3 million rows; this is the ""data table"")
tableB (2300 rows; this is the ""reference table"")  

Schemas -
tableA - id, field1, field2, field3, num_of_actual_items, num_of_possible_items
tableB - field1, field2, field3, num_of_possible_items  

Background
There are 14000 unique ids in tableA.
So a [select count(distinct id) from tableA] gives 14000 as the answer.  

There are 2300 possible combinations of field1, field2 and field3 and all of them with their corresponding num_of_possible_items are listed in tableB.  

tableA does not contain entries for an id when num_of_actual_items for a given ""field1-field2-field3"" combination is 0. This is what I am trying to fix.  

Ideally tableA should have 32,200,000 rows (14000 ids X 2300 combinations). The query I have is as follows:  

select A1.id, A1.field1, A1.field2, A1.field3, A1.num_of_actual_items, 
A1.num_of_possible_items
from tableA A1
union
select distinct A2.id, B.field1, B.field2, B.field3, 0 as 
num_of_actual_items, B.num_of_possible_items
from tableA A2, tableB B
where A2.field1 || A2.field2 || A2.field3 &lt;&gt; B.field1 || B.field2 || 
B.field3


The above query will give 2 rows for each id and corresponding field1-field2-field3 combination (one for the real num_of_actual_items and one for the artificially added 0 entry).
In the next step, the duplicate rows can be removed by doing an aggregate (max of num_of_actual_items) and the problem is solved.  

However, this solution only works when tableA has 5000-10000 rows. When I try it with the full 3 million rows, I get an ""out of spool space"" error.  

The above query actually populates a volatile table and I have created an indices on field1,field2 &amp; field3 and also collected stats on all 3.  

Any ideas what I can do to optimize/change the query for such large volumes?
I am using Teradata.  

Edit: I have added the suggestions by Andrew (limiting rows that don't match) and dnoeth (returning only distinct rows as opposed to all) but still no avail.
I keep running out of spool space.
",3
104,29726018,Converting dates and timestamps when inserting data into Teradata,"I am block-inserting data from Stata (a statistics package) into a Teradata database. I am having trouble converting dates and timestamps from Stata's native format to Teradata's.

Stata stores dates as days since 01/01/1960, so that 01jan1960 is 0 and 02jan1960 is 1. Timestamps are stored as milliseconds since 01jan1960 00:00:00.000, so that 1000 is 01jan1960 00:00:01. Here are some examples: 

          timestamp   Stata's tstamp  date           Stata's date  
2015-04-13 03:07:08   1744513628000   2015-04-13     20191  
2015-04-14 19:55:43   1744660543000   2015-04-14     20192  
2015-04-08 11:41:39   1744112499000   2015-04-08     20186  
2015-04-15 06:53:34   1744700014000   2015-04-15     20193  


I tried 2 approaches. The first involves converting the dates/timestamps to strings in Stata before inserting and then doing something like this once the data is inserted:

ALTER TABLE mytable ALTER date_variable DATETIME

However, I cannot figure out how to do the second part from the documentation I have and after searching the various fora.

The second approach is leaving the dates and timestamps as integers, and then doing some of conversion once the integers are inserted. Perhaps I can also pre-convert dates in Stata to TD's internal format with:

gen td_date = ((year(stata_dt)-1900)*10000 + month(stata_dt)*100 + day(stata_dt))


However, I am not sure what the formula for timestamps would be. I am also not sure how to do the second part (making the integers into dates/timestamps).  
",-1,-1,-1.0,"I am block-inserting data from Stata (a statistics package) into a Teradata database. I am having trouble converting dates and timestamps from Stata's native format to Teradata's.

Stata stores dates as days since 01/01/1960, so that 01jan1960 is 0 and 02jan1960 is 1. Timestamps are stored as milliseconds since 01jan1960 00:00:00.000, so that 1000 is 01jan1960 00:00:01. Here are some examples: 

          timestamp   Stata's tstamp  date           Stata's date  
2015-04-13 03:07:08   1744513628000   2015-04-13     20191  
2015-04-14 19:55:43   1744660543000   2015-04-14     20192  
2015-04-08 11:41:39   1744112499000   2015-04-08     20186  
2015-04-15 06:53:34   1744700014000   2015-04-15     20193  


I tried 2 approaches. The first involves converting the dates/timestamps to strings in Stata before inserting and then doing something like this once the data is inserted:

ALTER TABLE mytable ALTER date_variable DATETIME

However, I cannot figure out how to do the second part from the documentation I have and after searching the various fora.

The second approach is leaving the dates and timestamps as integers, and then doing some of conversion once the integers are inserted. Perhaps I can also pre-convert dates in Stata to TD's internal format with:

gen td_date = ((year(stata_dt)-1900)*10000 + month(stata_dt)*100 + day(stata_dt))


However, I am not sure what the formula for timestamps would be. I am also not sure how to do the second part (making the integers into dates/timestamps).  
",3
105,30073004,Teradata : Stored Procedure : parameter column name,"I'm looking for help with a stored procedure in teradata.  I want to update a whole table and for this I'm trying to use a for loop cursor. the problem is that my update is defined via column names passing through parameters to the SP.

I've seen it can be possible to use dynamic sql to do that but I haven't found any information on the subject concerning for loop cursor and dynamic sql. Is it possible with FOR LOOP CURSOR ? 
I've tried to do only the select and calculs with dynamic sql, it works fine but then the problem is to update the table from the cursor on the select. In this case how to update a table from my cursor?

I let you show my code.

loop cursor :

REPLACE PROCEDURE [database].calDELAI
 (
     IN dateDebut   VARCHAR(30),
     IN dateFin   VARCHAR(30),
     IN delay VARCHAR(30)
 )
 BEGIN
       DECLARE DATE_DEBUT_COLONNE VARCHAR(64);
       DECLARE DATE_FIN_COLONNE VARCHAR(64);

       SET DATE_DEBUT_COLONNE=dateDebut;
       SET DATE_FIN_COLONNE=dateFin;

       FOR for_loop_update AS cur_select_set CURSOR FOR
                       SELECT 
                       TMP.DATE_FIN_COLONNE-TMP.DATE_DEBUT_COLONNE
                       FROM [database].ORD_T_DETL_ORDR_DELAI AS TMP
                       /* the select is more complicated but here is the spirit of it.*/
        DO
                       IF (delay='DELAI1') THEN SET DELAI1=NB_JR_OUVRABLE;
                       END IF;
       END FOR ;
 END ;


The errors given by teradata are :

    SPL1027:E, Missing/Invalid SQL statement'E(3810):Column/Parameter '[database].TMP.DATE_FIN_COLONNE' does not exist.'.
    SPL2001:E, Undefined symbol 'DELAI1'.
    SPL2001:E, Undefined symbol 'NB_JR_OUVRABLE'.


Thanks in advance for your replies and your help.
",1,-1,-1.0,"I'm looking for help with a stored procedure in teradata.  I want to update a whole table and for this I'm trying to use a for loop cursor. the problem is that my update is defined via column names passing through parameters to the SP.

I've seen it can be possible to use dynamic sql to do that but I haven't found any information on the subject concerning for loop cursor and dynamic sql. Is it possible with FOR LOOP CURSOR ? 
I've tried to do only the select and calculs with dynamic sql, it works fine but then the problem is to update the table from the cursor on the select. In this case how to update a table from my cursor?

I let you show my code.

loop cursor :

REPLACE PROCEDURE [database].calDELAI
 (
     IN dateDebut   VARCHAR(30),
     IN dateFin   VARCHAR(30),
     IN delay VARCHAR(30)
 )
 BEGIN
       DECLARE DATE_DEBUT_COLONNE VARCHAR(64);
       DECLARE DATE_FIN_COLONNE VARCHAR(64);

       SET DATE_DEBUT_COLONNE=dateDebut;
       SET DATE_FIN_COLONNE=dateFin;

       FOR for_loop_update AS cur_select_set CURSOR FOR
                       SELECT 
                       TMP.DATE_FIN_COLONNE-TMP.DATE_DEBUT_COLONNE
                       FROM [database].ORD_T_DETL_ORDR_DELAI AS TMP
                       /* the select is more complicated but here is the spirit of it.*/
        DO
                       IF (delay='DELAI1') THEN SET DELAI1=NB_JR_OUVRABLE;
                       END IF;
       END FOR ;
 END ;


The errors given by teradata are :

    SPL1027:E, Missing/Invalid SQL statement'E(3810):Column/Parameter '[database].TMP.DATE_FIN_COLONNE' does not exist.'.
    SPL2001:E, Undefined symbol 'DELAI1'.
    SPL2001:E, Undefined symbol 'NB_JR_OUVRABLE'.


Thanks in advance for your replies and your help.
",3
106,30100592,teradata export from hive to teradata class not found,"I am trying to export from hive table to teradata using TDCH connector, I am getting below errror:-

15/05/07 08:01:03 INFO tool.ConnectorExportTool: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/metastore/api/MetaException
            at java.lang.Class.forName0(Native Method)
            at java.lang.Class.forName(Class.java:190)
            at com.teradata.connector.common.tool.ConnectorJobRunner.runJob(ConnectorJobRunner.java:81)
            at com.teradata.connector.common.tool.ConnectorExportTool.run(ConnectorExportTool.java:61)
            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
            at com.teradata.hadoop.tool.TeradataExportTool.main(TeradataExportTool.java:24)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:606)
            at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
            at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
    Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.metastore.api.MetaException
            at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
            at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
            at java.security.AccessController.doPrivileged(Native Method)
            at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
            at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
            at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
            ... 13 more


I understand from error that hive-metastore jar is missing. But it is already there in hive/lib folder

 hive-metastore.jar -&gt; hive-metastore-0.9.0.jar


Is already present in the path: /usr/hdp/2.2.4.2-2/hive/lib
",-1,-1,-1.0,"I am trying to export from hive table to teradata using TDCH connector, I am getting below errror:-

15/05/07 08:01:03 INFO tool.ConnectorExportTool: java.lang.NoClassDefFoundError: org/apache/hadoop/hive/metastore/api/MetaException
            at java.lang.Class.forName0(Native Method)
            at java.lang.Class.forName(Class.java:190)
            at com.teradata.connector.common.tool.ConnectorJobRunner.runJob(ConnectorJobRunner.java:81)
            at com.teradata.connector.common.tool.ConnectorExportTool.run(ConnectorExportTool.java:61)
            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
            at com.teradata.hadoop.tool.TeradataExportTool.main(TeradataExportTool.java:24)
            at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
            at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:606)
            at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
            at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
    Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.metastore.api.MetaException
            at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
            at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
            at java.security.AccessController.doPrivileged(Native Method)
            at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
            at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
            at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
            ... 13 more


I understand from error that hive-metastore jar is missing. But it is already there in hive/lib folder

 hive-metastore.jar -&gt; hive-metastore-0.9.0.jar


Is already present in the path: /usr/hdp/2.2.4.2-2/hive/lib
",0
107,30220889,Teradata - Numeric overflow occurred during computation,"I am having an issue with a calculation in one of my Teradata queries.  I am multiplying two numbers by each other but i am getting a ""Numeric overflow occurred during computation."" error when running the query.  I ran a type on both fields and they are DECIMAL(18,15) and DECIMAL(18,9).  I tried casting them both to DECIMAL(18,18) when i do the division but its still throwing errors.  Here is the calculation. UNITS is the 18,15 and PRICE is the 18,9. Can anyone please give me any tips on how to resolve this?

cast(UNITS as DECIMAL(18,18))* cast(PRICE as DECIMAL(18,18)) as  NEW_CALC


Thanks,

Craig
",-1,-1,-1.0,"I am having an issue with a calculation in one of my Teradata queries.  I am multiplying two numbers by each other but i am getting a ""Numeric overflow occurred during computation."" error when running the query.  I ran a type on both fields and they are DECIMAL(18,15) and DECIMAL(18,9).  I tried casting them both to DECIMAL(18,18) when i do the division but its still throwing errors.  Here is the calculation. UNITS is the 18,15 and PRICE is the 18,9. Can anyone please give me any tips on how to resolve this?

cast(UNITS as DECIMAL(18,18))* cast(PRICE as DECIMAL(18,18)) as  NEW_CALC


Thanks,

Craig
",3
108,30858546,Connect teradata JDBC driver on coldfusion 11,"I tried to install TERADATA JDBC on ColdFusion 11 using this link.
Everything is fine, but when I try to create the datasource, I get following errors:


  ""Connection verification failed for data source: TeraJDBC4
  java.sql.SQLException: No suitable driver available for TeraJDBC4,
  please check the driver setting in resources file, error:
  com.teradata.jdbc.TeraDriver The root cause was that:
  java.sql.SQLException: No suitable driver available for TeraJDBC4,
  please check the driver setting in resources file, error:
  com.teradata.jdbc.TeraDriver""


ColdFusion Class Path: C:\td\tdgssconfig.jar, C:\td\terajdbc4.jar

I am using ColdFusion 11 and WAMP web server. How can I solve this issue?
",-1,-1,-1.0,"I tried to install TERADATA JDBC on ColdFusion 11 using this link.
Everything is fine, but when I try to create the datasource, I get following errors:


  ""Connection verification failed for data source: TeraJDBC4
  java.sql.SQLException: No suitable driver available for TeraJDBC4,
  please check the driver setting in resources file, error:
  com.teradata.jdbc.TeraDriver The root cause was that:
  java.sql.SQLException: No suitable driver available for TeraJDBC4,
  please check the driver setting in resources file, error:
  com.teradata.jdbc.TeraDriver""


ColdFusion Class Path: C:\td\tdgssconfig.jar, C:\td\terajdbc4.jar

I am using ColdFusion 11 and WAMP web server. How can I solve this issue?
",1
109,30912253,SQL Teradata evaluation order of case when then else,"I want to run this query on the official Teradata Express for VMware Player (TDE 15.00.01 SLES 10 for VMware (40GB) with Viewpoint):

SELECT 'MaxValue' column_name,
       COUNT(""MaxValue"") AS count_value,
       COUNT(DISTINCT(""MaxValue"")) AS count_dist_value,
       MIN(""MaxValue"") AS min_value,
       MAX(""MaxValue"") AS max_value,
       CASE WHEN max_value &gt; 99999999999999 THEN 99999999999999
            ELSE SUM(""MaxValue"") END AS sum_value
FROM (SELECT TOP 100 * FROM ""DBC"".""IdCol"") AS xy;


But I get this error:


  Executed as Single statement.  Failed [2616 : 22003] Numeric overflow
  occurred during computation.   Elapsed time = 00:00:00.115 
  
  STATEMENT 1: Select Statement failed.


So my question is that why the ELSE statement is evaluated when the CASE logic is True? And how can I run this query? I want the COUNT, MIN, MAX, AVG, SUM etc. informations from unknown tables where I don't know if a column contains 20 digit long numbers or not.
Thank you!
",-1,-1,-1.0,"I want to run this query on the official Teradata Express for VMware Player (TDE 15.00.01 SLES 10 for VMware (40GB) with Viewpoint):

SELECT 'MaxValue' column_name,
       COUNT(""MaxValue"") AS count_value,
       COUNT(DISTINCT(""MaxValue"")) AS count_dist_value,
       MIN(""MaxValue"") AS min_value,
       MAX(""MaxValue"") AS max_value,
       CASE WHEN max_value &gt; 99999999999999 THEN 99999999999999
            ELSE SUM(""MaxValue"") END AS sum_value
FROM (SELECT TOP 100 * FROM ""DBC"".""IdCol"") AS xy;


But I get this error:


  Executed as Single statement.  Failed [2616 : 22003] Numeric overflow
  occurred during computation.   Elapsed time = 00:00:00.115 
  
  STATEMENT 1: Select Statement failed.


So my question is that why the ELSE statement is evaluated when the CASE logic is True? And how can I run this query? I want the COUNT, MIN, MAX, AVG, SUM etc. informations from unknown tables where I don't know if a column contains 20 digit long numbers or not.
Thank you!
",3
110,31108151,"I am getting this error ""ERROR: Teradata row not delivered (trget): Numeric overflow occurred during computation""","While executing teradata query in proc sql (SAS), I am getting this error 


  ERROR: Teradata row not delivered (trget): Numeric overflow occurred
  during computation.


I am using SAS EG 9.3.
I tried increasing the spool space and I tried with mode=teradat

The query is large and has a lot of sum, max and count functions. 
Can anybody tell me how to troubleshoot this?
",-1,-1,-1.0,"While executing teradata query in proc sql (SAS), I am getting this error 


  ERROR: Teradata row not delivered (trget): Numeric overflow occurred
  during computation.


I am using SAS EG 9.3.
I tried increasing the spool space and I tried with mode=teradat

The query is large and has a lot of sum, max and count functions. 
Can anybody tell me how to troubleshoot this?
",3
111,31257917,How to convert a teradata table to utf8,"I am trying to upload a csv file into my teradata table tbl which contains Chinese and Japanese characters and teradata is not able to read those characters. 

I tried:

ALTER TABLE tbl CONVERT TO CHARACTER SET utf8;


But i get this error:

[SQLState 42000] Syntax error: Expecting the word SET or RESET. 
Error Code: 3706 
Query = ALTER TABLE tbl CONVERT TO CHARACTER SET 
utf8; 

",-1,-1,-1.0,"I am trying to upload a csv file into my teradata table tbl which contains Chinese and Japanese characters and teradata is not able to read those characters. 

I tried:

ALTER TABLE tbl CONVERT TO CHARACTER SET utf8;


But i get this error:

[SQLState 42000] Syntax error: Expecting the word SET or RESET. 
Error Code: 3706 
Query = ALTER TABLE tbl CONVERT TO CHARACTER SET 
utf8; 

",3
112,31329420,How to resolve pythonodbc issue with Teradata in Ubuntu,"I am getting non text error with Pythonodbc in Teradata Ubuntu

`saranya@saranya-XPS-8500:~/Desktop$ python test.py`


Traceback (most recent call last):
  File ""test.py"", line 3, in 
    conn=pyodbc.connect('DRIVER={Teradata};DBCNAME=...**;UID=*****;PWD=*****;', ANSI=True, autocommit=True)

pyodbc.Error: ('632', '[632] 523 630 (0) (SQLDriverConnect)')

The solution provided in post Pyodbc Issue with Teradata is not helping.

Also,
exporting ODBCINI, NLSPATH, LD_LIBRARY_HOME,ODBC_HOME values are not helping either.

Any help will be appreciated
",-1,-1,-1.0,"I am getting non text error with Pythonodbc in Teradata Ubuntu

`saranya@saranya-XPS-8500:~/Desktop$ python test.py`


Traceback (most recent call last):
  File ""test.py"", line 3, in 
    conn=pyodbc.connect('DRIVER={Teradata};DBCNAME=...**;UID=*****;PWD=*****;', ANSI=True, autocommit=True)

pyodbc.Error: ('632', '[632] 523 630 (0) (SQLDriverConnect)')

The solution provided in post Pyodbc Issue with Teradata is not helping.

Also,
exporting ODBCINI, NLSPATH, LD_LIBRARY_HOME,ODBC_HOME values are not helping either.

Any help will be appreciated
",1
113,31406830,Does pandas 0.13.0 support Teradata server? I am trying to write a dataframe object to teradata using a pyodbc connection,"Does pandas 0.13.0 support Teradata ODBC connection? I am trying to create a table in teradata from a pandas dataframe object using a pyodbc connection. 

The code I am using is below:

import pyodbc
import pandas

connection=pyodbc.connect('DRIVER={Teradata};dbcname=dbcname;uid=userid;pwd=password;databasename=db_name;quietmode=yes',autocommit=True)
data=pandas.read_csv(data_file)
data.to_sql('table_name',con=connection,flavor=None)


I am getting a NotImplementedError.

Thanks in advance!!
",0,-1,-1.0,"Does pandas 0.13.0 support Teradata ODBC connection? I am trying to create a table in teradata from a pandas dataframe object using a pyodbc connection. 

The code I am using is below:

import pyodbc
import pandas

connection=pyodbc.connect('DRIVER={Teradata};dbcname=dbcname;uid=userid;pwd=password;databasename=db_name;quietmode=yes',autocommit=True)
data=pandas.read_csv(data_file)
data.to_sql('table_name',con=connection,flavor=None)


I am getting a NotImplementedError.

Thanks in advance!!
",1
114,31478994,Error while creating date column in tables- teradata,"proc sql;
connect to teradata as tera(mode=teradata server=oneview user=""&amp;teraid."" password=""&amp;terapwd."");

execute(CREATE MULTISET TABLE UD497.PAN_AM_EMAIL
(
    ATHNUM        DECIMAL(10,0),
    BLK_1_CDE      CHAR(1),
    BLK_2_CDE      CHAR(1),
    OPEN_DT         DATE,
    LANGUAGE      CHAR(7),
    MKTCELL       CHAR(2),
    PROJECT_ID     CHAR(15),
    CAMPAIGN        CHAR(35);
) PRIMARY INDEX(ATHNUM);
) by tera;


Error Message :


  ERROR: Teradata execute: Syntax error, expected something like a 'CHECK' keyword between ',' and the 'LANGUAGE' keyword.

",-1,-1,-1.0,"proc sql;
connect to teradata as tera(mode=teradata server=oneview user=""&amp;teraid."" password=""&amp;terapwd."");

execute(CREATE MULTISET TABLE UD497.PAN_AM_EMAIL
(
    ATHNUM        DECIMAL(10,0),
    BLK_1_CDE      CHAR(1),
    BLK_2_CDE      CHAR(1),
    OPEN_DT         DATE,
    LANGUAGE      CHAR(7),
    MKTCELL       CHAR(2),
    PROJECT_ID     CHAR(15),
    CAMPAIGN        CHAR(35);
) PRIMARY INDEX(ATHNUM);
) by tera;


Error Message :


  ERROR: Teradata execute: Syntax error, expected something like a 'CHECK' keyword between ',' and the 'LANGUAGE' keyword.

",1
115,31614149,Teradata Date Format,"I have created the following table:

CREATE SET TABLE test_hold.test_lct ,NO FALLBACK ,
 NO BEFORE JOURNAL,
 NO AFTER JOURNAL,
 DATABLOCKSIZE = 65024 BYTES, CHECKSUM = DEFAULT,
 DEFAULT MERGEBLOCKRATIO
 (
  LCT_NBR SMALLINT NOT NULL,
  RGN_NBR SMALLINT NOT NULL,
  bus_name CHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC,
  mail_add CHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC,
  city CHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC,
  zip CHAR(8) CHARACTER SET LATIN NOT CASESPECIFIC,
  OPN_DT DATE FORMAT 'YYYY-MM-DD',
  CSE_DT DATE FORMAT 'YYYY-MM-DD')
  UNIQUE PRIMARY INDEX I0050PI ( LCT_NBR );


The issue I am having is with the two date columns.  After loading records into Teradata from a flat file (which holds dates in YYYY-MM-DD format) using Informatica, I notice that the dates are actually of the format MM/DD/YYYY.

My mapping in Informatica uses a flat file as a source and my Teradata table as a target and simply converts the dates into date/time objects using an expression transformation with the following logic:

TO_DATE(i_Date, 'YYYY-MM-DD')


The expression doesn't seem to do anything, because if I do not convert the string into a date/time before loading into Teradata I get the same result.

Why would Teradata allow a record that contains a format not specified in the create statement?  I would expect the insert to fail and for there to be no records in the table.

Thank you,
",-1,-1,-1.0,"I have created the following table:

CREATE SET TABLE test_hold.test_lct ,NO FALLBACK ,
 NO BEFORE JOURNAL,
 NO AFTER JOURNAL,
 DATABLOCKSIZE = 65024 BYTES, CHECKSUM = DEFAULT,
 DEFAULT MERGEBLOCKRATIO
 (
  LCT_NBR SMALLINT NOT NULL,
  RGN_NBR SMALLINT NOT NULL,
  bus_name CHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC,
  mail_add CHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC,
  city CHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC,
  zip CHAR(8) CHARACTER SET LATIN NOT CASESPECIFIC,
  OPN_DT DATE FORMAT 'YYYY-MM-DD',
  CSE_DT DATE FORMAT 'YYYY-MM-DD')
  UNIQUE PRIMARY INDEX I0050PI ( LCT_NBR );


The issue I am having is with the two date columns.  After loading records into Teradata from a flat file (which holds dates in YYYY-MM-DD format) using Informatica, I notice that the dates are actually of the format MM/DD/YYYY.

My mapping in Informatica uses a flat file as a source and my Teradata table as a target and simply converts the dates into date/time objects using an expression transformation with the following logic:

TO_DATE(i_Date, 'YYYY-MM-DD')


The expression doesn't seem to do anything, because if I do not convert the string into a date/time before loading into Teradata I get the same result.

Why would Teradata allow a record that contains a format not specified in the create statement?  I would expect the insert to fail and for there to be no records in the table.

Thank you,
",3
116,31679892,Teradata syntax: unicode delimited identifier,"I am getting error while trying to run the query in Teradata 13.0 How to write this query? I converted this oracle query to Teradata query:

select sb.name,
       sb.address1, 
       sb.address2, 
       sb.city, 
       sb.stateprovince,
       sb.postalcode, 
       sb.country, 
       sb.cid, 
       sb.item, 
       sb.mcnum, 
       sb.dmh, 
       sb.hy, 
       sb.firstname, 
       sb.lastname, 
       sb.email, 
       sb.monikerexec, 
       sb.res,
 (select cid from (select * from abc.submissions where res is not null order by res asc) where name = sb.name and item &lt;&gt; sb.item and res &lt; sb.res and rownum =1) as mins,
  (select min(price) from ap.hist where name = sb.name) as minauc,
       sb.cat, 
       sb.uni
 from abc.submission sb
order by sb.item


Error:
",-1,-1,-1.0,"I am getting error while trying to run the query in Teradata 13.0 How to write this query? I converted this oracle query to Teradata query:

select sb.name,
       sb.address1, 
       sb.address2, 
       sb.city, 
       sb.stateprovince,
       sb.postalcode, 
       sb.country, 
       sb.cid, 
       sb.item, 
       sb.mcnum, 
       sb.dmh, 
       sb.hy, 
       sb.firstname, 
       sb.lastname, 
       sb.email, 
       sb.monikerexec, 
       sb.res,
 (select cid from (select * from abc.submissions where res is not null order by res asc) where name = sb.name and item &lt;&gt; sb.item and res &lt; sb.res and rownum =1) as mins,
  (select min(price) from ap.hist where name = sb.name) as minauc,
       sb.cat, 
       sb.uni
 from abc.submission sb
order by sb.item


Error:
",3
117,31699521,An error occurred while preparing the query for linked server - SQL Server + Teradata,"In SQL Server I have defined the following Job agent and the following step:

INSERT INTO dse.dwh_log
SELECT *
FROM
OPENQUERY(DWH_Production, 
'
select
    cast(cast(LogonDate as format ''yyyy-mm-dd'') as char(10)) || '' '' ||
    cast(cast(LogonTime as format ''99:99:99.999'') as char(12)) as LogonTime,
    UserName,
    substr(StatementText,1,8000) as RequestText
from
    p_sys_ms_logging.accesslog_hst
where      statementtype = ''select''
and databasename  in (''CDR'', ''cdr30_targetdb'')
and statementtext is not null
and logondate &gt; current_date - 2')


The error message I am getting is:


  Executed as user: XYZ. An error occurred while preparing the query
  select cast(cast(LogonDate as format 'yyyy-mm-dd') as char(10)) || ' ' || cast(cast(LogonTime as format '99:99:99.999') as char(12)) as LogonTime, UserName, substr(StatementText,1,8000) as RequestText from p_sys_ms_logging.accesslog_hst where statementtype = 'select' and databasename in ('CDR', 'cdr30_targetdb')  and statementtext is not null  and logondate > current_date - 2"" 
  
  for execution against OLE DB provider ""MSDASQL"" for linked server ""DWH_Production"".
  [SQLSTATE 42000] (Error 7321).
  NOTE: The step was retried the requested number of times (2) without succeeding. The step failed.


The duration is about an hour, but the query should run in around 5 minutes.
I am retrieving the data from the Teradata database, and transferring them to SQL Server.

Does anyone know why am I getting this message (on Google I did not find anything that could help me), and what I should do to fix it?
",-1,-1,-1.0,"In SQL Server I have defined the following Job agent and the following step:

INSERT INTO dse.dwh_log
SELECT *
FROM
OPENQUERY(DWH_Production, 
'
select
    cast(cast(LogonDate as format ''yyyy-mm-dd'') as char(10)) || '' '' ||
    cast(cast(LogonTime as format ''99:99:99.999'') as char(12)) as LogonTime,
    UserName,
    substr(StatementText,1,8000) as RequestText
from
    p_sys_ms_logging.accesslog_hst
where      statementtype = ''select''
and databasename  in (''CDR'', ''cdr30_targetdb'')
and statementtext is not null
and logondate &gt; current_date - 2')


The error message I am getting is:


  Executed as user: XYZ. An error occurred while preparing the query
  select cast(cast(LogonDate as format 'yyyy-mm-dd') as char(10)) || ' ' || cast(cast(LogonTime as format '99:99:99.999') as char(12)) as LogonTime, UserName, substr(StatementText,1,8000) as RequestText from p_sys_ms_logging.accesslog_hst where statementtype = 'select' and databasename in ('CDR', 'cdr30_targetdb')  and statementtext is not null  and logondate > current_date - 2"" 
  
  for execution against OLE DB provider ""MSDASQL"" for linked server ""DWH_Production"".
  [SQLSTATE 42000] (Error 7321).
  NOTE: The step was retried the requested number of times (2) without succeeding. The step failed.


The duration is about an hour, but the query should run in around 5 minutes.
I am retrieving the data from the Teradata database, and transferring them to SQL Server.

Does anyone know why am I getting this message (on Google I did not find anything that could help me), and what I should do to fix it?
",3
118,31732573,unixODBC connection with Teradata,"Has anyone ever got this to work on RHEL?
I have successfully installed Teradata 14.10 on my box. BTEQ works fine and I've tested the Teradata odbc connection using tdxodbc, which also successfully works. My problem is I want to connect via Python using it's pyodbc. I keep getting some cryptic error message:

[******@sdc01cunx09 ~]$ python helloworld.py
Hello World!
Traceback (most recent call last):
  File ""helloworld.py"", line 14, in &lt;module&gt;
    conn = pyodbc.connect(""DSN=MyTD; UID=*****; PWD=*****"")
pyodbc.Error: ('200', '[200] [unixODBC][eaaa[DCTrdt rvr o nuhifraint o n (0) (SQLDriverConnectW)')


Please help!! This is driving me N-U-T-S!!
",1,-1,-1.0,"Has anyone ever got this to work on RHEL?
I have successfully installed Teradata 14.10 on my box. BTEQ works fine and I've tested the Teradata odbc connection using tdxodbc, which also successfully works. My problem is I want to connect via Python using it's pyodbc. I keep getting some cryptic error message:

[******@sdc01cunx09 ~]$ python helloworld.py
Hello World!
Traceback (most recent call last):
  File ""helloworld.py"", line 14, in &lt;module&gt;
    conn = pyodbc.connect(""DSN=MyTD; UID=*****; PWD=*****"")
pyodbc.Error: ('200', '[200] [unixODBC][eaaa[DCTrdt rvr o nuhifraint o n (0) (SQLDriverConnectW)')


Please help!! This is driving me N-U-T-S!!
",1
119,32085536,sqoop import teradata to hive - CommandNeedRetryException,"I'm trying to import data from Teradata to hive using below Sqoop command:

sqoop import \
--connect ${connect} \
--username ${usr} \
--password ${pwd} \
--connection-manager org.apache.sqoop.teradata.TeradataConnManager \
--table ${tblname} \
--hive-import \
--hive-database ${db} \
--hive-table ${dist_tbl};


See error below:

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/CommandNeedRetryException
    at com.teradata.connector.common.tool.ConfigurationMappingUtils.importConfigurationMapping(ConfigurationMappingUtils.java:300)
    at org.apache.sqoop.teradata.TeradataSqoopImportHelper.runJob(TeradataSqoopImportHelper.java:360)
    at org.apache.sqoop.teradata.TeradataConnManager.importTable(TeradataConnManager.java:504)
    at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:497)
    at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601)
    at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
    at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
    at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
    at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
    at org.apache.sqoop.Sqoop.main(Sqoop.java:236)

Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.CommandNeedRetryException
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    ... 11 more

",-1,-1,-1.0,"I'm trying to import data from Teradata to hive using below Sqoop command:

sqoop import \
--connect ${connect} \
--username ${usr} \
--password ${pwd} \
--connection-manager org.apache.sqoop.teradata.TeradataConnManager \
--table ${tblname} \
--hive-import \
--hive-database ${db} \
--hive-table ${dist_tbl};


See error below:

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hive/ql/CommandNeedRetryException
    at com.teradata.connector.common.tool.ConfigurationMappingUtils.importConfigurationMapping(ConfigurationMappingUtils.java:300)
    at org.apache.sqoop.teradata.TeradataSqoopImportHelper.runJob(TeradataSqoopImportHelper.java:360)
    at org.apache.sqoop.teradata.TeradataConnManager.importTable(TeradataConnManager.java:504)
    at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:497)
    at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601)
    at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
    at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
    at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
    at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
    at org.apache.sqoop.Sqoop.main(Sqoop.java:236)

Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.CommandNeedRetryException
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    ... 11 more

",0
120,32349383,Loading Excel values to Teradata,"The Excel file I am using has cells which contains multiple lines typed in by using Alt+Enter. When this Excel is load into teradata, these multiple lines in single cell should reflect in single cell in the teradata table also. But instead, it is getting loaded as several rows. How to rectify this?
",-1,-1,-1.0,"The Excel file I am using has cells which contains multiple lines typed in by using Alt+Enter. When this Excel is load into teradata, these multiple lines in single cell should reflect in single cell in the teradata table also. But instead, it is getting loaded as several rows. How to rectify this?
",3
121,32463959,ODBC Teradata SQL Tool for OS X,"TLDR; best Mac SQL Query solution for ODBC Teradata version 14 with some instructions on how to set it up. 

Using ODBC version 14.x and Teradata DB version 14.x

Currently, running on a Windows 7 machine using Teradata SQL Assistant, though I am switching to a Mac and so far am unable to get OS X ODBC Management tool to function for my testing, nor could I get the download for the JE of SQL Assistant, as the site 404s. I tried Teradata Studio Express but it's not connecting to ODBC(it's using JDBC) and from what I am reading on the Teradata forums, there is no ODBC driver for Studio Express... with the SQL Assistant JE not available for download, I'm a lost on how I could be querying our Teradata Databases for testing on my Mac. 

Others have suggested anything from Excel to Eclipse should work for SQL querying as long as I have the ODBC driver installed (i seem to), I made sure it was even version 14 as well as trying 15. 

No dice... I think I am just too newb at this to figure it out without some guidance. 

Thanks and if this post is not articulate enough kindly let me know what details I am leaving out and I can add them. 

Thank you all so much! 
",1,1,-1.0,"TLDR; best Mac SQL Query solution for ODBC Teradata version 14 with some instructions on how to set it up. 

Using ODBC version 14.x and Teradata DB version 14.x

Currently, running on a Windows 7 machine using Teradata SQL Assistant, though I am switching to a Mac and so far am unable to get OS X ODBC Management tool to function for my testing, nor could I get the download for the JE of SQL Assistant, as the site 404s. I tried Teradata Studio Express but it's not connecting to ODBC(it's using JDBC) and from what I am reading on the Teradata forums, there is no ODBC driver for Studio Express... with the SQL Assistant JE not available for download, I'm a lost on how I could be querying our Teradata Databases for testing on my Mac. 

Others have suggested anything from Excel to Eclipse should work for SQL querying as long as I have the ODBC driver installed (i seem to), I made sure it was even version 14 as well as trying 15. 

No dice... I think I am just too newb at this to figure it out without some guidance. 

Thanks and if this post is not articulate enough kindly let me know what details I am leaving out and I can add them. 

Thank you all so much! 
",1
122,32988219,jaydebeapi teradata connection,"I am trying to connect to teradata database using jaydebeapi package.

conn = jaydebeapi.connect('com.teradata.jdbc.TeraDriver',
                          ['jdbc:teradata://ip/TMODE=ANSI,CHARSET=utf8', 'username', 'password'],
                          ['/teradata/tdgssconfig.jar',
                           '/terajdbc4.jar'])


Error while execution:

    TERAJDBC4 ERROR [main] com.teradata.jdbc.jdk6.JDK6_SQL_Connection@3d1a70a7 Connection to &lt;ip_address&gt; Wed Oct 07 13:47:29 IST 2015 socket orig=&lt;ip_address&gt; cid=22f79598 sess=0 java.net.SocketTimeoutException: connect timed out  at java.net.PlainSocketImpl.socketConnect(Native Method)  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)  at java.net.Socket.connect(Socket.java:579)  at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$ConnectThread.run(TDNetworkIOIF.java:1216) 
Traceback (most recent call last):
  File ""test_teradata.py"", line 32, in &lt;module&gt;
    teradataconn()
  File ""test_teradata.py"", line 26, in teradataconn
    '/home/abhishek/git/dblore/code/lib/teradata/terajdbc4.jar'])
  File ""/usr/local/lib/python2.7/dist-packages/jaydebeapi/__init__.py"", line 359, in connect
    jconn = _jdbc_connect(jclassname, jars, libs, *driver_args)
  File ""/usr/local/lib/python2.7/dist-packages/jaydebeapi/__init__.py"", line 183, in _jdbc_connect_jpype
    return jpype.java.sql.DriverManager.getConnection(*driver_args)
jpype._jexception.SQLExceptionPyRaisable: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.00.00.20] [Error 1277] [SQLState 08S01] Login timeout for Connection to &lt;ip_address&gt; Wed Oct 07 13:47:29 IST 2015 socket orig=&lt;ip_address&gt; cid=22f79598 sess=0 java.net.SocketTimeoutException: connect timed out  at java.net.PlainSocketImpl.socketConnect(Native Method)  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)  at java.net.Socket.connect(Socket.java:579)  at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$ConnectThread.run(TDNetworkIOIF.java:1216) 

",-1,-1,-1.0,"I am trying to connect to teradata database using jaydebeapi package.

conn = jaydebeapi.connect('com.teradata.jdbc.TeraDriver',
                          ['jdbc:teradata://ip/TMODE=ANSI,CHARSET=utf8', 'username', 'password'],
                          ['/teradata/tdgssconfig.jar',
                           '/terajdbc4.jar'])


Error while execution:

    TERAJDBC4 ERROR [main] com.teradata.jdbc.jdk6.JDK6_SQL_Connection@3d1a70a7 Connection to &lt;ip_address&gt; Wed Oct 07 13:47:29 IST 2015 socket orig=&lt;ip_address&gt; cid=22f79598 sess=0 java.net.SocketTimeoutException: connect timed out  at java.net.PlainSocketImpl.socketConnect(Native Method)  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)  at java.net.Socket.connect(Socket.java:579)  at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$ConnectThread.run(TDNetworkIOIF.java:1216) 
Traceback (most recent call last):
  File ""test_teradata.py"", line 32, in &lt;module&gt;
    teradataconn()
  File ""test_teradata.py"", line 26, in teradataconn
    '/home/abhishek/git/dblore/code/lib/teradata/terajdbc4.jar'])
  File ""/usr/local/lib/python2.7/dist-packages/jaydebeapi/__init__.py"", line 359, in connect
    jconn = _jdbc_connect(jclassname, jars, libs, *driver_args)
  File ""/usr/local/lib/python2.7/dist-packages/jaydebeapi/__init__.py"", line 183, in _jdbc_connect_jpype
    return jpype.java.sql.DriverManager.getConnection(*driver_args)
jpype._jexception.SQLExceptionPyRaisable: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.00.00.20] [Error 1277] [SQLState 08S01] Login timeout for Connection to &lt;ip_address&gt; Wed Oct 07 13:47:29 IST 2015 socket orig=&lt;ip_address&gt; cid=22f79598 sess=0 java.net.SocketTimeoutException: connect timed out  at java.net.PlainSocketImpl.socketConnect(Native Method)  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)  at java.net.Socket.connect(Socket.java:579)  at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$ConnectThread.run(TDNetworkIOIF.java:1216) 

",0
123,32999634,Sqoop error when connecting through teradata,"I am unable to execute the sqoop command with Teradata.

I am getting this error:


  Error 8017] [SQLState 28000] The UserId, Password or Account is invalid. 


Sqoop Command:

sqoop import --connect jdbc:teradata://TDPRODC/LOGMECH=LDAP 
             --driver com.teradata.jdbc.TeraDriver 
             --username svddssas 
             --password ' ' 
             --table ADW.GST_STST_V 
             --hive-import 
             --hive-table wins.gst_stst_v1 -m 1

",-1,-1,-1.0,"I am unable to execute the sqoop command with Teradata.

I am getting this error:


  Error 8017] [SQLState 28000] The UserId, Password or Account is invalid. 


Sqoop Command:

sqoop import --connect jdbc:teradata://TDPRODC/LOGMECH=LDAP 
             --driver com.teradata.jdbc.TeraDriver 
             --username svddssas 
             --password ' ' 
             --table ADW.GST_STST_V 
             --hive-import 
             --hive-table wins.gst_stst_v1 -m 1

",0
124,33534474,Load Historical data to teradata temporal table,"I have a task to load existing SQL Server table to Teradata temporal table. Existing table is a type 2 table and has many versions of record. I need to load them into teradata temporal table. I am planning to load version 1 1st and then update all other versions one by one. 
Difficulties i am having is that in existing table every record has start time and end time. I need to update that time in teradata temporal table as validity.

1st I am trying to insert and while insert i am not able to insert end time as less than current time. It report error as ""Check constraint violation"". Below is sample piece of code for creating table and inserting. 

I am yet to test updates as not able to do 1st step.

CREATE multiset TABLE EDW_T.edw_Contracts_History_Test
(
    ID INTEGER,
    Validity PERIOD(TIMESTAMP(3)) NOT NULL AS VALIDTIME
);

insert into EDW_T.edw_Contracts_History_Test(id,Validity) values(
1,period(cast('1996-01-20 05.00.00.000' as TIMESTAMP(3)), cast('2016-06-23 21.52.20.000' as TIMESTAMP(3))))
--this pass as 2016 is greater than current date
insert into EDW_T.edw_Contracts_History_Test(id,Validity) values(
1,period(cast('1996-01-20 05.00.00.000' as TIMESTAMP(3)), cast('2015-06-23 21.52.20.000' as TIMESTAMP(3))))
--This fails as i m trying to give end time less than current date.


Is there anyway to give end time as less than current date. any way to disable this constraint for time and then enable. 

Please help. Thanks!
",-1,-1,-1.0,"I have a task to load existing SQL Server table to Teradata temporal table. Existing table is a type 2 table and has many versions of record. I need to load them into teradata temporal table. I am planning to load version 1 1st and then update all other versions one by one. 
Difficulties i am having is that in existing table every record has start time and end time. I need to update that time in teradata temporal table as validity.

1st I am trying to insert and while insert i am not able to insert end time as less than current time. It report error as ""Check constraint violation"". Below is sample piece of code for creating table and inserting. 

I am yet to test updates as not able to do 1st step.

CREATE multiset TABLE EDW_T.edw_Contracts_History_Test
(
    ID INTEGER,
    Validity PERIOD(TIMESTAMP(3)) NOT NULL AS VALIDTIME
);

insert into EDW_T.edw_Contracts_History_Test(id,Validity) values(
1,period(cast('1996-01-20 05.00.00.000' as TIMESTAMP(3)), cast('2016-06-23 21.52.20.000' as TIMESTAMP(3))))
--this pass as 2016 is greater than current date
insert into EDW_T.edw_Contracts_History_Test(id,Validity) values(
1,period(cast('1996-01-20 05.00.00.000' as TIMESTAMP(3)), cast('2015-06-23 21.52.20.000' as TIMESTAMP(3))))
--This fails as i m trying to give end time less than current date.


Is there anyway to give end time as less than current date. any way to disable this constraint for time and then enable. 

Please help. Thanks!
",3
125,33551234,Teradata connection working for one site but not for the other,"I have configured a Teradata driver and connecting to TD from one website which works fine. But when we try to connect to TD using the same driver through a different website hosted on the same server I'm getting following error:


  ERROR [IM003] Specified driver could not be loaded due to system error 126: The specified module could not be found. (Teradata, D:\Apps\Program Files\Teradata\Client\14.10\ODBC Driver for Teradata nt-x8664\Lib\tdata32.dll). 
  Description: An unhandled exception occurred during the execution of the current web request. Please review the stack trace for more information about the error and where it originated in the code. 
  
  Exception Details: System.ServiceModel.FaultException`1[[System.ServiceModel.ExceptionDetail, System.ServiceModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089]]: ERROR [IM003] Specified driver could not be loaded due to system error 126: The specified module could not be found. (Teradata, D:\Apps\Program Files\Teradata\Client\14.10\ODBC Driver for Teradata nt-x8664\Lib\tdata32.dll).

",-1,-1,-1.0,"I have configured a Teradata driver and connecting to TD from one website which works fine. But when we try to connect to TD using the same driver through a different website hosted on the same server I'm getting following error:


  ERROR [IM003] Specified driver could not be loaded due to system error 126: The specified module could not be found. (Teradata, D:\Apps\Program Files\Teradata\Client\14.10\ODBC Driver for Teradata nt-x8664\Lib\tdata32.dll). 
  Description: An unhandled exception occurred during the execution of the current web request. Please review the stack trace for more information about the error and where it originated in the code. 
  
  Exception Details: System.ServiceModel.FaultException`1[[System.ServiceModel.ExceptionDetail, System.ServiceModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089]]: ERROR [IM003] Specified driver could not be loaded due to system error 126: The specified module could not be found. (Teradata, D:\Apps\Program Files\Teradata\Client\14.10\ODBC Driver for Teradata nt-x8664\Lib\tdata32.dll).

",1
126,33653408,Why doesn't Teradata identity column increment by 1?,"I have the following table 

CREATE SET TABLE myTab,FALLBACK,NO BEFORE JOURNAL,NO AFTER JOURNAL,CHECKSUM = DEFAULT,DEFAULT MERGEBLOCKRATIO
     (
      my_id BIGINT GENERATED ALWAYS AS IDENTITY
           (START WITH 1 
            INCREMENT BY 1 
            MINVALUE 1 
            MAXVALUE 922337203685477580 
            NO CYCLE),
      created_by VARCHAR(200) )
UNIQUE PRIMARY INDEX ( my_id );  


When I started insetring into this table (INSERT INTO myTab (created_by) VALUES ('test'), here are the values for my_id that got auto generated 

my_id
1
100,001
100,002
200,001
300,001
400,001
500,001
500,002
600,001


I was looking to get 1,2,3 . . . incremented by 1. I read the teradata explanation but still don't understand. Why does it increment by a random number ? This way I will reach the MAXVALUE 922337203685477580 pretty soon. What will happen then ? 
",-1,-1,-1.0,"I have the following table 

CREATE SET TABLE myTab,FALLBACK,NO BEFORE JOURNAL,NO AFTER JOURNAL,CHECKSUM = DEFAULT,DEFAULT MERGEBLOCKRATIO
     (
      my_id BIGINT GENERATED ALWAYS AS IDENTITY
           (START WITH 1 
            INCREMENT BY 1 
            MINVALUE 1 
            MAXVALUE 922337203685477580 
            NO CYCLE),
      created_by VARCHAR(200) )
UNIQUE PRIMARY INDEX ( my_id );  


When I started insetring into this table (INSERT INTO myTab (created_by) VALUES ('test'), here are the values for my_id that got auto generated 

my_id
1
100,001
100,002
200,001
300,001
400,001
500,001
500,002
600,001


I was looking to get 1,2,3 . . . incremented by 1. I read the teradata explanation but still don't understand. Why does it increment by a random number ? This way I will reach the MAXVALUE 922337203685477580 pretty soon. What will happen then ? 
",3
127,33775769,Teradata Orderer Analytical Function not allowed in group by clause,"I am getting the error


  Teradata Orderer Analytical Function not allowed in group by clause


when I run this query:

SELECT
    CC.CASE_ID as CASE_ID,
    FIRST_VALUE(CC.CASE_OWN_NM) OVER(PARTITION BY CC.CASE_ID) as FST_AGNT_CASE_OWN_NM,
    FIRST_VALUE(CC.LSTMOD_BY_AGNT_PRFL_NM) OVER(PARTITION BY CC.CASE_ID) as FST_AGNT_PRFL_NM,
    LAST_VALUE(CC.CASE_OWN_NM) OVER(PARTITION BY CC.CASE_ID) as LST_AGNT_CASE_OWN_NM,
    LAST_VALUE(CC.LSTMOD_BY_AGNT_PRFL_NM) OVER(PARTITION BY CC.CASE_ID) as LST_AGNT_PRFL_NM,
    case when CC.CASE_OWN_NM is not null then MIN(CC.REC_DTTM_PST) end as FST_AGNT_EDIT_DTTM,
    case when CC.CASE_OWN_NM is not null then MAX(CC.REC_DTTM_PST) end as LST_AGNT_EDIT_DTTM,
    case when CC.CASE_STS_CD='Open' then MIN(CC.REC_DTTM_PST) end as CASE_OPEN_DTTM,
    case when CC.CASE_STS_CD in ('Closed', 'Auto Closed') then MIN(CC.REC_DTTM_PST) end as CASE_CLSE_OR_AUTO_CLSE_DTTM,
    count(distinct CC.CASE_OWN_NM) as CASE_OWN_CHGS_IN_NUM,
    LAST_VALUE(CC.ESCL_RSN_TXT) OVER(PARTITION BY CC.CASE_ID) as ESCL_RSN_TXT,
    LAST_VALUE(CC.ESCL_DTLS_TXT) OVER(PARTITION BY CC.CASE_ID) as ESCL_DTLS_TXT
FROM
    EDW_KATAMARI_T.CNTCT_CASE CC
INNER JOIN
    EDW_KATAMARI_T.CNTCT_CASE_EXTN CCE ON CC.CNTCT_CASE_APND_KEY = CCE.CNTCT_CASE_APND_KEY
INNER JOIN
    EDW_STAGE_COMN_SRC.STG_CNTCT_CASE_DELTA DELTA on CC.CASE_ID = DELTA.CASE_ID
group by
    1,2,3,4,5
sample 10




I tried the answer by anwaar_hell and modified it a bit with a few more columns, but I'm still getting an error:


  ordered analytical function not allowed in subqueries 


Query:

SELECT
    distinct CC.CASE_ID as CASE_ID,
    CCC.FST_AGNT_CASE_OWN_NM,
    CCC.FST_AGNT_PRFL_NM,
    CCC.LST_AGNT_CASE_OWN_NM,
    CCC.LST_AGNT_PRFL_NM,
    CCC.FST_CHNL_NM,
    CCC.LST_CHNL_NM,
    CCC.LST_VEND_NM,
    case when MAX(CC.CASE_TFR_TO_L2)='1' then 'Yes' else 'No' end as ESCL_FL,
    case when CC.LSTMOD_BY_AGNT_ROLE_NM='L1' then count(distinct CC.LSTMOD_BY_AGNT_ROLE_NM) end as XFER_BTWN_L1_NUM,
    MAX(CC.CASE_SAVED_ORD_NUM) as SAVES_OR_MODS_ON_CASE_NUM,
    MIN(CC.REC_DTTM_PST) as FST_QUE_TIME_IN_SECS2,
    case when CC.RMTE_ASST_USED_IN&gt;=1 then 'Yes' else 'No' end as RMTE_SESS_FL,
    case when CC.OUTB_CALL_TYPE_CD='Outbound' then 1 else 0 end as IS_OUTB_CALL_TYPE_IN,
    case when CC.L2CALL_BK_SCEHDULED_PST_DT is NOT NULL then 'Yes' else 'No' end as L2_OUTB_CALL_SCHD_FL,
    MAX(CC.L2CALL_BK_SCEHDULED_PST_DT) as L2_CLBCK_SCEHDULED_DTTM,
    CC.PU_DTTM as LMI_PU_DTTM,
    CC.CLS_DTTM as LMI_CLS_DTTM,
  ( select
        FIRST_VALUE(CC.CASE_OWN_NM) OVER(PARTITION BY CC.CASE_ID) as FST_AGNT_CASE_OWN_NM,
        FIRST_VALUE(CC.LSTMOD_BY_AGNT_PRFL_NM) OVER(PARTITION BY CC.CASE_ID) as FST_AGNT_PRFL_NM,
        LAST_VALUE(CC.CASE_OWN_NM) OVER(PARTITION BY CC.CASE_ID) as LST_AGNT_CASE_OWN_NM,
        LAST_VALUE(CC.LSTMOD_BY_AGNT_PRFL_NM) OVER(PARTITION BY CC.CASE_ID) as LST_AGNT_PRFL_NM,
        FIRST_VALUE(CC.CHNL_NM) OVER(PARTITION BY CC.CASE_ID ) as FST_CHNL_NM,
        LAST_VALUE(CC.CHNL_NM) OVER(PARTITION BY CC.CASE_ID) as LST_CHNL_NM,
        LAST_VALUE(CCE.VEND_NM) OVER(PARTITION BY CC.CASE_ID) as LST_VEND_NM
    FROM
        EDW_KATAMARI_T.CNTCT_CASE CC
    INNER JOIN
        EDW_KATAMARI_T.CNTCT_CASE_EXTN CCE ON CC.CNTCT_CASE_APND_KEY = CCE.CNTCT_CASE_APND_KEY
    INNER JOIN
        EDW_STAGE_COMN_SRC.STG_CNTCT_CASE_DELTA DELTA on CC.CASE_ID = DELTA.CASE_ID
    where
        CC.CASE_ID='23268760'
  )
group by
    1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17

",-1,-1,-1.0,"I am getting the error


  Teradata Orderer Analytical Function not allowed in group by clause


when I run this query:

SELECT
    CC.CASE_ID as CASE_ID,
    FIRST_VALUE(CC.CASE_OWN_NM) OVER(PARTITION BY CC.CASE_ID) as FST_AGNT_CASE_OWN_NM,
    FIRST_VALUE(CC.LSTMOD_BY_AGNT_PRFL_NM) OVER(PARTITION BY CC.CASE_ID) as FST_AGNT_PRFL_NM,
    LAST_VALUE(CC.CASE_OWN_NM) OVER(PARTITION BY CC.CASE_ID) as LST_AGNT_CASE_OWN_NM,
    LAST_VALUE(CC.LSTMOD_BY_AGNT_PRFL_NM) OVER(PARTITION BY CC.CASE_ID) as LST_AGNT_PRFL_NM,
    case when CC.CASE_OWN_NM is not null then MIN(CC.REC_DTTM_PST) end as FST_AGNT_EDIT_DTTM,
    case when CC.CASE_OWN_NM is not null then MAX(CC.REC_DTTM_PST) end as LST_AGNT_EDIT_DTTM,
    case when CC.CASE_STS_CD='Open' then MIN(CC.REC_DTTM_PST) end as CASE_OPEN_DTTM,
    case when CC.CASE_STS_CD in ('Closed', 'Auto Closed') then MIN(CC.REC_DTTM_PST) end as CASE_CLSE_OR_AUTO_CLSE_DTTM,
    count(distinct CC.CASE_OWN_NM) as CASE_OWN_CHGS_IN_NUM,
    LAST_VALUE(CC.ESCL_RSN_TXT) OVER(PARTITION BY CC.CASE_ID) as ESCL_RSN_TXT,
    LAST_VALUE(CC.ESCL_DTLS_TXT) OVER(PARTITION BY CC.CASE_ID) as ESCL_DTLS_TXT
FROM
    EDW_KATAMARI_T.CNTCT_CASE CC
INNER JOIN
    EDW_KATAMARI_T.CNTCT_CASE_EXTN CCE ON CC.CNTCT_CASE_APND_KEY = CCE.CNTCT_CASE_APND_KEY
INNER JOIN
    EDW_STAGE_COMN_SRC.STG_CNTCT_CASE_DELTA DELTA on CC.CASE_ID = DELTA.CASE_ID
group by
    1,2,3,4,5
sample 10




I tried the answer by anwaar_hell and modified it a bit with a few more columns, but I'm still getting an error:


  ordered analytical function not allowed in subqueries 


Query:

SELECT
    distinct CC.CASE_ID as CASE_ID,
    CCC.FST_AGNT_CASE_OWN_NM,
    CCC.FST_AGNT_PRFL_NM,
    CCC.LST_AGNT_CASE_OWN_NM,
    CCC.LST_AGNT_PRFL_NM,
    CCC.FST_CHNL_NM,
    CCC.LST_CHNL_NM,
    CCC.LST_VEND_NM,
    case when MAX(CC.CASE_TFR_TO_L2)='1' then 'Yes' else 'No' end as ESCL_FL,
    case when CC.LSTMOD_BY_AGNT_ROLE_NM='L1' then count(distinct CC.LSTMOD_BY_AGNT_ROLE_NM) end as XFER_BTWN_L1_NUM,
    MAX(CC.CASE_SAVED_ORD_NUM) as SAVES_OR_MODS_ON_CASE_NUM,
    MIN(CC.REC_DTTM_PST) as FST_QUE_TIME_IN_SECS2,
    case when CC.RMTE_ASST_USED_IN&gt;=1 then 'Yes' else 'No' end as RMTE_SESS_FL,
    case when CC.OUTB_CALL_TYPE_CD='Outbound' then 1 else 0 end as IS_OUTB_CALL_TYPE_IN,
    case when CC.L2CALL_BK_SCEHDULED_PST_DT is NOT NULL then 'Yes' else 'No' end as L2_OUTB_CALL_SCHD_FL,
    MAX(CC.L2CALL_BK_SCEHDULED_PST_DT) as L2_CLBCK_SCEHDULED_DTTM,
    CC.PU_DTTM as LMI_PU_DTTM,
    CC.CLS_DTTM as LMI_CLS_DTTM,
  ( select
        FIRST_VALUE(CC.CASE_OWN_NM) OVER(PARTITION BY CC.CASE_ID) as FST_AGNT_CASE_OWN_NM,
        FIRST_VALUE(CC.LSTMOD_BY_AGNT_PRFL_NM) OVER(PARTITION BY CC.CASE_ID) as FST_AGNT_PRFL_NM,
        LAST_VALUE(CC.CASE_OWN_NM) OVER(PARTITION BY CC.CASE_ID) as LST_AGNT_CASE_OWN_NM,
        LAST_VALUE(CC.LSTMOD_BY_AGNT_PRFL_NM) OVER(PARTITION BY CC.CASE_ID) as LST_AGNT_PRFL_NM,
        FIRST_VALUE(CC.CHNL_NM) OVER(PARTITION BY CC.CASE_ID ) as FST_CHNL_NM,
        LAST_VALUE(CC.CHNL_NM) OVER(PARTITION BY CC.CASE_ID) as LST_CHNL_NM,
        LAST_VALUE(CCE.VEND_NM) OVER(PARTITION BY CC.CASE_ID) as LST_VEND_NM
    FROM
        EDW_KATAMARI_T.CNTCT_CASE CC
    INNER JOIN
        EDW_KATAMARI_T.CNTCT_CASE_EXTN CCE ON CC.CNTCT_CASE_APND_KEY = CCE.CNTCT_CASE_APND_KEY
    INNER JOIN
        EDW_STAGE_COMN_SRC.STG_CNTCT_CASE_DELTA DELTA on CC.CASE_ID = DELTA.CASE_ID
    where
        CC.CASE_ID='23268760'
  )
group by
    1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17

",3
128,33382386,Teradata: How to create views dynamically from database tables,"I am using Teradata BTEQ version 15.00.  I have the following SQL code.  The dynamical SQL is almost there, but the formet is a little off.

.Export Report File = CViews.sql

.Rtitle ''
 .Foldline on
 .Format Off
 .set heading '';
 .set heading off;
 .set UNDERLINE OFF;
 .Omit On 4,5

Select
 CASE When ColNo = 1
 THEN 'Replace View
$view_db_name.VW_'||Tbl.Tablename||' As locking row for access
Select ('
 Else '' END (Title '')
 , Cols.Columnname (Title '')
 , CASE WHEN RevColNo = 1 THEN ')
From $db_name.'||Tbl.Tablename||';'
 Else '' END (Title '')
 , Row_Number () Over(Partition By Tbl.Tablename
 Order By Cols.ColumnId) As ColNo
 , Row_Number () Over(Partition By Tbl.Tablename
 Order By Cols.ColumnId Desc) As RevColNo
 From DBC.Tables Tbl
 Join DBC.Columns Cols
 On Tbl.Databasename = Cols.Databasename
 And Tbl.Tablename = Cols.TableName
 Where Tbl.Databasename = '$db_name'
 And Tbl.Tablekind = 'T'
 Order By Tbl.Tablename, ColNo
 ;
.Export Reset

.Run File CViews.sql


Here are the results, but ""locking row for access"" was cut off, so i got errors when compiling SQL.

Replace View  VIEWS_TEST.VW_LOCATION                   As loc
   ID
   LOC_TYPE_ID
   NAME
   LATITUDE
   LONGITUDE
   ADDR1
   ADDR2
   CITY
   STATE
   COUNTRY

   )  From TABLES_TEST.LOCATION                  ;


You can see that the ""As Loc"" was cut off, so i got the following errors:

*** Failure 3707 Syntax error, expected something like a 'SELECT' keyword o
 r a 'LOCK' keyword or '(' or a 'TRANSACTIONTIME' keyword between the 'As' key word and the word 'loc'.
                Statement# 1, Info =81
 *** Total elapsed time was 1 second.


I tried different ways try to make it work, but failed.

Any suggestions?
",-1,-1,-1.0,"I am using Teradata BTEQ version 15.00.  I have the following SQL code.  The dynamical SQL is almost there, but the formet is a little off.

.Export Report File = CViews.sql

.Rtitle ''
 .Foldline on
 .Format Off
 .set heading '';
 .set heading off;
 .set UNDERLINE OFF;
 .Omit On 4,5

Select
 CASE When ColNo = 1
 THEN 'Replace View
$view_db_name.VW_'||Tbl.Tablename||' As locking row for access
Select ('
 Else '' END (Title '')
 , Cols.Columnname (Title '')
 , CASE WHEN RevColNo = 1 THEN ')
From $db_name.'||Tbl.Tablename||';'
 Else '' END (Title '')
 , Row_Number () Over(Partition By Tbl.Tablename
 Order By Cols.ColumnId) As ColNo
 , Row_Number () Over(Partition By Tbl.Tablename
 Order By Cols.ColumnId Desc) As RevColNo
 From DBC.Tables Tbl
 Join DBC.Columns Cols
 On Tbl.Databasename = Cols.Databasename
 And Tbl.Tablename = Cols.TableName
 Where Tbl.Databasename = '$db_name'
 And Tbl.Tablekind = 'T'
 Order By Tbl.Tablename, ColNo
 ;
.Export Reset

.Run File CViews.sql


Here are the results, but ""locking row for access"" was cut off, so i got errors when compiling SQL.

Replace View  VIEWS_TEST.VW_LOCATION                   As loc
   ID
   LOC_TYPE_ID
   NAME
   LATITUDE
   LONGITUDE
   ADDR1
   ADDR2
   CITY
   STATE
   COUNTRY

   )  From TABLES_TEST.LOCATION                  ;


You can see that the ""As Loc"" was cut off, so i got the following errors:

*** Failure 3707 Syntax error, expected something like a 'SELECT' keyword o
 r a 'LOCK' keyword or '(' or a 'TRANSACTIONTIME' keyword between the 'As' key word and the word 'loc'.
                Statement# 1, Info =81
 *** Total elapsed time was 1 second.


I tried different ways try to make it work, but failed.

Any suggestions?
",3
129,32958600,Teradata 15 Failure 5678 The user is not authorized to assign the default role,"I can execute the following using DBC, but failed when U used SYSDBA user.  I thought I have granted all rights to SYSDBA, but I must missed this one.

CREATE USER ""zz_01"" 
  FROM ALL_USERS AS
PASSWORD=""s#rMdfdgd1"",
PERMANENT = 0,
SPOOL = 0,
TEMPORARY = 0,
COLLATION = HOST,
ACCOUNT = 'sysdba',
NO FALLBACK,
NO BEFORE JOURNAL,
NO AFTER JOURNAL,
TIME ZONE = NULL,
DATEFORM = NULL,
DEFAULT CHARACTER SET LATIN,
DEFAULT ROLE = ""SL-TERADATA-TEST"",
PROFILE = PROFILE_DEFAULT_ACCOUNT;


*** Failure 5678 The user is not authorized to assign the default role 'SL-
 TERADATA-TEST'.
",-1,-1,-1.0,"I can execute the following using DBC, but failed when U used SYSDBA user.  I thought I have granted all rights to SYSDBA, but I must missed this one.

CREATE USER ""zz_01"" 
  FROM ALL_USERS AS
PASSWORD=""s#rMdfdgd1"",
PERMANENT = 0,
SPOOL = 0,
TEMPORARY = 0,
COLLATION = HOST,
ACCOUNT = 'sysdba',
NO FALLBACK,
NO BEFORE JOURNAL,
NO AFTER JOURNAL,
TIME ZONE = NULL,
DATEFORM = NULL,
DEFAULT CHARACTER SET LATIN,
DEFAULT ROLE = ""SL-TERADATA-TEST"",
PROFILE = PROFILE_DEFAULT_ACCOUNT;


*** Failure 5678 The user is not authorized to assign the default role 'SL-
 TERADATA-TEST'.
",3
130,32581755,Does Apache spark supports persisting dataframe (data) in teradata?,"I need to persist data in teradata and I am using spark to achieve this using jdbc connection.

But whenever I try to persist dataframe in teradata, each time spark try to create a new table inspite of using any SaveMode and the table creation operation gets failed. 

I dug down then found that spark checks if the table exist or not using below statement

Try(conn.prepareStatement(s""SELECT 1 FROM $table LIMIT 1"") .executeQuery().next()).isSuccess

This Limit keyword is not supported in Teradata so it gives the error and spark considers it as the table not present and try to create new table for each dataframe write operation. 

Any workaround for this or any other way to persist dataframe in teradata?
",-1,-1,-1.0,"I need to persist data in teradata and I am using spark to achieve this using jdbc connection.

But whenever I try to persist dataframe in teradata, each time spark try to create a new table inspite of using any SaveMode and the table creation operation gets failed. 

I dug down then found that spark checks if the table exist or not using below statement

Try(conn.prepareStatement(s""SELECT 1 FROM $table LIMIT 1"") .executeQuery().next()).isSuccess

This Limit keyword is not supported in Teradata so it gives the error and spark considers it as the table not present and try to create new table for each dataframe write operation. 

Any workaround for this or any other way to persist dataframe in teradata?
",3
131,32576048,Teradata: How to split a query to lessen spool size,"I'm having a problem running a very simple query in Teradata that essentially consists of:

SELECT COL1, COL2, COL3, COL4, COL5, COL6, COL7, COL8, COL9
FROM table1 LEFT OUTER JOIN table2 on table1.ID = table2.IDN;


I expect several million results, but am not precisely certain. The statistics indicate there are about ~8 Million rows.

After getting some resource errors I was informed that as a user I'm limited to 500,000 rows or a 37GB spool size. This query far surpasses both according to the EXPLAIN (as you can imagine). I'm looking for way to do the SELECT in chunks, if possible... unless there is something else that I'm fundamentally missing.

Is this a bad query by nature? Is there a way to split the query into multiple smaller queries?
",-1,-1,-1.0,"I'm having a problem running a very simple query in Teradata that essentially consists of:

SELECT COL1, COL2, COL3, COL4, COL5, COL6, COL7, COL8, COL9
FROM table1 LEFT OUTER JOIN table2 on table1.ID = table2.IDN;


I expect several million results, but am not precisely certain. The statistics indicate there are about ~8 Million rows.

After getting some resource errors I was informed that as a user I'm limited to 500,000 rows or a 37GB spool size. This query far surpasses both according to the EXPLAIN (as you can imagine). I'm looking for way to do the SELECT in chunks, if possible... unless there is something else that I'm fundamentally missing.

Is this a bad query by nature? Is there a way to split the query into multiple smaller queries?
",3
132,32299003,Datatype mismatch converting SAS numeric to Teradata BIGINT,"I have a SAS dataset with a numeric variable ACCT_ID (among other fields). Its attributes in a PROC CONTENTS are:

#    Variable                 Type    Len    Format    Informat    Label
1    ACCT_ID                  Num       8    19.       19.         ACCT_ID


I know that this field doesn't have any non-integer values in it, so I want to store it as a BIGINT in Teradata, and I've specified this with the dbtype data set option like this:

data td.output(dbtype=(ACCT_ID=""BIGINT"", &lt;etc etc&gt;));


However, this gives the following error:

ERROR: Datatype mismatch for column: ACCT_ID.


There are no missing or non-integer values in that field, and the error persists even if I round ACCT_ID using round(acct_id, 1) to explicitly remove any floating point values that could exist.

Strangely enough, no error is given if I assign this to be a DECIMAL(18,0) in Teradata rather than a BIGINT. I guess that could be one workaround, but I'd like to understand how I can create integer fields in Teradata from SAS numeric variables like this given that SAS doesn't distinguish types between integer and floating point.
",-1,-1,-1.0,"I have a SAS dataset with a numeric variable ACCT_ID (among other fields). Its attributes in a PROC CONTENTS are:

#    Variable                 Type    Len    Format    Informat    Label
1    ACCT_ID                  Num       8    19.       19.         ACCT_ID


I know that this field doesn't have any non-integer values in it, so I want to store it as a BIGINT in Teradata, and I've specified this with the dbtype data set option like this:

data td.output(dbtype=(ACCT_ID=""BIGINT"", &lt;etc etc&gt;));


However, this gives the following error:

ERROR: Datatype mismatch for column: ACCT_ID.


There are no missing or non-integer values in that field, and the error persists even if I round ACCT_ID using round(acct_id, 1) to explicitly remove any floating point values that could exist.

Strangely enough, no error is given if I assign this to be a DECIMAL(18,0) in Teradata rather than a BIGINT. I guess that could be one workaround, but I'd like to understand how I can create integer fields in Teradata from SAS numeric variables like this given that SAS doesn't distinguish types between integer and floating point.
",3
133,32264687,Syntax for GRANT statement in Teradata,"Help me to find out the Syntax for GRANT statement in Teradata

I have tried this GRANT select on database_Name to user_name, but its not working
",-1,-1,-1.0,"Help me to find out the Syntax for GRANT statement in Teradata

I have tried this GRANT select on database_Name to user_name, but its not working
",3
134,32017721,Use Fixed-Width Font in Teradata SQL Assistant Query Window?,"Really simple ask:

Teradata SQL Assistant's default font for code is variable-width. This is frustrating for formatting and readability reasons, among others. Is it possible to change the font to Courier New or something similar in the Query window? Google searches, perusing the options menus, and looking up the Teradata docs have yielded nothing...

Please help me code in monospaced font!
",0,-1,-1.0,"Really simple ask:

Teradata SQL Assistant's default font for code is variable-width. This is frustrating for formatting and readability reasons, among others. Is it possible to change the font to Courier New or something similar in the Query window? Google searches, perusing the options menus, and looking up the Teradata docs have yielded nothing...

Please help me code in monospaced font!
",3
135,31946694,Teradata Window/Rolling Sum under Multiple Conditions,"I'm working in Teradata SQL Assistant 14.10 and running into issues with the following problem:

I have a list of calculated elapsed times, and I need to create a column that flags when


a) the row for which the sum of the elapsed time exceeds 20 min for the first time
b) the row for which the sum of the elapsed time exceeds 15 min for each time after that


The difficulty here being that the rolling sum that would be used to set the flag would needs to go to 0 after each condition(s) have been met. See the below result set with the FLAG being the desired output column based on the above conditions, and the reason column explaining why it is to be flagged.

RN  REPORT_DT   SEG_CD  NUM_F   T1          T2          ELAPSED_TIME    FLAG    REASON
1   6/22/2015   STATION 881     18:33:00    17:30:00    63              1       63 &gt;= 20 min for first time
2   6/22/2015   STATION 881     18:45:00    18:33:00    12              0       12 &lt; 15
3   6/22/2015   STATION 881     19:00:00    18:45:00    15              1       12 + 15 &gt;= 15
4   6/22/2015   STATION 881     19:15:00    19:00:00    15              1       15 &gt;= 15
5   6/22/2015   STATION 881     19:30:00    19:15:00    15              1       15 &gt;= 15
6   6/22/2015   STATION 881     19:40:00    19:30:00    10              0       10 &lt; 15
7   6/22/2015   STATION 881     19:50:00    19:40:00    10              1       10 + 10 &gt;= 15
8   6/22/2015   STATION 881     20:00:00    19:50:00    10              0       10 &lt; 15
9   6/22/2015   STATION 881     20:10:00    20:00:00    10              1       10 + 10 &gt;= 15


I've tried a variety of SUM()OVER(PARTITION BY ORDER BY RESET WHEN) kind of queries which I feel is the right direction, but can't seem to get any desired results. 

Any advice is much appreciated! Thanks in advance!
",-1,-1,-1.0,"I'm working in Teradata SQL Assistant 14.10 and running into issues with the following problem:

I have a list of calculated elapsed times, and I need to create a column that flags when


a) the row for which the sum of the elapsed time exceeds 20 min for the first time
b) the row for which the sum of the elapsed time exceeds 15 min for each time after that


The difficulty here being that the rolling sum that would be used to set the flag would needs to go to 0 after each condition(s) have been met. See the below result set with the FLAG being the desired output column based on the above conditions, and the reason column explaining why it is to be flagged.

RN  REPORT_DT   SEG_CD  NUM_F   T1          T2          ELAPSED_TIME    FLAG    REASON
1   6/22/2015   STATION 881     18:33:00    17:30:00    63              1       63 &gt;= 20 min for first time
2   6/22/2015   STATION 881     18:45:00    18:33:00    12              0       12 &lt; 15
3   6/22/2015   STATION 881     19:00:00    18:45:00    15              1       12 + 15 &gt;= 15
4   6/22/2015   STATION 881     19:15:00    19:00:00    15              1       15 &gt;= 15
5   6/22/2015   STATION 881     19:30:00    19:15:00    15              1       15 &gt;= 15
6   6/22/2015   STATION 881     19:40:00    19:30:00    10              0       10 &lt; 15
7   6/22/2015   STATION 881     19:50:00    19:40:00    10              1       10 + 10 &gt;= 15
8   6/22/2015   STATION 881     20:00:00    19:50:00    10              0       10 &lt; 15
9   6/22/2015   STATION 881     20:10:00    20:00:00    10              1       10 + 10 &gt;= 15


I've tried a variety of SUM()OVER(PARTITION BY ORDER BY RESET WHEN) kind of queries which I feel is the right direction, but can't seem to get any desired results. 

Any advice is much appreciated! Thanks in advance!
",3
136,31363148,SQL Syntax: SQL Server vs. Teradata,"I've been querying against Teradata servers with SQL Assistant for years, but now have to work with a SQL Server. I've been stumbling over my code for hours, having a hard time figuring out which pieces of syntax need to be updated. 

Does anyone know of a good resource for converting logic? 

Here's an example -- I was loading .txt data into a temp table: 

In Teradata, the following works:

CREATE MULTISET TABLE USER_WORK.TABLE1 (
VAR1 CHAR(3)
,VAR2 CHAR(5)
,VAR3 DECIMAL(12,2)  )
PRIMARY INDEX (VAR1, VAR2);


In SQL Server, I was able to get the following to work:

CREATE TABLE #TABLE1 (
VAR1 VARCHAR(20)
,VAR2 VARCHAR(20)
,VAR3 VAR(20)  );


(Main differences: No ""Multiset""; all variables read in as VARCHAR &amp; and I couldn't get any length shorter than 20 to work; I couldn't figure out how to define a functional index)

Mostly wondering if there is some sort of pattern behind migrating the logic - it's painful to have to look up every single piece of failed code, and to sort out of it will actually run on SQL Server.
",-1,-1,-1.0,"I've been querying against Teradata servers with SQL Assistant for years, but now have to work with a SQL Server. I've been stumbling over my code for hours, having a hard time figuring out which pieces of syntax need to be updated. 

Does anyone know of a good resource for converting logic? 

Here's an example -- I was loading .txt data into a temp table: 

In Teradata, the following works:

CREATE MULTISET TABLE USER_WORK.TABLE1 (
VAR1 CHAR(3)
,VAR2 CHAR(5)
,VAR3 DECIMAL(12,2)  )
PRIMARY INDEX (VAR1, VAR2);


In SQL Server, I was able to get the following to work:

CREATE TABLE #TABLE1 (
VAR1 VARCHAR(20)
,VAR2 VARCHAR(20)
,VAR3 VAR(20)  );


(Main differences: No ""Multiset""; all variables read in as VARCHAR &amp; and I couldn't get any length shorter than 20 to work; I couldn't figure out how to define a functional index)

Mostly wondering if there is some sort of pattern behind migrating the logic - it's painful to have to look up every single piece of failed code, and to sort out of it will actually run on SQL Server.
",3
137,31011006,Inserting a timestamp(0) column on Teradata using Preparedstatement on Java,"Lets supose the folowing table:

CREATE MULTISET TABLE DATABASE.TABLE ,NO FALLBACK ,
   NO BEFORE JOURNAL,
   NO AFTER JOURNAL,
   CHECKSUM = DEFAULT,
   DEFAULT MERGEBLOCKRATIO
   (
     START_DATE TIMESTAMP(0) FORMAT 'DD-MMM-YYYYBHH:MI:SS',
     STATUS DECIMAL(5,0) ,
     PROCESS VARCHAR(100) CHARACTER SET LATIN CASESPECIFIC NOT NULL
   )
PRIMARY INDEX ( PROCESS)
UNIQUE INDEX ( PROCESS, START_DATE );


What I would like to do is insert into that table using PreparedStatement on Java, but it is not working.. =/

that is what I am doing on Java:

setConnection(); //IT IS OK, SO DONT WORRY!! =)

String insert = ""insert into DATABASE.TABLE (START_DATE , STATUS, PROCESS)"";

PreparedStatement  pstm = getConnection().prepareStatement(insert);

ArrayList&lt;Object&gt; values = new ArrayList&lt;Object&gt;();
values.add(0, new java.sql.Timestamp(calendar.getTime().getTime()));
values.add(1, new Integer(0));
values.add(2, ""TEST_PROCESS"");

for (int i = 0; i &lt; values.size(); i++)
{
    if (values.get(i).getClass().equals(Integer.class))
        pstm.setInt(i+1, (Integer) values.get(i));

    if (values.get(i).getClass().equals(String.class))
        pstm.setString(i+1, (String) values.get(i));

    if (values.get(i).getClass().equals(Timestamp.class))
        pstm.setTimestamp(i+1, (Timestamp) values.get(i));

    if (values.get(i).getClass().equals(Date.class))
        pstm.setDate(i+1, (Date) values.get(i));

    if (values.get(i).getClass().equals(Double.class))
        pstm.setDouble(i+1, (Double) values.get(i));

    if (values.get(i).getClass().equals(Float.class))
        pstm.setFloat(i+1, (Float) values.get(i));
}

pstm.execute();


And that is the error I am getting:

java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.05] [Error 5404] [SQLState HY000] Datetime field overflow.
at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:308)
at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:109)
at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:307)
at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:196)
at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:123)
at com.teradata.jdbc.jdbc_4.statemachine.PreparedStatementController.run(PreparedStatementController.java:46)
at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:386)
at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:328)
at com.teradata.jdbc.jdbc_4.TDPreparedStatement.doPrepExecute(TDPreparedStatement.java:165)
at com.teradata.jdbc.jdbc_4.TDPreparedStatement.execute(TDPreparedStatement.java:2598)
at ProcessStatus.set(ProcessStatus.java:71)
at CMain.main(CMain.java:42)


I know I could change the START_DATE data type to TIMESTAMP(3) for example, but it doesnt seems the tight thing to do, so thats why I am asking for your help.

Anyway, I appreciate your time and help. Thank you everybody!!!
",-1,-1,-1.0,"Lets supose the folowing table:

CREATE MULTISET TABLE DATABASE.TABLE ,NO FALLBACK ,
   NO BEFORE JOURNAL,
   NO AFTER JOURNAL,
   CHECKSUM = DEFAULT,
   DEFAULT MERGEBLOCKRATIO
   (
     START_DATE TIMESTAMP(0) FORMAT 'DD-MMM-YYYYBHH:MI:SS',
     STATUS DECIMAL(5,0) ,
     PROCESS VARCHAR(100) CHARACTER SET LATIN CASESPECIFIC NOT NULL
   )
PRIMARY INDEX ( PROCESS)
UNIQUE INDEX ( PROCESS, START_DATE );


What I would like to do is insert into that table using PreparedStatement on Java, but it is not working.. =/

that is what I am doing on Java:

setConnection(); //IT IS OK, SO DONT WORRY!! =)

String insert = ""insert into DATABASE.TABLE (START_DATE , STATUS, PROCESS)"";

PreparedStatement  pstm = getConnection().prepareStatement(insert);

ArrayList&lt;Object&gt; values = new ArrayList&lt;Object&gt;();
values.add(0, new java.sql.Timestamp(calendar.getTime().getTime()));
values.add(1, new Integer(0));
values.add(2, ""TEST_PROCESS"");

for (int i = 0; i &lt; values.size(); i++)
{
    if (values.get(i).getClass().equals(Integer.class))
        pstm.setInt(i+1, (Integer) values.get(i));

    if (values.get(i).getClass().equals(String.class))
        pstm.setString(i+1, (String) values.get(i));

    if (values.get(i).getClass().equals(Timestamp.class))
        pstm.setTimestamp(i+1, (Timestamp) values.get(i));

    if (values.get(i).getClass().equals(Date.class))
        pstm.setDate(i+1, (Date) values.get(i));

    if (values.get(i).getClass().equals(Double.class))
        pstm.setDouble(i+1, (Double) values.get(i));

    if (values.get(i).getClass().equals(Float.class))
        pstm.setFloat(i+1, (Float) values.get(i));
}

pstm.execute();


And that is the error I am getting:

java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.05] [Error 5404] [SQLState HY000] Datetime field overflow.
at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:308)
at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:109)
at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:307)
at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:196)
at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:123)
at com.teradata.jdbc.jdbc_4.statemachine.PreparedStatementController.run(PreparedStatementController.java:46)
at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:386)
at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:328)
at com.teradata.jdbc.jdbc_4.TDPreparedStatement.doPrepExecute(TDPreparedStatement.java:165)
at com.teradata.jdbc.jdbc_4.TDPreparedStatement.execute(TDPreparedStatement.java:2598)
at ProcessStatus.set(ProcessStatus.java:71)
at CMain.main(CMain.java:42)


I know I could change the START_DATE data type to TIMESTAMP(3) for example, but it doesnt seems the tight thing to do, so thats why I am asking for your help.

Anyway, I appreciate your time and help. Thank you everybody!!!
",3
138,33838913,Teradata 15: Failure 5616 The user is not authorized to grant or revoke role,"I am using a second most powerful DBA account SYSDBA to assign a user to a role.  Simple task, right, but i got this error. 

   grant ""$ROLE"" to ""$user_name"";


*** Failure 5616 The user is not authorized to grant or revoke role 'ROLE'

Then i search the web, Teradata has a nice document:

Only the user DBC, or a user granted a role WITH ADMIN OPTION, or a user with an applicable role (current role or a role nested within it) that is granted a role WITH ADMIN OPTION has the right to GRANT/REVOKE the role to/from another user or role. 

What is the SQL syntax that I can fix this issue?

I tried the following according to some research.  But did not work

 grant ""role_dba"" to sysdba with admin option;
 grant ""role_dba"" to sysdba;

",-1,-1,-1.0,"I am using a second most powerful DBA account SYSDBA to assign a user to a role.  Simple task, right, but i got this error. 

   grant ""$ROLE"" to ""$user_name"";


*** Failure 5616 The user is not authorized to grant or revoke role 'ROLE'

Then i search the web, Teradata has a nice document:

Only the user DBC, or a user granted a role WITH ADMIN OPTION, or a user with an applicable role (current role or a role nested within it) that is granted a role WITH ADMIN OPTION has the right to GRANT/REVOKE the role to/from another user or role. 

What is the SQL syntax that I can fix this issue?

I tried the following according to some research.  But did not work

 grant ""role_dba"" to sysdba with admin option;
 grant ""role_dba"" to sysdba;

",3
139,33890004,Connecting Java and Teradata via java error,"I am using 2 jars : 

1. tdgssconfig

2. terajdbc4

Class.forName(""com.teradata.jdbc.TeraDriver"");
connString = ""jdbc:teradata://"" + host + ""/database="" + db +       "",tmode=DEFAULT,charset=UTF8,LOGMECH=TD2"";
//DriverManager.registerDriver (new   com.teradata.jdbc.TeraDriver()); 
  DriverManager.setLoginTimeout(120);

this.conn = DriverManager.getConnection(connString, user,password);


On my computer which as also installed Teradata SQLAssistant the process is working and connecting teradata
but on the server which don't have the software ,the process is failed to connect tera data. I get the following error :

TERAJDBC4 ERROR [main] com.teradata.jdbc.jdk6.JDK6_SQL_Connection
",-1,-1,-1.0,"I am using 2 jars : 

1. tdgssconfig

2. terajdbc4

Class.forName(""com.teradata.jdbc.TeraDriver"");
connString = ""jdbc:teradata://"" + host + ""/database="" + db +       "",tmode=DEFAULT,charset=UTF8,LOGMECH=TD2"";
//DriverManager.registerDriver (new   com.teradata.jdbc.TeraDriver()); 
  DriverManager.setLoginTimeout(120);

this.conn = DriverManager.getConnection(connString, user,password);


On my computer which as also installed Teradata SQLAssistant the process is working and connecting teradata
but on the server which don't have the software ,the process is failed to connect tera data. I get the following error :

TERAJDBC4 ERROR [main] com.teradata.jdbc.jdk6.JDK6_SQL_Connection
",0
140,34139590,teradata jdbc jar not loading in spark,"I'm trying to load the teradata jar file in spark but can't get it to load. I start spark shell like this:

spark-shell --jars ~/*.jar --driver-class-path ~/*.jar


in there I have a jar file called terajdbc4.jar

when spark shell starts...I do this

scala&gt; sc.addJar(""terajdbc4.jar"")
15/12/07 12:27:55 INFO SparkContext: Added JAR terajdbc4.jar at http://1.2.4.4:41601/jars/terajdbc4.jar with timestamp 1449509275187

scala&gt; sc.jars
res1: Seq[String] = List(file:/home/user1/spark-cassandra-connector_2.10-1.0.0-beta1.jar)

scala&gt; 


but its not there in the jars. why is it missing still?

EDIT:

ok. I got the jar to load, but I'm getting this error:

java.lang.ClassNotFoundException: com.teradata.jdbc.TeraDriver


I do the following:

scala&gt; sc.jars
res4: Seq[String] = List(file:/home/user/terajdbc4.jar)

scala&gt; import com.teradata.jdbc.TeraDriver
import com.teradata.jdbc.TeraDriver

scala&gt; Class.forName(""com.teradata.jdbc.TeraDriver"")
res5: Class[_] = class com.teradata.jdbc.TeraDriver


and then this:

val jdbcDF = sqlContext.load(""jdbc"", Map(
  ""url"" -&gt; ""jdbc:teradata://dbinstn, TMODE=TERA, user=user1, password=pass1"",
  ""dbtable"" -&gt; ""db1a.table1a"",
  ""driver"" -&gt; ""com.teradata.jdbc.TeraDriver""))


and then I get this:

java.lang.ClassNotFoundException: com.teradata.jdbc.TeraDriver

",-1,-1,-1.0,"I'm trying to load the teradata jar file in spark but can't get it to load. I start spark shell like this:

spark-shell --jars ~/*.jar --driver-class-path ~/*.jar


in there I have a jar file called terajdbc4.jar

when spark shell starts...I do this

scala&gt; sc.addJar(""terajdbc4.jar"")
15/12/07 12:27:55 INFO SparkContext: Added JAR terajdbc4.jar at http://1.2.4.4:41601/jars/terajdbc4.jar with timestamp 1449509275187

scala&gt; sc.jars
res1: Seq[String] = List(file:/home/user1/spark-cassandra-connector_2.10-1.0.0-beta1.jar)

scala&gt; 


but its not there in the jars. why is it missing still?

EDIT:

ok. I got the jar to load, but I'm getting this error:

java.lang.ClassNotFoundException: com.teradata.jdbc.TeraDriver


I do the following:

scala&gt; sc.jars
res4: Seq[String] = List(file:/home/user/terajdbc4.jar)

scala&gt; import com.teradata.jdbc.TeraDriver
import com.teradata.jdbc.TeraDriver

scala&gt; Class.forName(""com.teradata.jdbc.TeraDriver"")
res5: Class[_] = class com.teradata.jdbc.TeraDriver


and then this:

val jdbcDF = sqlContext.load(""jdbc"", Map(
  ""url"" -&gt; ""jdbc:teradata://dbinstn, TMODE=TERA, user=user1, password=pass1"",
  ""dbtable"" -&gt; ""db1a.table1a"",
  ""driver"" -&gt; ""com.teradata.jdbc.TeraDriver""))


and then I get this:

java.lang.ClassNotFoundException: com.teradata.jdbc.TeraDriver

",0
141,34204794,LOAD HADOOP fails while pulling from Teradata,"I am using IBM BigInsights version 4.1.0. 

I used the below command to pull data from teradata.



LOAD HADOOP USING JDBC CONNECTION URL 'jdbc:teradata://&lt;&lt;ip_address&gt;&gt;/database=&lt;&lt;db_name&gt;&gt;' WITH PARAMETERS ('user' = '&lt;&lt;user_name&gt;&gt;','password'='&lt;&lt;password&gt;&gt;') FROM TABLE &lt;&lt;table_name&gt;&gt; COLUMNS (&lt;&lt;COL1, COL2, COL3, .... COLN&gt;&gt;) SPLIT COLUMN &lt;&lt;COLM&gt;&gt; INTO TABLE &lt;&lt;Target_bigsql_schema&gt;&gt;.&lt;&lt;target_bigsql_table&gt;&gt; APPEND WITH LOAD PROPERTIES ('tdch.enable'='true');




The error I am getting while executing the above command is below



2015-12-10 14:21:01,336 ERROR com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool [Thread-3] : Teradata Connector for Hadoop tool error.
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:88)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
        at java.lang.reflect.Method.invoke(Method.java:618)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.callTDCH(TDImportTool.java:104)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.run(TDImportTool.java:72)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
        at com.ibm.biginsights.ie.db.SqoopUtils.runSqoopTool(SqoopUtils.java:146)
        at com.ibm.biginsights.ie.db.DBImportImpl.importData(DBImportImpl.java:159)
        at com.ibm.biginsights.ie.impl.ImporterImpl.executeImport(ImporterImpl.java:504)
        at com.ibm.biginsights.ie.impl.ImporterImpl.executePerformImport(ImporterImpl.java:417)
        at com.ibm.biginsights.ie.impl.ImporterImpl.performImport(ImporterImpl.java:264)
        at com.ibm.biginsights.biga.udf.LoadTool.performImport(LoadTool.java:214)
        at com.ibm.biginsights.biga.udf.BIGSQL_DDL.doLoadStatement(BIGSQL_DDL.java:644)
        at com.ibm.biginsights.biga.udf.BIGSQL_DDL.processDDL(BIGSQL_DDL.java:207)
Caused by: com.teradata.connector.common.exception.ConnectorException: Hive table's InputFormat class is not supported
        at com.teradata.connector.common.tool.ConnectorJobRunner.runJob(ConnectorJobRunner.java:140)
        ... 17 more
2015-12-10 14:21:01,337 ERROR org.apache.sqoop.Sqoop [Thread-3] : Got exception running Sqoop: java.lang.RuntimeException: com.teradata.connector.common.exception.ConnectorException: Hive table's InputFormat class is not supported
2015-12-10 14:21:01,337 ERROR com.ibm.biginsights.ie.db.DBImportImpl [Thread-3] : Error during import
java.lang.RuntimeException: com.teradata.connector.common.exception.ConnectorException: Hive table's InputFormat class is not supported
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.callTDCH(TDImportTool.java:123)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.run(TDImportTool.java:72)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
        at com.ibm.biginsights.ie.db.SqoopUtils.runSqoopTool(SqoopUtils.java:146)
        at com.ibm.biginsights.ie.db.DBImportImpl.importData(DBImportImpl.java:159)
        at com.ibm.biginsights.ie.impl.ImporterImpl.executeImport(ImporterImpl.java:504)
        at com.ibm.biginsights.ie.impl.ImporterImpl.executePerformImport(ImporterImpl.java:417)
        at com.ibm.biginsights.ie.impl.ImporterImpl.performImport(ImporterImpl.java:264)
        at com.ibm.biginsights.biga.udf.LoadTool.performImport(LoadTool.java:214)
        at com.ibm.biginsights.biga.udf.BIGSQL_DDL.doLoadStatement(BIGSQL_DDL.java:644)
        at com.ibm.biginsights.biga.udf.BIGSQL_DDL.processDDL(BIGSQL_DDL.java:207)
Caused by: com.teradata.connector.common.exception.ConnectorException: Hive table's InputFormat class is not supported
        at com.teradata.connector.common.tool.ConnectorJobRunner.runJob(ConnectorJobRunner.java:140)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:88)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
        at java.lang.reflect.Method.invoke(Method.java:618)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.callTDCH(TDImportTool.java:104)
        ... 12 more
2015-12-10 14:21:01,337 ERROR com.ibm.biginsights.ie.db.DBImportImpl [Thread-3] : [BSL-0-18c443e19]: Error during import (Job Id = ):com.teradata.connector.common.exception.ConnectorException: Hive table's InputFormat class is not supported




Is there any possible resolution for this?
",-1,-1,-1.0,"I am using IBM BigInsights version 4.1.0. 

I used the below command to pull data from teradata.



LOAD HADOOP USING JDBC CONNECTION URL 'jdbc:teradata://&lt;&lt;ip_address&gt;&gt;/database=&lt;&lt;db_name&gt;&gt;' WITH PARAMETERS ('user' = '&lt;&lt;user_name&gt;&gt;','password'='&lt;&lt;password&gt;&gt;') FROM TABLE &lt;&lt;table_name&gt;&gt; COLUMNS (&lt;&lt;COL1, COL2, COL3, .... COLN&gt;&gt;) SPLIT COLUMN &lt;&lt;COLM&gt;&gt; INTO TABLE &lt;&lt;Target_bigsql_schema&gt;&gt;.&lt;&lt;target_bigsql_table&gt;&gt; APPEND WITH LOAD PROPERTIES ('tdch.enable'='true');




The error I am getting while executing the above command is below



2015-12-10 14:21:01,336 ERROR com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool [Thread-3] : Teradata Connector for Hadoop tool error.
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:88)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
        at java.lang.reflect.Method.invoke(Method.java:618)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.callTDCH(TDImportTool.java:104)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.run(TDImportTool.java:72)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
        at com.ibm.biginsights.ie.db.SqoopUtils.runSqoopTool(SqoopUtils.java:146)
        at com.ibm.biginsights.ie.db.DBImportImpl.importData(DBImportImpl.java:159)
        at com.ibm.biginsights.ie.impl.ImporterImpl.executeImport(ImporterImpl.java:504)
        at com.ibm.biginsights.ie.impl.ImporterImpl.executePerformImport(ImporterImpl.java:417)
        at com.ibm.biginsights.ie.impl.ImporterImpl.performImport(ImporterImpl.java:264)
        at com.ibm.biginsights.biga.udf.LoadTool.performImport(LoadTool.java:214)
        at com.ibm.biginsights.biga.udf.BIGSQL_DDL.doLoadStatement(BIGSQL_DDL.java:644)
        at com.ibm.biginsights.biga.udf.BIGSQL_DDL.processDDL(BIGSQL_DDL.java:207)
Caused by: com.teradata.connector.common.exception.ConnectorException: Hive table's InputFormat class is not supported
        at com.teradata.connector.common.tool.ConnectorJobRunner.runJob(ConnectorJobRunner.java:140)
        ... 17 more
2015-12-10 14:21:01,337 ERROR org.apache.sqoop.Sqoop [Thread-3] : Got exception running Sqoop: java.lang.RuntimeException: com.teradata.connector.common.exception.ConnectorException: Hive table's InputFormat class is not supported
2015-12-10 14:21:01,337 ERROR com.ibm.biginsights.ie.db.DBImportImpl [Thread-3] : Error during import
java.lang.RuntimeException: com.teradata.connector.common.exception.ConnectorException: Hive table's InputFormat class is not supported
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.callTDCH(TDImportTool.java:123)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.run(TDImportTool.java:72)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
        at com.ibm.biginsights.ie.db.SqoopUtils.runSqoopTool(SqoopUtils.java:146)
        at com.ibm.biginsights.ie.db.DBImportImpl.importData(DBImportImpl.java:159)
        at com.ibm.biginsights.ie.impl.ImporterImpl.executeImport(ImporterImpl.java:504)
        at com.ibm.biginsights.ie.impl.ImporterImpl.executePerformImport(ImporterImpl.java:417)
        at com.ibm.biginsights.ie.impl.ImporterImpl.performImport(ImporterImpl.java:264)
        at com.ibm.biginsights.biga.udf.LoadTool.performImport(LoadTool.java:214)
        at com.ibm.biginsights.biga.udf.BIGSQL_DDL.doLoadStatement(BIGSQL_DDL.java:644)
        at com.ibm.biginsights.biga.udf.BIGSQL_DDL.processDDL(BIGSQL_DDL.java:207)
Caused by: com.teradata.connector.common.exception.ConnectorException: Hive table's InputFormat class is not supported
        at com.teradata.connector.common.tool.ConnectorJobRunner.runJob(ConnectorJobRunner.java:140)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:88)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
        at java.lang.reflect.Method.invoke(Method.java:618)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.callTDCH(TDImportTool.java:104)
        ... 12 more
2015-12-10 14:21:01,337 ERROR com.ibm.biginsights.ie.db.DBImportImpl [Thread-3] : [BSL-0-18c443e19]: Error during import (Job Id = ):com.teradata.connector.common.exception.ConnectorException: Hive table's InputFormat class is not supported




Is there any possible resolution for this?
",0
142,34218247,tdimport utility fails to import data from teradata to hadoop,"I have tried tdimport utility to copy the data from teradata into hadoop but it is failing ""15/12/11 07:21:28 INFO processor.HiveOutputProcessor: hive table default.pos_rtl_str_test does not exist""

How could I specify the hive schema name here?



[biadmin@ehaasp-10035-master-3]/usr/iop/4.1.0.0/hive/bin&gt;/usr/iop/4.1.0.0/sqoop/bin/sqoop tdimport --connect jdbc:teradata://&lt;&lt;ipaddress&gt;&gt;/database=EDW01_V_LV_BASE --username &lt;&lt;username&gt;&gt; --password &lt;&lt;password&gt;&gt; --as-textfile --hive-table pos_rtl_str_test --table pos_rtl_str --columns ""RTL_STR_ID, RTL_STR_LANG_CD"" --split-by RTL_STR_ID
Warning: /usr/iop/4.1.0.0/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
15/12/11 07:20:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6_IBM_20
15/12/11 07:20:47 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
15/12/11 07:20:47 INFO common.ConnectorPlugin: load plugins in jar:file:/usr/iop/4.1.0.0/sqoop/lib/teradata-connector-1.4.1.jar!/teradata.connector.plugins.xml
15/12/11 07:20:47 WARN conf.HiveConf: HiveConf of name hive.heapsize does not exist
15/12/11 07:20:47 INFO hive.metastore: Trying to connect to metastore with URI thrift://&lt;&lt;masternode dns name&gt;&gt;:9083
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/iop/4.1.0.0/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/iop/4.1.0.0/zookeeper/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/12/11 07:20:48 INFO hive.metastore: Connected to metastore.
15/12/11 07:20:48 INFO processor.TeradataInputProcessor: input preprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor starts at:  1449818448723
15/12/11 07:20:49 INFO utils.TeradataUtils: the input database product is Teradata
15/12/11 07:20:49 INFO utils.TeradataUtils: the input database version is 14.10
15/12/11 07:20:49 INFO utils.TeradataUtils: the jdbc driver version is 15.0
15/12/11 07:21:07 INFO processor.TeradataInputProcessor: the teradata connector for hadoop version is: 1.4.1
15/12/11 07:21:07 INFO processor.TeradataInputProcessor: input jdbc properties are jdbc:teradata://&lt;&lt;ipaddress&gt;&gt;/database=&lt;&lt;database&gt;&gt;
15/12/11 07:21:27 INFO processor.TeradataInputProcessor: the number of mappers are 4
15/12/11 07:21:27 INFO processor.TeradataInputProcessor: input preprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor ends at:  1449818487899
15/12/11 07:21:27 INFO processor.TeradataInputProcessor: the total elapsed time of input preprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor is: 39s
15/12/11 07:21:28 WARN conf.HiveConf: HiveConf of name hive.heapsize does not exist
15/12/11 07:21:28 INFO hive.metastore: Trying to connect to metastore with URI thrift://ehaasp-10035-master-3.bi.services.bluemix.net:9083
15/12/11 07:21:28 INFO hive.metastore: Connected to metastore.
15/12/11 07:21:28 INFO processor.HiveOutputProcessor: hive table default.pos_rtl_str_test does not exist
15/12/11 07:21:28 WARN tool.ConnectorJobRunner: com.teradata.connector.common.exception.ConnectorException: The output post processor returns 1
15/12/11 07:21:28 INFO processor.TeradataInputProcessor: input postprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor starts at:  1449818488581
15/12/11 07:21:28 INFO processor.TeradataInputProcessor: input postprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor ends at:  1449818488581
15/12/11 07:21:28 INFO processor.TeradataInputProcessor: the total elapsed time of input postprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor is: 0s
15/12/11 07:21:28 ERROR wrapper.TDImportTool: Teradata Connector for Hadoop tool error.
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.callTDCH(TDImportTool.java:104)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.run(TDImportTool.java:72)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)
Caused by: com.teradata.connector.common.exception.ConnectorException: Import Hive table's column schema is missing
        at com.teradata.connector.common.tool.ConnectorJobRunner.runJob(ConnectorJobRunner.java:140)
        ... 12 more



",-1,-1,-1.0,"I have tried tdimport utility to copy the data from teradata into hadoop but it is failing ""15/12/11 07:21:28 INFO processor.HiveOutputProcessor: hive table default.pos_rtl_str_test does not exist""

How could I specify the hive schema name here?



[biadmin@ehaasp-10035-master-3]/usr/iop/4.1.0.0/hive/bin&gt;/usr/iop/4.1.0.0/sqoop/bin/sqoop tdimport --connect jdbc:teradata://&lt;&lt;ipaddress&gt;&gt;/database=EDW01_V_LV_BASE --username &lt;&lt;username&gt;&gt; --password &lt;&lt;password&gt;&gt; --as-textfile --hive-table pos_rtl_str_test --table pos_rtl_str --columns ""RTL_STR_ID, RTL_STR_LANG_CD"" --split-by RTL_STR_ID
Warning: /usr/iop/4.1.0.0/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
15/12/11 07:20:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6_IBM_20
15/12/11 07:20:47 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
15/12/11 07:20:47 INFO common.ConnectorPlugin: load plugins in jar:file:/usr/iop/4.1.0.0/sqoop/lib/teradata-connector-1.4.1.jar!/teradata.connector.plugins.xml
15/12/11 07:20:47 WARN conf.HiveConf: HiveConf of name hive.heapsize does not exist
15/12/11 07:20:47 INFO hive.metastore: Trying to connect to metastore with URI thrift://&lt;&lt;masternode dns name&gt;&gt;:9083
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/iop/4.1.0.0/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/iop/4.1.0.0/zookeeper/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/12/11 07:20:48 INFO hive.metastore: Connected to metastore.
15/12/11 07:20:48 INFO processor.TeradataInputProcessor: input preprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor starts at:  1449818448723
15/12/11 07:20:49 INFO utils.TeradataUtils: the input database product is Teradata
15/12/11 07:20:49 INFO utils.TeradataUtils: the input database version is 14.10
15/12/11 07:20:49 INFO utils.TeradataUtils: the jdbc driver version is 15.0
15/12/11 07:21:07 INFO processor.TeradataInputProcessor: the teradata connector for hadoop version is: 1.4.1
15/12/11 07:21:07 INFO processor.TeradataInputProcessor: input jdbc properties are jdbc:teradata://&lt;&lt;ipaddress&gt;&gt;/database=&lt;&lt;database&gt;&gt;
15/12/11 07:21:27 INFO processor.TeradataInputProcessor: the number of mappers are 4
15/12/11 07:21:27 INFO processor.TeradataInputProcessor: input preprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor ends at:  1449818487899
15/12/11 07:21:27 INFO processor.TeradataInputProcessor: the total elapsed time of input preprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor is: 39s
15/12/11 07:21:28 WARN conf.HiveConf: HiveConf of name hive.heapsize does not exist
15/12/11 07:21:28 INFO hive.metastore: Trying to connect to metastore with URI thrift://ehaasp-10035-master-3.bi.services.bluemix.net:9083
15/12/11 07:21:28 INFO hive.metastore: Connected to metastore.
15/12/11 07:21:28 INFO processor.HiveOutputProcessor: hive table default.pos_rtl_str_test does not exist
15/12/11 07:21:28 WARN tool.ConnectorJobRunner: com.teradata.connector.common.exception.ConnectorException: The output post processor returns 1
15/12/11 07:21:28 INFO processor.TeradataInputProcessor: input postprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor starts at:  1449818488581
15/12/11 07:21:28 INFO processor.TeradataInputProcessor: input postprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor ends at:  1449818488581
15/12/11 07:21:28 INFO processor.TeradataInputProcessor: the total elapsed time of input postprocessor com.teradata.connector.teradata.processor.TeradataSplitByHashProcessor is: 0s
15/12/11 07:21:28 ERROR wrapper.TDImportTool: Teradata Connector for Hadoop tool error.
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.callTDCH(TDImportTool.java:104)
        at com.ibm.biginsights.ie.sqoop.td.wrapper.TDImportTool.run(TDImportTool.java:72)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)
Caused by: com.teradata.connector.common.exception.ConnectorException: Import Hive table's column schema is missing
        at com.teradata.connector.common.tool.ConnectorJobRunner.runJob(ConnectorJobRunner.java:140)
        ... 12 more



",0
143,34319959,pagination in teradata in ascending order,"I am trying to paginate sql query in teradata. Currently I have query as shown below:

 SELECT RANK(name) as rank_,id,name,address FROM tbl_users ORDER BY name
 QUALIFY rank_ BETWEEN 1 and 5


I want to achieve pagination in ascending order with column 'name' but this query fetches last five rows of the result set. 
For eg if the values of column 'name' is like 'a','b','c','d','e'......'v','w','x','y','z'

I was expecting values in the order 'a','b','c','d','e' but i am getting 'v','w','x','y','z'.
How do I fix this?
",0,-1,-1.0,"I am trying to paginate sql query in teradata. Currently I have query as shown below:

 SELECT RANK(name) as rank_,id,name,address FROM tbl_users ORDER BY name
 QUALIFY rank_ BETWEEN 1 and 5


I want to achieve pagination in ascending order with column 'name' but this query fetches last five rows of the result set. 
For eg if the values of column 'name' is like 'a','b','c','d','e'......'v','w','x','y','z'

I was expecting values in the order 'a','b','c','d','e' but i am getting 'v','w','x','y','z'.
How do I fix this?
",3
144,34454426,sql query in teradata,"I am working on a query that will result in returning Names of top ten people from table with highest length.
The query is as :

SELECT name, COUNT(*) As frequency
FROM    loadbise.student
GROUP   BY name
ORDER   BY COUNT(*) DESC
Where   name is Not Null
sample 10;


It returns me top ten from bottom. I want to select top ten from most highest value to 2nd most highest value and so on.
Top function is not working on teradata 6 (my version).
",-1,-1,-1.0,"I am working on a query that will result in returning Names of top ten people from table with highest length.
The query is as :

SELECT name, COUNT(*) As frequency
FROM    loadbise.student
GROUP   BY name
ORDER   BY COUNT(*) DESC
Where   name is Not Null
sample 10;


It returns me top ten from bottom. I want to select top ten from most highest value to 2nd most highest value and so on.
Top function is not working on teradata 6 (my version).
",3
145,34542019,Oracle To Teradata Record Validation,"Project Background:

I am part of data migration project. Data is to be migrated from one platform(oracle) to another platform(teradata). My project requirement is that I have to compare whole data of each table between these two databases. But my problem is that client is not allowing us to create temp table at target database. So unable to use minus query for full data validation at target side.

When table has less data suppose less than 100,000 row. In this case it is easy for me to compare table data using excel(after importing in two different excel and then compare using excel in built compare tool or macro) But when row count is more than 100,000 suppose 10 or 200,000. In this case I am unable to use excel for full data validation.

Project belongs to banking sector so client is not allowing us to use any third party tool for data comparison.

My question is ""How do I validate full data between two different database platform without using minus query in data migration project"" Please help me in this scenario.
",1,-1,-1.0,"Project Background:

I am part of data migration project. Data is to be migrated from one platform(oracle) to another platform(teradata). My project requirement is that I have to compare whole data of each table between these two databases. But my problem is that client is not allowing us to create temp table at target database. So unable to use minus query for full data validation at target side.

When table has less data suppose less than 100,000 row. In this case it is easy for me to compare table data using excel(after importing in two different excel and then compare using excel in built compare tool or macro) But when row count is more than 100,000 suppose 10 or 200,000. In this case I am unable to use excel for full data validation.

Project belongs to banking sector so client is not allowing us to use any third party tool for data comparison.

My question is ""How do I validate full data between two different database platform without using minus query in data migration project"" Please help me in this scenario.
",3
146,34597592,Find the first instance of a null value in a group as long as no non null comes after - Teradata SQL,"I am trying to find the very first row where a certain field is null but the caveat is there cannot be a non-null coming after.  If there isn't a null value or a non-null comes after the null then I do not want to return that one at all. I am using Teradata SQL and the following mock dataset should illustrate what I am looking for.

ID | Date      | Field_Of_Interest
A  | 1/1/2015  | 1
A  | 2/1/2015  | 1
A  | 3/1/2015  | 
A  | 4/1/2015  | 
A  | 5/1/2015  | 
B  | 1/1/2015  | 1
B  | 2/1/2015  | 1
B  | 3/1/2015  | 
B  | 4/1/2015  | 1
B  | 5/1/2015  | 
C  | 1/1/2015  | 1
C  | 2/1/2015  | 1
C  | 3/1/2015  | 1
C  | 4/1/2015  | 1
C  | 5/1/2015  | 1
D  | 1/1/2015  | 1
D  | 2/1/2015  | 1
D  | 3/1/2015  | 
D  | 4/1/2015  | 
D  | 5/1/2015  | 1


Desired Result:

ID | Date      
A  | 3/1/2015
B  | 5/1/2015


Since C and D have a non-null for the last record I do not want them all all.

Where I run into trouble are situations like B or D where I can't just take the minimum of the date field where Field_Of_Interest is null.  Another thought I had was to find the min where null and the max where not null and if the date for the min was greater than that of the max use that.  The problem there is in B where a non-null came after a null and then it went back to null.

Any ideas?
",-1,0,-1.0,"I am trying to find the very first row where a certain field is null but the caveat is there cannot be a non-null coming after.  If there isn't a null value or a non-null comes after the null then I do not want to return that one at all. I am using Teradata SQL and the following mock dataset should illustrate what I am looking for.

ID | Date      | Field_Of_Interest
A  | 1/1/2015  | 1
A  | 2/1/2015  | 1
A  | 3/1/2015  | 
A  | 4/1/2015  | 
A  | 5/1/2015  | 
B  | 1/1/2015  | 1
B  | 2/1/2015  | 1
B  | 3/1/2015  | 
B  | 4/1/2015  | 1
B  | 5/1/2015  | 
C  | 1/1/2015  | 1
C  | 2/1/2015  | 1
C  | 3/1/2015  | 1
C  | 4/1/2015  | 1
C  | 5/1/2015  | 1
D  | 1/1/2015  | 1
D  | 2/1/2015  | 1
D  | 3/1/2015  | 
D  | 4/1/2015  | 
D  | 5/1/2015  | 1


Desired Result:

ID | Date      
A  | 3/1/2015
B  | 5/1/2015


Since C and D have a non-null for the last record I do not want them all all.

Where I run into trouble are situations like B or D where I can't just take the minimum of the date field where Field_Of_Interest is null.  Another thought I had was to find the min where null and the max where not null and if the date for the min was greater than that of the max use that.  The problem there is in B where a non-null came after a null and then it went back to null.

Any ideas?
",3
147,34853567,How to iterate properly over cursor in teradata sql?,"How to iterate  correctly over cursor in Teradata SQL?

while sql code = 0 
fetch cursor_name into (...)
do something...
end while;


This doesn't work properly. I had heard about something like 
for loop cursor, but I found only comparison to casual cursor in documentation, there is no declaration example NOWHERE.
",-1,-1,-1.0,"How to iterate  correctly over cursor in Teradata SQL?

while sql code = 0 
fetch cursor_name into (...)
do something...
end while;


This doesn't work properly. I had heard about something like 
for loop cursor, but I found only comparison to casual cursor in documentation, there is no declaration example NOWHERE.
",3
148,34874254,Error in sqoop action in oozie while fetching data from teradata to hive,"I am using HDP 2.3. I am getting the following error in sqoop action in oozie while fetching data from teradata to hive.

Sqoop action in my workflow.xml:

 &lt;sqoop xmlns=""uri:oozie:sqoop-action:0.3""&gt;
        &lt;job-tracker&gt;${hadoop_jobTrackerURL}&lt;/job-tracker&gt;
            &lt;name-node&gt;${hadoop_nameNodeURL}&lt;/name-node&gt;
            &lt;job-xml&gt;lib/hive-site.xml&lt;/job-xml&gt;

  &lt;configuration&gt;
        &lt;property&gt;
              &lt;name&gt;oozie.launcher.mapreduce.user.classpath.first&lt;/name&gt;
                    &lt;value&gt;true&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
             &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${hadoop_yarnQueueName}&lt;/value&gt;
       &lt;/property&gt;
  &lt;/configuration&gt;

  &lt;arg&gt;import&lt;/arg&gt;
  &lt;arg&gt;--connect&lt;/arg&gt;
  &lt;arg&gt;jdbc:teradata://192.168.145.129/DBS_PORT=1025,DATABASE=DS_TBL_DB&lt;/arg&gt;
  &lt;arg&gt;--username&lt;/arg&gt;
  &lt;arg&gt;dbc&lt;/arg&gt;
  &lt;arg&gt;--password&lt;/arg&gt;
  &lt;arg&gt;dbc&lt;/arg&gt;
  &lt;arg&gt;--driver&lt;/arg&gt;
  &lt;arg&gt;com.teradata.jdbc.TeraDriver&lt;/arg&gt;
  &lt;arg&gt;--query&lt;/arg&gt;
  &lt;arg&gt;select * from ds_tbl_db.catalog_page WHERE $CONDITIONS&lt;/arg&gt;
  &lt;arg&gt;--hive-import&lt;/arg&gt;
  &lt;arg&gt;--hive-table&lt;/arg&gt;
  &lt;arg&gt;catalog_page123&lt;/arg&gt;
  &lt;arg&gt;--target-dir&lt;/arg&gt;
  &lt;arg&gt;/user/root/db/catalog_page1234&lt;/arg&gt;
  &lt;arg&gt;-m&lt;/arg&gt;
  &lt;arg&gt;1&lt;/arg&gt;
  &lt;arg&gt;--verbose&lt;/arg&gt;




NOTE: I added tdgssconfig.jar &amp; terajdbc4.jar and all hive dependency jars in /share/lib. Also tried with including the dependencies in the lib folder for workflow.

Error Stack: 


  ERROR [main] tool.ImportTool (ImportTool.java:run(613)) - Encountered
  IOException running import job: java.io.IOException: Cannot run
  program ""hive"": error=13, Permission denied
          at java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)
          at java.lang.Runtime.exec(Runtime.java:617)
          at java.lang.Runtime.exec(Runtime.java:528)
          at org.apache.sqoop.util.Executor.exec(Executor.java:76)
          at org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:391)
          at org.apache.sqoop.hive.HiveImport.executeScript(HiveImport.java:344)
          at org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:245)
          at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:514)
          at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605)
          at org.apache.sqoop.Sqoop.run(Sqoop.java:148)
          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
          at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:184)
          at org.apache.sqoop.Sqoop.runTool(Sqoop.java:226)
          at org.apache.sqoop.Sqoop.runTool(Sqoop.java:235)
          at org.apache.sqoop.Sqoop.main(Sqoop.java:244)
          at org.apache.oozie.action.hadoop.SqoopMain.runSqoopJob(SqoopMain.java:197)
          at org.apache.oozie.action.hadoop.SqoopMain.run(SqoopMain.java:177)
          at org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:47)
          at org.apache.oozie.action.hadoop.SqoopMain.main(SqoopMain.java:46)
          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
          at java.lang.reflect.Method.invoke(Method.java:606)
          at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:236)
          at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
          at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
          at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
          at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
          at java.security.AccessController.doPrivileged(Native Method)
          at javax.security.auth.Subject.doAs(Subject.java:415)
          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
          at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: java.io.IOException: error=13, Permission denied
          at java.lang.UNIXProcess.forkAndExec(Native Method)
          at java.lang.UNIXProcess.(UNIXProcess.java:186)
          at java.lang.ProcessImpl.start(ProcessImpl.java:130)
          at java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)
          ... 31 more

",-1,-1,-1.0,"I am using HDP 2.3. I am getting the following error in sqoop action in oozie while fetching data from teradata to hive.

Sqoop action in my workflow.xml:

 &lt;sqoop xmlns=""uri:oozie:sqoop-action:0.3""&gt;
        &lt;job-tracker&gt;${hadoop_jobTrackerURL}&lt;/job-tracker&gt;
            &lt;name-node&gt;${hadoop_nameNodeURL}&lt;/name-node&gt;
            &lt;job-xml&gt;lib/hive-site.xml&lt;/job-xml&gt;

  &lt;configuration&gt;
        &lt;property&gt;
              &lt;name&gt;oozie.launcher.mapreduce.user.classpath.first&lt;/name&gt;
                    &lt;value&gt;true&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
             &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${hadoop_yarnQueueName}&lt;/value&gt;
       &lt;/property&gt;
  &lt;/configuration&gt;

  &lt;arg&gt;import&lt;/arg&gt;
  &lt;arg&gt;--connect&lt;/arg&gt;
  &lt;arg&gt;jdbc:teradata://192.168.145.129/DBS_PORT=1025,DATABASE=DS_TBL_DB&lt;/arg&gt;
  &lt;arg&gt;--username&lt;/arg&gt;
  &lt;arg&gt;dbc&lt;/arg&gt;
  &lt;arg&gt;--password&lt;/arg&gt;
  &lt;arg&gt;dbc&lt;/arg&gt;
  &lt;arg&gt;--driver&lt;/arg&gt;
  &lt;arg&gt;com.teradata.jdbc.TeraDriver&lt;/arg&gt;
  &lt;arg&gt;--query&lt;/arg&gt;
  &lt;arg&gt;select * from ds_tbl_db.catalog_page WHERE $CONDITIONS&lt;/arg&gt;
  &lt;arg&gt;--hive-import&lt;/arg&gt;
  &lt;arg&gt;--hive-table&lt;/arg&gt;
  &lt;arg&gt;catalog_page123&lt;/arg&gt;
  &lt;arg&gt;--target-dir&lt;/arg&gt;
  &lt;arg&gt;/user/root/db/catalog_page1234&lt;/arg&gt;
  &lt;arg&gt;-m&lt;/arg&gt;
  &lt;arg&gt;1&lt;/arg&gt;
  &lt;arg&gt;--verbose&lt;/arg&gt;




NOTE: I added tdgssconfig.jar &amp; terajdbc4.jar and all hive dependency jars in /share/lib. Also tried with including the dependencies in the lib folder for workflow.

Error Stack: 


  ERROR [main] tool.ImportTool (ImportTool.java:run(613)) - Encountered
  IOException running import job: java.io.IOException: Cannot run
  program ""hive"": error=13, Permission denied
          at java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)
          at java.lang.Runtime.exec(Runtime.java:617)
          at java.lang.Runtime.exec(Runtime.java:528)
          at org.apache.sqoop.util.Executor.exec(Executor.java:76)
          at org.apache.sqoop.hive.HiveImport.executeExternalHiveScript(HiveImport.java:391)
          at org.apache.sqoop.hive.HiveImport.executeScript(HiveImport.java:344)
          at org.apache.sqoop.hive.HiveImport.importTable(HiveImport.java:245)
          at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:514)
          at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605)
          at org.apache.sqoop.Sqoop.run(Sqoop.java:148)
          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
          at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:184)
          at org.apache.sqoop.Sqoop.runTool(Sqoop.java:226)
          at org.apache.sqoop.Sqoop.runTool(Sqoop.java:235)
          at org.apache.sqoop.Sqoop.main(Sqoop.java:244)
          at org.apache.oozie.action.hadoop.SqoopMain.runSqoopJob(SqoopMain.java:197)
          at org.apache.oozie.action.hadoop.SqoopMain.run(SqoopMain.java:177)
          at org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:47)
          at org.apache.oozie.action.hadoop.SqoopMain.main(SqoopMain.java:46)
          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
          at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
          at java.lang.reflect.Method.invoke(Method.java:606)
          at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:236)
          at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
          at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
          at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
          at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
          at java.security.AccessController.doPrivileged(Native Method)
          at javax.security.auth.Subject.doAs(Subject.java:415)
          at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
          at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: java.io.IOException: error=13, Permission denied
          at java.lang.UNIXProcess.forkAndExec(Native Method)
          at java.lang.UNIXProcess.(UNIXProcess.java:186)
          at java.lang.ProcessImpl.start(ProcessImpl.java:130)
          at java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)
          ... 31 more

",0
149,34910613,Teradata LEFT() function issue,"I run the following query against Teradata using SAS, and it works fine 

SELECT LEFT(first_name,7) 
FROM testTab 


However, when I run the same query in Sql Assistant it throws an error : Expected something between keyword SELECT and keyword LEFT. 

On my other computer, the above code runs on Sql Assistant. 

So, my question is, why does the LEFT function work sometimes, and sometimes it doesn't ? 

There are a lot of workarounds, but I want to know what the hell is going on with this LEFT function ? 
",-1,-1,-1.0,"I run the following query against Teradata using SAS, and it works fine 

SELECT LEFT(first_name,7) 
FROM testTab 


However, when I run the same query in Sql Assistant it throws an error : Expected something between keyword SELECT and keyword LEFT. 

On my other computer, the above code runs on Sql Assistant. 

So, my question is, why does the LEFT function work sometimes, and sometimes it doesn't ? 

There are a lot of workarounds, but I want to know what the hell is going on with this LEFT function ? 
",3
150,35120257,OLE DB provider for Teradata 15.10.04?,"I'm setting up a brand new system and decided to install TTU 15.10.04 (my old machine had TTU 14).  When I run my apps, I get this error:

The 'TDOLEDB' provider is not registered on the local machine. 

I used to get this error on earlier versions, but all I had to do was make sure my app was running in 32-bit mode.  After checking everything multiple times and not being able to isolate the problem, I searched for the OLE DB installation folder on my new machine, but have not been able to find it.  So I checked my old machine and found that it was installed here:

C:\Program Files (x86)\Teradata\Client\14.00\OLE DB Provider for Teradata

I have no such equivalent folder on my new machine.  The only thing I have is OLE DB Access Module, but I am sure that's not it.  I have concluded that I do not have the OLE DB provider installed at all and cannot seem to find out where to get it.  It's as if it has disappeared.  Any help would be great
",-1,-1,-1.0,"I'm setting up a brand new system and decided to install TTU 15.10.04 (my old machine had TTU 14).  When I run my apps, I get this error:

The 'TDOLEDB' provider is not registered on the local machine. 

I used to get this error on earlier versions, but all I had to do was make sure my app was running in 32-bit mode.  After checking everything multiple times and not being able to isolate the problem, I searched for the OLE DB installation folder on my new machine, but have not been able to find it.  So I checked my old machine and found that it was installed here:

C:\Program Files (x86)\Teradata\Client\14.00\OLE DB Provider for Teradata

I have no such equivalent folder on my new machine.  The only thing I have is OLE DB Access Module, but I am sure that's not it.  I have concluded that I do not have the OLE DB provider installed at all and cannot seem to find out where to get it.  It's as if it has disappeared.  Any help would be great
",1
151,35204076,Execute Dynamic DDL statement in teradata stored procedure?,"How to Execute Dynamic DDL statements IN TERADATA?

CREATE PROCEDURE DROP_INDEXES(IN indexs varchar(1000),IN p_database VARCHAR  (8000),IN p_table varchar(8000))
BEGIN
    DECLARE L_SQL VARCHAR(400);
    SET L_SQL= 'DROP INDEX '||trim(indexs)||' ON '||trim(db_name)|| '.'|| trim(tablename); 
    EXECUTE IMMEDIATE L_SQL;        
END ;


I need to call this child_procedure(DROP_INDEXES) from parent procedure, but during executing of the parent_procedure, after executing this procedure  

 CALL DROP_INDEXES(indexs,db_name,tablename); 


automatically gets exit from the parent_procedure, the next statement is not executing from parent_procedure.

This is the error i'm getting:

Executed as Single statement.  Failed [3722 : HY000] SP_DROP_INDEXES:
Only a COMMIT WORK or null statement is legal after a DDL Statement. 
Elapsed time = 00:00:00.326 


Kindly do help me regarding my issue.

Thanks in advance.
",-1,-1,-1.0,"How to Execute Dynamic DDL statements IN TERADATA?

CREATE PROCEDURE DROP_INDEXES(IN indexs varchar(1000),IN p_database VARCHAR  (8000),IN p_table varchar(8000))
BEGIN
    DECLARE L_SQL VARCHAR(400);
    SET L_SQL= 'DROP INDEX '||trim(indexs)||' ON '||trim(db_name)|| '.'|| trim(tablename); 
    EXECUTE IMMEDIATE L_SQL;        
END ;


I need to call this child_procedure(DROP_INDEXES) from parent procedure, but during executing of the parent_procedure, after executing this procedure  

 CALL DROP_INDEXES(indexs,db_name,tablename); 


automatically gets exit from the parent_procedure, the next statement is not executing from parent_procedure.

This is the error i'm getting:

Executed as Single statement.  Failed [3722 : HY000] SP_DROP_INDEXES:
Only a COMMIT WORK or null statement is legal after a DDL Statement. 
Elapsed time = 00:00:00.326 


Kindly do help me regarding my issue.

Thanks in advance.
",3
152,35207741,Teradata table creation and select statements fail when using LPAD and RPAD,"Background: The application I'm working on is not using any character delimiters. Fields are fixed length. Alphanumeric fields have to be left justified and space filled to the right, and numeric fields are right justified and zero-filled to the left.

I've been trying to accomplish this by using the RPAD and LPAD functions. The problem I'm running into is the error Teradata is displaying, ""Response Row size or Constant Row size overflow"". Each record if 4000 Bytes, and (from what I've read) the maximum size for each record in Teradata is 64KB, so I'm well under the maximum Teradata-allowed length.

Here is a small sample of the code that is generating an error;

SELECT
    RPAD(t1.MemberNbr, 20, ' ') AS MemberNbr
    ,RPAD(t1.LastName, 35, ' ') AS LastName
    ,RPAD(t1.FirstName, 25, ' ') AS FirstName
,CAST(t1.B_Day AS DATE FORMAT 'YYYYMMDD') (char(8)) AS BirthDay
FROM someTable AS t1


Can anyone explain to me why this isn't working? Thanks
",-1,-1,-1.0,"Background: The application I'm working on is not using any character delimiters. Fields are fixed length. Alphanumeric fields have to be left justified and space filled to the right, and numeric fields are right justified and zero-filled to the left.

I've been trying to accomplish this by using the RPAD and LPAD functions. The problem I'm running into is the error Teradata is displaying, ""Response Row size or Constant Row size overflow"". Each record if 4000 Bytes, and (from what I've read) the maximum size for each record in Teradata is 64KB, so I'm well under the maximum Teradata-allowed length.

Here is a small sample of the code that is generating an error;

SELECT
    RPAD(t1.MemberNbr, 20, ' ') AS MemberNbr
    ,RPAD(t1.LastName, 35, ' ') AS LastName
    ,RPAD(t1.FirstName, 25, ' ') AS FirstName
,CAST(t1.B_Day AS DATE FORMAT 'YYYYMMDD') (char(8)) AS BirthDay
FROM someTable AS t1


Can anyone explain to me why this isn't working? Thanks
",3
153,35217223,Sqoop: converting string to date when exporting data from HDFS to Teradata,"I have a table in teradata and one column, update_date, is defined as DATE. Format is YYYYMMDD. I need to upload files in HDFS to the table. The files are comma separated text files. The column that corresponds to update_date is a string of format 'YYYYMMDD'.

When I tried to export the data using command:

sqoop export --connect jdbc:teradata://tdwc/DATABASE=sandbox --username xxxx --password yyyy --table mytable --export-dir hftp://gdoop-namenode/path/to/myfiles/ 


The command failed with error 

java.io.IOException: Can't export data, please check failed map task logs
at org.apache.sqoop.mapreduce.TextExportMapper.map(TextExportMapper.java:112)
at org.apache.sqoop.mapreduce.TextExportMapper.map(TextExportMapper.java:39)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:140)
at org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:672)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:330)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438) 
at org.apache.hadoop.mapred.Child.main(Child.java:262)  
Caused by: java.lang.RuntimeException: Can't parse input data: '20160129'


How can I resolve this without changing the schema of the table?
",-1,-1,-1.0,"I have a table in teradata and one column, update_date, is defined as DATE. Format is YYYYMMDD. I need to upload files in HDFS to the table. The files are comma separated text files. The column that corresponds to update_date is a string of format 'YYYYMMDD'.

When I tried to export the data using command:

sqoop export --connect jdbc:teradata://tdwc/DATABASE=sandbox --username xxxx --password yyyy --table mytable --export-dir hftp://gdoop-namenode/path/to/myfiles/ 


The command failed with error 

java.io.IOException: Can't export data, please check failed map task logs
at org.apache.sqoop.mapreduce.TextExportMapper.map(TextExportMapper.java:112)
at org.apache.sqoop.mapreduce.TextExportMapper.map(TextExportMapper.java:39)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:140)
at org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:672)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:330)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438) 
at org.apache.hadoop.mapred.Child.main(Child.java:262)  
Caused by: java.lang.RuntimeException: Can't parse input data: '20160129'


How can I resolve this without changing the schema of the table?
",0
154,35292661,IN operator is not working when using list as a input to teradata query in mule,"I have a POC where I am taking data from teradata and updating it in salesforce. I am taking input from itemids which separated by comma like '2345','456'.

I'm first getting this value and setting it as payload. But in query we cannot use it directly. I've tried another solution of appending '' to values and ','. Hence payload becomes a string like '2345','456'. But when I'm passing this in query I'm getting 'bad parameter'. Anyone has any solution for it?
",-1,-1,-1.0,"I have a POC where I am taking data from teradata and updating it in salesforce. I am taking input from itemids which separated by comma like '2345','456'.

I'm first getting this value and setting it as payload. But in query we cannot use it directly. I've tried another solution of appending '' to values and ','. Hence payload becomes a string like '2345','456'. But when I'm passing this in query I'm getting 'bad parameter'. Anyone has any solution for it?
",3
155,35316367,data extraction using sqoop from Teradata,"I've been trying a sqoop import on a 2TB table from Teradata DB to Hive. It's full table import I'm trying of. The query generated in background is very simple select:

SELECT A, B, C FROM TABLE WHERE (A&gt;=0) AND (A&lt;100000);


The range is decided based on min, max values of a column and the number of mappers I provide in the sqoop query of course.

The question might not be very sqoop specific but because it is used in the activity, I'm tagging it as well here. It's the spool space in Teradata DB that gets full and reaches upto a shocking 8 TB. The table hardly is 2 TB, while the spool occupied reaches 8. 

I'm really not sure what goes on in the Teradata DB background that the spool shoots up this high. Does anybody has any idea on what's actually going on in the background and any workaround on this?

Thanks.
",-1,-1,-1.0,"I've been trying a sqoop import on a 2TB table from Teradata DB to Hive. It's full table import I'm trying of. The query generated in background is very simple select:

SELECT A, B, C FROM TABLE WHERE (A&gt;=0) AND (A&lt;100000);


The range is decided based on min, max values of a column and the number of mappers I provide in the sqoop query of course.

The question might not be very sqoop specific but because it is used in the activity, I'm tagging it as well here. It's the spool space in Teradata DB that gets full and reaches upto a shocking 8 TB. The table hardly is 2 TB, while the spool occupied reaches 8. 

I'm really not sure what goes on in the Teradata DB background that the spool shoots up this high. Does anybody has any idea on what's actually going on in the background and any workaround on this?

Thanks.
",3
156,35380084,Filtering out a group from a Teradata / SQL statement originated from a concatenate,"I concatenated the year and month values of a date and I'm trying to exclude one combination from the output with a HAVING statement. However, it continues to show up.

SELECT EXTRACT(YEAR from saledate) || EXTRACT(MONTH from saledate) AS yearmonth, SUM(amt)
FROM trnsact
GROUP BY yearmonth
HAVING yearmonth&lt;&gt;'2005 8'
;


I started using != in the HAVING statement but the Teradata documentation suggested to use any of the following:

&lt;&gt;
^=
NE
NOT=


I've tried all of them and the group that I'm trying to exclude still shows up. I also tried using a WHERE statement instead.

I know I could avoid the concatenate and just create two conditions but I would like to understand why is this route not working and hopefully fix it!

Why is the WHERE or HAVING not filtering out concatenate?
",-1,-1,-1.0,"I concatenated the year and month values of a date and I'm trying to exclude one combination from the output with a HAVING statement. However, it continues to show up.

SELECT EXTRACT(YEAR from saledate) || EXTRACT(MONTH from saledate) AS yearmonth, SUM(amt)
FROM trnsact
GROUP BY yearmonth
HAVING yearmonth&lt;&gt;'2005 8'
;


I started using != in the HAVING statement but the Teradata documentation suggested to use any of the following:

&lt;&gt;
^=
NE
NOT=


I've tried all of them and the group that I'm trying to exclude still shows up. I also tried using a WHERE statement instead.

I know I could avoid the concatenate and just create two conditions but I would like to understand why is this route not working and hopefully fix it!

Why is the WHERE or HAVING not filtering out concatenate?
",3
157,35389577,Connecting HDP from Teradata Studio Express 15.10,"I've been trying to connect my HDP cluster(2.1) from Teradata Studio Express 15.10 via Knox but couldn't succeed. All the configurations namely the JDBC hostname, port, user/password, WebHCat host,port I'm providing as correct. The JDBC connection is working perfectly if I test it from the beeline shell.

beeline&gt; !connect jdbc:hive2://xx.xx.xxx.xxx:10000
scan complete in 11ms
Connecting to jdbc:hive2://xx.xx.xxx.xxx:10000
Enter username for jdbc:hive2://xx.xx.xxx.xxx:10000: hive
Enter password for jdbc:hive2://xx.xx.xxx.xxx:10000: ****
Connected to: Apache Hive (version 0.13.0.2.1.2.2-516)
Driver: Hive JDBC (version 0.13.0.2.1.2.2-516)
Transaction isolation: TRANSACTION_REPEATABLE_READ


However I don't find my conviction on WebHCat username it's asking of. Not sure what's the default one for HDP or how a new one could be defined. The custom webhcat-site.xml properties ""webhcat.proxyuser.hue.groups"" &amp; ""webhcat.proxyuser.hue.hosts"" won't help me out here.

When I try the connectivity from Teradata Studio Express GUI, it gives the following error:

java.lang.Exception: Could not establish connection to jdbc:hive2://xx.xx.xxx.xxx:10000/default?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice: org.apache.http.conn.HttpHostConnectException: Connection to http://xx.xx.xxx.xxx:10000 refused
    at com.teradata.datatools.hadoop.hive.connectivity.HiveConnection.openJdbcConnection(HiveConnection.java:286)
    at com.teradata.datatools.hadoop.hive.connectivity.HiveConnection.createConnection(HiveConnection.java:199)
    at org.eclipse.datatools.connectivity.DriverConnectionBase.internalCreateConnection(DriverConnectionBase.java:105)
    at org.eclipse.datatools.connectivity.DriverConnectionBase.open(DriverConnectionBase.java:54)
    at com.teradata.datatools.hadoop.hive.connectivity.HiveConnection.open(HiveConnection.java:144)
    at com.teradata.datatools.hadoop.hive.connectivity.HivePingFactory.createJdbcConnection(HivePingFactory.java:44)
    at com.teradata.datatools.hadoop.hive.connectivity.PingJdbcJob.createTestConnection(PingJdbcJob.java:30)
    at com.teradata.datatools.hadoop.hive.connectivity.PingJob.run(PingJob.java:42)
    at org.eclipse.core.internal.jobs.Worker.run(Worker.java:54)


Anybody got any idea on what's happening here? Any pointers would be appreciated.
",-1,-1,-1.0,"I've been trying to connect my HDP cluster(2.1) from Teradata Studio Express 15.10 via Knox but couldn't succeed. All the configurations namely the JDBC hostname, port, user/password, WebHCat host,port I'm providing as correct. The JDBC connection is working perfectly if I test it from the beeline shell.

beeline&gt; !connect jdbc:hive2://xx.xx.xxx.xxx:10000
scan complete in 11ms
Connecting to jdbc:hive2://xx.xx.xxx.xxx:10000
Enter username for jdbc:hive2://xx.xx.xxx.xxx:10000: hive
Enter password for jdbc:hive2://xx.xx.xxx.xxx:10000: ****
Connected to: Apache Hive (version 0.13.0.2.1.2.2-516)
Driver: Hive JDBC (version 0.13.0.2.1.2.2-516)
Transaction isolation: TRANSACTION_REPEATABLE_READ


However I don't find my conviction on WebHCat username it's asking of. Not sure what's the default one for HDP or how a new one could be defined. The custom webhcat-site.xml properties ""webhcat.proxyuser.hue.groups"" &amp; ""webhcat.proxyuser.hue.hosts"" won't help me out here.

When I try the connectivity from Teradata Studio Express GUI, it gives the following error:

java.lang.Exception: Could not establish connection to jdbc:hive2://xx.xx.xxx.xxx:10000/default?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice: org.apache.http.conn.HttpHostConnectException: Connection to http://xx.xx.xxx.xxx:10000 refused
    at com.teradata.datatools.hadoop.hive.connectivity.HiveConnection.openJdbcConnection(HiveConnection.java:286)
    at com.teradata.datatools.hadoop.hive.connectivity.HiveConnection.createConnection(HiveConnection.java:199)
    at org.eclipse.datatools.connectivity.DriverConnectionBase.internalCreateConnection(DriverConnectionBase.java:105)
    at org.eclipse.datatools.connectivity.DriverConnectionBase.open(DriverConnectionBase.java:54)
    at com.teradata.datatools.hadoop.hive.connectivity.HiveConnection.open(HiveConnection.java:144)
    at com.teradata.datatools.hadoop.hive.connectivity.HivePingFactory.createJdbcConnection(HivePingFactory.java:44)
    at com.teradata.datatools.hadoop.hive.connectivity.PingJdbcJob.createTestConnection(PingJdbcJob.java:30)
    at com.teradata.datatools.hadoop.hive.connectivity.PingJob.run(PingJob.java:42)
    at org.eclipse.core.internal.jobs.Worker.run(Worker.java:54)


Anybody got any idea on what's happening here? Any pointers would be appreciated.
",0
158,35706223,Teradata Hadoop Connector using split by amp,"I am using TDCH for pulling data from Teradata to  hadoop cluster using TDCH.
In TDCH i am using split by amp option .I have 120 amp Teradata System and in my TDCH script i am defining 30 mappers. So each will pull data from 4 AMP .I am getting like below queries :-

'Select ""NAME"" ,""ADRESS"" FROM FROM tdampcopy(ON ""TABLENAME"" USING AMPList(136,137,138,139)) AS THCALIAS1 .


It is creating 30 queries with 30 session ids. When they are running it on Teradata System , it is causing skewness as in each query only 4 amps are invloved and DBAs are killing these queries. Any pointers to how we can overcome with this.
",0,-1,-1.0,"I am using TDCH for pulling data from Teradata to  hadoop cluster using TDCH.
In TDCH i am using split by amp option .I have 120 amp Teradata System and in my TDCH script i am defining 30 mappers. So each will pull data from 4 AMP .I am getting like below queries :-

'Select ""NAME"" ,""ADRESS"" FROM FROM tdampcopy(ON ""TABLENAME"" USING AMPList(136,137,138,139)) AS THCALIAS1 .


It is creating 30 queries with 30 session ids. When they are running it on Teradata System , it is causing skewness as in each query only 4 amps are invloved and DBAs are killing these queries. Any pointers to how we can overcome with this.
",3
159,35884934,Sample Java Program to call Teradata Stored Procedure,"I want to call Teradata store procedure from Java program, Can anyone have the sample java program ?

Due to some restriction,I couldn't create some UDF functions inside Teradata, so I developed those function as stored procedure in Teradata. 

Now, I need to call those procedures using Java Program.

Friends, These is my java program, 

public static String executeme(String x,String y) throws SQLException, ClassNotFoundException
 {
     System.out.println("" Message: 2 "" );
     String connectionString = ""jdbc:teradata://192.168.0.0/xyz,tmode=ANSI,charset=UTF8,DBS_PORT=1025,DATABASE=xyz"";
     String user = ""xyz"";
     String passwd = ""xxx"";
     System.out.println("" Message: 3 after url "" +x );
     String sCall = ""{CALL ""+x+""(?,?)}"";
     System.out.println("" Message: after call str "" +sCall );
     String result = null;

     try
     {
         Class.forName(""com.teradata.jdbc.TeraDriver"");

         Connection con = DriverManager.getConnection(connectionString, user, passwd);  
         System.out.println("" Message: after con ""+con  );
         CallableStatement cStmt = con.prepareCall(sCall);
         cStmt.setString(1, y);
         cStmt.registerOutParameter(2, Types.VARCHAR);          
         result = cStmt.getString(2);   

         return result;  


     }       
     catch(ClassNotFoundException e)
     {
         System.out.println("" Message: "" + e);
         return e.toString();    
     }



 }
public static void main(String[] args) throws SQLException, ClassNotFoundException {
    // TODO Auto-generated method stub

    String outs=null; 
    System.out.println("" Message: 1 "" );
    outs = testing.executeme(""sample_name"", ""ABCD"");
     System.out.println(outs);
}


But, while running this program, i'm getting as error:

Message: 1 
Message: 2 
Message: 3 after url ment_f_swithin
Message: after con com.teradata.jdbc.jdk6.JDK6_SQL_Connection@85ede7b
Exception in thread ""main"" java.sql.SQLException: [Teradata Database] [TeraJDBC 14.10.00.42] [Error 5510] [SQLState HY000] Invalid session mode for   procedure execution.
at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException  (ErrorFactory.java:308)
at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:109)
at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:307)
at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:196)
at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:123)
at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:114)
at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:386)
at com.teradata.jdbc.jdbc_4.TDStatement.prepareRequest(TDStatement.java:573)
at com.teradata.jdbc.jdbc_4.TDPreparedStatement.&lt;init&gt;(TDPreparedStatement.java:117)
at com.teradata.jdbc.jdk6.JDK6_SQL_PreparedStatement.&lt;init&gt;(JDK6_SQL_PreparedStatement.java:29)
at com.teradata.jdbc.jdk6.JDK6_SQL_CallableStatement.&lt;init&gt;(JDK6_SQL_CallableStatement.java:23)
at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.constructCallableStatement(JDK6_SQL_Connection.java:87)
at com.teradata.jdbc.jdbc_4.TDSession.prepareCall(TDSession.java:1373)
at com.teradata.jdbc.jdbc_4.TDSession.prepareCall(TDSession.java:1408)
at com.teradata.jdbc.jdbc_4.TDSession.prepareCall(TDSession.java:1394)
at java_func.testing.executeme(testing.java:32)
at java_func.testing.main(testing.java:55)


I tried using all the 3 modes ANSI,TERA,BTET. but still i'm getting the same error, kindly guide me please
",-1,-1,-1.0,"I want to call Teradata store procedure from Java program, Can anyone have the sample java program ?

Due to some restriction,I couldn't create some UDF functions inside Teradata, so I developed those function as stored procedure in Teradata. 

Now, I need to call those procedures using Java Program.

Friends, These is my java program, 

public static String executeme(String x,String y) throws SQLException, ClassNotFoundException
 {
     System.out.println("" Message: 2 "" );
     String connectionString = ""jdbc:teradata://192.168.0.0/xyz,tmode=ANSI,charset=UTF8,DBS_PORT=1025,DATABASE=xyz"";
     String user = ""xyz"";
     String passwd = ""xxx"";
     System.out.println("" Message: 3 after url "" +x );
     String sCall = ""{CALL ""+x+""(?,?)}"";
     System.out.println("" Message: after call str "" +sCall );
     String result = null;

     try
     {
         Class.forName(""com.teradata.jdbc.TeraDriver"");

         Connection con = DriverManager.getConnection(connectionString, user, passwd);  
         System.out.println("" Message: after con ""+con  );
         CallableStatement cStmt = con.prepareCall(sCall);
         cStmt.setString(1, y);
         cStmt.registerOutParameter(2, Types.VARCHAR);          
         result = cStmt.getString(2);   

         return result;  


     }       
     catch(ClassNotFoundException e)
     {
         System.out.println("" Message: "" + e);
         return e.toString();    
     }



 }
public static void main(String[] args) throws SQLException, ClassNotFoundException {
    // TODO Auto-generated method stub

    String outs=null; 
    System.out.println("" Message: 1 "" );
    outs = testing.executeme(""sample_name"", ""ABCD"");
     System.out.println(outs);
}


But, while running this program, i'm getting as error:

Message: 1 
Message: 2 
Message: 3 after url ment_f_swithin
Message: after con com.teradata.jdbc.jdk6.JDK6_SQL_Connection@85ede7b
Exception in thread ""main"" java.sql.SQLException: [Teradata Database] [TeraJDBC 14.10.00.42] [Error 5510] [SQLState HY000] Invalid session mode for   procedure execution.
at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException  (ErrorFactory.java:308)
at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:109)
at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:307)
at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:196)
at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:123)
at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:114)
at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:386)
at com.teradata.jdbc.jdbc_4.TDStatement.prepareRequest(TDStatement.java:573)
at com.teradata.jdbc.jdbc_4.TDPreparedStatement.&lt;init&gt;(TDPreparedStatement.java:117)
at com.teradata.jdbc.jdk6.JDK6_SQL_PreparedStatement.&lt;init&gt;(JDK6_SQL_PreparedStatement.java:29)
at com.teradata.jdbc.jdk6.JDK6_SQL_CallableStatement.&lt;init&gt;(JDK6_SQL_CallableStatement.java:23)
at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.constructCallableStatement(JDK6_SQL_Connection.java:87)
at com.teradata.jdbc.jdbc_4.TDSession.prepareCall(TDSession.java:1373)
at com.teradata.jdbc.jdbc_4.TDSession.prepareCall(TDSession.java:1408)
at com.teradata.jdbc.jdbc_4.TDSession.prepareCall(TDSession.java:1394)
at java_func.testing.executeme(testing.java:32)
at java_func.testing.main(testing.java:55)


I tried using all the 3 modes ANSI,TERA,BTET. but still i'm getting the same error, kindly guide me please
",0
160,35924605,Connecting to Teradata on mac via Python ODBC,"I have successfully gone through the process of installing pyodbc for Python 2.7, however when I run the following code:

cnx = pyodbc.connect(""DRIVER={Teradata};DBCNAME=&lt;DBCNAME&gt;;DATABASE=&lt;DB&gt;;UID=%s;PWD=%s"" % (username, password), autocommit=True, ANSI=True)


I get the following error:


  pyodbc.Error: ('00000', '[00000] [iODBC][Driver
  Manager]dlopen(/Library/Application
  Support/teradata/client/ODBC/lib/tdata.dylib, 6): Library not loaded:
  libtdparse.dylib\n  Referenced from: /Library/Application
  Support/teradata/client/ODBC/lib/tdata.dylib\n  Reason: unsafe use of
  relative (0) (SQLDriverConnect)')


Note: this is a different error than what appears in this post.
I am on OS X El Capitan 10.13, if that matters.
",-1,-1,-1.0,"I have successfully gone through the process of installing pyodbc for Python 2.7, however when I run the following code:

cnx = pyodbc.connect(""DRIVER={Teradata};DBCNAME=&lt;DBCNAME&gt;;DATABASE=&lt;DB&gt;;UID=%s;PWD=%s"" % (username, password), autocommit=True, ANSI=True)


I get the following error:


  pyodbc.Error: ('00000', '[00000] [iODBC][Driver
  Manager]dlopen(/Library/Application
  Support/teradata/client/ODBC/lib/tdata.dylib, 6): Library not loaded:
  libtdparse.dylib\n  Referenced from: /Library/Application
  Support/teradata/client/ODBC/lib/tdata.dylib\n  Reason: unsafe use of
  relative (0) (SQLDriverConnect)')


Note: this is a different error than what appears in this post.
I am on OS X El Capitan 10.13, if that matters.
",1
161,35938320,Connecting Python with Teradata using Teradata module,"I have installed python 2.7.0 and Teradata module on Windows 7. I am not able to connect and query TD from python.

pip install Teradata

Now I want to import teradata module in my source code and perform operations like -


Firing queries to teradata and get result set.
Check if connection is made to teradata.


Please help me writing code for the same as I am new to Python and there is no information available with me to connect to teradata.
",1,-1,-1.0,"I have installed python 2.7.0 and Teradata module on Windows 7. I am not able to connect and query TD from python.

pip install Teradata

Now I want to import teradata module in my source code and perform operations like -


Firing queries to teradata and get result set.
Check if connection is made to teradata.


Please help me writing code for the same as I am new to Python and there is no information available with me to connect to teradata.
",1
162,35999727,"Import of data from Teradata to SQL Server times out, regardless of Teradata timeout settings","I am using the SQL Server Import and Export wizard to pull data from Teradata into a SQL Server table:


In SSMS, I right-click my target database and select ""Import Data""
In ""Choose a Data Source"", I select "".NET Framework Data Provider for Teradata""
I change the Command Timeout and Connection Timeout to a high number, say, 3000 (I've also tried 0 and -1). 
I set the SQL Server data source
I specify the query to select the data (query takes 15 min+ to run in Toad)
I set the option to create the destination table in SQL Server
I execute the package.
The package fails in pre-execute after ~40 seconds (error below).


I can run this query fine from an SSIS package in Visual Studio, or if I add it to a job. Setting the command timeout there makes a difference. I want to be able to run one-offs directly from SSMS, without going to the trouble of creating an SSIS package every time.

I have no connection timeout limit on the SQL Server side (set to 0).

Error Text:

Pre-execute (Error)
Messages
Error 0xc0047062: Data Flow Task 1: Teradata.Client.Provider.TdException (0x80004005): [.NET Data Provider for Teradata] [100038] Command did not complete within the time specified (timeout).
[Teradata Database] [3110] The transaction was aborted by the user.
[Socket Transport] [115003] The receive operation timed out. ---&gt; System.Net.Sockets.SocketException (0x80004005): A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond
   at System.Net.Sockets.Socket.Receive(Byte[] buffer, Int32 offset, Int32 size, SocketFlags socketFlags)
   at Teradata.Client.Provider.WpTcpTransport.ReadLanHeader(Buffer buffer, Int32 timeout, Int32 readBytes)
   at Teradata.Client.Provider.WpTcpTransport.ReadLanHeader(Buffer buffer, Int32 timeout, Int32 readBytes)
   at Teradata.Client.Provider.WpTcpTransport.Receive(Buffer buffer, Int32 timeout)
   at Teradata.Client.Provider.WpSession.Receive(Buffer buffer, Int32 timeout)
   at Teradata.Client.Provider.WpMessageManager.Receive(Int32 timeout)
   at Teradata.Client.Provider.WpStartRequestManager.ReceiveStartMessage()
   at Teradata.Client.Provider.WpStartRequestManager.Action(ManagerActions step)
   at Teradata.Client.Provider.WpStartRequestManager.RedriveAction(ManagerActions step)
   at Teradata.Client.Provider.WpStartRequestManager.Action()
   at Teradata.Client.Provider.Request.ExecuteStartRequest(String commandText, TeraTypeBase[][] parameters, ExecutionMode executionMode, Boolean asynchronous, Boolean isTrustedRequest)
   at Teradata.Client.Provider.TdCommand.ExecuteRequest(CommandBehavior cmdBehavior, Boolean asynchronousCall)
   at Teradata.Client.Provider.TdCommand.ExecuteReader(CommandBehavior behavior)
   at Teradata.Client.Provider.TdCommand.ExecuteDbDataReader(CommandBehavior behavior)
   at System.Data.Common.DbCommand.System.Data.IDbCommand.ExecuteReader(CommandBehavior behavior)
   at Microsoft.SqlServer.Dts.Pipeline.DataReaderSourceAdapter.PreExecute()
   at Microsoft.SqlServer.Dts.Pipeline.ManagedComponentHost.HostPreExecute(IDTSManagedComponentWrapper100 wrapper) (SQL Server Import and Export Wizard)

Error 0xc004701a: Data Flow Task 1: Source - Query failed the pre-execute phase and returned error code 0x80004005.
 (SQL Server Import and Export Wizard)

Information 0x4004300b: Data Flow Task 1: ""Destination - MercTestTeradataPull"" wrote 0 rows.
 (SQL Server Import and Export Wizard)

",-1,-1,-1.0,"I am using the SQL Server Import and Export wizard to pull data from Teradata into a SQL Server table:


In SSMS, I right-click my target database and select ""Import Data""
In ""Choose a Data Source"", I select "".NET Framework Data Provider for Teradata""
I change the Command Timeout and Connection Timeout to a high number, say, 3000 (I've also tried 0 and -1). 
I set the SQL Server data source
I specify the query to select the data (query takes 15 min+ to run in Toad)
I set the option to create the destination table in SQL Server
I execute the package.
The package fails in pre-execute after ~40 seconds (error below).


I can run this query fine from an SSIS package in Visual Studio, or if I add it to a job. Setting the command timeout there makes a difference. I want to be able to run one-offs directly from SSMS, without going to the trouble of creating an SSIS package every time.

I have no connection timeout limit on the SQL Server side (set to 0).

Error Text:

Pre-execute (Error)
Messages
Error 0xc0047062: Data Flow Task 1: Teradata.Client.Provider.TdException (0x80004005): [.NET Data Provider for Teradata] [100038] Command did not complete within the time specified (timeout).
[Teradata Database] [3110] The transaction was aborted by the user.
[Socket Transport] [115003] The receive operation timed out. ---&gt; System.Net.Sockets.SocketException (0x80004005): A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond
   at System.Net.Sockets.Socket.Receive(Byte[] buffer, Int32 offset, Int32 size, SocketFlags socketFlags)
   at Teradata.Client.Provider.WpTcpTransport.ReadLanHeader(Buffer buffer, Int32 timeout, Int32 readBytes)
   at Teradata.Client.Provider.WpTcpTransport.ReadLanHeader(Buffer buffer, Int32 timeout, Int32 readBytes)
   at Teradata.Client.Provider.WpTcpTransport.Receive(Buffer buffer, Int32 timeout)
   at Teradata.Client.Provider.WpSession.Receive(Buffer buffer, Int32 timeout)
   at Teradata.Client.Provider.WpMessageManager.Receive(Int32 timeout)
   at Teradata.Client.Provider.WpStartRequestManager.ReceiveStartMessage()
   at Teradata.Client.Provider.WpStartRequestManager.Action(ManagerActions step)
   at Teradata.Client.Provider.WpStartRequestManager.RedriveAction(ManagerActions step)
   at Teradata.Client.Provider.WpStartRequestManager.Action()
   at Teradata.Client.Provider.Request.ExecuteStartRequest(String commandText, TeraTypeBase[][] parameters, ExecutionMode executionMode, Boolean asynchronous, Boolean isTrustedRequest)
   at Teradata.Client.Provider.TdCommand.ExecuteRequest(CommandBehavior cmdBehavior, Boolean asynchronousCall)
   at Teradata.Client.Provider.TdCommand.ExecuteReader(CommandBehavior behavior)
   at Teradata.Client.Provider.TdCommand.ExecuteDbDataReader(CommandBehavior behavior)
   at System.Data.Common.DbCommand.System.Data.IDbCommand.ExecuteReader(CommandBehavior behavior)
   at Microsoft.SqlServer.Dts.Pipeline.DataReaderSourceAdapter.PreExecute()
   at Microsoft.SqlServer.Dts.Pipeline.ManagedComponentHost.HostPreExecute(IDTSManagedComponentWrapper100 wrapper) (SQL Server Import and Export Wizard)

Error 0xc004701a: Data Flow Task 1: Source - Query failed the pre-execute phase and returned error code 0x80004005.
 (SQL Server Import and Export Wizard)

Information 0x4004300b: Data Flow Task 1: ""Destination - MercTestTeradataPull"" wrote 0 rows.
 (SQL Server Import and Export Wizard)

",0
163,36132167,Teradata external Java Stored Procedure error: No suitable driver found for jdbc:default:connection,"I wrote a Java stored procedure, packed it into a jar and installed it into the Teradata database. I want to use the default database connection as described here. Most of the code was generated by the Teradata wizard for stored procedures.

public class TestSql {
    public static void getEntryById(int id, String[] resultStrings) throws SQLException {
        Connection con = DriverManager.getConnection(""jdbc:default:connection"");

    String sql = ""SELECT x FROM TEST_TABLE WHERE ID = "" + id + "";"";

    Statement stmt = (Statement) con.createStatement();
    ResultSet rs1 = ((java.sql.Statement) stmt).executeQuery(sql);
    rs1.next();
    String resultString = rs1.getString(1);
    stmt.close();
    con.close();

    resultStrings[0] = resultString;
    }
}


I installed the jar:

CALL SQLJ.REPLACE_JAR('CJ!/my/path/Teradata-SqlTest.jar','test');


And created the procedure:

REPLACE PROCEDURE ""db"".""getEntryById"" (
    IN ""id"" INTEGER,
    OUT ""resultString"" VARCHAR(1024))
    LANGUAGE JAVA
    MODIFIES SQL DATA
    PARAMETER STYLE JAVA
    EXTERNAL NAME 'test:my.package.TestSql.getEntryById(int,java.lang.String[])';


Now when I call this procedure, I get this error message: 


  Executed as Single statement.  Failed [7827 : 39001] Java SQL Exception SQLSTATE 39001: Invalid SQL state (08001: No suitable driver found for jdbc:default:connection).


Now when I log off from Teradata and log on again and call the procedure, the error message becomes:


  Executed as Single statement.  Failed [7827 : 39001] A default connection for a Java Stored Procedure has not been established for this thread.). 


What is the problem here? I'm connecting to Teradata using the Eclipse plugin. Teradata v. 15.0.1.01.
",-1,-1,-1.0,"I wrote a Java stored procedure, packed it into a jar and installed it into the Teradata database. I want to use the default database connection as described here. Most of the code was generated by the Teradata wizard for stored procedures.

public class TestSql {
    public static void getEntryById(int id, String[] resultStrings) throws SQLException {
        Connection con = DriverManager.getConnection(""jdbc:default:connection"");

    String sql = ""SELECT x FROM TEST_TABLE WHERE ID = "" + id + "";"";

    Statement stmt = (Statement) con.createStatement();
    ResultSet rs1 = ((java.sql.Statement) stmt).executeQuery(sql);
    rs1.next();
    String resultString = rs1.getString(1);
    stmt.close();
    con.close();

    resultStrings[0] = resultString;
    }
}


I installed the jar:

CALL SQLJ.REPLACE_JAR('CJ!/my/path/Teradata-SqlTest.jar','test');


And created the procedure:

REPLACE PROCEDURE ""db"".""getEntryById"" (
    IN ""id"" INTEGER,
    OUT ""resultString"" VARCHAR(1024))
    LANGUAGE JAVA
    MODIFIES SQL DATA
    PARAMETER STYLE JAVA
    EXTERNAL NAME 'test:my.package.TestSql.getEntryById(int,java.lang.String[])';


Now when I call this procedure, I get this error message: 


  Executed as Single statement.  Failed [7827 : 39001] Java SQL Exception SQLSTATE 39001: Invalid SQL state (08001: No suitable driver found for jdbc:default:connection).


Now when I log off from Teradata and log on again and call the procedure, the error message becomes:


  Executed as Single statement.  Failed [7827 : 39001] A default connection for a Java Stored Procedure has not been established for this thread.). 


What is the problem here? I'm connecting to Teradata using the Eclipse plugin. Teradata v. 15.0.1.01.
",3
164,36366816,SAS OLEDB connection and Teradata Volatile table,"I have some SAS developers who do not have SAS access for Teradata Installed

and they are not on SAS query  grid . Instead they use OLEDB to connect to Teradata. 
OLEDB does not seem to like create volatile table statement. Has anyone had some luck using it and get VT's in teradata to work 

This is what's installed on the SAS W2K serveer 

     proc setinit; run;


  ---Base SAS Software
            30DEC2016
    ---SAS/STAT
            30DEC2016
    ---SAS/GRAPH
            30DEC2016
    ---SAS/Secure 168-bit
            30DEC2016
    ---SAS/Secure Windows
            30DEC2016
    ---SAS/ACCESS Interface to PC Files
            30DEC2016
    ---SAS/ACCESS Interface to ODBC
            30DEC2016
    ---SAS/ACCESS Interface to OLE DB
            30DEC2016
    ---SAS Workspace Server for Local Access
            30DEC2016
    ---High Performance Suite
            30DEC2016


I can see ODBC but I not sure.. is that supposed to mean ODBC driver for TD and how to use is instead of OLEDB ?

(create multiset volatile table VT (C1 integer, C2 date))


The above test code fails with message that it expects a column name on a create table statement. The extra R &amp; L Parenthesis is because this is embedded in SAS 
but if I remove the Volatile statement , it will  run fine.

Here is the detailed error 

    proc sql;
  connect to OLEDB(Provider='MSDASQL' Extended_Properties='DRIVER={Teradata};DBCNAME=SITEPRD;AUTHENTICATION=ldap' UID=""&amp;DMID"" PWD=""&amp;DMPWD"");
  execute (create multiset volatile table idlist (my_id integer, mydate date)
  ON COMMIT PRESERVE ROWS) by teradata;
  execute (COMMIT WORK) by teradata;
  insert into idlist
  select distinct MyId_sas, mydate
  from mysource;
quit; 3:52 PM 
And got this output: 3:52 PM 
proc sql;
28     connect to OLEDB(Provider='MSDASQL' Extended_Properties='DRIVER={Teradata};
28 ! DBCNAME=SITEPRD;AUTHENTICATION=ldap' UID=""&amp;DMID"" PWD=""&amp;DMPWD"");
SYMBOLGEN:  Macro variable DMID resolves to ConfusedUser
SYMBOLGEN:  Macro variable DMPWD resolves to Youbetcha!

29     execute (create multiset volatile table idlist (my_id integer, mydate date)
30     ON COMMIT PRESERVE ROWS) by teradata;
ERROR: The TERADATA engine cannot be found.
ERROR: A Connection to the teradata DBMS is not currently supported, or is not installed at
       your site.
31     execute (COMMIT WORK) by teradata;
ERROR: The TERADATA engine cannot be found.
ERROR: A Connection to the teradata DBMS is not currently supported, or is not installed at
       your site.
32     insert into idlist
33     select distinct MyId_sas, mydate
34     from mysource;
ERROR: File WORK.idlist.DATA does not exist.
NOTE: SGIO processing active for file WORK.mysource.DATA.
35   quit;
NOTE: The SAS System stopped processing this step because of errors.
NOTE: PROCEDURE SQL used (Total process time):
      real time           9.19 seconds
      cpu time            1.75 seconds 

",-1,-1,-1.0,"I have some SAS developers who do not have SAS access for Teradata Installed

and they are not on SAS query  grid . Instead they use OLEDB to connect to Teradata. 
OLEDB does not seem to like create volatile table statement. Has anyone had some luck using it and get VT's in teradata to work 

This is what's installed on the SAS W2K serveer 

     proc setinit; run;


  ---Base SAS Software
            30DEC2016
    ---SAS/STAT
            30DEC2016
    ---SAS/GRAPH
            30DEC2016
    ---SAS/Secure 168-bit
            30DEC2016
    ---SAS/Secure Windows
            30DEC2016
    ---SAS/ACCESS Interface to PC Files
            30DEC2016
    ---SAS/ACCESS Interface to ODBC
            30DEC2016
    ---SAS/ACCESS Interface to OLE DB
            30DEC2016
    ---SAS Workspace Server for Local Access
            30DEC2016
    ---High Performance Suite
            30DEC2016


I can see ODBC but I not sure.. is that supposed to mean ODBC driver for TD and how to use is instead of OLEDB ?

(create multiset volatile table VT (C1 integer, C2 date))


The above test code fails with message that it expects a column name on a create table statement. The extra R &amp; L Parenthesis is because this is embedded in SAS 
but if I remove the Volatile statement , it will  run fine.

Here is the detailed error 

    proc sql;
  connect to OLEDB(Provider='MSDASQL' Extended_Properties='DRIVER={Teradata};DBCNAME=SITEPRD;AUTHENTICATION=ldap' UID=""&amp;DMID"" PWD=""&amp;DMPWD"");
  execute (create multiset volatile table idlist (my_id integer, mydate date)
  ON COMMIT PRESERVE ROWS) by teradata;
  execute (COMMIT WORK) by teradata;
  insert into idlist
  select distinct MyId_sas, mydate
  from mysource;
quit; 3:52 PM 
And got this output: 3:52 PM 
proc sql;
28     connect to OLEDB(Provider='MSDASQL' Extended_Properties='DRIVER={Teradata};
28 ! DBCNAME=SITEPRD;AUTHENTICATION=ldap' UID=""&amp;DMID"" PWD=""&amp;DMPWD"");
SYMBOLGEN:  Macro variable DMID resolves to ConfusedUser
SYMBOLGEN:  Macro variable DMPWD resolves to Youbetcha!

29     execute (create multiset volatile table idlist (my_id integer, mydate date)
30     ON COMMIT PRESERVE ROWS) by teradata;
ERROR: The TERADATA engine cannot be found.
ERROR: A Connection to the teradata DBMS is not currently supported, or is not installed at
       your site.
31     execute (COMMIT WORK) by teradata;
ERROR: The TERADATA engine cannot be found.
ERROR: A Connection to the teradata DBMS is not currently supported, or is not installed at
       your site.
32     insert into idlist
33     select distinct MyId_sas, mydate
34     from mysource;
ERROR: File WORK.idlist.DATA does not exist.
NOTE: SGIO processing active for file WORK.mysource.DATA.
35   quit;
NOTE: The SAS System stopped processing this step because of errors.
NOTE: PROCEDURE SQL used (Total process time):
      real time           9.19 seconds
      cpu time            1.75 seconds 

",3
165,36070443,Exception in export data from hcatalog to teradata,"I am trying to export table to Teradata, using Sqoop.
Table data was created in mapreduce job in parquet format, then I created external table pointing to this data using Impala.

Here is comand i am running: 

sqoop export --connect jdbc:teradata://&lt;ip&gt;/DATABASE=TESTDB --username &lt;username&gt; --password &lt;password&gt; \
    --hcatalog-database parq_xml --hcatalog-table &lt;table_name&gt; --table &lt;table_name&gt;


But it fails with next exception (showing only tail):

16/03/17 23:45:21 INFO hcat.SqoopHCatUtilities: Adding to job classpath: file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hive/lib/hive-jdbc-standalone.jar
16/03/17 23:45:21 INFO hcat.SqoopHCatUtilities: Adding to job classpath: file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hive/lib/zookeeper.jar
16/03/17 23:45:21 INFO hcat.SqoopHCatUtilities: Adding to job classpath: file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hive/lib/commons-collections-3.2.1.jar
16/03/17 23:45:21 INFO hcat.SqoopHCatUtilities: Adding to job classpath: file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hive/lib/logredactor-1.0.2.jar
16/03/17 23:45:21 INFO hcat.SqoopHCatUtilities: Adding jar files under /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/bin/../lib/sqoop/../hive-hcatalog/share/hcatalog/storage-handlers to distributed cache (recursively)
16/03/17 23:45:21 WARN hcat.SqoopHCatUtilities: No files under /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/bin/../lib/sqoop/../hive-hcatalog/share/hcatalog/storage-handlers to add to distributed cache for hcatalog job
16/03/17 23:45:21 INFO common.ConnectorPlugin: load plugins in file:/home/ovlasyuk/xml/teradata.connector.plugins.xml
16/03/17 23:45:21 ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.IllegalArgumentException: Can not create a Path from a null string
java.lang.IllegalArgumentException: Can not create a Path from a null string
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:123)
        at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:135)
        at com.cloudera.connector.teradata.exports.ExportJob.configureOutputFormat(ExportJob.java:176)
        at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:425)
        at com.cloudera.connector.teradata.TeradataManager.exportTable(TeradataManager.java:97)
        at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:81)
        at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:100)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)


What is wrong?
",-1,-1,-1.0,"I am trying to export table to Teradata, using Sqoop.
Table data was created in mapreduce job in parquet format, then I created external table pointing to this data using Impala.

Here is comand i am running: 

sqoop export --connect jdbc:teradata://&lt;ip&gt;/DATABASE=TESTDB --username &lt;username&gt; --password &lt;password&gt; \
    --hcatalog-database parq_xml --hcatalog-table &lt;table_name&gt; --table &lt;table_name&gt;


But it fails with next exception (showing only tail):

16/03/17 23:45:21 INFO hcat.SqoopHCatUtilities: Adding to job classpath: file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hive/lib/hive-jdbc-standalone.jar
16/03/17 23:45:21 INFO hcat.SqoopHCatUtilities: Adding to job classpath: file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hive/lib/zookeeper.jar
16/03/17 23:45:21 INFO hcat.SqoopHCatUtilities: Adding to job classpath: file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hive/lib/commons-collections-3.2.1.jar
16/03/17 23:45:21 INFO hcat.SqoopHCatUtilities: Adding to job classpath: file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hive/lib/logredactor-1.0.2.jar
16/03/17 23:45:21 INFO hcat.SqoopHCatUtilities: Adding jar files under /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/bin/../lib/sqoop/../hive-hcatalog/share/hcatalog/storage-handlers to distributed cache (recursively)
16/03/17 23:45:21 WARN hcat.SqoopHCatUtilities: No files under /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/bin/../lib/sqoop/../hive-hcatalog/share/hcatalog/storage-handlers to add to distributed cache for hcatalog job
16/03/17 23:45:21 INFO common.ConnectorPlugin: load plugins in file:/home/ovlasyuk/xml/teradata.connector.plugins.xml
16/03/17 23:45:21 ERROR sqoop.Sqoop: Got exception running Sqoop: java.lang.IllegalArgumentException: Can not create a Path from a null string
java.lang.IllegalArgumentException: Can not create a Path from a null string
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:123)
        at org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:135)
        at com.cloudera.connector.teradata.exports.ExportJob.configureOutputFormat(ExportJob.java:176)
        at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:425)
        at com.cloudera.connector.teradata.TeradataManager.exportTable(TeradataManager.java:97)
        at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:81)
        at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:100)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
        at org.apache.sqoop.Sqoop.main(Sqoop.java:236)


What is wrong?
",0
166,35731117,Teradata jdbc executeBatch throws a weird error?,"I use jdbc to load raw data sets to Teradata. I has been working PERFECT until recently. 

Here is the code 

    try { 
        prst.executeBatch(); 
        } 
        catch (SQLException ex) {  
            System.out.println(""Batch outside the loop error: "");
             while (ex != null)
                {
                    System.out.println("" Error code: "" + ex.getErrorCode());
                    System.out.println("" SQL State: "" + ex.getSQLState());
                    System.out.println("" Message: "" + ex.getMessage());
                    ex.printStackTrace();
                    System.out.println();
                    ex = ex.getNextException();
                }

        }


But yesterday the same code statred throwing an error. Here is the error 

    [Teradata JDBC Driver] [TeraJDBC 13.00.00.16] [Error 1339] [SQLState HY000] 
A failure occurred while executing a PreparedStatement batch request. 
The parameter set was not executed and should be resubmitted
 individually using the PreparedStatement executeUpdate method 


I checked using getNextException() but all I get is the same message  

A failure occurred while executing a PreparedStatement batch request. 
    The parameter set was not executed and should be resubmitted
     individually using the PreparedStatement executeUpdate method  


It's repeating the same stuff all over again without any further details. I tried to decrease the batch size to a minimum as recomended here  but still no result. 

What could possible cause this error  ?  How to overcome it ?  
",-1,-1,-1.0,"I use jdbc to load raw data sets to Teradata. I has been working PERFECT until recently. 

Here is the code 

    try { 
        prst.executeBatch(); 
        } 
        catch (SQLException ex) {  
            System.out.println(""Batch outside the loop error: "");
             while (ex != null)
                {
                    System.out.println("" Error code: "" + ex.getErrorCode());
                    System.out.println("" SQL State: "" + ex.getSQLState());
                    System.out.println("" Message: "" + ex.getMessage());
                    ex.printStackTrace();
                    System.out.println();
                    ex = ex.getNextException();
                }

        }


But yesterday the same code statred throwing an error. Here is the error 

    [Teradata JDBC Driver] [TeraJDBC 13.00.00.16] [Error 1339] [SQLState HY000] 
A failure occurred while executing a PreparedStatement batch request. 
The parameter set was not executed and should be resubmitted
 individually using the PreparedStatement executeUpdate method 


I checked using getNextException() but all I get is the same message  

A failure occurred while executing a PreparedStatement batch request. 
    The parameter set was not executed and should be resubmitted
     individually using the PreparedStatement executeUpdate method  


It's repeating the same stuff all over again without any further details. I tried to decrease the batch size to a minimum as recomended here  but still no result. 

What could possible cause this error  ?  How to overcome it ?  
",0
167,36449948,Teradata: [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows,"I have connected to my Teradata database via JDBC and  am trying to use the FASTLOAD utility of Teradata to insert records into a table (with prepared statements and batch). Eg:

connection = DriverManager.getConnection(""jdbc:teradata://192.168.1.110/TYPE=FASTLOAD"", ""admin"", ""admin"");

String sql = ""INSERT INTO table (RANDOM_INTEGER) VALUES (?)"";
PreparedStatement preparedStatement = connection.prepareStatement(sql);

int numberOfRecordsToInsert = 100000;

        for (int i = 0; i &lt; numberOfRecordsToInsert; i++) {

            preparedStatement.setInt(1, 5);
            preparedStatement.addBatch();

        }

preparedStatement.executeBatch();

connection.commit();    


With this (I've tried this on many different tables with different data types) I get the following error when it gets to the executeBatch() line:


  Exception in thread ""main"" java.sql.BatchUpdateException: [Teradata
  JDBC Driver] [TeraJDBC 15.10.00.14] [Error 1154] [SQLState HY000] A
  failure occurred while inserting the batch of rows destined for
  database table ""admin"".""table"". Details of the failure can be
  found in the exception chain that is accessible with getNextException.
    at
  com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:148)
    at
  com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:132)
    at
  com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2202)
    at teradata.fastload.main(fastload.java:62) Caused by:
  java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.10.00.14]
  [Error 1147] [SQLState HY000] The next failure(s) in the exception
  chain occurred while beginning FastLoad of database table
  ""admin"".""table""   at
  com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:94)
    at
  com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:69)
    at
  com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.beginFastLoad(FastLoadManagerPreparedStatement.java:836)
    at
  com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2070)
    ... 1 more


I don't know how to even call the getNextException that the error says. 
",-1,-1,-1.0,"I have connected to my Teradata database via JDBC and  am trying to use the FASTLOAD utility of Teradata to insert records into a table (with prepared statements and batch). Eg:

connection = DriverManager.getConnection(""jdbc:teradata://192.168.1.110/TYPE=FASTLOAD"", ""admin"", ""admin"");

String sql = ""INSERT INTO table (RANDOM_INTEGER) VALUES (?)"";
PreparedStatement preparedStatement = connection.prepareStatement(sql);

int numberOfRecordsToInsert = 100000;

        for (int i = 0; i &lt; numberOfRecordsToInsert; i++) {

            preparedStatement.setInt(1, 5);
            preparedStatement.addBatch();

        }

preparedStatement.executeBatch();

connection.commit();    


With this (I've tried this on many different tables with different data types) I get the following error when it gets to the executeBatch() line:


  Exception in thread ""main"" java.sql.BatchUpdateException: [Teradata
  JDBC Driver] [TeraJDBC 15.10.00.14] [Error 1154] [SQLState HY000] A
  failure occurred while inserting the batch of rows destined for
  database table ""admin"".""table"". Details of the failure can be
  found in the exception chain that is accessible with getNextException.
    at
  com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:148)
    at
  com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:132)
    at
  com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2202)
    at teradata.fastload.main(fastload.java:62) Caused by:
  java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.10.00.14]
  [Error 1147] [SQLState HY000] The next failure(s) in the exception
  chain occurred while beginning FastLoad of database table
  ""admin"".""table""   at
  com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:94)
    at
  com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:69)
    at
  com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.beginFastLoad(FastLoadManagerPreparedStatement.java:836)
    at
  com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2070)
    ... 1 more


I don't know how to even call the getNextException that the error says. 
",0
168,36586069,Teradata create table auto-increment column error,"I am trying to import my CSV file into Teradata using Teradata's Fastload script. 
I also tried adding an auto-increment column.
This is my CSV file:

Word,country,sale,week
hi,USA,26.17,11/22/15-11/28/15
bye,USA,16.5,11/22/15-11/28/15


code snippet

String tableName = ""my_db.mytable"";
    String createTable = ""CREATE TABLE "" + tableName + "","" +
                            ""NO FALLBACK,"" +
                            ""NO BEFORE JOURNAL,"" +
                            ""NO AFTER JOURNAL,"" +
                            ""CHECKSUM = DEFAULT"" +
                            ""("" +
                            "" id decimal(10,0) NOT NULL GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1 MINVALUE 1 MAXVALUE 2147483647 NO CYCLE),""+
                            "" word VARCHAR(500) CHARACTER SET UNICODE,"" +
                            "" country VARCHAR(50),"" +
                            "" sale FLOAT,"" +
                            "" week VARCHAR(30)"" +
                            "") "" +
                          ""PRIMARY INDEX (id)"";

    // INSERT statement
    String insertTable = ""INSERT INTO "" + tableName + "" VALUES(?,?,?,?,?)"";


Error i got:

Row 1 in FastLoad table my_db.mytable_ERR_1 contains the following data: 
ErrorCode=2673
ErrorFieldName=F_id
ActualDataParcelLength=55
DataParcel: byte array length 55 (0x37), offset 0 (0x0), dump length 55 (0x37)

",-1,-1,-1.0,"I am trying to import my CSV file into Teradata using Teradata's Fastload script. 
I also tried adding an auto-increment column.
This is my CSV file:

Word,country,sale,week
hi,USA,26.17,11/22/15-11/28/15
bye,USA,16.5,11/22/15-11/28/15


code snippet

String tableName = ""my_db.mytable"";
    String createTable = ""CREATE TABLE "" + tableName + "","" +
                            ""NO FALLBACK,"" +
                            ""NO BEFORE JOURNAL,"" +
                            ""NO AFTER JOURNAL,"" +
                            ""CHECKSUM = DEFAULT"" +
                            ""("" +
                            "" id decimal(10,0) NOT NULL GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1 MINVALUE 1 MAXVALUE 2147483647 NO CYCLE),""+
                            "" word VARCHAR(500) CHARACTER SET UNICODE,"" +
                            "" country VARCHAR(50),"" +
                            "" sale FLOAT,"" +
                            "" week VARCHAR(30)"" +
                            "") "" +
                          ""PRIMARY INDEX (id)"";

    // INSERT statement
    String insertTable = ""INSERT INTO "" + tableName + "" VALUES(?,?,?,?,?)"";


Error i got:

Row 1 in FastLoad table my_db.mytable_ERR_1 contains the following data: 
ErrorCode=2673
ErrorFieldName=F_id
ActualDataParcelLength=55
DataParcel: byte array length 55 (0x37), offset 0 (0x0), dump length 55 (0x37)

",3
169,36706056,Python Teradata auto increment starts with a 6 digit number instead of 1?,"I want to create a table in Teradata that uses auto-increment in the ID column. The code works, but i just dont know why the ID starts like 100001, 100002.. ect despite stating a start with 1 increment by 1.

This is my code:

cur = connection.cursor()


create table

create_stmt = """"""CREATE TABLE my_table,
                                NO FALLBACK,
                                NO BEFORE JOURNAL,
                                NO AFTER JOURNAL,
                                CHECKSUM = DEFAULT
                                ( 
                                 id INTEGER NOT NULL GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1 MINVALUE 1 MAXVALUE 2147483647 NO CYCLE),
                                word VARCHAR(500) CHARACTER SET UNICODE, 
                                country VARCHAR(50) 
                                ) 
                                PRIMARY INDEX (id);""""""
cur.execute(create_stmt)


insert into table

insert_stmt = """"""INSERT INTO my_table (word,country) VALUES(?,?);""""""
mydata=(""hello"",""USA"")
my_query=cur.execute(insert_stmt,my_data)


select all to test

sel_all_stmt=""sel * from my_table""
cur.execute(sel_all_stmt)
result = cur.fetchall()


I ran the insert statement a few times to see what the auto increment look like and this is what i got:

[(100001, u'hello', u'USA'), (1, u'hello', u'USA'), (200001, u'hello', u'USA'), (400001, u'hello', u'USA'), (300001, u'hello', u'USA')]


Any idea why the id is so long?
",-1,-1,-1.0,"I want to create a table in Teradata that uses auto-increment in the ID column. The code works, but i just dont know why the ID starts like 100001, 100002.. ect despite stating a start with 1 increment by 1.

This is my code:

cur = connection.cursor()


create table

create_stmt = """"""CREATE TABLE my_table,
                                NO FALLBACK,
                                NO BEFORE JOURNAL,
                                NO AFTER JOURNAL,
                                CHECKSUM = DEFAULT
                                ( 
                                 id INTEGER NOT NULL GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1 MINVALUE 1 MAXVALUE 2147483647 NO CYCLE),
                                word VARCHAR(500) CHARACTER SET UNICODE, 
                                country VARCHAR(50) 
                                ) 
                                PRIMARY INDEX (id);""""""
cur.execute(create_stmt)


insert into table

insert_stmt = """"""INSERT INTO my_table (word,country) VALUES(?,?);""""""
mydata=(""hello"",""USA"")
my_query=cur.execute(insert_stmt,my_data)


select all to test

sel_all_stmt=""sel * from my_table""
cur.execute(sel_all_stmt)
result = cur.fetchall()


I ran the insert statement a few times to see what the auto increment look like and this is what i got:

[(100001, u'hello', u'USA'), (1, u'hello', u'USA'), (200001, u'hello', u'USA'), (400001, u'hello', u'USA'), (300001, u'hello', u'USA')]


Any idea why the id is so long?
",3
170,36845710,Teradata stored procedure with dynamic parameters called from R script,"I need to extract some data from Teradata to process in R.  I have around 84 Dep/sec keys with most of them having a different time span so my thought was to create a stored procedure in Teradata that will accept the Dep, Sec and Dates as parameters. I could then loop over the list in R calling the SP each time to create my data set.

The SP I have created to test this idea is a very simple one but I can't get it to work.

CREATE PROCEDURE procTest4 (IntN integer)
BEGIN
CALL DBC.SysExecSQL('SELECT top' || IntN || '*
from TableName');
END;


Teradata does create the SP but I don't know how to execute it and pass the paramters to it.  When I try:
    Call procText4(10)
I get the following error:

5568: SQL statement is not supported within a stored procedure. 

The only other option for me is to create the SQL string in R and then run it from there but there is multiple passes of SQL which create volatile tables and the RODBC package doesn't seem to like them, plus it's a very messy way of doing it.

Any help is much appreciated.
",1,-1,-1.0,"I need to extract some data from Teradata to process in R.  I have around 84 Dep/sec keys with most of them having a different time span so my thought was to create a stored procedure in Teradata that will accept the Dep, Sec and Dates as parameters. I could then loop over the list in R calling the SP each time to create my data set.

The SP I have created to test this idea is a very simple one but I can't get it to work.

CREATE PROCEDURE procTest4 (IntN integer)
BEGIN
CALL DBC.SysExecSQL('SELECT top' || IntN || '*
from TableName');
END;


Teradata does create the SP but I don't know how to execute it and pass the paramters to it.  When I try:
    Call procText4(10)
I get the following error:

5568: SQL statement is not supported within a stored procedure. 

The only other option for me is to create the SQL string in R and then run it from there but there is multiple passes of SQL which create volatile tables and the RODBC package doesn't seem to like them, plus it's a very messy way of doing it.

Any help is much appreciated.
",3
171,36866076,Insert Into table Teradata dynamic stored procedure SQL,"I am trying to create a Stored Procedure in Teradata that will accept various arguments.  My query has 4 passes of SQL where it creates 3 lots of volatile tables.  Within the Select statements is the SQL that I need to be dynamic and this is where I run into problems.

Here is my SQL:

CREATE PROCEDURE ""mydb"".""test_sp20"" (DepID integer) --DepID is my parameter
DYNAMIC RESULT SETS 1 SQL SECURITY OWNER
BEGIN

DECLARE q1 VARCHAR(10000);
DECLARE cur1 CURSOR WITH RETURN ONLY TO client FOR s1;

CREATE VOLATILE TABLE mydb.tbl_1 , no fallback, no log(
Consumer_Unit_Id    INTEGER, 
Price_Promotion_Id  INTEGER, 
Promotion_Id    INTEGER)
primary index (Consumer_Unit_Id, Price_Promotion_Id, Promotion_Id) on commit preserve rows ;

INSERT INTO mydb.tbl_1
SELECT * FROM mydb.tbl_1
SET q1 = 'Select * from mydb.tbl_1'
PREPARE s1 FROM q1;
OPEN cur1;
END;


This works fine as a SP with static SQL but I need the Select statements to be dynamic in that within them I have Department and Sections parameters that I want to be able to pass accross. e.g.

INSERT INTO mydb.tbl_1
SQL = 'Select * from mydb.tbl_1 where Department_ID = ' || DepID ||'


I've also tried:

SQL = 'INSERT INTO Select * from mydb.tbl_1 where Department_ID = ' || DepID ||'


In both cases I get the following error:

7683:  TEST_SP20:Invalid statement specified inside a dynamic declare cursor/SQL statement


I seem to be able to have a dynamic SQL string but I can't have a INSERT INTO SELECT string??

@dnoeth helped me get this started so any more help is much appreciated.
",1,-1,-1.0,"I am trying to create a Stored Procedure in Teradata that will accept various arguments.  My query has 4 passes of SQL where it creates 3 lots of volatile tables.  Within the Select statements is the SQL that I need to be dynamic and this is where I run into problems.

Here is my SQL:

CREATE PROCEDURE ""mydb"".""test_sp20"" (DepID integer) --DepID is my parameter
DYNAMIC RESULT SETS 1 SQL SECURITY OWNER
BEGIN

DECLARE q1 VARCHAR(10000);
DECLARE cur1 CURSOR WITH RETURN ONLY TO client FOR s1;

CREATE VOLATILE TABLE mydb.tbl_1 , no fallback, no log(
Consumer_Unit_Id    INTEGER, 
Price_Promotion_Id  INTEGER, 
Promotion_Id    INTEGER)
primary index (Consumer_Unit_Id, Price_Promotion_Id, Promotion_Id) on commit preserve rows ;

INSERT INTO mydb.tbl_1
SELECT * FROM mydb.tbl_1
SET q1 = 'Select * from mydb.tbl_1'
PREPARE s1 FROM q1;
OPEN cur1;
END;


This works fine as a SP with static SQL but I need the Select statements to be dynamic in that within them I have Department and Sections parameters that I want to be able to pass accross. e.g.

INSERT INTO mydb.tbl_1
SQL = 'Select * from mydb.tbl_1 where Department_ID = ' || DepID ||'


I've also tried:

SQL = 'INSERT INTO Select * from mydb.tbl_1 where Department_ID = ' || DepID ||'


In both cases I get the following error:

7683:  TEST_SP20:Invalid statement specified inside a dynamic declare cursor/SQL statement


I seem to be able to have a dynamic SQL string but I can't have a INSERT INTO SELECT string??

@dnoeth helped me get this started so any more help is much appreciated.
",3
172,36870631,Using a Teradata UDF in SAS Implicit Sql Pass Thru,"I  am   trying to use  a Teradata  UDF (User Defined Function)  in a SAS Implicit SQL which establishes the connection to  Teradata using LIBNAME  Statement.Assume  that  the function is called PTY_DECRYPT and is defined in a Database called TEST in Teradata. The Purpose  of this function  is to decrypt values in a Column of a View in Teradata.

What   works  is using the UDF  in an Explicit  Sql .Below  I am using the function on a column called SSN_NBR in a view called V_TEST_PERS present in the Database called SAMPLE.

Explcit Sql:

Options  debug=DBMS_TIMERS sastrace=',,,d'
sastraceloc=saslog no$stsuffix fullstimer;


Proc Sql;
Connect to TERADATA(User=XXXXX pwd=XXXXX server=XXXXX);
Create Table Final as
select  *  from connection to teradata
(
Select
sub_id, 
SSN_NBR,
TEST.PTY_DECRYPT(SSN_NBR,'T_ssn_test',400,0,0 ) as SSN_NBR_Decrypt
from SAMPLE.V_TEST_PERS
);
disconnect from teradata;
Quit;


But  I would like to use the same function in an Implicit SQL but  it does not work.  Any ideas as to how  to make it work in an Implicit Sql with minimum  changes to the Implicit SQL?

Implicit Sql

Options  debug=DBMS_TIMERS sastrace=',,,d'
sastraceloc=saslog no$stsuffix fullstimer;

Libname Td Teradata  User=XXXXX pwd=XXXXX server=XXXXX database=SAMPLE ;

Proc sql;
Create  table Final as
select
sub_id, 
SSN_NBR,
TEST.PTY_DECRYPT(SSN_NBR,'T_ssn_test',400,0,0 ) as SSN_NBR_Decrypt

from Td.V_TEST_PERS;
Quit;

",-1,1,-1.0,"I  am   trying to use  a Teradata  UDF (User Defined Function)  in a SAS Implicit SQL which establishes the connection to  Teradata using LIBNAME  Statement.Assume  that  the function is called PTY_DECRYPT and is defined in a Database called TEST in Teradata. The Purpose  of this function  is to decrypt values in a Column of a View in Teradata.

What   works  is using the UDF  in an Explicit  Sql .Below  I am using the function on a column called SSN_NBR in a view called V_TEST_PERS present in the Database called SAMPLE.

Explcit Sql:

Options  debug=DBMS_TIMERS sastrace=',,,d'
sastraceloc=saslog no$stsuffix fullstimer;


Proc Sql;
Connect to TERADATA(User=XXXXX pwd=XXXXX server=XXXXX);
Create Table Final as
select  *  from connection to teradata
(
Select
sub_id, 
SSN_NBR,
TEST.PTY_DECRYPT(SSN_NBR,'T_ssn_test',400,0,0 ) as SSN_NBR_Decrypt
from SAMPLE.V_TEST_PERS
);
disconnect from teradata;
Quit;


But  I would like to use the same function in an Implicit SQL but  it does not work.  Any ideas as to how  to make it work in an Implicit Sql with minimum  changes to the Implicit SQL?

Implicit Sql

Options  debug=DBMS_TIMERS sastrace=',,,d'
sastraceloc=saslog no$stsuffix fullstimer;

Libname Td Teradata  User=XXXXX pwd=XXXXX server=XXXXX database=SAMPLE ;

Proc sql;
Create  table Final as
select
sub_id, 
SSN_NBR,
TEST.PTY_DECRYPT(SSN_NBR,'T_ssn_test',400,0,0 ) as SSN_NBR_Decrypt

from Td.V_TEST_PERS;
Quit;

",3
173,36906484,How to take Teradata Database Build?,"I'm working in Teradata Database Express 14.0

Now, I want to take a particular database build(backup all objects like tables, sp, views, udf, indexes, triggers, etc).

I tried this way, but i'm getting error:

TDExpress14.10.03_Sles11:~ # bteq
Enter your logon or BTEQ command:
.logon localhost/xyz
Password:xxx

BTEQ -- Enter your SQL request or BTEQ command:

ARCHIVE DATA TABLES (xyz)ALL,
RELEASE LOCK,
FILEDEF=(tddumps,/var/tddumps/dump.%UEN%.out);



  ERROR:


ARCHIVE DATA TABLES(MENTISAGENT)ALL,
         $
*** Failure 3706 Syntax error: expected something between the beginning of
the request and the word 'ARCHIVE'.
            Statement# 2, Info =10
*** Total elapsed time was 1 second.


Can, anyone kindly help me to figure it out this error, and guide me how to take Teradata database backup?
",1,-1,-1.0,"I'm working in Teradata Database Express 14.0

Now, I want to take a particular database build(backup all objects like tables, sp, views, udf, indexes, triggers, etc).

I tried this way, but i'm getting error:

TDExpress14.10.03_Sles11:~ # bteq
Enter your logon or BTEQ command:
.logon localhost/xyz
Password:xxx

BTEQ -- Enter your SQL request or BTEQ command:

ARCHIVE DATA TABLES (xyz)ALL,
RELEASE LOCK,
FILEDEF=(tddumps,/var/tddumps/dump.%UEN%.out);



  ERROR:


ARCHIVE DATA TABLES(MENTISAGENT)ALL,
         $
*** Failure 3706 Syntax error: expected something between the beginning of
the request and the word 'ARCHIVE'.
            Statement# 2, Info =10
*** Total elapsed time was 1 second.


Can, anyone kindly help me to figure it out this error, and guide me how to take Teradata database backup?
",3
174,37003323,VBA code to copy a Teradata query result into Pdf file,"I am having a doubt in Excel VBA. Is it possible to get the result of a query executed in Teradata exported into Pdf file? Can please someone share the VBA code for this?

I am having all the connections and other stuffs done. I am only stuck when exporting the result into a Pdf file from the Teradata result window

Thanks,
Philip
",0,-1,-1.0,"I am having a doubt in Excel VBA. Is it possible to get the result of a query executed in Teradata exported into Pdf file? Can please someone share the VBA code for this?

I am having all the connections and other stuffs done. I am only stuck when exporting the result into a Pdf file from the Teradata result window

Thanks,
Philip
",3
175,37050656,WSO2 CEP Event tables with Teradata DB connection throws Null Exception,"I’ve try to use event tables in Siddhi engine with connection to Teradata Database through JDBC Teradata driver.

Teradata Database runs as virtual machine on the same machine where WSO2CEP installed.

First of all I’ve put Teradata JDBC driver terajdbc4.jar in the

&lt;WSO2CEP_HOME&gt;/repository/components/lib/


directory, and additional part of driver tdgssconfig.jar put into 

&lt;WSO2CEP_HOME&gt;/bin


directory. Then I’ve restarted CEP server and  made connection in the “Datasources” with next parameters:

Datasource type: RDBMS
Name: Teradata
Datasource provider: default
Driver: com.teradata.jdbc.TeraDriver
URL: jdbc:teradata://192.168.96.128/LOG=DEBUG
User name: xxxxx
Password: xxxxx


Test connection works fine.

I’ve created a new Execution Plan which join incoming data flow with Teradata db.

@Export('pcrf_out:1.0.0')
define stream pcrf_out (meta_event string, meta_user_id int, value int);

@Import('pcrf_in_filtered:1.0.0')
define stream pcrf_in_filtered (meta_event string, meta_user_id int, value int);

@From(eventtable='rdbms', datasource.name='Teradata', table.name='user_profile')
define table user_profile (user_id int, usual_traf_usg int);

/* JOIN WITH RELATIONAL TABLE */

from pcrf_in_filtered as a join user_profile as b
    on a.meta_user_id == b.user_id and b.usual_traf_usg==100
select a.meta_event as meta_event, a.meta_user_id as meta_user_id, a.value as value
&lt;i&gt;insert into pcrf_out;


When I run script validation then I see an error message: Null

Audit.log contains next message:

ERROR {AUDIT_LOG}-  Illegal access attempt at [2016-05-04 08:19:38,0813] from IP address null while trying to authenticate access to service EventProcessorAdminService


Wso2carbon.log

TID: [-1234] [] [2016-05-05 13:26:37,361] ERROR {org.wso2.carbon.event.processor.admin.EventProcessorAdminService} -  Exception when validating execution plan 
org.wso2.carbon.event.processor.core.exception.ExecutionPlanConfigurationException: Error while initialising the connection, null
    at org.wso2.carbon.event.processor.core.internal.util.helper.EventProcessorHelper.validateExecutionPlan(EventProcessorHelper.java:193)
    at org.wso2.carbon.event.processor.core.internal.CarbonEventProcessorService.validateExecutionPlan(CarbonEventProcessorService.java:493)
    at org.wso2.carbon.event.processor.admin.EventProcessorAdminService.validateExecutionPlan(EventProcessorAdminService.java:329)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.axis2.rpc.receivers.RPCUtil.invokeServiceClass(RPCUtil.java:212)
    at org.apache.axis2.rpc.receivers.RPCMessageReceiver.invokeBusinessLogic(RPCMessageReceiver.java:117)
    at org.apache.axis2.receivers.AbstractInOutMessageReceiver.invokeBusinessLogic(AbstractInOutMessageReceiver.java:40)
    at org.apache.axis2.receivers.AbstractMessageReceiver.receive(AbstractMessageReceiver.java:110)
    at org.apache.axis2.engine.AxisEngine.receive(AxisEngine.java:180)
    at org.apache.axis2.transport.local.LocalTransportReceiver.processMessage(LocalTransportReceiver.java:169)
    at org.apache.axis2.transport.local.LocalTransportReceiver.processMessage(LocalTransportReceiver.java:82)
    at org.wso2.carbon.core.transports.local.CarbonLocalTransportSender.finalizeSendWithToAddress(CarbonLocalTransportSender.java:45)
    at org.apache.axis2.transport.local.LocalTransportSender.invoke(LocalTransportSender.java:77)
    at org.apache.axis2.engine.AxisEngine.send(AxisEngine.java:442)
    at org.apache.axis2.description.OutInAxisOperationClient.send(OutInAxisOperation.java:430)
    at org.apache.axis2.description.OutInAxisOperationClient.executeImpl(OutInAxisOperation.java:225)
    at org.apache.axis2.client.OperationClient.execute(OperationClient.java:149)
    at org.wso2.carbon.event.processor.stub.EventProcessorAdminServiceStub.validateExecutionPlan(EventProcessorAdminServiceStub.java:2207)
    at org.apache.jsp.eventprocessor.validate_005fsiddhi_005fqueries_005fajaxprocessor_jsp._jspService(validate_005fsiddhi_005fqueries_005fajaxprocessor_jsp.java:73)
    at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
    at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:432)
    at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:395)
    at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:339)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
    at org.wso2.carbon.ui.JspServlet.service(JspServlet.java:155)
    at org.wso2.carbon.ui.TilesJspServlet.service(TilesJspServlet.java:80)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
    at org.eclipse.equinox.http.helper.ContextPathServletAdaptor.service(ContextPathServletAdaptor.java:37)
    at org.eclipse.equinox.http.servlet.internal.ServletRegistration.service(ServletRegistration.java:61)
    at org.eclipse.equinox.http.servlet.internal.ProxyServlet.processAlias(ProxyServlet.java:128)
    at org.eclipse.equinox.http.servlet.internal.ProxyServlet.service(ProxyServlet.java:68)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
    at org.wso2.carbon.tomcat.ext.servlet.DelegationServlet.service(DelegationServlet.java:68)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
    at org.wso2.carbon.ui.filters.CSRFPreventionFilter.doFilter(CSRFPreventionFilter.java:88)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
    at org.wso2.carbon.ui.filters.CRLFPreventionFilter.doFilter(CRLFPreventionFilter.java:59)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
    at org.wso2.carbon.tomcat.ext.filter.CharacterSetFilter.doFilter(CharacterSetFilter.java:61)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:504)
    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
    at org.wso2.carbon.tomcat.ext.valves.CompositeValve.continueInvocation(CompositeValve.java:99)
    at org.wso2.carbon.tomcat.ext.valves.CarbonTomcatValve$1.invoke(CarbonTomcatValve.java:47)
    at org.wso2.carbon.webapp.mgt.TenantLazyLoaderValve.invoke(TenantLazyLoaderValve.java:57)
    at org.wso2.carbon.event.receiver.core.internal.tenantmgt.TenantLazyLoaderValve.invoke(TenantLazyLoaderValve.java:48)
    at org.wso2.carbon.tomcat.ext.valves.TomcatValveContainer.invokeValves(TomcatValveContainer.java:47)
    at org.wso2.carbon.tomcat.ext.valves.CompositeValve.invoke(CompositeValve.java:62)
    at org.wso2.carbon.tomcat.ext.valves.CarbonStuckThreadDetectionValve.invoke(CarbonStuckThreadDetectionValve.java:159)
    at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
    at org.wso2.carbon.tomcat.ext.valves.CarbonContextCreatorValve.invoke(CarbonContextCreatorValve.java:57)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)
    at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1074)
    at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1739)
    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1698)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.wso2.siddhi.core.exception.ExecutionPlanRuntimeException: Error while initialising the connection, null
    at org.wso2.siddhi.extension.eventtable.rdbms.DBHandler.&lt;init&gt;(DBHandler.java:79)
    at org.wso2.siddhi.extension.eventtable.RDBMSEventTable.init(RDBMSEventTable.java:119)
    at org.wso2.siddhi.core.util.parser.helper.DefinitionParserHelper.addEventTable(DefinitionParserHelper.java:99)
    at org.wso2.siddhi.core.util.ExecutionPlanRuntimeBuilder.defineTable(ExecutionPlanRuntimeBuilder.java:74)
    at org.wso2.siddhi.core.util.parser.ExecutionPlanParser.defineTableDefinitions(ExecutionPlanParser.java:194)
    at org.wso2.siddhi.core.util.parser.ExecutionPlanParser.parse(ExecutionPlanParser.java:140)
    at org.wso2.siddhi.core.SiddhiManager.validateExecutionPlan(SiddhiManager.java:69)
    at org.wso2.siddhi.core.SiddhiManager.validateExecutionPlan(SiddhiManager.java:75)
    at org.wso2.carbon.event.processor.core.internal.util.helper.EventProcessorHelper.validateExecutionPlan(EventProcessorHelper.java:191)
    ... 74 more
Caused by: java.sql.SQLException
    at org.apache.tomcat.jdbc.pool.PooledConnection.connectUsingDriver(PooledConnection.java:254)
    at org.apache.tomcat.jdbc.pool.PooledConnection.connect(PooledConnection.java:182)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.createConnection(ConnectionPool.java:701)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:635)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.getConnection(ConnectionPool.java:188)
    at org.apache.tomcat.jdbc.pool.DataSourceProxy.getConnection(DataSourceProxy.java:127)
    at org.wso2.siddhi.extension.eventtable.rdbms.DBHandler.&lt;init&gt;(DBHandler.java:73)
    ... 82 more
Caused by: java.lang.NullPointerException
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at org.apache.tomcat.jdbc.pool.PooledConnection.connectUsingDriver(PooledConnection.java:246)
    ... 88 more


The same Execution Plan works fine with Mysql connection.

Can anybody help me with it?
Thanks!
",1,-1,-1.0,"I’ve try to use event tables in Siddhi engine with connection to Teradata Database through JDBC Teradata driver.

Teradata Database runs as virtual machine on the same machine where WSO2CEP installed.

First of all I’ve put Teradata JDBC driver terajdbc4.jar in the

&lt;WSO2CEP_HOME&gt;/repository/components/lib/


directory, and additional part of driver tdgssconfig.jar put into 

&lt;WSO2CEP_HOME&gt;/bin


directory. Then I’ve restarted CEP server and  made connection in the “Datasources” with next parameters:

Datasource type: RDBMS
Name: Teradata
Datasource provider: default
Driver: com.teradata.jdbc.TeraDriver
URL: jdbc:teradata://192.168.96.128/LOG=DEBUG
User name: xxxxx
Password: xxxxx


Test connection works fine.

I’ve created a new Execution Plan which join incoming data flow with Teradata db.

@Export('pcrf_out:1.0.0')
define stream pcrf_out (meta_event string, meta_user_id int, value int);

@Import('pcrf_in_filtered:1.0.0')
define stream pcrf_in_filtered (meta_event string, meta_user_id int, value int);

@From(eventtable='rdbms', datasource.name='Teradata', table.name='user_profile')
define table user_profile (user_id int, usual_traf_usg int);

/* JOIN WITH RELATIONAL TABLE */

from pcrf_in_filtered as a join user_profile as b
    on a.meta_user_id == b.user_id and b.usual_traf_usg==100
select a.meta_event as meta_event, a.meta_user_id as meta_user_id, a.value as value
&lt;i&gt;insert into pcrf_out;


When I run script validation then I see an error message: Null

Audit.log contains next message:

ERROR {AUDIT_LOG}-  Illegal access attempt at [2016-05-04 08:19:38,0813] from IP address null while trying to authenticate access to service EventProcessorAdminService


Wso2carbon.log

TID: [-1234] [] [2016-05-05 13:26:37,361] ERROR {org.wso2.carbon.event.processor.admin.EventProcessorAdminService} -  Exception when validating execution plan 
org.wso2.carbon.event.processor.core.exception.ExecutionPlanConfigurationException: Error while initialising the connection, null
    at org.wso2.carbon.event.processor.core.internal.util.helper.EventProcessorHelper.validateExecutionPlan(EventProcessorHelper.java:193)
    at org.wso2.carbon.event.processor.core.internal.CarbonEventProcessorService.validateExecutionPlan(CarbonEventProcessorService.java:493)
    at org.wso2.carbon.event.processor.admin.EventProcessorAdminService.validateExecutionPlan(EventProcessorAdminService.java:329)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.axis2.rpc.receivers.RPCUtil.invokeServiceClass(RPCUtil.java:212)
    at org.apache.axis2.rpc.receivers.RPCMessageReceiver.invokeBusinessLogic(RPCMessageReceiver.java:117)
    at org.apache.axis2.receivers.AbstractInOutMessageReceiver.invokeBusinessLogic(AbstractInOutMessageReceiver.java:40)
    at org.apache.axis2.receivers.AbstractMessageReceiver.receive(AbstractMessageReceiver.java:110)
    at org.apache.axis2.engine.AxisEngine.receive(AxisEngine.java:180)
    at org.apache.axis2.transport.local.LocalTransportReceiver.processMessage(LocalTransportReceiver.java:169)
    at org.apache.axis2.transport.local.LocalTransportReceiver.processMessage(LocalTransportReceiver.java:82)
    at org.wso2.carbon.core.transports.local.CarbonLocalTransportSender.finalizeSendWithToAddress(CarbonLocalTransportSender.java:45)
    at org.apache.axis2.transport.local.LocalTransportSender.invoke(LocalTransportSender.java:77)
    at org.apache.axis2.engine.AxisEngine.send(AxisEngine.java:442)
    at org.apache.axis2.description.OutInAxisOperationClient.send(OutInAxisOperation.java:430)
    at org.apache.axis2.description.OutInAxisOperationClient.executeImpl(OutInAxisOperation.java:225)
    at org.apache.axis2.client.OperationClient.execute(OperationClient.java:149)
    at org.wso2.carbon.event.processor.stub.EventProcessorAdminServiceStub.validateExecutionPlan(EventProcessorAdminServiceStub.java:2207)
    at org.apache.jsp.eventprocessor.validate_005fsiddhi_005fqueries_005fajaxprocessor_jsp._jspService(validate_005fsiddhi_005fqueries_005fajaxprocessor_jsp.java:73)
    at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
    at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:432)
    at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:395)
    at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:339)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
    at org.wso2.carbon.ui.JspServlet.service(JspServlet.java:155)
    at org.wso2.carbon.ui.TilesJspServlet.service(TilesJspServlet.java:80)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
    at org.eclipse.equinox.http.helper.ContextPathServletAdaptor.service(ContextPathServletAdaptor.java:37)
    at org.eclipse.equinox.http.servlet.internal.ServletRegistration.service(ServletRegistration.java:61)
    at org.eclipse.equinox.http.servlet.internal.ProxyServlet.processAlias(ProxyServlet.java:128)
    at org.eclipse.equinox.http.servlet.internal.ProxyServlet.service(ProxyServlet.java:68)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
    at org.wso2.carbon.tomcat.ext.servlet.DelegationServlet.service(DelegationServlet.java:68)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
    at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
    at org.wso2.carbon.ui.filters.CSRFPreventionFilter.doFilter(CSRFPreventionFilter.java:88)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
    at org.wso2.carbon.ui.filters.CRLFPreventionFilter.doFilter(CRLFPreventionFilter.java:59)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
    at org.wso2.carbon.tomcat.ext.filter.CharacterSetFilter.doFilter(CharacterSetFilter.java:61)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
    at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:504)
    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
    at org.wso2.carbon.tomcat.ext.valves.CompositeValve.continueInvocation(CompositeValve.java:99)
    at org.wso2.carbon.tomcat.ext.valves.CarbonTomcatValve$1.invoke(CarbonTomcatValve.java:47)
    at org.wso2.carbon.webapp.mgt.TenantLazyLoaderValve.invoke(TenantLazyLoaderValve.java:57)
    at org.wso2.carbon.event.receiver.core.internal.tenantmgt.TenantLazyLoaderValve.invoke(TenantLazyLoaderValve.java:48)
    at org.wso2.carbon.tomcat.ext.valves.TomcatValveContainer.invokeValves(TomcatValveContainer.java:47)
    at org.wso2.carbon.tomcat.ext.valves.CompositeValve.invoke(CompositeValve.java:62)
    at org.wso2.carbon.tomcat.ext.valves.CarbonStuckThreadDetectionValve.invoke(CarbonStuckThreadDetectionValve.java:159)
    at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
    at org.wso2.carbon.tomcat.ext.valves.CarbonContextCreatorValve.invoke(CarbonContextCreatorValve.java:57)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)
    at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1074)
    at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1739)
    at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1698)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.wso2.siddhi.core.exception.ExecutionPlanRuntimeException: Error while initialising the connection, null
    at org.wso2.siddhi.extension.eventtable.rdbms.DBHandler.&lt;init&gt;(DBHandler.java:79)
    at org.wso2.siddhi.extension.eventtable.RDBMSEventTable.init(RDBMSEventTable.java:119)
    at org.wso2.siddhi.core.util.parser.helper.DefinitionParserHelper.addEventTable(DefinitionParserHelper.java:99)
    at org.wso2.siddhi.core.util.ExecutionPlanRuntimeBuilder.defineTable(ExecutionPlanRuntimeBuilder.java:74)
    at org.wso2.siddhi.core.util.parser.ExecutionPlanParser.defineTableDefinitions(ExecutionPlanParser.java:194)
    at org.wso2.siddhi.core.util.parser.ExecutionPlanParser.parse(ExecutionPlanParser.java:140)
    at org.wso2.siddhi.core.SiddhiManager.validateExecutionPlan(SiddhiManager.java:69)
    at org.wso2.siddhi.core.SiddhiManager.validateExecutionPlan(SiddhiManager.java:75)
    at org.wso2.carbon.event.processor.core.internal.util.helper.EventProcessorHelper.validateExecutionPlan(EventProcessorHelper.java:191)
    ... 74 more
Caused by: java.sql.SQLException
    at org.apache.tomcat.jdbc.pool.PooledConnection.connectUsingDriver(PooledConnection.java:254)
    at org.apache.tomcat.jdbc.pool.PooledConnection.connect(PooledConnection.java:182)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.createConnection(ConnectionPool.java:701)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:635)
    at org.apache.tomcat.jdbc.pool.ConnectionPool.getConnection(ConnectionPool.java:188)
    at org.apache.tomcat.jdbc.pool.DataSourceProxy.getConnection(DataSourceProxy.java:127)
    at org.wso2.siddhi.extension.eventtable.rdbms.DBHandler.&lt;init&gt;(DBHandler.java:73)
    ... 82 more
Caused by: java.lang.NullPointerException
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at org.apache.tomcat.jdbc.pool.PooledConnection.connectUsingDriver(PooledConnection.java:246)
    ... 88 more


The same Execution Plan works fine with Mysql connection.

Can anybody help me with it?
Thanks!
",0
176,36792868,BTEQ: Save Teradata error in logfile,"My goal is to automate a deployment of SQL scripts to Teradata via BTEQ. So far my script is working. However, I would like to generate a log file where possible failures are captured.

.LOGON tdserver/username,pw
.EXPORT file=\logfile.txt;

.run file = \Desktop\test\test.sql;

.LOGOFF
.EXIT


My SQL script will create a VIEW. When this view for example already exists I see an error in the BTEQ command window:

*** Failure 3804 View 'ViewName' already exists.

I would like to have this TD Message in my log file. I tried several tings, have been looking for 3 hours but unfortunately without success.
",-1,-1,-1.0,"My goal is to automate a deployment of SQL scripts to Teradata via BTEQ. So far my script is working. However, I would like to generate a log file where possible failures are captured.

.LOGON tdserver/username,pw
.EXPORT file=\logfile.txt;

.run file = \Desktop\test\test.sql;

.LOGOFF
.EXIT


My SQL script will create a VIEW. When this view for example already exists I see an error in the BTEQ command window:

*** Failure 3804 View 'ViewName' already exists.

I would like to have this TD Message in my log file. I tried several tings, have been looking for 3 hours but unfortunately without success.
",3
177,36765046,Teradata : Teradata not taking table-name for query,"I am working on Teradata SQL query, and in that query I am performing a join. Unfortunately, terradata is not accepting the table name as it is because there is a . or period in table name.

Query :

Insert TEST (NAME) VALUES((
sel
 smallname||' '||bigName
 ,upper(smallname)
 ,upper(bigName)
from domain.sourceTable as a
join  domain2.destinationtable as b on b.someId=a.otherId))


Error log :

5628: Column smallname not found in domain.a or domain2.b


What am I doing wrong? Any ideas.
",-1,-1,-1.0,"I am working on Teradata SQL query, and in that query I am performing a join. Unfortunately, terradata is not accepting the table name as it is because there is a . or period in table name.

Query :

Insert TEST (NAME) VALUES((
sel
 smallname||' '||bigName
 ,upper(smallname)
 ,upper(bigName)
from domain.sourceTable as a
join  domain2.destinationtable as b on b.someId=a.otherId))


Error log :

5628: Column smallname not found in domain.a or domain2.b


What am I doing wrong? Any ideas.
",3
178,36760622,Array defintion in Teradata Aggregate UDF,"I'm trying to create a Aggregate UDF function in teradata.

As a part of it im trying to decalre an array in intermediate storage.

It keeps on throwing the below error when im trying to link it to teradata.

Executed as Single statement.  Failed [7504 : HY000] in UDF/XSP/UDM DBC.IRR: SQLSTATE U0001: 
Elapsed time = 00:00:01.628 

STATEMENT 1: Select Statement failed. 


Here is my code which im incorporating the array.

Is there any thing wrong with the syntax?

#include &lt;sqltypes_td.h&gt;
#include &lt;string.h&gt;
#include &lt;math.h&gt;
typedef struct agr_storage {
        FLOAT count;
        FLOAT val1,val2,val3,val4;
    FLOAT res[100]; // This is my array
} AGR_Storage;

",-1,-1,-1.0,"I'm trying to create a Aggregate UDF function in teradata.

As a part of it im trying to decalre an array in intermediate storage.

It keeps on throwing the below error when im trying to link it to teradata.

Executed as Single statement.  Failed [7504 : HY000] in UDF/XSP/UDM DBC.IRR: SQLSTATE U0001: 
Elapsed time = 00:00:01.628 

STATEMENT 1: Select Statement failed. 


Here is my code which im incorporating the array.

Is there any thing wrong with the syntax?

#include &lt;sqltypes_td.h&gt;
#include &lt;string.h&gt;
#include &lt;math.h&gt;
typedef struct agr_storage {
        FLOAT count;
        FLOAT val1,val2,val3,val4;
    FLOAT res[100]; // This is my array
} AGR_Storage;

",3
179,37317534,Teradata Week Calculation,"I have a query in teradata where i am trying to get the week number from a specific date in the format yyyymmdd (20160201). We have a calendar table (not the teradata one because we count weeks slightly differently) which allows you to join the date and export the results

When i run the query with static dates so for the example below runs fine

FROM table_main AL1
 JOIN cal_table cal
  ON  AL1.run_date = cal.cal_dateyyyymmdd
WHERE AL1.run_date &gt;= 20160201
AND AL1.run_date &lt; 20160220


When i try to generalize the statement to the previous week

FROM table_main AL1
 JOIN cal_table cal
  ON  AL1.run_date = cal.cal_dateyyyymmdd
WHERE AL1.run_date &gt;= CAST(CAST(((DATE-DAYOFWEEK(DATE)-5) (FORMAT 'YYYYMMDD')) AS CHAR(8)) AS INT)
AND AL1.run_date &lt; CAST(CAST(((DATE-DAYOFWEEK(DATE)+ 1) (FORMAT 'YYYYMMDD')) AS CHAR(8)) AS INT)


I get the error


  SELECT Failed. 3706:  Syntax error: expected something between ')' and '-'. 


Has anyone ever seen this before?
",-1,-1,-1.0,"I have a query in teradata where i am trying to get the week number from a specific date in the format yyyymmdd (20160201). We have a calendar table (not the teradata one because we count weeks slightly differently) which allows you to join the date and export the results

When i run the query with static dates so for the example below runs fine

FROM table_main AL1
 JOIN cal_table cal
  ON  AL1.run_date = cal.cal_dateyyyymmdd
WHERE AL1.run_date &gt;= 20160201
AND AL1.run_date &lt; 20160220


When i try to generalize the statement to the previous week

FROM table_main AL1
 JOIN cal_table cal
  ON  AL1.run_date = cal.cal_dateyyyymmdd
WHERE AL1.run_date &gt;= CAST(CAST(((DATE-DAYOFWEEK(DATE)-5) (FORMAT 'YYYYMMDD')) AS CHAR(8)) AS INT)
AND AL1.run_date &lt; CAST(CAST(((DATE-DAYOFWEEK(DATE)+ 1) (FORMAT 'YYYYMMDD')) AS CHAR(8)) AS INT)


I get the error


  SELECT Failed. 3706:  Syntax error: expected something between ')' and '-'. 


Has anyone ever seen this before?
",3
180,37468973,Teradata-sqlalchemy not using database when given,"I'm trying to connect to our internal Teradata database, using flask and sqlAlchemy along with a custom engine called sqlalchemy teradata. I put the database into the create_engine function likes so.

engine = sqlalchemy.create_engine('teradata://username:pw@server_name/database')


I've setup my dialect just like in the tests

registry.register(""tdalchemy"", ""sqlalchemy_teradata.dialect"", ""TeradataDialect"")


I'm getting a.

DatabaseError: (teradata.api.DatabaseError) (3807, u""[42S02] [Teradata][ODBC Teradata Driver][Teradata Database] Object 'table_name' does not exist


I can make raw sql queries just fine, I can also have alchemy do a query I construct and it pulls the data. I'm not sure what all is preventing things from working properly at all. When I test a similar call but looking at a database in an psql server it works just fine and pulls from that db without issue.

Also the pypi page says there is supposed to be an test/orm_test.py but it doesn't seem to have it.
",-1,-1,-1.0,"I'm trying to connect to our internal Teradata database, using flask and sqlAlchemy along with a custom engine called sqlalchemy teradata. I put the database into the create_engine function likes so.

engine = sqlalchemy.create_engine('teradata://username:pw@server_name/database')


I've setup my dialect just like in the tests

registry.register(""tdalchemy"", ""sqlalchemy_teradata.dialect"", ""TeradataDialect"")


I'm getting a.

DatabaseError: (teradata.api.DatabaseError) (3807, u""[42S02] [Teradata][ODBC Teradata Driver][Teradata Database] Object 'table_name' does not exist


I can make raw sql queries just fine, I can also have alchemy do a query I construct and it pulls the data. I'm not sure what all is preventing things from working properly at all. When I test a similar call but looking at a database in an psql server it works just fine and pulls from that db without issue.

Also the pypi page says there is supposed to be an test/orm_test.py but it doesn't seem to have it.
",1
181,37556713,How to recover from network error when running a large Teradata query?,"I have a java job that runs a query on Teradata and pushes the results to a local database. It's a large query (>80M records) and can take hours to finish (The slowness is not due to Teradata, but the local DB). Because it takes so long, there is a chance that it gets interrupted by a network error or something. When that happens I get this exception:

org.skife.jdbi.v2.exceptions.ResultSetException: Unable to advance result set

If the failure occurs a few hours into the query then it cannot rerun the query because the job needs to deliver the result before a specific time every day. Is there a way to resume the query after such failure? I'm not sure if pagination is a good option because the query involves joining a few tables and the tables are updated frequently.
",-1,-1,-1.0,"I have a java job that runs a query on Teradata and pushes the results to a local database. It's a large query (>80M records) and can take hours to finish (The slowness is not due to Teradata, but the local DB). Because it takes so long, there is a chance that it gets interrupted by a network error or something. When that happens I get this exception:

org.skife.jdbi.v2.exceptions.ResultSetException: Unable to advance result set

If the failure occurs a few hours into the query then it cannot rerun the query because the job needs to deliver the result before a specific time every day. Is there a way to resume the query after such failure? I'm not sure if pagination is a good option because the query involves joining a few tables and the tables are updated frequently.
",3
182,37627991,how to download and install teradata on windows 10,"Can any one help me install ""Teradata"" on Windows 10. A step by step guide will be much helpful.

I tried this in Google but didnt get any useful link.
",1,-1,-1.0,"Can any one help me install ""Teradata"" on Windows 10. A step by step guide will be much helpful.

I tried this in Google but didnt get any useful link.
",1
183,37777512,Teradata Python module cursor result set is exhausted after one iteration,"I am experimenting Teradata python module tutorial here 

I am executing a query and I want to iterate over the result set several times. The problem is that if i iterate over the result set once, I can't do it again. Looks like the result set is exhausted and no more available for any further computation.

Please see the below code for details and suggest how can I preserve the result set.

import teradata

class DB():
    def __init__(self):
        udaExec = teradata.UdaExec (appName=""HelloWorld"", version=""1.0"",logConsole=False)
        session = udaExec.connect(method=""odbc"", system=""tddemo"",username=""dbc"", password=""dbc"")
        self.session = session

    def fun1(self):
        rows = self.session.execute(""SELECT  databasename, ownername  FROM DBC.DATABASES  where DatabaseName='financial'"")
        return rows

db = DB()
rows = db.fun1()

# This loop prints accurate result like
#Row 1: [financial, Samples]
for row in rows:
    print(row)

# This loop does not print anything
for row in rows:
    print(row)  

# This line also gets printed
print(""The End"")

",-1,-1,-1.0,"I am experimenting Teradata python module tutorial here 

I am executing a query and I want to iterate over the result set several times. The problem is that if i iterate over the result set once, I can't do it again. Looks like the result set is exhausted and no more available for any further computation.

Please see the below code for details and suggest how can I preserve the result set.

import teradata

class DB():
    def __init__(self):
        udaExec = teradata.UdaExec (appName=""HelloWorld"", version=""1.0"",logConsole=False)
        session = udaExec.connect(method=""odbc"", system=""tddemo"",username=""dbc"", password=""dbc"")
        self.session = session

    def fun1(self):
        rows = self.session.execute(""SELECT  databasename, ownername  FROM DBC.DATABASES  where DatabaseName='financial'"")
        return rows

db = DB()
rows = db.fun1()

# This loop prints accurate result like
#Row 1: [financial, Samples]
for row in rows:
    print(row)

# This loop does not print anything
for row in rows:
    print(row)  

# This line also gets printed
print(""The End"")

",1
184,37781974,Teradata Python module show table statement does not work,"I am experimenting with Teradata-Python module and I am new to python

I am trying to fetch table ddl with the help of SHOW TABLE statement and it behaves weird and only returns a few words out of whole DDL. Please see my attempt and error below

import teradata
class DB():
    def __init__(self):
        udaExec = teradata.UdaExec (appName=""test"", version=""1.0"",logConsole=False)
        session = udaExec.connect(method=""odbc"", system=""tddemo"",username=""dbc"", password=""dbc"")
        self.session = session

    def fun1(self):
        # session.execute(""create table financial.dummytable1(a varchar(10))"")
        rows = self.session.execute(""SHOW TABLE financial.dummytable1"")
        for row in rows:
            print(row)
db = DB()
db.fun1()

print(""---The End---"")


Here is the unexpected result

PRIMARY INDEX ( a );]HARACTER SET LATIN NOT CASESPECIFIC)CK ,
---The End---


The desired result

CREATE SET TABLE financial.dummytable1 ,NO FALLBACK ,
     NO BEFORE JOURNAL,
     NO AFTER JOURNAL,
     CHECKSUM = DEFAULT,
     DEFAULT MERGEBLOCKRATIO
     (
      a VARCHAR(10) CHARACTER SET LATIN NOT CASESPECIFIC)
PRIMARY INDEX ( a );
---The End---


Please help me understand whats going on here.
",1,-1,-1.0,"I am experimenting with Teradata-Python module and I am new to python

I am trying to fetch table ddl with the help of SHOW TABLE statement and it behaves weird and only returns a few words out of whole DDL. Please see my attempt and error below

import teradata
class DB():
    def __init__(self):
        udaExec = teradata.UdaExec (appName=""test"", version=""1.0"",logConsole=False)
        session = udaExec.connect(method=""odbc"", system=""tddemo"",username=""dbc"", password=""dbc"")
        self.session = session

    def fun1(self):
        # session.execute(""create table financial.dummytable1(a varchar(10))"")
        rows = self.session.execute(""SHOW TABLE financial.dummytable1"")
        for row in rows:
            print(row)
db = DB()
db.fun1()

print(""---The End---"")


Here is the unexpected result

PRIMARY INDEX ( a );]HARACTER SET LATIN NOT CASESPECIFIC)CK ,
---The End---


The desired result

CREATE SET TABLE financial.dummytable1 ,NO FALLBACK ,
     NO BEFORE JOURNAL,
     NO AFTER JOURNAL,
     CHECKSUM = DEFAULT,
     DEFAULT MERGEBLOCKRATIO
     (
      a VARCHAR(10) CHARACTER SET LATIN NOT CASESPECIFIC)
PRIMARY INDEX ( a );
---The End---


Please help me understand whats going on here.
",1
185,37816763,Teradata client for Ruby,"i'm kind of begginer in ruby and my question is how to remote a Teradata database using ruby, and for that I found a gem that does the job  but I have no idea how to make it work.

The gem I'm talking about is teradata-cli (0.0.12), and it requires CLIV2 for teradata, a set of callable services to do stuff on teradata databases. 

So I couldn't manage to make the gem work because I don't know how to manage dependencies, any hints to do that ? 
",-1,-1,-1.0,"i'm kind of begginer in ruby and my question is how to remote a Teradata database using ruby, and for that I found a gem that does the job  but I have no idea how to make it work.

The gem I'm talking about is teradata-cli (0.0.12), and it requires CLIV2 for teradata, a set of callable services to do stuff on teradata databases. 

So I couldn't manage to make the gem work because I don't know how to manage dependencies, any hints to do that ? 
",1
186,37871607,Python Query From Teradata pyodbc Module,"import pyodbc
conn = pyodbc.connect('DRIVER={Teradata};DBCNAME=&lt;DATABASE_NAME&gt;;UID=&lt;UID&gt;;PWD=&lt;UID&gt;;QUIETMODE=YES;')


What actually goes in the Driver={} part?  Is this where I put the teradata .jar file such as terajdbc4.jar?  

But this doesnt work, I get the error:

pyodbc.Error: ('01000', ""[01000] [unixODBC][Driver Manager]Can't open lib 'terajdbc4.jar' : file not found (0) (SQLDriverConnect)"")

",-1,-1,-1.0,"import pyodbc
conn = pyodbc.connect('DRIVER={Teradata};DBCNAME=&lt;DATABASE_NAME&gt;;UID=&lt;UID&gt;;PWD=&lt;UID&gt;;QUIETMODE=YES;')


What actually goes in the Driver={} part?  Is this where I put the teradata .jar file such as terajdbc4.jar?  

But this doesnt work, I get the error:

pyodbc.Error: ('01000', ""[01000] [unixODBC][Driver Manager]Can't open lib 'terajdbc4.jar' : file not found (0) (SQLDriverConnect)"")

",1
187,38221369,Teradata TPT_INFRA: TPT02640: Error: Conflicting column count,"I am facing issues in Teradata parallel transporter while loading data from TEST to DEV environment. The table structure are the same, however I face the error


  TPT_INFRA: TPT02640: Error: Conflicting column count. Source column
  count (20) Target column count (13). EXPORT_OPERATOR: TPT12108: Output
  Schema does not match data from SELECT statement

",-1,-1,-1.0,"I am facing issues in Teradata parallel transporter while loading data from TEST to DEV environment. The table structure are the same, however I face the error


  TPT_INFRA: TPT02640: Error: Conflicting column count. Source column
  count (20) Target column count (13). EXPORT_OPERATOR: TPT12108: Output
  Schema does not match data from SELECT statement

",3
188,38315266,Reading (even joining) a very large (1.1bn row) table in Enterprise Guide from Teradata,"Hopefully you guys can help with what I'm hoping is quite a simple question for those in the know!

I live (well, work) in SAS Enterprise Guide and am trying to perform a simple left join against a table in Teradata.

The table is extremely large (700+ columns, 1.1bn rows) and so far I have been connecting via a LIBNAME statement at the top of my program, followed by the usual PROC SQL to read the data.

The issue I am having is its is extremely slow. I performed the join successfully using 90 rows on the left table and it took 3 hours to complete. The real table I want to use has something like 15,000 rows.

I have tried to connect via the SQL Pass-Through method, but this throws a hosts file error, which I can't fix due to corporate security limitations.

Has anyone had any experience performing this kind of task?

I should mention that I can run a simple select * query in Teradata SQL Assistant is just over 1 minute (16,666,666 obs/s!) so the limitation must be somewhere between SAS/Teradata, or even SAS itself.

I'm sorry I haven't posted actual code snippets as they're on my work machine but this has been bugging me for ages so thought I'd see if I'm missing any tricks.

Thanks in advance for your help.
",1,-1,-1.0,"Hopefully you guys can help with what I'm hoping is quite a simple question for those in the know!

I live (well, work) in SAS Enterprise Guide and am trying to perform a simple left join against a table in Teradata.

The table is extremely large (700+ columns, 1.1bn rows) and so far I have been connecting via a LIBNAME statement at the top of my program, followed by the usual PROC SQL to read the data.

The issue I am having is its is extremely slow. I performed the join successfully using 90 rows on the left table and it took 3 hours to complete. The real table I want to use has something like 15,000 rows.

I have tried to connect via the SQL Pass-Through method, but this throws a hosts file error, which I can't fix due to corporate security limitations.

Has anyone had any experience performing this kind of task?

I should mention that I can run a simple select * query in Teradata SQL Assistant is just over 1 minute (16,666,666 obs/s!) so the limitation must be somewhere between SAS/Teradata, or even SAS itself.

I'm sorry I haven't posted actual code snippets as they're on my work machine but this has been bugging me for ages so thought I'd see if I'm missing any tricks.

Thanks in advance for your help.
",3
189,38336419,Sqoop export from hive to teradata does not work with timestamp field,"I am trying to export a file from hive which has one field as timestamp ('2016-05-21 02:00:00') to teradata. 
The datatype in teradata is timestamp(0) which can expect a similar format.

When i try to export it using sqoop, it gives string to timestamp conversion error.
Any workarounds this would greatly help.

Note: 


String in hive to Varchar(256) in teradata works. 
String in hive to timestamp(0) in teradata fails.
Timestamp in hive to timestamp(0) in teradata fails.


Logs :

16/07/12 12:24:20 INFO mapreduce.Job:  map 0% reduce 0%
16/07/12 12:24:27 INFO mapreduce.Job: Task Id : attempt_1467306662019_8607_m_000001_0, Status : FAILED
Error: java.lang.RuntimeException: Unparseable date: ""2016-07-11 22:36:55""
    at com.teradata.connector.common.converter.ConnectorDataTypeConverter$StringFMTTZToTimestampTZ.convert(ConnectorDataTypeConverter.java:1679)
    at com.teradata.connector.teradata.converter.TeradataConverter.convert(TeradataConverter.java:143)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:106)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:65)
    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:658)
    at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
    at com.teradata.connector.common.ConnectorMMapper.map(ConnectorMMapper.java:129)
    at com.teradata.connector.common.ConnectorMMapper.run(ConnectorMMapper.java:117)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

",1,-1,-1.0,"I am trying to export a file from hive which has one field as timestamp ('2016-05-21 02:00:00') to teradata. 
The datatype in teradata is timestamp(0) which can expect a similar format.

When i try to export it using sqoop, it gives string to timestamp conversion error.
Any workarounds this would greatly help.

Note: 


String in hive to Varchar(256) in teradata works. 
String in hive to timestamp(0) in teradata fails.
Timestamp in hive to timestamp(0) in teradata fails.


Logs :

16/07/12 12:24:20 INFO mapreduce.Job:  map 0% reduce 0%
16/07/12 12:24:27 INFO mapreduce.Job: Task Id : attempt_1467306662019_8607_m_000001_0, Status : FAILED
Error: java.lang.RuntimeException: Unparseable date: ""2016-07-11 22:36:55""
    at com.teradata.connector.common.converter.ConnectorDataTypeConverter$StringFMTTZToTimestampTZ.convert(ConnectorDataTypeConverter.java:1679)
    at com.teradata.connector.teradata.converter.TeradataConverter.convert(TeradataConverter.java:143)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:106)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:65)
    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:658)
    at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
    at com.teradata.connector.common.ConnectorMMapper.map(ConnectorMMapper.java:129)
    at com.teradata.connector.common.ConnectorMMapper.run(ConnectorMMapper.java:117)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)

",0
190,38474659,Teradata LIKE ANY causing duplicates,"I have a query which looks something like below which aim is to see if a two letter code exists in a string. If it does, set a column to yes, if it doesnt set a column to no

SELECT ID,
    case when Table_a.item LIKE ANY ('%AA%','%AB%','%AC%','%AD%','%AE%','%FF%',' %GG%','%HR%','%TR%','%ST%','%VL%') THEN 'YES' else 'NO' end AS foo,
item 
FROM Table_a


Unfortunately its causing rows to appear twice

ID      foo    item     
112     yes    AA-FF-TT-RR
112     no     AA-FF-TT-RR


Does anyone know why. 
Am i misusing the LIKE ANY function
We are not on Teradata 14 yet

Thank you for your time
",-1,-1,-1.0,"I have a query which looks something like below which aim is to see if a two letter code exists in a string. If it does, set a column to yes, if it doesnt set a column to no

SELECT ID,
    case when Table_a.item LIKE ANY ('%AA%','%AB%','%AC%','%AD%','%AE%','%FF%',' %GG%','%HR%','%TR%','%ST%','%VL%') THEN 'YES' else 'NO' end AS foo,
item 
FROM Table_a


Unfortunately its causing rows to appear twice

ID      foo    item     
112     yes    AA-FF-TT-RR
112     no     AA-FF-TT-RR


Does anyone know why. 
Am i misusing the LIKE ANY function
We are not on Teradata 14 yet

Thank you for your time
",3
191,38152103,how to avoid a join in teradata,"I have kind of impossible question : i am working with teradata, i have a table from which i have to take a field, BUT i need for this to make a join with another table that does not contain a field to make a join with the first table.
So my question is : is there any way to do my request and to take the field i need with another solution than a join ? For me, it seems impossible, but maybe there is a way ( or ways) that i don't know ?

I hope that my question is clear.

PS : please don't tell me unoptimized solutions ;)

Cheers !

BLG.
",-1,-1,-1.0,"I have kind of impossible question : i am working with teradata, i have a table from which i have to take a field, BUT i need for this to make a join with another table that does not contain a field to make a join with the first table.
So my question is : is there any way to do my request and to take the field i need with another solution than a join ? For me, it seems impossible, but maybe there is a way ( or ways) that i don't know ?

I hope that my question is clear.

PS : please don't tell me unoptimized solutions ;)

Cheers !

BLG.
",3
192,38114854,Teradata-cli ruby gem is not getting installed on Windows,"I am trying to connect to Teradata using ruby script, for which i require teradata-cli ruby gem. But the gem is not getting installed and following error is getting thrown:


  gem install teradata-cli


Temporarily enhancing PATH to include DevKit...
Building native extensions.  This could take a while...
ERROR:  Error installing teradata-cli:
        ERROR: Failed to build gem native extension.

    C:/Vendor/ruby-2.0.0-p451/bin/ruby.exe extconf.rb
checking for ruby.h... yes
checking for main() in -lwincli32... no
*** extconf.rb failed ***
Could not create Makefile due to some reason, probably lack of necessary
libraries and/or headers.  Check the mkmf.log file for more details.  
You may need configuration options.

Provided configuration options:
        --with-opt-dir
        --without-opt-dir
        --with-opt-include
        --without-opt-include=${opt-dir}/include
        --with-opt-lib
        --without-opt-lib=${opt-dir}/lib
        --with-make-prog
        --without-make-prog
        --srcdir=.
        --curdir
        --ruby=C:/Vendor/ruby-2.0.0-p451/bin/ruby
        --with-cli-dir
        --without-cli-dir
        --with-cli-include
        --without-cli-include=${cli-dir}/include
        --with-cli-lib
        --without-cli-lib=${cli-dir}/
        --with-wincli32lib
        --without-wincli32lib

extconf failed, exit code 1

Gem files will remain installed in C:/Vendor/ruby-2.0.0-p451/lib/ruby/gems/2.0.0/gems/teradata-cli-0.0.12 for inspection.
Results logged to C:/Vendor/ruby-2.0.0-p451/lib/ruby/gems/2.0.0/extensions/x86-mingw32/2.0.0/teradata-cli-0.0.12/gem_make.out


To resolve above issue, I installed cliv2 on my machine and again tried to install the gem as follows:


  gem install teradata-cli -- '--with-cli-lib=""C:\Teradata_cliv2_x64\Teradata\Client\14.00\CLIv2\lib"" --with-cli-include=""C:\Teradata_cliv2_x64\Teradata\Client\14.00\CLIv2\inc""'


where the lib and include are the directories where Cliv2 client is installed.

Now the error shown above is not thrown but following error is displayed: 

C:/Vendor/ruby-2.0.0-p451/bin/ruby.exe extconf.rb --with-cli-lib=""C:\Teradata_cliv2_x64\Teradata\Client\14.00\CLIv2\lib"" --with-cli-in
de=""C:\Teradata_cliv2_x64\Teradata\Client\14.00\CLIv2\inc""
checking for ruby.h... yes
checking for main() in -lwincli32... yes
creating Makefile

make ""DESTDIR="" clean

make ""DESTDIR=""
generating cli-i386-mingw32.def
compiling cli.c
cli.c: In function 'status_name':
cli.c:269:17: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:270:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:271:19: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:272:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:273:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:274:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:275:24: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:276:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:277:23: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:278:24: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:279:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:280:26: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:281:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:282:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:283:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:284:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:285:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:286:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:287:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c: In function 'flavor_name':
cli.c:308:18: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:309:25: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:310:19: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:311:19: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:312:23: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:313:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:314:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:315:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:316:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:317:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:318:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:319:27: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:320:25: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:321:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:322:27: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:323:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:324:24: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:325:17: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:326:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:327:24: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:328:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:329:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:330:23: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:331:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:332:23: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:333:24: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:334:19: warning: return discards 'const' qualifier from pointer target type [enabled by default]
In file included from cli.c:20:0:
cli.c: At top level:
C:\Teradata_cliv2_x64\Teradata\Client\14.00\CLIv2\inc/dbcarea.h:270:13: warning: 'D8XIIIRX' defined but not used [-Wunused-variable]
linking shared-object teradata-cli/cli.so
cli.o: In function `logoff':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:172: undefined reference to `DBCHCL@12'
cli.o: In function `cli_cleanup':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:194: undefined reference to `DBCHCLN@8'
cli.o: In function `cli_initialize':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:100: undefined reference to `DBCHINI@12'
cli.o: In function `dispatch':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:258: undefined reference to `DBCHCL@12'
cli.o: In function `logoff':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:172: undefined reference to `DBCHCL@12'
cli.o: In function `dispatch':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:258: undefined reference to `DBCHCL@12'
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:258: undefined reference to `DBCHCL@12'
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:258: undefined reference to `DBCHCL@12'
collect2.exe: error: ld returned 1 exit status
make: *** [cli.so] Error 1

make failed, exit code 2

Gem files will remain installed in C:/Vendor/ruby-2.0.0-p451/lib/ruby/gems/2.0.0/gems/teradata-cli-0.0.12 for inspection.
Results logged to C:/Vendor/ruby-2.0.0-p451/lib/ruby/gems/2.0.0/extensions/x86-mingw32/2.0.0/teradata-cli-0.0.12/gem_make.out


Please guide how this issue can be resolved.
",-1,-1,-1.0,"I am trying to connect to Teradata using ruby script, for which i require teradata-cli ruby gem. But the gem is not getting installed and following error is getting thrown:


  gem install teradata-cli


Temporarily enhancing PATH to include DevKit...
Building native extensions.  This could take a while...
ERROR:  Error installing teradata-cli:
        ERROR: Failed to build gem native extension.

    C:/Vendor/ruby-2.0.0-p451/bin/ruby.exe extconf.rb
checking for ruby.h... yes
checking for main() in -lwincli32... no
*** extconf.rb failed ***
Could not create Makefile due to some reason, probably lack of necessary
libraries and/or headers.  Check the mkmf.log file for more details.  
You may need configuration options.

Provided configuration options:
        --with-opt-dir
        --without-opt-dir
        --with-opt-include
        --without-opt-include=${opt-dir}/include
        --with-opt-lib
        --without-opt-lib=${opt-dir}/lib
        --with-make-prog
        --without-make-prog
        --srcdir=.
        --curdir
        --ruby=C:/Vendor/ruby-2.0.0-p451/bin/ruby
        --with-cli-dir
        --without-cli-dir
        --with-cli-include
        --without-cli-include=${cli-dir}/include
        --with-cli-lib
        --without-cli-lib=${cli-dir}/
        --with-wincli32lib
        --without-wincli32lib

extconf failed, exit code 1

Gem files will remain installed in C:/Vendor/ruby-2.0.0-p451/lib/ruby/gems/2.0.0/gems/teradata-cli-0.0.12 for inspection.
Results logged to C:/Vendor/ruby-2.0.0-p451/lib/ruby/gems/2.0.0/extensions/x86-mingw32/2.0.0/teradata-cli-0.0.12/gem_make.out


To resolve above issue, I installed cliv2 on my machine and again tried to install the gem as follows:


  gem install teradata-cli -- '--with-cli-lib=""C:\Teradata_cliv2_x64\Teradata\Client\14.00\CLIv2\lib"" --with-cli-include=""C:\Teradata_cliv2_x64\Teradata\Client\14.00\CLIv2\inc""'


where the lib and include are the directories where Cliv2 client is installed.

Now the error shown above is not thrown but following error is displayed: 

C:/Vendor/ruby-2.0.0-p451/bin/ruby.exe extconf.rb --with-cli-lib=""C:\Teradata_cliv2_x64\Teradata\Client\14.00\CLIv2\lib"" --with-cli-in
de=""C:\Teradata_cliv2_x64\Teradata\Client\14.00\CLIv2\inc""
checking for ruby.h... yes
checking for main() in -lwincli32... yes
creating Makefile

make ""DESTDIR="" clean

make ""DESTDIR=""
generating cli-i386-mingw32.def
compiling cli.c
cli.c: In function 'status_name':
cli.c:269:17: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:270:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:271:19: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:272:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:273:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:274:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:275:24: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:276:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:277:23: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:278:24: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:279:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:280:26: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:281:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:282:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:283:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:284:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:285:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:286:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:287:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c: In function 'flavor_name':
cli.c:308:18: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:309:25: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:310:19: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:311:19: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:312:23: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:313:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:314:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:315:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:316:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:317:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:318:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:319:27: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:320:25: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:321:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:322:27: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:323:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:324:24: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:325:17: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:326:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:327:24: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:328:20: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:329:21: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:330:23: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:331:22: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:332:23: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:333:24: warning: return discards 'const' qualifier from pointer target type [enabled by default]
cli.c:334:19: warning: return discards 'const' qualifier from pointer target type [enabled by default]
In file included from cli.c:20:0:
cli.c: At top level:
C:\Teradata_cliv2_x64\Teradata\Client\14.00\CLIv2\inc/dbcarea.h:270:13: warning: 'D8XIIIRX' defined but not used [-Wunused-variable]
linking shared-object teradata-cli/cli.so
cli.o: In function `logoff':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:172: undefined reference to `DBCHCL@12'
cli.o: In function `cli_cleanup':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:194: undefined reference to `DBCHCLN@8'
cli.o: In function `cli_initialize':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:100: undefined reference to `DBCHINI@12'
cli.o: In function `dispatch':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:258: undefined reference to `DBCHCL@12'
cli.o: In function `logoff':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:172: undefined reference to `DBCHCL@12'
cli.o: In function `dispatch':
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:258: undefined reference to `DBCHCL@12'
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:258: undefined reference to `DBCHCL@12'
C:\Vendor\ruby-2.0.0-p451\lib\ruby\gems\2.0.0\gems\teradata-cli-0.0.12\ext\teradata-cli\cli/cli.c:258: undefined reference to `DBCHCL@12'
collect2.exe: error: ld returned 1 exit status
make: *** [cli.so] Error 1

make failed, exit code 2

Gem files will remain installed in C:/Vendor/ruby-2.0.0-p451/lib/ruby/gems/2.0.0/gems/teradata-cli-0.0.12 for inspection.
Results logged to C:/Vendor/ruby-2.0.0-p451/lib/ruby/gems/2.0.0/extensions/x86-mingw32/2.0.0/teradata-cli-0.0.12/gem_make.out


Please guide how this issue can be resolved.
",2
193,38061298,Writing JOIN Commands on Teradata,"q) Use COUNT and DISTINCT to determine how many distinct skus there are
in the skuinfo, skstinfo, and trnsact tables. Which skus are common to all tables, or unique to specific tables?  

A) I was trying to find the solution to the above q in TERADATA. The first part was simple i was able to run three commands and got the distinct skus (stock keeping unit).  

Now to find the common sku in all three table i am running the command but having errors :

SELECT COUNT(DISTINCT a.sku),COUNT(DISTINCT b.sku),COUNT(DISTINCT c.sku)
FROM skuinfo a INNER JOIN skstinfo b INNER JOIN trnsact c
ON a.sku=b.sku AND b.sku=c.sku;// Why is there error if i use **Where** in Place of ON?



**Error Occurred . . .**
[com.teradata.commons.datatools.sqlparsers.common.ParseException: Encountered "";"" at line 3, column 31. Was expecting one of: ""and"" ... ""at"" ... ""cross"" ... ""day"" ... ""full"" ... ""hour"" ... ""inner"" ... ""join"" ... ""left"" ... ""minute"" ... ""month"" ... ""on"" ... ""or"" ... ""right"" ... ""second"" ... ""timezone_hour"" ... ""timezone_minute"" ... ""year"" ... ""||"" ... ""("" ... ""**"" ... ""+"" ... ""-"" ... ""*"" ... ""/"" ... ""mod"" ... ""."" ... ""["" ... ]


Please let me know why is my syntax not working, though most forums say the query is right. Thanks
",-1,-1,-1.0,"q) Use COUNT and DISTINCT to determine how many distinct skus there are
in the skuinfo, skstinfo, and trnsact tables. Which skus are common to all tables, or unique to specific tables?  

A) I was trying to find the solution to the above q in TERADATA. The first part was simple i was able to run three commands and got the distinct skus (stock keeping unit).  

Now to find the common sku in all three table i am running the command but having errors :

SELECT COUNT(DISTINCT a.sku),COUNT(DISTINCT b.sku),COUNT(DISTINCT c.sku)
FROM skuinfo a INNER JOIN skstinfo b INNER JOIN trnsact c
ON a.sku=b.sku AND b.sku=c.sku;// Why is there error if i use **Where** in Place of ON?



**Error Occurred . . .**
[com.teradata.commons.datatools.sqlparsers.common.ParseException: Encountered "";"" at line 3, column 31. Was expecting one of: ""and"" ... ""at"" ... ""cross"" ... ""day"" ... ""full"" ... ""hour"" ... ""inner"" ... ""join"" ... ""left"" ... ""minute"" ... ""month"" ... ""on"" ... ""or"" ... ""right"" ... ""second"" ... ""timezone_hour"" ... ""timezone_minute"" ... ""year"" ... ""||"" ... ""("" ... ""**"" ... ""+"" ... ""-"" ... ""*"" ... ""/"" ... ""mod"" ... ""."" ... ""["" ... ]


Please let me know why is my syntax not working, though most forums say the query is right. Thanks
",3
194,38046897,Teradata JDBC executeBatch errorhandling,"I am inserting data into a teradata table using executeBatch method. Currently if one insert in the batch fails all the other inserts in the batch also fails and no records end up being inserted. How can I change this behaviour to let the other inserts in the batch succeed if any inserts fails and the some ability to track the rejected records.

PS: I have ensured that TMODE is set to TERA and autocommit enabled.

UPDATE:

target table definition.

CREATE SET TABLE mydb.mytable ,NO FALLBACK ,
     NO BEFORE JOURNAL,
     NO AFTER JOURNAL,
     CHECKSUM = DEFAULT,
     DEFAULT MERGEBLOCKRATIO
     (
      col1 INTEGER,
      col2 VARCHAR(10) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL)
PRIMARY INDEX ( col1 );


Below is the sample scala code. As you can see, this batch contains 5 insert statements. The First insert is set to fail because it is trying to insert null into an not null field (col2). The other 4 inserts dont have any issues and should succeed. But as you can see from below all 5 inserts in the batch failed. Is there any way we can make other inserts succeed?. As stated above tmode is tera and autocommit is enabled. if there is no way other than re-submitting all failed queries individually then we would have to reduce the batch size and settle for lower throughput. 

Class.forName(""com.teradata.jdbc.TeraDriver"");
val conn = DriverManager.getConnection(""jdbc:teradata://teradata-server/mydb,tmode=TERA"",""username"",""password"")
val insertSQL =  ""INSERT INTO mydb.mytable VALUES (?,?)""
val stmt = conn.prepareStatement(insertSQL)


stmt.setInt(1,1)
stmt.setNull(2,Types.VARCHAR)  // Inserting Null here. This insert will fail
stmt.addBatch()

stmt.setInt(1,2)
stmt.setString(2,""XXX"")
stmt.addBatch()

stmt.setInt(1,3)
stmt.setString(2,""YYY"")
stmt.addBatch()

stmt.setInt(1,4)
stmt.setString(2,""ZZZ"")
stmt.addBatch()

stmt.setInt(1,5)
stmt.setString(2,""ABC"")
stmt.addBatch()

try {
val res = stmt.executeBatch()
println(res.mkString("",""))
}
catch {
 case th: BatchUpdateException =&gt; {
        println(th.getUpdateCounts().mkString("",""))
 }
}


Result

-3,-3,-3,-3,-3
",-1,-1,-1.0,"I am inserting data into a teradata table using executeBatch method. Currently if one insert in the batch fails all the other inserts in the batch also fails and no records end up being inserted. How can I change this behaviour to let the other inserts in the batch succeed if any inserts fails and the some ability to track the rejected records.

PS: I have ensured that TMODE is set to TERA and autocommit enabled.

UPDATE:

target table definition.

CREATE SET TABLE mydb.mytable ,NO FALLBACK ,
     NO BEFORE JOURNAL,
     NO AFTER JOURNAL,
     CHECKSUM = DEFAULT,
     DEFAULT MERGEBLOCKRATIO
     (
      col1 INTEGER,
      col2 VARCHAR(10) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL)
PRIMARY INDEX ( col1 );


Below is the sample scala code. As you can see, this batch contains 5 insert statements. The First insert is set to fail because it is trying to insert null into an not null field (col2). The other 4 inserts dont have any issues and should succeed. But as you can see from below all 5 inserts in the batch failed. Is there any way we can make other inserts succeed?. As stated above tmode is tera and autocommit is enabled. if there is no way other than re-submitting all failed queries individually then we would have to reduce the batch size and settle for lower throughput. 

Class.forName(""com.teradata.jdbc.TeraDriver"");
val conn = DriverManager.getConnection(""jdbc:teradata://teradata-server/mydb,tmode=TERA"",""username"",""password"")
val insertSQL =  ""INSERT INTO mydb.mytable VALUES (?,?)""
val stmt = conn.prepareStatement(insertSQL)


stmt.setInt(1,1)
stmt.setNull(2,Types.VARCHAR)  // Inserting Null here. This insert will fail
stmt.addBatch()

stmt.setInt(1,2)
stmt.setString(2,""XXX"")
stmt.addBatch()

stmt.setInt(1,3)
stmt.setString(2,""YYY"")
stmt.addBatch()

stmt.setInt(1,4)
stmt.setString(2,""ZZZ"")
stmt.addBatch()

stmt.setInt(1,5)
stmt.setString(2,""ABC"")
stmt.addBatch()

try {
val res = stmt.executeBatch()
println(res.mkString("",""))
}
catch {
 case th: BatchUpdateException =&gt; {
        println(th.getUpdateCounts().mkString("",""))
 }
}


Result

-3,-3,-3,-3,-3
",3
195,37723162,Not able to connect VMware teradata database from windows 8(Host Machine) java application,"I have installed in windows8 teradata in VMwere by following bellow link

http://kosmisch.net/Blog/DataAndBusinessIntelligence/Archive/2014/6/29/1a7ee23474544efb9e2318c7a771f74f.html

Now i am trying to connect the db from windows java application, but i am not able to connect it.
i have given below credentials

con = DriverManager.getConnection(""jdbc:teradata://192.168.10.182:1025/database=DBC"", ""dbc"", ""dbc"");


getting exception like

2016-06-09.13:15:42.126 TERAJDBC4 ERROR [main]
com.teradata.jdbc.jdk6.JDK6_SQL_Connection@264b898 Connection to 192.168.128.128:1025 Thu Jun 09 13:15:42 IST 2016
socket orig=192.168.128.128:1025 cid=cd63cd3 sess=0 java.net.UnknownHostException: 192.168.128.128:1025: invalid IPv6 address 
at java.net.InetAddress.getAllByName(InetAddress.java:1141)   at java.net.InetAddress.getAllByName(InetAddress.java:1098) 
at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$Lookup.&lt;init&gt;(TDNetworkIOIF.java:201)   at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.connectToHost
(TDNetworkIOIF.java:301)   at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.createSocketConnection(TDNetworkIOIF.java:149)
at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.&lt;init&gt;(TDNetworkIOIF.java:135)
at com.teradata.jdbc.jdbc.GenericTeradataConnection.getIO(GenericTeradataConnection.java:130)
at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:98)   at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:207)
at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:35)  
at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)   
at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:179)   
at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:169)   
at com.teradata.jdbc.TeraDriver.doConnect(TeraDriver.java:234)  
at com.teradata.jdbc.TeraDriver.connect(TeraDriver.java:160)   
at java.sql.DriverManager.getConnection(DriverManager.java:571)   
at java.sql.DriverManager.getConnection(DriverManager.java:215)   
at xteradatademo.TeradataExampl.buildCon(TeradataExampl.java:33)   
at xteradatademo.TeradataExampl.main(TeradataExampl.java:19)  


Can any one please help me.
",-1,-1,-1.0,"I have installed in windows8 teradata in VMwere by following bellow link

http://kosmisch.net/Blog/DataAndBusinessIntelligence/Archive/2014/6/29/1a7ee23474544efb9e2318c7a771f74f.html

Now i am trying to connect the db from windows java application, but i am not able to connect it.
i have given below credentials

con = DriverManager.getConnection(""jdbc:teradata://192.168.10.182:1025/database=DBC"", ""dbc"", ""dbc"");


getting exception like

2016-06-09.13:15:42.126 TERAJDBC4 ERROR [main]
com.teradata.jdbc.jdk6.JDK6_SQL_Connection@264b898 Connection to 192.168.128.128:1025 Thu Jun 09 13:15:42 IST 2016
socket orig=192.168.128.128:1025 cid=cd63cd3 sess=0 java.net.UnknownHostException: 192.168.128.128:1025: invalid IPv6 address 
at java.net.InetAddress.getAllByName(InetAddress.java:1141)   at java.net.InetAddress.getAllByName(InetAddress.java:1098) 
at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$Lookup.&lt;init&gt;(TDNetworkIOIF.java:201)   at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.connectToHost
(TDNetworkIOIF.java:301)   at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.createSocketConnection(TDNetworkIOIF.java:149)
at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.&lt;init&gt;(TDNetworkIOIF.java:135)
at com.teradata.jdbc.jdbc.GenericTeradataConnection.getIO(GenericTeradataConnection.java:130)
at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:98)   at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:207)
at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:35)  
at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)   
at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:179)   
at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:169)   
at com.teradata.jdbc.TeraDriver.doConnect(TeraDriver.java:234)  
at com.teradata.jdbc.TeraDriver.connect(TeraDriver.java:160)   
at java.sql.DriverManager.getConnection(DriverManager.java:571)   
at java.sql.DriverManager.getConnection(DriverManager.java:215)   
at xteradatademo.TeradataExampl.buildCon(TeradataExampl.java:33)   
at xteradatademo.TeradataExampl.main(TeradataExampl.java:19)  


Can any one please help me.
",0
196,38692335,Connecting to Teradata with pyodbc on OS X,"I am new to Mac and having issues trying to get pyodbc to work. I have installed:


Teradata ODBC driver from https://downloads.teradata.com/download/connectivity/teradata-odbc-driver-for-mac-os-x 
unixodbc via brew 
pyodbc via pip install


when I try to create a connection using the following connection string:

pyodbc.connect('DRIVER={Teradata};DBCName=XXX;DATABASE=XXX;Authentication=TD2;UID=XXX;PWD=XXX')

I get this error:

Error: ('01000', ""[01000] [unixODBC][Driver Manager]Can't open lib 'Teradata' : file not found (0) (SQLDriverConnect)"")

I have no idea where to go from here. Also, I am not sure if it is relevant but I am using virtualenvwrapper to create my python environment.

I see this similar question (return error is different). Connect Python to Teradata in mac with pyodbc
and ran the export statements but I am still getting the same error
",-1,-1,-1.0,"I am new to Mac and having issues trying to get pyodbc to work. I have installed:


Teradata ODBC driver from https://downloads.teradata.com/download/connectivity/teradata-odbc-driver-for-mac-os-x 
unixodbc via brew 
pyodbc via pip install


when I try to create a connection using the following connection string:

pyodbc.connect('DRIVER={Teradata};DBCName=XXX;DATABASE=XXX;Authentication=TD2;UID=XXX;PWD=XXX')

I get this error:

Error: ('01000', ""[01000] [unixODBC][Driver Manager]Can't open lib 'Teradata' : file not found (0) (SQLDriverConnect)"")

I have no idea where to go from here. Also, I am not sure if it is relevant but I am using virtualenvwrapper to create my python environment.

I see this similar question (return error is different). Connect Python to Teradata in mac with pyodbc
and ran the export statements but I am still getting the same error
",1
197,38883830,Correct way to configure connection between Teradata and python on Linux RHEL,"I installed teradata module for python2.7, the teradata client 15.00, also set the environment variables ODBCINI, ODBCINST and LD_LIBRARY_PATH correctly. But when I create my connection in my py script:

odbclib=""/opt/teradata/client/15.00/odbc_64/lib/libodbc.so""
udaExec = teradata.UdaExec (appName=""terapp"", version=""1.0"", logConsole=True, odbcLibPath=odbclib)
session = udaExec.connect(method=""odbc"", system=""XXX.XX.XX.XX"",username=user, password=pass)


I got this: 

File ""build/bdist.linux-x86_64/egg/teradata/udaexec.py"", line 183, in connect
  File ""build/bdist.linux-x86_64/egg/teradata/tdodbc.py"", line 374, in __init__
  File ""build/bdist.linux-x86_64/egg/teradata/tdodbc.py"", line 206, in checkStatus
 teradata.api.DatabaseError: (0, u'[IM003] [DataDirect][ODBC lib] Specified driver could not be loaded')


Please, any help clever people
",-1,-1,-1.0,"I installed teradata module for python2.7, the teradata client 15.00, also set the environment variables ODBCINI, ODBCINST and LD_LIBRARY_PATH correctly. But when I create my connection in my py script:

odbclib=""/opt/teradata/client/15.00/odbc_64/lib/libodbc.so""
udaExec = teradata.UdaExec (appName=""terapp"", version=""1.0"", logConsole=True, odbcLibPath=odbclib)
session = udaExec.connect(method=""odbc"", system=""XXX.XX.XX.XX"",username=user, password=pass)


I got this: 

File ""build/bdist.linux-x86_64/egg/teradata/udaexec.py"", line 183, in connect
  File ""build/bdist.linux-x86_64/egg/teradata/tdodbc.py"", line 374, in __init__
  File ""build/bdist.linux-x86_64/egg/teradata/tdodbc.py"", line 206, in checkStatus
 teradata.api.DatabaseError: (0, u'[IM003] [DataDirect][ODBC lib] Specified driver could not be loaded')


Please, any help clever people
",1
198,39416993,Best practices for creating large tables in Teradata,"I have a table with 100 million rows I'm trying to modify so it can be maintained and queried effectively.  We get about a million new rows for the table each month.
I thought I had it set up correctly with the primary index distributing rows equally and range partitions, one of which is by date, but when I tried altering the table to the current date to add a new partition, it took too long.

Now I'm not sure what else I should change.  My understanding is that each field participating in the PI and partitions should be indexed and have statistics collected for the parsing engine, even though maintaining those indexes and statistics takes time and space.

I've thought about extending the range of dates for a couple of years to avoid altering the partitions, but I've also read that Teradata doesn't recommend that practice.

So what else should I try?  Here's my create statement:

CREATE SET TABLE STAGE.PartD
     DATABLOCKSIZE = 1048064 BYTES
     (
      Efctv_uniq_id CHAR(13) NOT NULL,
      Cntrct_num CHAR(5) NOT NULL COMPRESS ('H2407','H2416','H2417','H2419','H2422','H2425','H2450','H2456','H2457','H2458','H2459','H2462','H5703','S0522','S4802','S5597','S5601','S5617','S5644','S5660','S5743','S5768','S5803','S5810','S5820','S5884','S5921','S5932','S5960','S5967','S7694'),
      Pbp_id SMALLINT NOT NULL COMPRESS (1 ,2 ,24 ,25 ,42 ,50 ,59 ,83 ,94 ,370 ,122 ,123 ,145 ,162 ,247 ),
      Hicn VARCHAR(20) NOT NULL,
      Cardhldr_id VARCHAR(20) COMPRESS '',
      Ptnt_date_of_birth DATE FORMAT 'YY/MM/DD' COMPRESS ,
      Ptnt_gender_cd CHAR(1) NOT NULL COMPRESS ('0','1','2'),
      Date_of_srvc DATE FORMAT 'YY/MM/DD' NOT NULL,
      Paid_dt DATE FORMAT 'YY/MM/DD' COMPRESS ,
      Product_srvc_id VARCHAR(19) NOT NULL COMPRESS ('~','62175011843','00088222033'),
      Srvc_prvdr_id_qlfyr CHAR(2) NOT NULL COMPRESS ('01','07','99'),
      Srvc_prvdr_id VARCHAR(15) NOT NULL,
      Fill_num BYTEINT NOT NULL COMPRESS (0 ,1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10 ,11 ,12 ,13 ,14 ),
      Dspnsng_stus CHAR(1) NOT NULL COMPRESS ' ',
      Cmpnd_cd CHAR(1) COMPRESS '1',
      Daw_prod_slctn_cd CHAR(1) COMPRESS '0',
      Qty_dspnsd CHAR(10) NOT NULL COMPRESS ('0000004000','0000010000','0000014000','0000015000','0000020000','0000028000','0000030000','0000031000','0000045000','0000056000','0000060000','0000090000','0000100000','0000120000','0000180000'),
      Days_suply SMALLINT COMPRESS (0 ,3 ,5 ,7 ,10 ,14 ,15 ,20 ,25 ,28 ,30 ,31 ,34 ,90 ),
      Prscrb_id_qlfyr CHAR(2) COMPRESS ('  ','01','12'),
      Prscrb_id VARCHAR(15) NOT NULL COMPRESS '',
      Drug_cvrg_stus_cd CHAR(1) COMPRESS (' ','C','E','O'),
      Adjsmt_del_cd CHAR(1) COMPRESS (' ','A','D'),
      Non_stand_frmt_cd CHAR(1) COMPRESS (' ','B','C','X'),
      Prcng_excptn_cd CHAR(1) COMPRESS (' ','M','O'),
      Uniq_id CHAR(13) NOT NULL,
      FinalVersionInd CHAR(1),
      LoadID SMALLINT NOT NULL)
PRIMARY INDEX pi ( Efctv_uniq_id ,LoadID )
PARTITION BY ( RANGE_N(LoadID  BETWEEN 1  AND 10000  EACH 1 ),
                            RANGE_N(Date_of_srvc  BETWEEN DATE '2007-01-01' AND ADD_MONTHS((DATE ),(-1 )) EACH INTERVAL '1' MONTH ) )
UNIQUE INDEX ui ( Efctv_uniq_id ,LoadID )
INDEX Efctv_uniq_id ( Efctv_uniq_id )
INDEX Date_of_srvc ( Date_of_srvc )
INDEX LoadID ( LoadID );


I need the UI in the staging environment so it doesn't allow the same data to get loaded more than once.  I've taken that index out of the table in the prod environment.

avgCurrentPerm is 722027691, maxPeakPerm is 730772992, and skewFactor is 0.094412.

Thanks for any help.



changed it to:

PRIMARY INDEX pi ( Efctv_uniq_id ,LoadID )
PARTITION BY ( 
RANGE_N(LoadID  BETWEEN 644  AND 10000  EACH 1 ),
RANGE_N(Date_of_srvc  BETWEEN DATE '2007-01-01' AND DATE '2030-01-01' 
EACH INTERVAL '1' MONTH ) );


seeing:
avgCurrentPerm 377035904    maxPeakPerm 377372160   skewFactor 0.089105

and collecting these stats:

COLLECT STATS 
COLUMN (PARTITION), 
COLUMN (loadid), 
COLUMN (Efctv_uniq_id), 
COLUMN (Date_of_srvc) 
ON STAGE.PartD;




and I'm getting the impression that creating unique indexes and partition ranges using the current date is a bad idea?
",1,-1,-1.0,"I have a table with 100 million rows I'm trying to modify so it can be maintained and queried effectively.  We get about a million new rows for the table each month.
I thought I had it set up correctly with the primary index distributing rows equally and range partitions, one of which is by date, but when I tried altering the table to the current date to add a new partition, it took too long.

Now I'm not sure what else I should change.  My understanding is that each field participating in the PI and partitions should be indexed and have statistics collected for the parsing engine, even though maintaining those indexes and statistics takes time and space.

I've thought about extending the range of dates for a couple of years to avoid altering the partitions, but I've also read that Teradata doesn't recommend that practice.

So what else should I try?  Here's my create statement:

CREATE SET TABLE STAGE.PartD
     DATABLOCKSIZE = 1048064 BYTES
     (
      Efctv_uniq_id CHAR(13) NOT NULL,
      Cntrct_num CHAR(5) NOT NULL COMPRESS ('H2407','H2416','H2417','H2419','H2422','H2425','H2450','H2456','H2457','H2458','H2459','H2462','H5703','S0522','S4802','S5597','S5601','S5617','S5644','S5660','S5743','S5768','S5803','S5810','S5820','S5884','S5921','S5932','S5960','S5967','S7694'),
      Pbp_id SMALLINT NOT NULL COMPRESS (1 ,2 ,24 ,25 ,42 ,50 ,59 ,83 ,94 ,370 ,122 ,123 ,145 ,162 ,247 ),
      Hicn VARCHAR(20) NOT NULL,
      Cardhldr_id VARCHAR(20) COMPRESS '',
      Ptnt_date_of_birth DATE FORMAT 'YY/MM/DD' COMPRESS ,
      Ptnt_gender_cd CHAR(1) NOT NULL COMPRESS ('0','1','2'),
      Date_of_srvc DATE FORMAT 'YY/MM/DD' NOT NULL,
      Paid_dt DATE FORMAT 'YY/MM/DD' COMPRESS ,
      Product_srvc_id VARCHAR(19) NOT NULL COMPRESS ('~','62175011843','00088222033'),
      Srvc_prvdr_id_qlfyr CHAR(2) NOT NULL COMPRESS ('01','07','99'),
      Srvc_prvdr_id VARCHAR(15) NOT NULL,
      Fill_num BYTEINT NOT NULL COMPRESS (0 ,1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,10 ,11 ,12 ,13 ,14 ),
      Dspnsng_stus CHAR(1) NOT NULL COMPRESS ' ',
      Cmpnd_cd CHAR(1) COMPRESS '1',
      Daw_prod_slctn_cd CHAR(1) COMPRESS '0',
      Qty_dspnsd CHAR(10) NOT NULL COMPRESS ('0000004000','0000010000','0000014000','0000015000','0000020000','0000028000','0000030000','0000031000','0000045000','0000056000','0000060000','0000090000','0000100000','0000120000','0000180000'),
      Days_suply SMALLINT COMPRESS (0 ,3 ,5 ,7 ,10 ,14 ,15 ,20 ,25 ,28 ,30 ,31 ,34 ,90 ),
      Prscrb_id_qlfyr CHAR(2) COMPRESS ('  ','01','12'),
      Prscrb_id VARCHAR(15) NOT NULL COMPRESS '',
      Drug_cvrg_stus_cd CHAR(1) COMPRESS (' ','C','E','O'),
      Adjsmt_del_cd CHAR(1) COMPRESS (' ','A','D'),
      Non_stand_frmt_cd CHAR(1) COMPRESS (' ','B','C','X'),
      Prcng_excptn_cd CHAR(1) COMPRESS (' ','M','O'),
      Uniq_id CHAR(13) NOT NULL,
      FinalVersionInd CHAR(1),
      LoadID SMALLINT NOT NULL)
PRIMARY INDEX pi ( Efctv_uniq_id ,LoadID )
PARTITION BY ( RANGE_N(LoadID  BETWEEN 1  AND 10000  EACH 1 ),
                            RANGE_N(Date_of_srvc  BETWEEN DATE '2007-01-01' AND ADD_MONTHS((DATE ),(-1 )) EACH INTERVAL '1' MONTH ) )
UNIQUE INDEX ui ( Efctv_uniq_id ,LoadID )
INDEX Efctv_uniq_id ( Efctv_uniq_id )
INDEX Date_of_srvc ( Date_of_srvc )
INDEX LoadID ( LoadID );


I need the UI in the staging environment so it doesn't allow the same data to get loaded more than once.  I've taken that index out of the table in the prod environment.

avgCurrentPerm is 722027691, maxPeakPerm is 730772992, and skewFactor is 0.094412.

Thanks for any help.



changed it to:

PRIMARY INDEX pi ( Efctv_uniq_id ,LoadID )
PARTITION BY ( 
RANGE_N(LoadID  BETWEEN 644  AND 10000  EACH 1 ),
RANGE_N(Date_of_srvc  BETWEEN DATE '2007-01-01' AND DATE '2030-01-01' 
EACH INTERVAL '1' MONTH ) );


seeing:
avgCurrentPerm 377035904    maxPeakPerm 377372160   skewFactor 0.089105

and collecting these stats:

COLLECT STATS 
COLUMN (PARTITION), 
COLUMN (loadid), 
COLUMN (Efctv_uniq_id), 
COLUMN (Date_of_srvc) 
ON STAGE.PartD;




and I'm getting the impression that creating unique indexes and partition ranges using the current date is a bad idea?
",3
199,39882352,Python connect to Teradata in AWS EC2,"I want to pull data from a Teradata instance. Client code runs Python2.7+ on AWS EC2 instance.

I installed unixODBC driver and sudo pip install teradata but I am still getting the following exception:

  File ""/usr/local/lib/python2.7/site-packages/teradata/tdodbc.py"", line 369, in determineDriver
""Available drivers: {}"".format(dbType, "","".join(drivers)))
teradata.api.InterfaceError: ('DRIVER_NOT_FOUND', ""No driver found for 'Teradata'.  Available drivers: PostgreSQL,MySQL"")


The code is as follows:

import sys
import teradata
# my own imports
td = TeradataClient(DEFAULT_HOSTNAME, DEFAULT_USERNAME, DEFAULT_PASSWORD)
td.select(query, outfile)


The TeradataClient class I created which calls Teradata is as follows:

class TeradataClient:
    def __init__(self, hostname, username, password):
        self._hostname = hostname
        self._username = username
        self._password = password
        self._udaExec = teradata.UdaExec(appName=""MyApp"", version=""1.0"", logConsole=False)


    def select(self, query, outfile, sep=DEFAULT_SEPARATOR, nullstr=DEFAULT_NULL_STR):
        with self._udaExec.connect(method=""odbc"", system=self._hostname, username=self._username,
                                          password=self._password) as session:
            print 'Connection to Teradata established'
            with open(outfile,'w') as fp:
                with session.cursor() as cursor:
                    for row in cursor.execute(query):
                        lineparts = [str(x if x!=None else nullstr) for x in row]
                        fp.write('%s\n' %sep.join(lineparts))


How can I fix this? Is there another ODBC driver that needs to be installed?
",1,-1,-1.0,"I want to pull data from a Teradata instance. Client code runs Python2.7+ on AWS EC2 instance.

I installed unixODBC driver and sudo pip install teradata but I am still getting the following exception:

  File ""/usr/local/lib/python2.7/site-packages/teradata/tdodbc.py"", line 369, in determineDriver
""Available drivers: {}"".format(dbType, "","".join(drivers)))
teradata.api.InterfaceError: ('DRIVER_NOT_FOUND', ""No driver found for 'Teradata'.  Available drivers: PostgreSQL,MySQL"")


The code is as follows:

import sys
import teradata
# my own imports
td = TeradataClient(DEFAULT_HOSTNAME, DEFAULT_USERNAME, DEFAULT_PASSWORD)
td.select(query, outfile)


The TeradataClient class I created which calls Teradata is as follows:

class TeradataClient:
    def __init__(self, hostname, username, password):
        self._hostname = hostname
        self._username = username
        self._password = password
        self._udaExec = teradata.UdaExec(appName=""MyApp"", version=""1.0"", logConsole=False)


    def select(self, query, outfile, sep=DEFAULT_SEPARATOR, nullstr=DEFAULT_NULL_STR):
        with self._udaExec.connect(method=""odbc"", system=self._hostname, username=self._username,
                                          password=self._password) as session:
            print 'Connection to Teradata established'
            with open(outfile,'w') as fp:
                with session.cursor() as cursor:
                    for row in cursor.execute(query):
                        lineparts = [str(x if x!=None else nullstr) for x in row]
                        fp.write('%s\n' %sep.join(lineparts))


How can I fix this? Is there another ODBC driver that needs to be installed?
",1
200,39883162,Teradata - Two Joins to a CTE StrTok_Split_To_Table function - Error 3807,"This question is related to the excellent answer to this question: Teradata REGEXP_SPLIT_TO_TABLE Input Parameter. Below is the simplest example I can create.

I have a TERADATA query with two CTEs (WITH clauses). The first CTE contains a STRTOK_SPLIT_TO_TABLE function that refers to the second CTE which collects a parameter from the user. The body of the query has a SELECT statement that references the first CTE and gets a column of split parameters. This works great:

WITH 
    SPLIT_PARAMS(PARAM) AS
    (SELECT
        TEST_TABLE.SPLIT_PARAMS
    FROM
        TABLE (StrTok_Split_To_Table(1, PARAMS.INPUT_PARAMS, '|')
            RETURNS (outkey INTEGER, TOKENNUM INTEGER, SPLIT_PARAMS VARCHAR(8192) CHARACTER SET Unicode)) AS TEST_TABLE)
    ,

    PARAMS (INPUT_PARAMS) AS
    (SELECT
        '?InputParams' AS INPUT_PARAMS
    )

SELECT 
    SPLIT_PARAMS.PARAMS
FROM SPLIT_PARAMS


However, I want to be able to refer to the SPLIT_PARAMS CTE more than once. When I do that I get a [3807] object 'PARAMS' does not exist error:

WITH 
    SPLIT_PARAMS(PARAM) AS
    (SELECT
        TEST_TABLE.SPLIT_PARAMS
    FROM
        TABLE (STRTOK_SPLIT_TO_TABLE(1, PARAMS.INPUT_PARAMS, '|')
            RETURNS (outkey INTEGER, TOKENNUM INTEGER, SPLIT_PARAMS VARCHAR(8192) CHARACTER SET UNICODE)) AS TEST_TABLE)
    ,

    PARAMS (INPUT_PARAMS) AS
    (SELECT
        '?InputParams' AS INPUT_PARAMS
    )

SELECT 
    SP1.PARAM,
    SP2.PARAM
FROM SPLIT_PARAMS SP1
    CROSS JOIN SPLIT_PARAMS SP2


I've tried a bunch of things, like putting two SPLIT_PARAMS subqueries in the main query and using the old-style JOINs referred to in the answer to the previous question. However, any attempt to JOIN to the SPLIT_PARAMS CTE more than once yields this error. (My actual setup is three CTEs deep, but the result is the same - ""PARAMS does not exist.""
",-1,-1,-1.0,"This question is related to the excellent answer to this question: Teradata REGEXP_SPLIT_TO_TABLE Input Parameter. Below is the simplest example I can create.

I have a TERADATA query with two CTEs (WITH clauses). The first CTE contains a STRTOK_SPLIT_TO_TABLE function that refers to the second CTE which collects a parameter from the user. The body of the query has a SELECT statement that references the first CTE and gets a column of split parameters. This works great:

WITH 
    SPLIT_PARAMS(PARAM) AS
    (SELECT
        TEST_TABLE.SPLIT_PARAMS
    FROM
        TABLE (StrTok_Split_To_Table(1, PARAMS.INPUT_PARAMS, '|')
            RETURNS (outkey INTEGER, TOKENNUM INTEGER, SPLIT_PARAMS VARCHAR(8192) CHARACTER SET Unicode)) AS TEST_TABLE)
    ,

    PARAMS (INPUT_PARAMS) AS
    (SELECT
        '?InputParams' AS INPUT_PARAMS
    )

SELECT 
    SPLIT_PARAMS.PARAMS
FROM SPLIT_PARAMS


However, I want to be able to refer to the SPLIT_PARAMS CTE more than once. When I do that I get a [3807] object 'PARAMS' does not exist error:

WITH 
    SPLIT_PARAMS(PARAM) AS
    (SELECT
        TEST_TABLE.SPLIT_PARAMS
    FROM
        TABLE (STRTOK_SPLIT_TO_TABLE(1, PARAMS.INPUT_PARAMS, '|')
            RETURNS (outkey INTEGER, TOKENNUM INTEGER, SPLIT_PARAMS VARCHAR(8192) CHARACTER SET UNICODE)) AS TEST_TABLE)
    ,

    PARAMS (INPUT_PARAMS) AS
    (SELECT
        '?InputParams' AS INPUT_PARAMS
    )

SELECT 
    SP1.PARAM,
    SP2.PARAM
FROM SPLIT_PARAMS SP1
    CROSS JOIN SPLIT_PARAMS SP2


I've tried a bunch of things, like putting two SPLIT_PARAMS subqueries in the main query and using the old-style JOINs referred to in the answer to the previous question. However, any attempt to JOIN to the SPLIT_PARAMS CTE more than once yields this error. (My actual setup is three CTEs deep, but the result is the same - ""PARAMS does not exist.""
",3
201,39970385,How to pass column names into subquery from an outer query in teradata,"So in TSQL, I can do something like 

select * from tbl1 a 
where a.key1 = (select b.key1 from tbl2 b where a.key1 = b.key1 and a.key2 = b.key2)


I tried to implement the same concept in teradata, but no luck. How can I do this in teradata?
",-1,-1,-1.0,"So in TSQL, I can do something like 

select * from tbl1 a 
where a.key1 = (select b.key1 from tbl2 b where a.key1 = b.key1 and a.key2 = b.key2)


I tried to implement the same concept in teradata, but no luck. How can I do this in teradata?
",3
202,40152876,Error in importing table from teradata to hive using teradata-connector,"I am trying to import myemp table from Teradata to hive using the teradata-connector and same error occurs for hortonworks-teradata-connector but it 
works well without using connectors by using JDBC. 

hadoop com.teradata.hadoop.tool.TeradataImportTool \
-libjars $LIB_JARS \
-url jdbc:teradata://192.168.2.129/database=mydb \
-username kd \
-password exa \
-classname com.teradata.jdbc.TeraDriver \
-jobtype hive \
-fileformat rcfile \
-targettable tab1 \
-sourcetable myemp


Error:

16/10/20 11:10:17 INFO tool.ConnectorImportTool: ConnectorImportTool starts at 1476961817658
16/10/20 11:10:19 INFO common.ConnectorPlugin: load plugins in jar:file:/usr/hdp/2.4.0.0-169/sqoop/lib/teradata-connector-1.4.1-hadoop2.jar!/teradata.connector.plugins.xml
16/10/20 11:10:19 INFO hive.metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
16/10/20 11:10:19 INFO tool.ConnectorImportTool: java.lang.NoSuchMethodError: org.apache.hadoop.hive.shims.HadoopShims.getUGIForConf(Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/security/UserGroupInformation;
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:292)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:163)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:103)
at com.teradata.connector.hive.utils.HiveUtils.isHiveOutputTablePartitioned(HiveUtils.java:1075)
at com.teradata.connector.common.tool.ConnectorImportTool.processArgs(ConnectorImportTool.java:625)
at com.teradata.connector.common.tool.ConnectorImportTool.run(ConnectorImportTool.java:58)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
at com.teradata.hadoop.tool.TeradataImportTool.main(TeradataImportTool.java:29)

16/10/20 11:10:19 INFO tool.ConnectorImportTool: job completed with exit code 10000

",-1,-1,-1.0,"I am trying to import myemp table from Teradata to hive using the teradata-connector and same error occurs for hortonworks-teradata-connector but it 
works well without using connectors by using JDBC. 

hadoop com.teradata.hadoop.tool.TeradataImportTool \
-libjars $LIB_JARS \
-url jdbc:teradata://192.168.2.129/database=mydb \
-username kd \
-password exa \
-classname com.teradata.jdbc.TeraDriver \
-jobtype hive \
-fileformat rcfile \
-targettable tab1 \
-sourcetable myemp


Error:

16/10/20 11:10:17 INFO tool.ConnectorImportTool: ConnectorImportTool starts at 1476961817658
16/10/20 11:10:19 INFO common.ConnectorPlugin: load plugins in jar:file:/usr/hdp/2.4.0.0-169/sqoop/lib/teradata-connector-1.4.1-hadoop2.jar!/teradata.connector.plugins.xml
16/10/20 11:10:19 INFO hive.metastore: Trying to connect to metastore with URI thrift://sandbox.hortonworks.com:9083
16/10/20 11:10:19 INFO tool.ConnectorImportTool: java.lang.NoSuchMethodError: org.apache.hadoop.hive.shims.HadoopShims.getUGIForConf(Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/security/UserGroupInformation;
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:292)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:163)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:103)
at com.teradata.connector.hive.utils.HiveUtils.isHiveOutputTablePartitioned(HiveUtils.java:1075)
at com.teradata.connector.common.tool.ConnectorImportTool.processArgs(ConnectorImportTool.java:625)
at com.teradata.connector.common.tool.ConnectorImportTool.run(ConnectorImportTool.java:58)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
at com.teradata.hadoop.tool.TeradataImportTool.main(TeradataImportTool.java:29)

16/10/20 11:10:19 INFO tool.ConnectorImportTool: job completed with exit code 10000

",0
203,40359181,How to get a count of number of occurrence of a substring in a string in teradata?,"I have a column in a teradata table with string values like ""page1-->page2-->page1-->page3-->page1--page2-->...""

I want to search for a specific page and get the number of occurrence of the page in the string. I couldn't find any function that gives this result.
",-1,-1,-1.0,"I have a column in a teradata table with string values like ""page1-->page2-->page1-->page3-->page1--page2-->...""

I want to search for a specific page and get the number of occurrence of the page in the string. I couldn't find any function that gives this result.
",3
204,40580355,Using documentParser function in Teradata Aster,"I'm working with Teradata's Aster and am trying to parse a pdf(or html) file such that it is inserted into a table in the Beehive database in Aster. The entire pdf should correspond to a single row of data in the table.

This is to be done by using one of Aster's SQL-MR functions called documentParser. This will produce a text file(.rtf) containing a single row produced by parsing all the chapters from the pdf file, which would be then loaded into the table in Beehive.

I have been given this script that shows the use of documentParser and other steps involved in this parsing process - 

/* SHELL INSTRUCTIONS */
--transform file in b64 (change file names to your relevant file)

base64 pp.pdf&gt;pp.b64

--prepare a loadfile
rm my_load_file.txt


-- get the content of the file
var=$(cat pp.b64)

-- put in file
echo \""""pp.b64""\"""",""\""""$var""\"" &gt;&gt; ""my_load_file.txt""


-- create staging table 
act -U db_superuser -w db_superuser -d beehive -c ""drop table if exists public.cf_load_file;""
act -U db_superuser -w db_superuser -d beehive -c ""create dimension table public.cf_load_file(file_name varchar, content varchar);""


-- load into staging table
ncluster_loader -U db_superuser -w db_superuser -d beehive --csv --verbose public.cf_load_file my_load_file.txt


-- use document parser to load the clean text (you will need to create the table beforehand)

act -U db_superuser -w db_superuser -d beehive -c ""INSERT INTO got_data.cf_got_text_data (file_name, content) SELECT * FROM documentParser (ON public.cf_load_file documentCol ('content') mode ('text'));""

--done


However, I am stuck on the last step of the script because it looks like there is no function called documentParser in the list of functions that are available in Aster. This is the error I get - 

ERROR:  function ""documentparser"" does not exist


I tried to search for this function several times with the command \dF, but did not get any match.

I've attached a picture which present the gist of what I'm trying to do. 

SQL-MR Document Parser

I would appreciate any help if any one has any experience with this.
",1,-1,-1.0,"I'm working with Teradata's Aster and am trying to parse a pdf(or html) file such that it is inserted into a table in the Beehive database in Aster. The entire pdf should correspond to a single row of data in the table.

This is to be done by using one of Aster's SQL-MR functions called documentParser. This will produce a text file(.rtf) containing a single row produced by parsing all the chapters from the pdf file, which would be then loaded into the table in Beehive.

I have been given this script that shows the use of documentParser and other steps involved in this parsing process - 

/* SHELL INSTRUCTIONS */
--transform file in b64 (change file names to your relevant file)

base64 pp.pdf&gt;pp.b64

--prepare a loadfile
rm my_load_file.txt


-- get the content of the file
var=$(cat pp.b64)

-- put in file
echo \""""pp.b64""\"""",""\""""$var""\"" &gt;&gt; ""my_load_file.txt""


-- create staging table 
act -U db_superuser -w db_superuser -d beehive -c ""drop table if exists public.cf_load_file;""
act -U db_superuser -w db_superuser -d beehive -c ""create dimension table public.cf_load_file(file_name varchar, content varchar);""


-- load into staging table
ncluster_loader -U db_superuser -w db_superuser -d beehive --csv --verbose public.cf_load_file my_load_file.txt


-- use document parser to load the clean text (you will need to create the table beforehand)

act -U db_superuser -w db_superuser -d beehive -c ""INSERT INTO got_data.cf_got_text_data (file_name, content) SELECT * FROM documentParser (ON public.cf_load_file documentCol ('content') mode ('text'));""

--done


However, I am stuck on the last step of the script because it looks like there is no function called documentParser in the list of functions that are available in Aster. This is the error I get - 

ERROR:  function ""documentparser"" does not exist


I tried to search for this function several times with the command \dF, but did not get any match.

I've attached a picture which present the gist of what I'm trying to do. 

SQL-MR Document Parser

I would appreciate any help if any one has any experience with this.
",3
205,40585558,Sqoop export to Teradata failing with error - Task attempt failed to report status for 600 seconds. Killing,"I am facing tasks attempts failing with below error, related to Teradata export (batch insert) jobs.Other jobs exporting data to Oracle etc. are running fine.

Task attempt_1234_m_000000_0 failed to report status for 600 seconds. Killing!,
java.lang.Throwable: Child Error at           org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:250) 
Caused by: ExitCodeException exitCode=255: at  org.apache.hadoop.util.Shell.runCommand(Shell.java:543) 
 at org.apache.hadoop.util.Shell.run(Shell.java:460) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:707)
 at org.apache.hadoop.mapred.LinuxTaskController.createLogDir(LinuxTaskController.java:313) at org.apache.hadoop.mapred.TaskRunner.prepareLogFiles(TaskRunner.java:295)
 at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:215)


I can also see below error message from task stdout logs :
""main"" prio=10 tid=0x00007f8824018800 nid=0x3395 runnable [0x00007f882bffb000]
    java.lang.Thread.State: RUNNABLE
    at java.net.SocketInputStream.socketRead0(Native Method)
    at java.net.SocketInputStream.read(SocketInputStream.java:152)
    at java.net.SocketInputStream.read(SocketInputStream.java:122)
    at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:693)
    at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:774)

Hadoop version : Hadoop 2.5.0-cdh5.3.8

Specifically it will be really helpful if you could tell me  why this issue is happening?
Is this an issue related to limit in number of connections  to Teradata ?
",-1,-1,-1.0,"I am facing tasks attempts failing with below error, related to Teradata export (batch insert) jobs.Other jobs exporting data to Oracle etc. are running fine.

Task attempt_1234_m_000000_0 failed to report status for 600 seconds. Killing!,
java.lang.Throwable: Child Error at           org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:250) 
Caused by: ExitCodeException exitCode=255: at  org.apache.hadoop.util.Shell.runCommand(Shell.java:543) 
 at org.apache.hadoop.util.Shell.run(Shell.java:460) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:707)
 at org.apache.hadoop.mapred.LinuxTaskController.createLogDir(LinuxTaskController.java:313) at org.apache.hadoop.mapred.TaskRunner.prepareLogFiles(TaskRunner.java:295)
 at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:215)


I can also see below error message from task stdout logs :
""main"" prio=10 tid=0x00007f8824018800 nid=0x3395 runnable [0x00007f882bffb000]
    java.lang.Thread.State: RUNNABLE
    at java.net.SocketInputStream.socketRead0(Native Method)
    at java.net.SocketInputStream.read(SocketInputStream.java:152)
    at java.net.SocketInputStream.read(SocketInputStream.java:122)
    at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:693)
    at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:774)

Hadoop version : Hadoop 2.5.0-cdh5.3.8

Specifically it will be really helpful if you could tell me  why this issue is happening?
Is this an issue related to limit in number of connections  to Teradata ?
",0
206,41005565,teradata stored procedure error writing into file,"I am having issue with replace or executing stored procedure in teradata. This error suddenly appears. The stored procedure I have created before does not work at all suddenly.

When I try to replace a procedure I am getting Error 5547: failed to get the file size and when try to execute an existing procedure I get Error 7645: RTSExecSP: Error writing into file. This procedure worked fine previously. I am using TD 15.00.02.06.

I have looked into the error definition for 7645 error in the TD site and it says that: 


  ""This error is reported when a problem is encountered while writing
  into the stored procedure object code file or log files during SP or
  DB spoil operations. This can occur when the directory has no space.
  This is a system disk directory NOT part of the database.""


Can anyone suggest me where in the TD OS I should look for this space?

here is a simple stored procedure I tried to compile

CREATE PROCEDURE [MyDb].InsertSalary ( 
   IN in_EmployeeNo INTEGER, IN in_Gross INTEGER, 
   IN in_Deduction INTEGER, IN in_NetPay INTEGER

) 
BEGIN 
   INSERT INTO [MyDb].Salary ( 
      EmployeeNo, 
      Gross, 
      Deduction, 
      NetPay 
   ) 
   VALUES ( 
      :in_EmployeeNo, 
      :in_Gross, 
      :in_Deduction, 
      :in_NetPay 
   ); 
END;

",-1,-1,-1.0,"I am having issue with replace or executing stored procedure in teradata. This error suddenly appears. The stored procedure I have created before does not work at all suddenly.

When I try to replace a procedure I am getting Error 5547: failed to get the file size and when try to execute an existing procedure I get Error 7645: RTSExecSP: Error writing into file. This procedure worked fine previously. I am using TD 15.00.02.06.

I have looked into the error definition for 7645 error in the TD site and it says that: 


  ""This error is reported when a problem is encountered while writing
  into the stored procedure object code file or log files during SP or
  DB spoil operations. This can occur when the directory has no space.
  This is a system disk directory NOT part of the database.""


Can anyone suggest me where in the TD OS I should look for this space?

here is a simple stored procedure I tried to compile

CREATE PROCEDURE [MyDb].InsertSalary ( 
   IN in_EmployeeNo INTEGER, IN in_Gross INTEGER, 
   IN in_Deduction INTEGER, IN in_NetPay INTEGER

) 
BEGIN 
   INSERT INTO [MyDb].Salary ( 
      EmployeeNo, 
      Gross, 
      Deduction, 
      NetPay 
   ) 
   VALUES ( 
      :in_EmployeeNo, 
      :in_Gross, 
      :in_Deduction, 
      :in_NetPay 
   ); 
END;

",3
207,41149234,SqlAlchemy can't find Teradata ODBC driver on RHEL 7,"I am trying to use SQLAlchemy to connect to our Teradata environment and execute a query.  I ran the script on a Windows 7 machine using an Anaconda Python 2.7 environment and Jupyter notebook.  When I moved this to our Linux server and ran it in an Anaconda Python 2.7 environment it doesn't work.  It complains that it can't find the driver Teradata.  I have ran this as root and my non-root user.

I am trying to run this in a Jupyter notebook running on the server

from sqlalchemy import create_engine
# connect
td_engine = create_engine('teradata://username:password@teradata_database:22/')

# execute sql
sql=""SELECT * FROM sometable""
result = td_engine.execute(sql)


Here is the traceback

Here is what I've done on the RHEL 7 server inside a py27 env

conda install sqlalchemy

pip install teradata

pip install sqlalchemy-teradata

yum install unixODBC

I downloaded the latest Teradata ODBC Driver for Linux

I extracted the file

tar -zxf tdodbc1510__linux_indep_15.10.01.04-1.tar.gz

This gives me three more tar.gz files and a text file of the same name.

I then extract each of these tar.gz files

tar -zxf tdicu1510__linux_indep.15.10.04-1.tar.gz

tar -zxf tdodbc1510__linux_indep.15.10.01.04-1.tar.gz

tar -zxf TeraGSS_linux-x64_linux_indep.15.10.03.02-1.tar.gz

This creates a directory for each root name (tdicu1510, tdodbc1510, TeraGSS)

I cd into each directory and switch to korn shell

/usr/bin/ksh

for the rpm's inside the three directories I install them

rpm -ihv tdicu1510_linux_x64-15.10.03.02-1.noarch.rpm

rpm -ihv TeraGSS_linux_x64-15.10.03.02-1.noarch.rpm

rpm -ihv tdodbc1510_linux_x64-15.10.03.02-1.noarch.rpm

This creates an /opt/teradata directory

I add this to /usr/local/etc/odbcinst.ini

[Teradata]
Driver=/opt/teradata/client/15.10/odbc_64/lib/tdata.so
APILevel=CORE
ConnectFunctions=YYY
DriverODBCVer=3.51
SQLLevel=1


I add this to /usr/local/etc/odbc.ini

[ODBC Data Sources]
TDDSN=tdata.so

[ODBC]
InstallDir=/opt/teradata/client/15.10/odbc_64
Trace=0
TraceDll=/opt/teradata/client/15.10/odbc_64/lib/odbctrac.so
TraceFile=/usr/teradata_logs/odbcusr/trace.log
TraceAutoStop=0

[TDDSN]
Driver=/opt/teradata/client/15.10/odbc_64/lib/tdata.so
Description=Teradata database
DBCName=&lt;MachineName or ip&gt;
LastUser=
Username=
Password=
Database=


Here is my ~/.bashrc

export PATH=""/opt/miniconda2/bin:$PATH
export ORACLE_HOME=""/usr/lib/oracle/12.1/client64""
export PATH=""$PATH:$ORACLE_HOME/bin""
export LD_LIBRARY_PATH=""$ORACLE_HOME/lib""
export TNS_ADMIN=""$ORACLE_HOME/network/admin""
export TERADATA=""/opt/teradata/client/15.10""
export ODBCINI=""/opt/teradata/client/15.10/odbc_64/odbc.ini""
export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:$TERADATA/lib64""


echo $LD_LIBRARY_PATH shows /usr/lib/oracle/12.1/client64/lib:/opt/teradata/client/15.10/lib64

Any ideas what I am doing wrong?  Why can't SQLAlchemy see the Teradata ODBC driver on the Linux server?
",-1,-1,-1.0,"I am trying to use SQLAlchemy to connect to our Teradata environment and execute a query.  I ran the script on a Windows 7 machine using an Anaconda Python 2.7 environment and Jupyter notebook.  When I moved this to our Linux server and ran it in an Anaconda Python 2.7 environment it doesn't work.  It complains that it can't find the driver Teradata.  I have ran this as root and my non-root user.

I am trying to run this in a Jupyter notebook running on the server

from sqlalchemy import create_engine
# connect
td_engine = create_engine('teradata://username:password@teradata_database:22/')

# execute sql
sql=""SELECT * FROM sometable""
result = td_engine.execute(sql)


Here is the traceback

Here is what I've done on the RHEL 7 server inside a py27 env

conda install sqlalchemy

pip install teradata

pip install sqlalchemy-teradata

yum install unixODBC

I downloaded the latest Teradata ODBC Driver for Linux

I extracted the file

tar -zxf tdodbc1510__linux_indep_15.10.01.04-1.tar.gz

This gives me three more tar.gz files and a text file of the same name.

I then extract each of these tar.gz files

tar -zxf tdicu1510__linux_indep.15.10.04-1.tar.gz

tar -zxf tdodbc1510__linux_indep.15.10.01.04-1.tar.gz

tar -zxf TeraGSS_linux-x64_linux_indep.15.10.03.02-1.tar.gz

This creates a directory for each root name (tdicu1510, tdodbc1510, TeraGSS)

I cd into each directory and switch to korn shell

/usr/bin/ksh

for the rpm's inside the three directories I install them

rpm -ihv tdicu1510_linux_x64-15.10.03.02-1.noarch.rpm

rpm -ihv TeraGSS_linux_x64-15.10.03.02-1.noarch.rpm

rpm -ihv tdodbc1510_linux_x64-15.10.03.02-1.noarch.rpm

This creates an /opt/teradata directory

I add this to /usr/local/etc/odbcinst.ini

[Teradata]
Driver=/opt/teradata/client/15.10/odbc_64/lib/tdata.so
APILevel=CORE
ConnectFunctions=YYY
DriverODBCVer=3.51
SQLLevel=1


I add this to /usr/local/etc/odbc.ini

[ODBC Data Sources]
TDDSN=tdata.so

[ODBC]
InstallDir=/opt/teradata/client/15.10/odbc_64
Trace=0
TraceDll=/opt/teradata/client/15.10/odbc_64/lib/odbctrac.so
TraceFile=/usr/teradata_logs/odbcusr/trace.log
TraceAutoStop=0

[TDDSN]
Driver=/opt/teradata/client/15.10/odbc_64/lib/tdata.so
Description=Teradata database
DBCName=&lt;MachineName or ip&gt;
LastUser=
Username=
Password=
Database=


Here is my ~/.bashrc

export PATH=""/opt/miniconda2/bin:$PATH
export ORACLE_HOME=""/usr/lib/oracle/12.1/client64""
export PATH=""$PATH:$ORACLE_HOME/bin""
export LD_LIBRARY_PATH=""$ORACLE_HOME/lib""
export TNS_ADMIN=""$ORACLE_HOME/network/admin""
export TERADATA=""/opt/teradata/client/15.10""
export ODBCINI=""/opt/teradata/client/15.10/odbc_64/odbc.ini""
export LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:$TERADATA/lib64""


echo $LD_LIBRARY_PATH shows /usr/lib/oracle/12.1/client64/lib:/opt/teradata/client/15.10/lib64

Any ideas what I am doing wrong?  Why can't SQLAlchemy see the Teradata ODBC driver on the Linux server?
",1
208,41181038,Python pyodbc cursor execution fails on Teradata,"I have a Python script which runs successfully from my Windows workstation and I am trying to migrate it to a Unix server. The script connects to a Teradata database using pyodbc package and executes a bunch of queries. When it is execute from the server, it triggers the following error message:


  Error: ('HY000', 'The driver did not supply an error!')


I am able to consistently reproduce the error with the following code snippet executed on the server:

import pyodbc
oConnexion = pyodbc.connect(""Driver={Teradata};DBCNAME=myserver;UID=myuser;PWD=mypassword"", autocommit=True)
print(""Connected"")
oCursor = oConnexion.cursor()
oCursor.execute(""select 1"")
print(""Success"")


Configuration:


Python 3.5.2
Pyodbc 3.1.2b2
UnixODBC Driver Manager
Teradata 15.10


After enabling ODBC logging and running a simple SELECT query, I have noticed the following Invalid cursor GeTypeInfo errors:

Data Type = SQL_VARCHAR
[ODBC][57920][1481847636.278776][SQLGetTypeInfo.c][190]Error: 24000
[ODBC][57920][1481847636.278815][SQLGetTypeInfo.c][168]
                Entry:
                        Statement = 0x1bc69e0
                        Data Type = Unknown(-9)
[ODBC][57920][1481847636.278839][SQLGetTypeInfo.c][190]Error: 24000
[ODBC][57920][1481847636.278873][SQLGetTypeInfo.c][168]
                Entry:
                        Statement = 0x1bc69e0
                        Data Type = SQL_BINARY
[ODBC][57920][1481847636.278896][SQLGetTypeInfo.c][190]Error: 24000


Also, trying to list the connection attributes using the following code:

for attr in vars(pyodbc):
        print (attr)
        value = oConnexion.getinfo(getattr(pyodbc, attr))
        print('{:&lt;40s} | {}'.format(attr, value))


Fails with:

SQL_DESCRIBE_PARAMETER
Traceback (most recent call last):
  File ""test.py"", line 28, in &lt;module&gt;
    value = oConnexion.getinfo(getattr(pyodbc, attr))
pyodbc.Error: ('IM001', '[IM001] [unixODBC][Driver Manager]Driver does not support this function (0) (SQLGetInfo)')

",-1,-1,-1.0,"I have a Python script which runs successfully from my Windows workstation and I am trying to migrate it to a Unix server. The script connects to a Teradata database using pyodbc package and executes a bunch of queries. When it is execute from the server, it triggers the following error message:


  Error: ('HY000', 'The driver did not supply an error!')


I am able to consistently reproduce the error with the following code snippet executed on the server:

import pyodbc
oConnexion = pyodbc.connect(""Driver={Teradata};DBCNAME=myserver;UID=myuser;PWD=mypassword"", autocommit=True)
print(""Connected"")
oCursor = oConnexion.cursor()
oCursor.execute(""select 1"")
print(""Success"")


Configuration:


Python 3.5.2
Pyodbc 3.1.2b2
UnixODBC Driver Manager
Teradata 15.10


After enabling ODBC logging and running a simple SELECT query, I have noticed the following Invalid cursor GeTypeInfo errors:

Data Type = SQL_VARCHAR
[ODBC][57920][1481847636.278776][SQLGetTypeInfo.c][190]Error: 24000
[ODBC][57920][1481847636.278815][SQLGetTypeInfo.c][168]
                Entry:
                        Statement = 0x1bc69e0
                        Data Type = Unknown(-9)
[ODBC][57920][1481847636.278839][SQLGetTypeInfo.c][190]Error: 24000
[ODBC][57920][1481847636.278873][SQLGetTypeInfo.c][168]
                Entry:
                        Statement = 0x1bc69e0
                        Data Type = SQL_BINARY
[ODBC][57920][1481847636.278896][SQLGetTypeInfo.c][190]Error: 24000


Also, trying to list the connection attributes using the following code:

for attr in vars(pyodbc):
        print (attr)
        value = oConnexion.getinfo(getattr(pyodbc, attr))
        print('{:&lt;40s} | {}'.format(attr, value))


Fails with:

SQL_DESCRIBE_PARAMETER
Traceback (most recent call last):
  File ""test.py"", line 28, in &lt;module&gt;
    value = oConnexion.getinfo(getattr(pyodbc, attr))
pyodbc.Error: ('IM001', '[IM001] [unixODBC][Driver Manager]Driver does not support this function (0) (SQLGetInfo)')

",1
209,41279153,Teradata spool space issue on running a sub query with Count,"I am using below query to calculate business days between two dates for all the order numbers. Business days are already available in the teradata table Common_WorkingCalendar. But, i'm also facing spool space issue while i execute the query. I have ample space available in my data lab. Need to optimize the query. Appreciate any inputs.

SELECT 
tx.""OrderNumber"",
(SELECT COUNT(1) FROM Common_WorkingCalendar
WHERE CalDate between Cast(tx.""TimeStamp"" as date) and Cast(mf.ShipDate as  date)) as BusDays
from StoreFulfillment ff
inner join StoreTransmission tx 
            on tx.OrderNumber = ff.OrderNumber
        inner join StoreMerchandiseFulfillment mf 
            on mf.OrderNumber = ff.OrderNumber

",-1,-1,-1.0,"I am using below query to calculate business days between two dates for all the order numbers. Business days are already available in the teradata table Common_WorkingCalendar. But, i'm also facing spool space issue while i execute the query. I have ample space available in my data lab. Need to optimize the query. Appreciate any inputs.

SELECT 
tx.""OrderNumber"",
(SELECT COUNT(1) FROM Common_WorkingCalendar
WHERE CalDate between Cast(tx.""TimeStamp"" as date) and Cast(mf.ShipDate as  date)) as BusDays
from StoreFulfillment ff
inner join StoreTransmission tx 
            on tx.OrderNumber = ff.OrderNumber
        inner join StoreMerchandiseFulfillment mf 
            on mf.OrderNumber = ff.OrderNumber

",3
210,41342009,Teradata Drivers not found on linux,"I am trying to setup Teradata drivers on linux and trying to access the database from PHP(codeigniter) linux.

$connection = odbc_connect('something@domain.com','USERNAME', 'PASSWORD');


I get 

Message: odbc_connect(): SQL error: [unixODBC][Driver Manager]Can't open lib '/opt/teradata/client/14.10/odbc_64/lib/tdata.so' : file not found, SQL state 01000 in SQLConnect


I have checked the following:

1)

 /opt/teradata/client/14.10/odbc_64/lib# ldd tdata.so   
            linux-vdso.so.1 =&gt;  (0x00007fff7f39a000)
            libstdc++.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f8cb43f9000)
            libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f8cb41e3000)
            libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f8cb3fc4000)
            libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f8cb3dc0000)
            librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f8cb3bb8000)
            libnsl.so.1 =&gt; /lib/x86_64-linux-gnu/libnsl.so.1 (0x00007f8cb399d000)
            libodbcinst.so =&gt; /opt/teradata/client/14.10/odbc_64/lib/libodbcinst.so (0x00007f8cb377f000)
            libddicu25.so =&gt; /opt/teradata/client/14.10/odbc_64/lib/libddicu25.so (0x00007f8cb2888000)
            libtdparse.so =&gt; /opt/teradata/client/14.10/odbc_64/lib/libtdparse.so (0x00007f8cb2712000)
            libicudatatd.so.46 =&gt; /opt/teradata/client/14.10/tdicu/lib64/libicudatatd.so.46 (0x00007f8cb1ec8000)
            libicuuctd.so.46 =&gt; /opt/teradata/client/14.10/tdicu/lib64/libicuuctd.so.46 (0x00007f8cb1c3b000)
            libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f8cb1934000)
            libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f8cb156f000)
            /lib64/ld-linux-x86-64.so.2 (0x00007f8cb4ae5000)


So, I can confirm that no library files are missing. But, still i get the above error.  I have followed the steps here.

2) odbc.ini

[ODBC Data Sources]
TDDSN=tdata.so

[ODBC]
InstallDir=/opt/teradata/client/14.10/odbc_64
Trace=0
TraceDll=/opt/teradata/client/14.10/odbc_64/lib/odbctrac.so
TraceFile=/usr/joe/odbcusr/trace.log
TraceAutoStop=0

[TDDSN]
Driver=/opt/teradata/client/14.10/odbc_64/lib/tdata.so
Description=Teradata database
DBCName=something@domain.com
Username=XXX
Password=XXX
Database=XXX


3) 

isql -v tddsn &lt;username&gt; &lt;password&gt;
[ISQL]ERROR: Could not SQLConnect


Any help would be appreciated.
",-1,-1,-1.0,"I am trying to setup Teradata drivers on linux and trying to access the database from PHP(codeigniter) linux.

$connection = odbc_connect('something@domain.com','USERNAME', 'PASSWORD');


I get 

Message: odbc_connect(): SQL error: [unixODBC][Driver Manager]Can't open lib '/opt/teradata/client/14.10/odbc_64/lib/tdata.so' : file not found, SQL state 01000 in SQLConnect


I have checked the following:

1)

 /opt/teradata/client/14.10/odbc_64/lib# ldd tdata.so   
            linux-vdso.so.1 =&gt;  (0x00007fff7f39a000)
            libstdc++.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f8cb43f9000)
            libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f8cb41e3000)
            libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f8cb3fc4000)
            libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f8cb3dc0000)
            librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f8cb3bb8000)
            libnsl.so.1 =&gt; /lib/x86_64-linux-gnu/libnsl.so.1 (0x00007f8cb399d000)
            libodbcinst.so =&gt; /opt/teradata/client/14.10/odbc_64/lib/libodbcinst.so (0x00007f8cb377f000)
            libddicu25.so =&gt; /opt/teradata/client/14.10/odbc_64/lib/libddicu25.so (0x00007f8cb2888000)
            libtdparse.so =&gt; /opt/teradata/client/14.10/odbc_64/lib/libtdparse.so (0x00007f8cb2712000)
            libicudatatd.so.46 =&gt; /opt/teradata/client/14.10/tdicu/lib64/libicudatatd.so.46 (0x00007f8cb1ec8000)
            libicuuctd.so.46 =&gt; /opt/teradata/client/14.10/tdicu/lib64/libicuuctd.so.46 (0x00007f8cb1c3b000)
            libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f8cb1934000)
            libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f8cb156f000)
            /lib64/ld-linux-x86-64.so.2 (0x00007f8cb4ae5000)


So, I can confirm that no library files are missing. But, still i get the above error.  I have followed the steps here.

2) odbc.ini

[ODBC Data Sources]
TDDSN=tdata.so

[ODBC]
InstallDir=/opt/teradata/client/14.10/odbc_64
Trace=0
TraceDll=/opt/teradata/client/14.10/odbc_64/lib/odbctrac.so
TraceFile=/usr/joe/odbcusr/trace.log
TraceAutoStop=0

[TDDSN]
Driver=/opt/teradata/client/14.10/odbc_64/lib/tdata.so
Description=Teradata database
DBCName=something@domain.com
Username=XXX
Password=XXX
Database=XXX


3) 

isql -v tddsn &lt;username&gt; &lt;password&gt;
[ISQL]ERROR: Could not SQLConnect


Any help would be appreciated.
",1
211,41473724,Teradata system views tuning,"I tried to connect teradata using sqlalchemy in order to use read_sql and to_sql methods form pandas.

However, the connection is so slow. Even simple stuff, such as pd.read_sql('select current_date'), will cost more than 30 seconds to complete. 

I don't really understand why this is so slow. If anyone experienced similar issue before, please tell me why and how you solved this. Thanks!

Updates:
I tried cProfile and sqlTAP and realize the slowness is due to some queries that the dialect generated. The has_table meathod will run a query to dbc.tablesvx view and this query will take around 100 seconds to finish while the view is only about 55k rows. For pd.to_sql, this has_table might be called more than once and some other queries to system tables will be required.  

-- query from has_table method   
SELECT tablename
FROM dbc.tablesvx
WHERE DatabaseName=?
  AND TableName=? 
-- query form drop_table method
SELECT tablename
FROM dbc.TablesVX
WHERE DatabaseName = ?
  AND (tablekind='T'
       OR tablekind='O')


It seems like all I need to do is some tuning to the system tables to make the queries run faster. However, our sql help persons told me that those system tables are already in the best performance. Is this possible? Is there anyone who have done any tuning to teradata DBC view? Thanks.
",1,-1,-1.0,"I tried to connect teradata using sqlalchemy in order to use read_sql and to_sql methods form pandas.

However, the connection is so slow. Even simple stuff, such as pd.read_sql('select current_date'), will cost more than 30 seconds to complete. 

I don't really understand why this is so slow. If anyone experienced similar issue before, please tell me why and how you solved this. Thanks!

Updates:
I tried cProfile and sqlTAP and realize the slowness is due to some queries that the dialect generated. The has_table meathod will run a query to dbc.tablesvx view and this query will take around 100 seconds to finish while the view is only about 55k rows. For pd.to_sql, this has_table might be called more than once and some other queries to system tables will be required.  

-- query from has_table method   
SELECT tablename
FROM dbc.tablesvx
WHERE DatabaseName=?
  AND TableName=? 
-- query form drop_table method
SELECT tablename
FROM dbc.TablesVX
WHERE DatabaseName = ?
  AND (tablekind='T'
       OR tablekind='O')


It seems like all I need to do is some tuning to the system tables to make the queries run faster. However, our sql help persons told me that those system tables are already in the best performance. Is this possible? Is there anyone who have done any tuning to teradata DBC view? Thanks.
",3
212,41510229,Teradata ODBC driver not connecting via Python but able to connect via VBA,"I have a strange problem where i am able to connect to Teradata using VBA but not able to connect via python from same teradata driver.

Below is the code snippet:

1) VBA

connection_string = ""Driver={Teradata};"" &amp; ""DBCName="" &amp; dsn_name &amp; "";Database="" &amp; database_name &amp; ""; User ID ="" &amp; user_name &amp; "";Password="" &amp; password


and i am able to connect to teradata successfully.

2) Python

import pyodbc
dsn_name=""td_dev""
user_name=""test""
password=""test""
db = pyodbc.connect('DSN=' + dsn_name + ';UID='+ user_name +';PWD=' + password + ';')


this statement throws me below error-

('IM003', '[IM003] Specified driver could not be loaded due to system error  126
: The specified module could not be found. (Teradata, C:\\Program Files (x86)\\T
eradata\\Client\\13.10\\ODBC Driver for Teradata\\Lib\\tdata32.dll). (160) (SQLD
riverConnect)')


I have tried various methods like : Re-Installing TD drivers, setting up the environment variables.

But the question remains, how i am able to connect via excel VBA but not python.
",-1,-1,-1.0,"I have a strange problem where i am able to connect to Teradata using VBA but not able to connect via python from same teradata driver.

Below is the code snippet:

1) VBA

connection_string = ""Driver={Teradata};"" &amp; ""DBCName="" &amp; dsn_name &amp; "";Database="" &amp; database_name &amp; ""; User ID ="" &amp; user_name &amp; "";Password="" &amp; password


and i am able to connect to teradata successfully.

2) Python

import pyodbc
dsn_name=""td_dev""
user_name=""test""
password=""test""
db = pyodbc.connect('DSN=' + dsn_name + ';UID='+ user_name +';PWD=' + password + ';')


this statement throws me below error-

('IM003', '[IM003] Specified driver could not be loaded due to system error  126
: The specified module could not be found. (Teradata, C:\\Program Files (x86)\\T
eradata\\Client\\13.10\\ODBC Driver for Teradata\\Lib\\tdata32.dll). (160) (SQLD
riverConnect)')


I have tried various methods like : Re-Installing TD drivers, setting up the environment variables.

But the question remains, how i am able to connect via excel VBA but not python.
",1
213,41896716,Max date between two dates teradata,"I am running the following proc sql to pull out the max date.

Proc sql;
   Connect to TERADATA (login details);
   Create table dates as 
   Select * from connection to TERADATA
  ( select max (date1,'2011-12-31') from table1
);
Quit;

Error:
Syntax error: expected something between the word      'date1' and ','


Can someone help me where I am doing wrong?
",-1,-1,-1.0,"I am running the following proc sql to pull out the max date.

Proc sql;
   Connect to TERADATA (login details);
   Create table dates as 
   Select * from connection to TERADATA
  ( select max (date1,'2011-12-31') from table1
);
Quit;

Error:
Syntax error: expected something between the word      'date1' and ','


Can someone help me where I am doing wrong?
",3
214,41951074,Issue Connecting to Teradata using Python,"I need to connect to the Teradata database using python. I have used the below code:

import pyodbc
import teradata



cnxn = pyodbc.connect('DRIVER={Teradata};SERVER=&lt;*ServerName*&gt;;DATABASE=&lt;*Database Name*&gt;;UID=&lt;*User ID*&gt;;PWD=&lt;*Password*&gt;',ansi=True, autocommit=True)

cur = cnxn.cursor()


But on executing, I am getting the error as :


  Error: ('28000', '[28000] [Teradata][ODBC Teradata Driver] Not enough
  information to log on (0) (SQLDriverConnect); [28000] [Teradata][ODBC
  Teradata Driver] Not enough information to log on (0)')


What I am missing here ? What else needs to be included to set up the connection ?

Also, is there any other way to set up the connection. While looking, I have come across teradata.UdaExec(). Can this also be used?
",-1,-1,-1.0,"I need to connect to the Teradata database using python. I have used the below code:

import pyodbc
import teradata



cnxn = pyodbc.connect('DRIVER={Teradata};SERVER=&lt;*ServerName*&gt;;DATABASE=&lt;*Database Name*&gt;;UID=&lt;*User ID*&gt;;PWD=&lt;*Password*&gt;',ansi=True, autocommit=True)

cur = cnxn.cursor()


But on executing, I am getting the error as :


  Error: ('28000', '[28000] [Teradata][ODBC Teradata Driver] Not enough
  information to log on (0) (SQLDriverConnect); [28000] [Teradata][ODBC
  Teradata Driver] Not enough information to log on (0)')


What I am missing here ? What else needs to be included to set up the connection ?

Also, is there any other way to set up the connection. While looking, I have come across teradata.UdaExec(). Can this also be used?
",1
215,42002109,Teradata locks table for read while accessing to table through view with access rights,"This is really weird. 

I have  2 view, one with access rights and other with read rights to table.

replace view v1_read as locking row for read
select id1 from t1;

replace view v2_access as locking row for access
select id2 from t2;


Then I  run such select query and look explain plan:

select id1, id2
from v2_access left join v1_read on v2_access.id2=v1_read.id1;


Teradata blocks table t2 for read, but it should block for access. Is it bug?

I can say Teradata to start it block for access manually this way:

lock table t2 for access -- before the query


However there is the problem with such solution - administrators does not give such grants.


TD Release: 15.10.03.07
TD Version: 15.10.03.09


How  can I fix that?
",-1,-1,-1.0,"This is really weird. 

I have  2 view, one with access rights and other with read rights to table.

replace view v1_read as locking row for read
select id1 from t1;

replace view v2_access as locking row for access
select id2 from t2;


Then I  run such select query and look explain plan:

select id1, id2
from v2_access left join v1_read on v2_access.id2=v1_read.id1;


Teradata blocks table t2 for read, but it should block for access. Is it bug?

I can say Teradata to start it block for access manually this way:

lock table t2 for access -- before the query


However there is the problem with such solution - administrators does not give such grants.


TD Release: 15.10.03.07
TD Version: 15.10.03.09


How  can I fix that?
",3
216,42208233,sas load into Teradata Date format,"I am having an issue when loading SAS dates into Teradata. The date format in SAS was '01Jan2017'D, and after loading the dates in the teradata would show a string of number like 23050, but not in a date format. And I used put function before loading to change the format in SAS and also after loading I used cast date format function in Teradata, but neither of them worked. 

Does anybody have the solution for this?

Thanks a lot for answering!
",-1,-1,-1.0,"I am having an issue when loading SAS dates into Teradata. The date format in SAS was '01Jan2017'D, and after loading the dates in the teradata would show a string of number like 23050, but not in a date format. And I used put function before loading to change the format in SAS and also after loading I used cast date format function in Teradata, but neither of them worked. 

Does anybody have the solution for this?

Thanks a lot for answering!
",3
217,42259209,Teradata Concatenate multiple string columns format as timestamp,"I am still fairly new to Teradata so forgive this but I have two columns one with the date and one with a 4 digit varchar as the time (24hour)

below is what I use to concatenate the fields to make it readable but I want to make the result come out as a valid timestamp so I can perform calculations.

cast(SCHEDULE_DATE as date format 'yyyy-mm-dd') || ' ' || substr(START_TIME,0,3) || ':' || substr(START_TIME,2,2) 

This is an example of the results I get from the above query. 
2017-01-25 13:30

when I run the query like this 

cast(cast(SCHEDULE_DATE as date format 'yyyy-mm-dd') || ' ' || substr(START_TIME,0,3) || ':' || substr(START_TIME,2,2) as Timestamp ) as TESTVALUE


I get invalid TimeStamp 
",0,-1,-1.0,"I am still fairly new to Teradata so forgive this but I have two columns one with the date and one with a 4 digit varchar as the time (24hour)

below is what I use to concatenate the fields to make it readable but I want to make the result come out as a valid timestamp so I can perform calculations.

cast(SCHEDULE_DATE as date format 'yyyy-mm-dd') || ' ' || substr(START_TIME,0,3) || ':' || substr(START_TIME,2,2) 

This is an example of the results I get from the above query. 
2017-01-25 13:30

when I run the query like this 

cast(cast(SCHEDULE_DATE as date format 'yyyy-mm-dd') || ' ' || substr(START_TIME,0,3) || ':' || substr(START_TIME,2,2) as Timestamp ) as TESTVALUE


I get invalid TimeStamp 
",3
218,42348494,how to change the teradata server port number?,"The default port for teradata is 1025. I want to change that. 

I have launched the teradata server on ec2. I tried to change it via the ""Teradata Administrator Tool"", but it does not have such an option.

I could not find any teradata admin tools/tutorials that explains how to change the default port number.
",-1,-1,-1.0,"The default port for teradata is 1025. I want to change that. 

I have launched the teradata server on ec2. I tried to change it via the ""Teradata Administrator Tool"", but it does not have such an option.

I could not find any teradata admin tools/tutorials that explains how to change the default port number.
",1
219,42376142,Teradata MERGE yielding no results when executed through SQLAlchemy,"I'm attempting to use python with sqlalchemy to download some data, create a temporary staging table on a Teradata Server, then MERGEing that table into another table which I've created to permanently store this data. I'm using sql = slqalchemy.text(merge) and td_engine.execute(sql) where merge is a string similar to the below:

MERGE INTO perm_table as p
USING temp_table as t
ON p.Id = t.Id
WHEN MATCHED THEN
UPDATE
SET col1 = t.col1,
col2 = t.col2,
...
col50 = t.col50
WHEN NOT MATCHED THEN
INSERT (col1,
col2,
...
col50)
VALUES (t.col1,
t.col2,
...
t.col50)


The script runs all the way to the end without error and the SQL executes properly through Teradata Studio, but for some reason the table won't update when I execute it through SQLAlchemy. However, I've also run different SQL expressions, like the insert that populated perm_table from the same python script and it worked fine. Maybe there's something specific to the MERGE and SQLAlchemy combo?
",-1,-1,-1.0,"I'm attempting to use python with sqlalchemy to download some data, create a temporary staging table on a Teradata Server, then MERGEing that table into another table which I've created to permanently store this data. I'm using sql = slqalchemy.text(merge) and td_engine.execute(sql) where merge is a string similar to the below:

MERGE INTO perm_table as p
USING temp_table as t
ON p.Id = t.Id
WHEN MATCHED THEN
UPDATE
SET col1 = t.col1,
col2 = t.col2,
...
col50 = t.col50
WHEN NOT MATCHED THEN
INSERT (col1,
col2,
...
col50)
VALUES (t.col1,
t.col2,
...
t.col50)


The script runs all the way to the end without error and the SQL executes properly through Teradata Studio, but for some reason the table won't update when I execute it through SQLAlchemy. However, I've also run different SQL expressions, like the insert that populated perm_table from the same python script and it worked fine. Maybe there's something specific to the MERGE and SQLAlchemy combo?
",3
220,42496826,PHP 64 bit - Teradata ODBC driver failure - system error 126 'specified module cannot be found' -,"I have installed PHP 5.6.30 64 bit on Windows Server 2012 along with Teradata ODBC driver. 

Whenever I test the PHP connection I receive an error 


  Warning: odbc_connect(): SQL error: Specified driver could not be loaded due to system error 126: The specified module could not be found. (Teradata, C:\Program Files\Teradata\Client\15.00\ODBC Driver for Teradata nt-x8664\Lib\tdata32.dll)., SQL state IM003 in SQLConnect 


EDIT: C:\Program Files\Teradata\Client\15.00\ODBC Driver for Teradata nt-x8664\Lib\tdata32.dll does definitely exist on that specific path. Also, using tdxodbc.exe to test the ODBC connection works normally using tdata32.dll, so it appears to be some issue with PHP.

I have installed Teradata ODBC via GSS > ICU > ODBC from the Utilities package based on a related thread however still experiencing the same error. I can make a successful 32-bit ODBC connection via SQL assistant. I have tried a DSN / DSN-less ODBC connection via PHP but same error occurs.

I have been unable to find any clear indication of what the issue is - any idea? 
",-1,-1,-1.0,"I have installed PHP 5.6.30 64 bit on Windows Server 2012 along with Teradata ODBC driver. 

Whenever I test the PHP connection I receive an error 


  Warning: odbc_connect(): SQL error: Specified driver could not be loaded due to system error 126: The specified module could not be found. (Teradata, C:\Program Files\Teradata\Client\15.00\ODBC Driver for Teradata nt-x8664\Lib\tdata32.dll)., SQL state IM003 in SQLConnect 


EDIT: C:\Program Files\Teradata\Client\15.00\ODBC Driver for Teradata nt-x8664\Lib\tdata32.dll does definitely exist on that specific path. Also, using tdxodbc.exe to test the ODBC connection works normally using tdata32.dll, so it appears to be some issue with PHP.

I have installed Teradata ODBC via GSS > ICU > ODBC from the Utilities package based on a related thread however still experiencing the same error. I can make a successful 32-bit ODBC connection via SQL assistant. I have tried a DSN / DSN-less ODBC connection via PHP but same error occurs.

I have been unable to find any clear indication of what the issue is - any idea? 
",1
221,42502016,Angular 2 teradata covalent: how to reduse material 2 toolbar height,"how to reduce toolbar size in angular material 2??

I am using Teradata covalent for angular 2 UI.Teradata covalent itself using angular2. 

Angular Material 2 toolbar size is too big for me.

So I want to decrease the size of the toolbar. I have tried to look in CSS in my dev console but I didn't find any padding option. I found height property and I try to reduce that height but didn't work for me.

if you want to look at the demo here https://teradata.github.io/covalent/#/components/material-components

scroll down to Toolbar.
",-1,-1,-1.0,"how to reduce toolbar size in angular material 2??

I am using Teradata covalent for angular 2 UI.Teradata covalent itself using angular2. 

Angular Material 2 toolbar size is too big for me.

So I want to decrease the size of the toolbar. I have tried to look in CSS in my dev console but I didn't find any padding option. I found height property and I try to reduce that height but didn't work for me.

if you want to look at the demo here https://teradata.github.io/covalent/#/components/material-components

scroll down to Toolbar.
",2
222,42599411,Connecting to Teradata using Python,"I am trying to connect to teradata server and load a dataframe into a table using python. Here is my code -

import sqlalchemy 

engine = sqlalchemy.create_engine(""teradata://username:passwor@hostname:port/"")

f3.to_sql(con=engine, name='sample', if_exists='replace', schema = 'schema_name')


But I am getting the following error -

InterfaceError: (teradata.api.InterfaceError) ('DRIVER_NOT_FOUND', ""No driver found for 'Teradata'.  Available drivers: SQL Server,SQL Server Native Client 11.0,ODBC Driver 13 for SQL Server"")


Can anybody help me to figure out whats wrong in my approach?
",-1,-1,-1.0,"I am trying to connect to teradata server and load a dataframe into a table using python. Here is my code -

import sqlalchemy 

engine = sqlalchemy.create_engine(""teradata://username:passwor@hostname:port/"")

f3.to_sql(con=engine, name='sample', if_exists='replace', schema = 'schema_name')


But I am getting the following error -

InterfaceError: (teradata.api.InterfaceError) ('DRIVER_NOT_FOUND', ""No driver found for 'Teradata'.  Available drivers: SQL Server,SQL Server Native Client 11.0,ODBC Driver 13 for SQL Server"")


Can anybody help me to figure out whats wrong in my approach?
",1
223,42617395,Teradata error SPL 1076 - The right parenthesis in parameter declaration is missing,"I have created a blank table in Teradata called EMPLOYEE

CREATE MULTISET VOLATILE TABLE EMPLOYEE ( 
   EmployeeNo INTEGER, 
   FirstName VARCHAR(30), 
   LastName VARCHAR(30), 
   DOB DATE FORMAT 'YYYY-MM-DD', 
   JoinedDate DATE FORMAT 'YYYY-MM-DD', 
   DepartmentNo BYTEINT 
) 
PRIMARY INDEX ( EmployeeNo ) On COMMIT PRESERVE ROWS;


Table is created. 

Now i am trying to define a procedure to enter data into the table.

CREATE PROCEDURE InsertEmployee ( 
   IN in_EmployeeNo INTEGER, IN in_FirstName VARCHAR(30), 
   IN in_LastName VARCHAR(30), IN in_DOB DATE FORMAT 'YYYY-MM-DD', 
   IN in_JoinedDate DATE FORMAT 'YYYY-MM-DD', IN in_DepartmentNo BYTEINT
) 
BEGIN 
   INSERT INTO EMPLOYEE( 
      EmployeeNo, 
      FirstName, 
      LastName, 
      DOB, 
      JoinedDate, 
      DepartmentNo
   ) 
   VALUES ( 
      :in_EmployeeNo, 
       :in_LastName, 
      :in_FirstName, 
      :in_DOB,
      :in_JoinedDate,  
      :in_DepartmentNo
   ); 
END;


This is where I repeatedly get 2 errors:

SPL1076:E(L3), The right parenthesis in parameter declaration is missing.
SPL1048:E(L3), Unexpected text ';' in place of SPL statement.

",-1,-1,-1.0,"I have created a blank table in Teradata called EMPLOYEE

CREATE MULTISET VOLATILE TABLE EMPLOYEE ( 
   EmployeeNo INTEGER, 
   FirstName VARCHAR(30), 
   LastName VARCHAR(30), 
   DOB DATE FORMAT 'YYYY-MM-DD', 
   JoinedDate DATE FORMAT 'YYYY-MM-DD', 
   DepartmentNo BYTEINT 
) 
PRIMARY INDEX ( EmployeeNo ) On COMMIT PRESERVE ROWS;


Table is created. 

Now i am trying to define a procedure to enter data into the table.

CREATE PROCEDURE InsertEmployee ( 
   IN in_EmployeeNo INTEGER, IN in_FirstName VARCHAR(30), 
   IN in_LastName VARCHAR(30), IN in_DOB DATE FORMAT 'YYYY-MM-DD', 
   IN in_JoinedDate DATE FORMAT 'YYYY-MM-DD', IN in_DepartmentNo BYTEINT
) 
BEGIN 
   INSERT INTO EMPLOYEE( 
      EmployeeNo, 
      FirstName, 
      LastName, 
      DOB, 
      JoinedDate, 
      DepartmentNo
   ) 
   VALUES ( 
      :in_EmployeeNo, 
       :in_LastName, 
      :in_FirstName, 
      :in_DOB,
      :in_JoinedDate,  
      :in_DepartmentNo
   ); 
END;


This is where I repeatedly get 2 errors:

SPL1076:E(L3), The right parenthesis in parameter declaration is missing.
SPL1048:E(L3), Unexpected text ';' in place of SPL statement.

",3
224,42300600,Write R DataFrame to Teradata using RJDBC,"We want to insert a dataframe into a table in teradata. so we connect to the database
drv =     JDBC(&quot;com.teradata.jdbc.TeraDriver&quot;,&quot;C:\\Users\\~\\TeradataJDBCDDriver\\terajdb    c4.jar;C:\\Users\\~\\TeradataJDBCDDriver\\tdgssconfig.jar&quot;)  
conn = dbConnect(drv,&quot;jdbc:teradata://###&quot;,&quot;username&quot;,&quot;password&quot;) 

So we created a dataframe:
column1&lt;-c(1,2,3,4)
column2&lt;-c(&quot;bar1&quot;,&quot;bar2&quot;,&quot;bar3&quot;,&quot;bar4&quot;)
df=data.frame(column1,column2)

And now we want to write the dataframe to a table in teradata. We tried three approaches
first try where we define the table we want to create with its datatypes
dbWriteTable(conn,&quot;temp.test_table(a int,b varchar(100))&quot;, df)

Then the following error appears:
&quot;Error in .local(conn, statement, ...) : execute JDBC update query failed in     dbSendUpdate ([Teradata Database] [TeraJDBC 15.10.00.33] [Error 3706] [SQLState     42000] Syntax error: expected something between ')' and '('.)&quot;

The second approach we create a table first and try to append the dataframe second.
dbSendUpdate(conn,&quot;create table temp.test_table(a int,b varchar(100))&quot;)
dbWriteTable(conn,&quot;temp.test_table&quot;, df,row.names=F,overwrite=F,append = T)

This leads to the following error:
Error in .local(conn, name, value, ...) : Cannot append to a non-existing table `temp.test_table'

When we check if the table exists using dbListTables() the table does exist however. (but not if we use dbExistTable())
In the third try we try to overwrite instead of append the data.
dbWriteTable(conn,&quot;temp.test_table&quot;, df,row.names=F,overwrite=T,append = F)

Which leads to the following error:
Error in .local(conn, statement, ...) : execute JDBC update query failed in dbSendUpdate ([Teradata Database] [TeraJDBC 15.10.00.33] [Error 3803] [SQLState 42S01] Table 'test_table' already exists.)

now it states that the table already exists (that should be fine as, we want to overwrite it). But the table is not overwritten.
all in all we're looking for a solution to write a dataframe from r into a teradata database.
Anyone knows how to deal with this issue?
",-1,-1,-1.0,"We want to insert a dataframe into a table in teradata. so we connect to the database
drv =     JDBC(&quot;com.teradata.jdbc.TeraDriver&quot;,&quot;C:\\Users\\~\\TeradataJDBCDDriver\\terajdb    c4.jar;C:\\Users\\~\\TeradataJDBCDDriver\\tdgssconfig.jar&quot;)  
conn = dbConnect(drv,&quot;jdbc:teradata://###&quot;,&quot;username&quot;,&quot;password&quot;) 

So we created a dataframe:
column1&lt;-c(1,2,3,4)
column2&lt;-c(&quot;bar1&quot;,&quot;bar2&quot;,&quot;bar3&quot;,&quot;bar4&quot;)
df=data.frame(column1,column2)

And now we want to write the dataframe to a table in teradata. We tried three approaches
first try where we define the table we want to create with its datatypes
dbWriteTable(conn,&quot;temp.test_table(a int,b varchar(100))&quot;, df)

Then the following error appears:
&quot;Error in .local(conn, statement, ...) : execute JDBC update query failed in     dbSendUpdate ([Teradata Database] [TeraJDBC 15.10.00.33] [Error 3706] [SQLState     42000] Syntax error: expected something between ')' and '('.)&quot;

The second approach we create a table first and try to append the dataframe second.
dbSendUpdate(conn,&quot;create table temp.test_table(a int,b varchar(100))&quot;)
dbWriteTable(conn,&quot;temp.test_table&quot;, df,row.names=F,overwrite=F,append = T)

This leads to the following error:
Error in .local(conn, name, value, ...) : Cannot append to a non-existing table `temp.test_table'

When we check if the table exists using dbListTables() the table does exist however. (but not if we use dbExistTable())
In the third try we try to overwrite instead of append the data.
dbWriteTable(conn,&quot;temp.test_table&quot;, df,row.names=F,overwrite=T,append = F)

Which leads to the following error:
Error in .local(conn, statement, ...) : execute JDBC update query failed in dbSendUpdate ([Teradata Database] [TeraJDBC 15.10.00.33] [Error 3803] [SQLState 42S01] Table 'test_table' already exists.)

now it states that the table already exists (that should be fine as, we want to overwrite it). But the table is not overwritten.
all in all we're looking for a solution to write a dataframe from r into a teradata database.
Anyone knows how to deal with this issue?
",3
225,42150954,Type Casting error while importing Decimal values from Teradata to Hive(parquet) using TDCH,"I am trying to import decimal data from Teradata to Hive Parquet format using TDCH, but it's giving type casting error. Not sure why it is trying to cast decimal to string. I am using Decimal data type at both side i.e. Hive and Teradata both. Same thing is happening with Timestamp field as well. This is happening while the table is in Parquet format in Hive, similar thing is working fine for RCfile format. Any help please?
",-1,-1,-1.0,"I am trying to import decimal data from Teradata to Hive Parquet format using TDCH, but it's giving type casting error. Not sure why it is trying to cast decimal to string. I am using Decimal data type at both side i.e. Hive and Teradata both. Same thing is happening with Timestamp field as well. This is happening while the table is in Parquet format in Hive, similar thing is working fine for RCfile format. Any help please?
",3
226,40657753,Spring boot Hibernate Teradata Unable to determine Dialect to use,"Application.properties :

spring.jpa.database-platform=org.hibernate.dialect.TeradataDialect


Data source bean :

@Bean  
public DataSource dataSource() {
  return  DataSourceBuilder
        .create()
            .driverClassName(""com.teradata.jdbc.TeraDriver"")
       .username(""dbc"")
     .password(""dbc"")
      .url(""jdbc:teradata://name/DBC"")

       .build();

    }


But I am getting this error :

Caused by: org.hibernate.HibernateException: Access to DialectResolutionInfo cannot be null when 'hibernate.dialect' not set

",-1,-1,-1.0,"Application.properties :

spring.jpa.database-platform=org.hibernate.dialect.TeradataDialect


Data source bean :

@Bean  
public DataSource dataSource() {
  return  DataSourceBuilder
        .create()
            .driverClassName(""com.teradata.jdbc.TeraDriver"")
       .username(""dbc"")
     .password(""dbc"")
      .url(""jdbc:teradata://name/DBC"")

       .build();

    }


But I am getting this error :

Caused by: org.hibernate.HibernateException: Access to DialectResolutionInfo cannot be null when 'hibernate.dialect' not set

",0
227,40653520,Spring Boot and Teradata UnsatisfiedDependencyException,"I am trying to create a simple Spring application with Teradata database.

It was working with mySql database and driver but after changing to Teradata driver/databased  i receive following exception : 

org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Tomcat.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.tomcat.jdbc.pool.DataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalArgumentException: URL must start with 'jdbc'


Application Properties :

spring.datasource.url =jdbc:teradata://servername/db
spring.datasource.username = dbc
spring.datasource.password = dbc
spring.datasource.driverClassName=com.ncr.teradata.TeraDriver

",-1,-1,-1.0,"I am trying to create a simple Spring application with Teradata database.

It was working with mySql database and driver but after changing to Teradata driver/databased  i receive following exception : 

org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration': Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Tomcat.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.tomcat.jdbc.pool.DataSource]: Factory method 'dataSource' threw exception; nested exception is java.lang.IllegalArgumentException: URL must start with 'jdbc'


Application Properties :

spring.datasource.url =jdbc:teradata://servername/db
spring.datasource.username = dbc
spring.datasource.password = dbc
spring.datasource.driverClassName=com.ncr.teradata.TeraDriver

",0
228,40438315,perform in situ updates in teradata,"What does it mean to perform in situ updates in teradata? I could not find anything via google. Any feedback would be very much appreciated.

PS:

This is an excerpt from a best practice doc I am digesting:

""Update processing of large amounts of data is perhaps the most inefficient operation that is routinely performed on Teradata. In almost all cases it is more efficient to insert the changed rows to another table rather than updating them in situ.""

DONT'T: Perform in situ updates or deletes unless unavoidable.
",1,-1,-1.0,"What does it mean to perform in situ updates in teradata? I could not find anything via google. Any feedback would be very much appreciated.

PS:

This is an excerpt from a best practice doc I am digesting:

""Update processing of large amounts of data is perhaps the most inefficient operation that is routinely performed on Teradata. In almost all cases it is more efficient to insert the changed rows to another table rather than updating them in situ.""

DONT'T: Perform in situ updates or deletes unless unavoidable.
",3
229,40294736,Removing a trailing period from improperly joined Teradata column,"I have a Teradata table that I've inherited that was not formatted in a great manner.

ID
123456789.
234567890.


I've tried:

TRIM(new_card_srgt_id (FORMAT 'Z(17)9')) 


but my version of Teradata gave a funny error: 

Format string has combination of numeric, character, and GRAPHIC values.


Any suggestions welcomed.

UPDATE: The suggestion to use TRIM(trailing '.' from ID) results in a numeric overflow when I went to cast it. Any other way to fix it.
",-1,-1,-1.0,"I have a Teradata table that I've inherited that was not formatted in a great manner.

ID
123456789.
234567890.


I've tried:

TRIM(new_card_srgt_id (FORMAT 'Z(17)9')) 


but my version of Teradata gave a funny error: 

Format string has combination of numeric, character, and GRAPHIC values.


Any suggestions welcomed.

UPDATE: The suggestion to use TRIM(trailing '.' from ID) results in a numeric overflow when I went to cast it. Any other way to fix it.
",3
230,40287139,SQL script fails import in Teradata 15 but succeeds in 14,"I recently updated one of my two work computers to Teradata v15. Given the radical difference in the GUI between Teradata SQL Assistant v14 &amp; Teradata Studio v15 I wanted to try and run a script of mine as a test case to figure out how things had changed. Unfortunately I didn't get very far, when I attempted to import some needed data from a tab delimited TXT file the import failed with the error ""[5666 : HY000] LOBs are not allowed in indexes."".

The field that is supposed to be getting filled is defined as VARCHAR(11), all the values in the TXT are 11 characters (they're fixed IDs). The data successfully imported when I ran the same logic in TSA 14. That being the case I'm thinking (hoping) something is off in my settings, but Google hasn't turned up any results that are helpful.

For reference the SQL in question is below:

CREATE SET TABLE HP_TX_OWN_TABLES.SRI_TX088, NO FALLBACK , 
    NO BEFORE JOURNAL, 
    NO AFTER JOURNAL, 
    CHECKSUM = DEFAULT 
    ( 
        MEM_ID VARCHAR(11)   
    ) 
PRIMARY INDEX (MEM_ID);

INSERT INTO HP_TX_OWN_TABLES.SRI_TX088 VALUES (?);

",1,-1,-1.0,"I recently updated one of my two work computers to Teradata v15. Given the radical difference in the GUI between Teradata SQL Assistant v14 &amp; Teradata Studio v15 I wanted to try and run a script of mine as a test case to figure out how things had changed. Unfortunately I didn't get very far, when I attempted to import some needed data from a tab delimited TXT file the import failed with the error ""[5666 : HY000] LOBs are not allowed in indexes."".

The field that is supposed to be getting filled is defined as VARCHAR(11), all the values in the TXT are 11 characters (they're fixed IDs). The data successfully imported when I ran the same logic in TSA 14. That being the case I'm thinking (hoping) something is off in my settings, but Google hasn't turned up any results that are helpful.

For reference the SQL in question is below:

CREATE SET TABLE HP_TX_OWN_TABLES.SRI_TX088, NO FALLBACK , 
    NO BEFORE JOURNAL, 
    NO AFTER JOURNAL, 
    CHECKSUM = DEFAULT 
    ( 
        MEM_ID VARCHAR(11)   
    ) 
PRIMARY INDEX (MEM_ID);

INSERT INTO HP_TX_OWN_TABLES.SRI_TX088 VALUES (?);

",3
231,40265559,Hibernate config -- Teradata -- DBC.UDTInfo,"I am attempting to interface my java application with Teradata, but when I call org.hibernate.cfg.Configuration#buildSessionFactory, I get the following error:

ERROR: [Teradata Database] [TeraJDBC 15.10.00.05] [Error 5315] [SQLState HY000] The user does not have SELECT access to DBC.UDTInfo.TypeName

After googling around a bit, it seems that the only solution that anyone talks about is just getting select access to DBC.UDTInfo.  But, supposing that this is not an option for me, is there another way around this problem?

(This is a crosspost from forums.hibernate.org -- still waiting for the admin to review and post it)

The full stack trace...

Oct 26, 2016 9:57:05 AM org.hibernate.Version logVersion  
INFO: HHH000412: Hibernate Core {5.2.3.Final}  
Oct 26, 2016 9:57:05 AM org.hibernate.cfg.Environment &lt;clinit&gt;  
INFO: HHH000206: hibernate.properties not found  
Oct 26, 2016 9:57:05 AM org.hibernate.cfg.Environment buildBytecodeProvider  
INFO: HHH000021: Bytecode provider name : javassist  
Oct 26, 2016 9:57:05 AM org.hibernate.annotations.common.reflection.java.JavaReflectionManager &lt;clinit&gt;  
INFO: HCANN000001: Hibernate Commons Annotations {5.0.1.Final}  
Oct 26, 2016 9:57:06 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl configure  
WARN: HHH10001002: Using Hibernate built-in connection pool (not for production use!)  
Oct 26, 2016 9:57:06 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl buildCreator  
INFO: HHH10001005: using driver [com.teradata.jdbc.TeraDriver] at URL [jdbc:teradata://&lt;serveraddr&gt;]  
Oct 26, 2016 9:57:06 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl buildCreator  
INFO: HHH10001001: Connection properties: {user=****, password=****}  
Oct 26, 2016 9:57:06 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl buildCreator  
INFO: HHH10001003: Autocommit mode: false  
Oct 26, 2016 9:57:06 AM org.hibernate.engine.jdbc.connections.internal.PooledConnections &lt;init&gt;  
INFO: HHH000115: Hibernate connection pool size: 20 (min=1)  
Oct 26, 2016 9:57:08 AM org.hibernate.dialect.Dialect &lt;init&gt;  
INFO: HHH000400: Using dialect: org.hibernate.dialect.TeradataDialect  
Oct 26, 2016 9:57:09 AM org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl useContextualLobCreation  
INFO: HHH000424: Disabling contextual LOB creation as createClob() method threw error : java.lang.reflect.InvocationTargetException  
Oct 26, 2016 9:57:09 AM org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl getIsolatedConnection  
INFO: HHH10001501: Connection obtained from JdbcConnectionAccess [org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess@e22bdd] for (non-JTA) DDL execution was not in auto-commit mode; the Connection 'local transaction' will be committed and the Connection will be set into auto-commit mode.  
Oct 26, 2016 9:57:10 AM org.hibernate.engine.jdbc.spi.SqlExceptionHelper logExceptions  
WARN: SQL Error: 5315, SQLState: HY000  
Oct 26, 2016 9:57:10 AM org.hibernate.engine.jdbc.spi.SqlExceptionHelper logExceptions  
ERROR: [Teradata Database] [TeraJDBC 15.10.00.05] [Error 5315] [SQLState HY000] The user does not have SELECT access to DBC.UDTInfo.TypeName.  
Oct 26, 2016 9:57:10 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl stop  
INFO: HHH10001008: Cleaning up connection pool [jdbc:teradata://&lt;serveraddr&gt;]  
Exception in thread ""main"" org.hibernate.exception.GenericJDBCException: Error accessing column metadata: &lt;schemaname&gt;.cr_test1  
. . . at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:47)  
. . . at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:111)  
. . . at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:97)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.convertSQLException(InformationExtractorJdbcDatabaseMetaDataImpl.java:98)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.addColumns(InformationExtractorJdbcDatabaseMetaDataImpl.java:580)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.extractTableInformation(InformationExtractorJdbcDatabaseMetaDataImpl.java:206)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.processTableResults(InformationExtractorJdbcDatabaseMetaDataImpl.java:395)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.getTables(InformationExtractorJdbcDatabaseMetaDataImpl.java:337)  
. . . at org.hibernate.tool.schema.extract.internal.DatabaseInformationImpl.getTablesInformation(DatabaseInformationImpl.java:120)  
. . . at org.hibernate.tool.schema.internal.GroupedSchemaMigratorImpl.performTablesMigration(GroupedSchemaMigratorImpl.java:65)  
. . . at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.performMigration(AbstractSchemaMigrator.java:203)  
. . . at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.doMigration(AbstractSchemaMigrator.java:110)  
. . . at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:177)  
. . . at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:66)  
. . . at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:309)  
. . . at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:493)  
. . . at org.hibernate.cfg.Configuration.buildSessionFactory(Configuration.java:710)  
. . . at org.hibernate.cfg.Configuration.buildSessionFactory(Configuration.java:726)  
. . . at &lt;mypackage&gt;.StoreData.experiment00(StoreData.java:45)  
. . . at &lt;mypackage&gt;.StoreData.main(StoreData.java:15)  
Caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.05] [Error 5315] [SQLState HY000] The user does not have SELECT access to DBC.UDTInfo.TypeName.  
. . . at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:308)  
. . . at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:109)  
. . . at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:307)  
. . . at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:196)  
. . . at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:123)  
. . . at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:114)  
. . . at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:386)  
. . . at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:328)  
. . . at com.teradata.jdbc.jdbc_4.TDStatement.doNonPrepExecuteQuery(TDStatement.java:316)  
. . . at com.teradata.jdbc.jdbc_4.TDStatement.executeQuery(TDStatement.java:1105)  
. . . at com.teradata.jdbc.TeraDatabaseMetaData.getColumns(TeraDatabaseMetaData.java:3297)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.addColumns(InformationExtractorJdbcDatabaseMetaDataImpl.java:553)  
. . . ... 15 more

",1,-1,-1.0,"I am attempting to interface my java application with Teradata, but when I call org.hibernate.cfg.Configuration#buildSessionFactory, I get the following error:

ERROR: [Teradata Database] [TeraJDBC 15.10.00.05] [Error 5315] [SQLState HY000] The user does not have SELECT access to DBC.UDTInfo.TypeName

After googling around a bit, it seems that the only solution that anyone talks about is just getting select access to DBC.UDTInfo.  But, supposing that this is not an option for me, is there another way around this problem?

(This is a crosspost from forums.hibernate.org -- still waiting for the admin to review and post it)

The full stack trace...

Oct 26, 2016 9:57:05 AM org.hibernate.Version logVersion  
INFO: HHH000412: Hibernate Core {5.2.3.Final}  
Oct 26, 2016 9:57:05 AM org.hibernate.cfg.Environment &lt;clinit&gt;  
INFO: HHH000206: hibernate.properties not found  
Oct 26, 2016 9:57:05 AM org.hibernate.cfg.Environment buildBytecodeProvider  
INFO: HHH000021: Bytecode provider name : javassist  
Oct 26, 2016 9:57:05 AM org.hibernate.annotations.common.reflection.java.JavaReflectionManager &lt;clinit&gt;  
INFO: HCANN000001: Hibernate Commons Annotations {5.0.1.Final}  
Oct 26, 2016 9:57:06 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl configure  
WARN: HHH10001002: Using Hibernate built-in connection pool (not for production use!)  
Oct 26, 2016 9:57:06 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl buildCreator  
INFO: HHH10001005: using driver [com.teradata.jdbc.TeraDriver] at URL [jdbc:teradata://&lt;serveraddr&gt;]  
Oct 26, 2016 9:57:06 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl buildCreator  
INFO: HHH10001001: Connection properties: {user=****, password=****}  
Oct 26, 2016 9:57:06 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl buildCreator  
INFO: HHH10001003: Autocommit mode: false  
Oct 26, 2016 9:57:06 AM org.hibernate.engine.jdbc.connections.internal.PooledConnections &lt;init&gt;  
INFO: HHH000115: Hibernate connection pool size: 20 (min=1)  
Oct 26, 2016 9:57:08 AM org.hibernate.dialect.Dialect &lt;init&gt;  
INFO: HHH000400: Using dialect: org.hibernate.dialect.TeradataDialect  
Oct 26, 2016 9:57:09 AM org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl useContextualLobCreation  
INFO: HHH000424: Disabling contextual LOB creation as createClob() method threw error : java.lang.reflect.InvocationTargetException  
Oct 26, 2016 9:57:09 AM org.hibernate.resource.transaction.backend.jdbc.internal.DdlTransactionIsolatorNonJtaImpl getIsolatedConnection  
INFO: HHH10001501: Connection obtained from JdbcConnectionAccess [org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator$ConnectionProviderJdbcConnectionAccess@e22bdd] for (non-JTA) DDL execution was not in auto-commit mode; the Connection 'local transaction' will be committed and the Connection will be set into auto-commit mode.  
Oct 26, 2016 9:57:10 AM org.hibernate.engine.jdbc.spi.SqlExceptionHelper logExceptions  
WARN: SQL Error: 5315, SQLState: HY000  
Oct 26, 2016 9:57:10 AM org.hibernate.engine.jdbc.spi.SqlExceptionHelper logExceptions  
ERROR: [Teradata Database] [TeraJDBC 15.10.00.05] [Error 5315] [SQLState HY000] The user does not have SELECT access to DBC.UDTInfo.TypeName.  
Oct 26, 2016 9:57:10 AM org.hibernate.engine.jdbc.connections.internal.DriverManagerConnectionProviderImpl stop  
INFO: HHH10001008: Cleaning up connection pool [jdbc:teradata://&lt;serveraddr&gt;]  
Exception in thread ""main"" org.hibernate.exception.GenericJDBCException: Error accessing column metadata: &lt;schemaname&gt;.cr_test1  
. . . at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:47)  
. . . at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:111)  
. . . at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:97)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.convertSQLException(InformationExtractorJdbcDatabaseMetaDataImpl.java:98)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.addColumns(InformationExtractorJdbcDatabaseMetaDataImpl.java:580)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.extractTableInformation(InformationExtractorJdbcDatabaseMetaDataImpl.java:206)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.processTableResults(InformationExtractorJdbcDatabaseMetaDataImpl.java:395)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.getTables(InformationExtractorJdbcDatabaseMetaDataImpl.java:337)  
. . . at org.hibernate.tool.schema.extract.internal.DatabaseInformationImpl.getTablesInformation(DatabaseInformationImpl.java:120)  
. . . at org.hibernate.tool.schema.internal.GroupedSchemaMigratorImpl.performTablesMigration(GroupedSchemaMigratorImpl.java:65)  
. . . at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.performMigration(AbstractSchemaMigrator.java:203)  
. . . at org.hibernate.tool.schema.internal.AbstractSchemaMigrator.doMigration(AbstractSchemaMigrator.java:110)  
. . . at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.performDatabaseAction(SchemaManagementToolCoordinator.java:177)  
. . . at org.hibernate.tool.schema.spi.SchemaManagementToolCoordinator.process(SchemaManagementToolCoordinator.java:66)  
. . . at org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:309)  
. . . at org.hibernate.boot.internal.SessionFactoryBuilderImpl.build(SessionFactoryBuilderImpl.java:493)  
. . . at org.hibernate.cfg.Configuration.buildSessionFactory(Configuration.java:710)  
. . . at org.hibernate.cfg.Configuration.buildSessionFactory(Configuration.java:726)  
. . . at &lt;mypackage&gt;.StoreData.experiment00(StoreData.java:45)  
. . . at &lt;mypackage&gt;.StoreData.main(StoreData.java:15)  
Caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 15.10.00.05] [Error 5315] [SQLState HY000] The user does not have SELECT access to DBC.UDTInfo.TypeName.  
. . . at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:308)  
. . . at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:109)  
. . . at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:307)  
. . . at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:196)  
. . . at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:123)  
. . . at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:114)  
. . . at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:386)  
. . . at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:328)  
. . . at com.teradata.jdbc.jdbc_4.TDStatement.doNonPrepExecuteQuery(TDStatement.java:316)  
. . . at com.teradata.jdbc.jdbc_4.TDStatement.executeQuery(TDStatement.java:1105)  
. . . at com.teradata.jdbc.TeraDatabaseMetaData.getColumns(TeraDatabaseMetaData.java:3297)  
. . . at org.hibernate.tool.schema.extract.internal.InformationExtractorJdbcDatabaseMetaDataImpl.addColumns(InformationExtractorJdbcDatabaseMetaDataImpl.java:553)  
. . . ... 15 more

",0
232,40170664,sqoop to teradata - column length issue,"I am trying to sqoop a table's data from HIVE to Teradata and got the error 

Error: com.teradata.connector.common.exception.ConnectorException: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.00.00.20] [Error 1186] [SQLState HY000] Parameter 8 length is 67618 bytes, which is greater than the maximum 64000 bytes that can be set.

Can anyone please suggest exactly what change I have to do here? Column-8 is too long string in HIVE table and that is why I have defined the data type in TERADATA as VARCHAR(50000), but still it is failing.

Error: com.teradata.connector.common.exception.ConnectorException: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.00.00.20] [Error 1186] [SQLState HY000] Parameter 8 length is 67618 bytes, which is greater than the maximum 64000 bytes that can be set.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:94)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:74)
    at com.teradata.jdbc.jdbc_4.TDPreparedStatement.internalSetString(TDPreparedStatement.java:1121)
    at com.teradata.jdbc.jdbc_4.TDPreparedStatement.setString(TDPreparedStatement.java:1095)
    at com.teradata.jdbc.jdbc_4.TDPreparedStatement.setObject(TDPreparedStatement.java:1631)
    at com.teradata.connector.teradata.TeradataObjectArrayWritable.write(TeradataObjectArrayWritable.java:232)
    at com.teradata.connector.teradata.TeradataBatchInsertOutputFormat$TeradataRecordWriter.write(TeradataBatchInsertOutputFormat.java:142)
    at com.teradata.connector.teradata.TeradataBatchInsertOutputFormat$TeradataRecordWriter.write(TeradataBatchInsertOutputFormat.java:114)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:107)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:65)
    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:658)
    at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
    at com.teradata.connector.common.ConnectorMMapper.map(ConnectorMMapper.java:129)
    at com.teradata.connector.common.ConnectorMMapper.run(ConnectorMMapper.java:117)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)

    at com.teradata.connector.teradata.TeradataBatchInsertOutputFormat$TeradataRecordWriter.write(TeradataBatchInsertOutputFormat.java:151)
    at com.teradata.connector.teradata.TeradataBatchInsertOutputFormat$TeradataRecordWriter.write(TeradataBatchInsertOutputFormat.java:114)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:107)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:65)
    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:658)
    at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
    at com.teradata.connector.common.ConnectorMMapper.map(ConnectorMMapper.java:129)
    at com.teradata.connector.common.ConnectorMMapper.run(ConnectorMMapper.java:117)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)

",-1,-1,-1.0,"I am trying to sqoop a table's data from HIVE to Teradata and got the error 

Error: com.teradata.connector.common.exception.ConnectorException: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.00.00.20] [Error 1186] [SQLState HY000] Parameter 8 length is 67618 bytes, which is greater than the maximum 64000 bytes that can be set.

Can anyone please suggest exactly what change I have to do here? Column-8 is too long string in HIVE table and that is why I have defined the data type in TERADATA as VARCHAR(50000), but still it is failing.

Error: com.teradata.connector.common.exception.ConnectorException: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.00.00.20] [Error 1186] [SQLState HY000] Parameter 8 length is 67618 bytes, which is greater than the maximum 64000 bytes that can be set.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:94)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:74)
    at com.teradata.jdbc.jdbc_4.TDPreparedStatement.internalSetString(TDPreparedStatement.java:1121)
    at com.teradata.jdbc.jdbc_4.TDPreparedStatement.setString(TDPreparedStatement.java:1095)
    at com.teradata.jdbc.jdbc_4.TDPreparedStatement.setObject(TDPreparedStatement.java:1631)
    at com.teradata.connector.teradata.TeradataObjectArrayWritable.write(TeradataObjectArrayWritable.java:232)
    at com.teradata.connector.teradata.TeradataBatchInsertOutputFormat$TeradataRecordWriter.write(TeradataBatchInsertOutputFormat.java:142)
    at com.teradata.connector.teradata.TeradataBatchInsertOutputFormat$TeradataRecordWriter.write(TeradataBatchInsertOutputFormat.java:114)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:107)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:65)
    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:658)
    at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
    at com.teradata.connector.common.ConnectorMMapper.map(ConnectorMMapper.java:129)
    at com.teradata.connector.common.ConnectorMMapper.run(ConnectorMMapper.java:117)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)

    at com.teradata.connector.teradata.TeradataBatchInsertOutputFormat$TeradataRecordWriter.write(TeradataBatchInsertOutputFormat.java:151)
    at com.teradata.connector.teradata.TeradataBatchInsertOutputFormat$TeradataRecordWriter.write(TeradataBatchInsertOutputFormat.java:114)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:107)
    at com.teradata.connector.common.ConnectorOutputFormat$ConnectorFileRecordWriter.write(ConnectorOutputFormat.java:65)
    at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:658)
    at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
    at com.teradata.connector.common.ConnectorMMapper.map(ConnectorMMapper.java:129)
    at com.teradata.connector.common.ConnectorMMapper.run(ConnectorMMapper.java:117)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)

",0
233,39991274,Not able to access volatile table in teradata.net mode,"I am using Teredata.net to connect to my SQL assistant as i have some issues with my odbc connection.

The problem i am facing is i am able to create a volatile table but when i do a select * it says table doesn't exist. I am not sure if that has anything to do with the connection as i have never used teradata.net connection before. below is my table syntax;

 CREATE MULTISET VOLATILE TABLE VT
(
COL1 VARCHAR(100)
) 
ON COMMIT PRESERVE ROWS;


Can any one help me here.

Regards,
Amit
",-1,-1,-1.0,"I am using Teredata.net to connect to my SQL assistant as i have some issues with my odbc connection.

The problem i am facing is i am able to create a volatile table but when i do a select * it says table doesn't exist. I am not sure if that has anything to do with the connection as i have never used teradata.net connection before. below is my table syntax;

 CREATE MULTISET VOLATILE TABLE VT
(
COL1 VARCHAR(100)
) 
ON COMMIT PRESERVE ROWS;


Can any one help me here.

Regards,
Amit
",3
234,39760130,Teradata distance between two geo-coordinates using ST_SPHERICALDISTANCE,"I have read that in Teradata Version 13 and onward you can use ST_SPHERICALDISTANCE for calculating geo-distance.
I written a simple test query in Teradata:

Select
Cast('POINT(-35.0000, 150.0000)' As ST_GEOMETRY) As location1,
Cast('POINT(-35.0000, 149.0000)' As ST_GEOMETRY) As location2,
location1.ST_SPHERICALDISTANCE(location2) As Distance_In_km


But received following error:  


  Teradata row not delivered (trget): 13
  OGRGeometryFactoryX::createFromWkt failed.


Is this an issue with the query or configuration?
Thanks
",-1,-1,-1.0,"I have read that in Teradata Version 13 and onward you can use ST_SPHERICALDISTANCE for calculating geo-distance.
I written a simple test query in Teradata:

Select
Cast('POINT(-35.0000, 150.0000)' As ST_GEOMETRY) As location1,
Cast('POINT(-35.0000, 149.0000)' As ST_GEOMETRY) As location2,
location1.ST_SPHERICALDISTANCE(location2) As Distance_In_km


But received following error:  


  Teradata row not delivered (trget): 13
  OGRGeometryFactoryX::createFromWkt failed.


Is this an issue with the query or configuration?
Thanks
",3
235,39683363,Teradata BTEQ conditional not working as expected,"I have a rest. where I need to check
if a teradata table exists. If yes then delete the records
If no then create the table as per DDL

The issue is that ..both the statements are getting executed and the conditions I want to run are not successfully run

select * from doc.tablesV
where database = DATABASE_NAME 
and table = TABLE_NAME ;   

.if activitycount = 1 then .GoTo del_tab ;   
.if activitycount = 0 then .GoTo create_tab ;
.LABEL del_tab ;   
delete  table DATABASE_NAME.TABLE_NAME;
.LABEL create_tab;
create multisite table ...;


In the log I see this message
For the first run (when no table exists)

activity count =0
.label del_tab
skipped
.label create_tab
Go to create_tab


del_tab command is executed (though it says skipped)
and it fails (as no table exists)

Is there any mistake in my conditional logic? or Is there any limitation to the conditional logic in BTEQ?
Thanks
Pari
",-1,-1,-1.0,"I have a rest. where I need to check
if a teradata table exists. If yes then delete the records
If no then create the table as per DDL

The issue is that ..both the statements are getting executed and the conditions I want to run are not successfully run

select * from doc.tablesV
where database = DATABASE_NAME 
and table = TABLE_NAME ;   

.if activitycount = 1 then .GoTo del_tab ;   
.if activitycount = 0 then .GoTo create_tab ;
.LABEL del_tab ;   
delete  table DATABASE_NAME.TABLE_NAME;
.LABEL create_tab;
create multisite table ...;


In the log I see this message
For the first run (when no table exists)

activity count =0
.label del_tab
skipped
.label create_tab
Go to create_tab


del_tab command is executed (though it says skipped)
and it fails (as no table exists)

Is there any mistake in my conditional logic? or Is there any limitation to the conditional logic in BTEQ?
Thanks
Pari
",3
236,39614883,Connecting to teradata from ubuntu 14.04,"After following the instructions from http://crashthatch.tumblr.com/post/66957708538/teradata-odbc-connection-using-python-on-ubuntu, installed 15.00.05.14 version of teradata odbc version.

&gt;&gt;&gt; import pyodbc
&gt;&gt;&gt; pyodbc.pooling = False
&gt;&gt;&gt; pyodbc.connect('DRIVER={Teradata};DBCNAME=teradata.xx.xxx.com;UID=myname;PWD=pwd;QUIETMODE=YES;')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
pyodbc.OperationalError: ('HYT00', '[HYT00] [Teradata][Unix system error]  110 Socket error - No response received when attempting to connect to the Teradata server (110) (SQLDriverConnect)')


$ odbcinst -j
unixODBC 2.2.14
odbcinst: symbol lookup error: odbcinst: undefined symbol: odbcinst_system_file_name

$ python -c ""import pyodbc; print pyodbc.version""
3.0.6


what should be the workaround for this problem?
",-1,-1,-1.0,"After following the instructions from http://crashthatch.tumblr.com/post/66957708538/teradata-odbc-connection-using-python-on-ubuntu, installed 15.00.05.14 version of teradata odbc version.

&gt;&gt;&gt; import pyodbc
&gt;&gt;&gt; pyodbc.pooling = False
&gt;&gt;&gt; pyodbc.connect('DRIVER={Teradata};DBCNAME=teradata.xx.xxx.com;UID=myname;PWD=pwd;QUIETMODE=YES;')
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
pyodbc.OperationalError: ('HYT00', '[HYT00] [Teradata][Unix system error]  110 Socket error - No response received when attempting to connect to the Teradata server (110) (SQLDriverConnect)')


$ odbcinst -j
unixODBC 2.2.14
odbcinst: symbol lookup error: odbcinst: undefined symbol: odbcinst_system_file_name

$ python -c ""import pyodbc; print pyodbc.version""
3.0.6


what should be the workaround for this problem?
",1
237,39576583,multiple common table expressions in teradata,"According to this teradata v14.0 supports multiple common table expressions. So the simplified script as follows:

WITH CTE1 AS
(
    SELECT       
                X                                       
        FROM SomeTable
)
,CTE2 AS
(
        SELECT top 10 X FROM CTE1
)
SELECT * FROM CTE2


should work. Unfortunately, I get:

Object 'CTE1' does not exist.


Any ideas?
",0,-1,-1.0,"According to this teradata v14.0 supports multiple common table expressions. So the simplified script as follows:

WITH CTE1 AS
(
    SELECT       
                X                                       
        FROM SomeTable
)
,CTE2 AS
(
        SELECT top 10 X FROM CTE1
)
SELECT * FROM CTE2


should work. Unfortunately, I get:

Object 'CTE1' does not exist.


Any ideas?
",3
238,39570366,Teradata : Update table values with join and if clause,"I am working on a Teradata system in which I have 2 tables, Apple as A &amp; Ball as B. Apple has 2 columns primId(integer) and updateValue(INteger). B only has primId(integer). What I am trying to do is, when A.primId=B.primId, then set updateValue=1 else to 0. 

Table Apple:
Columns : Primary-key primId(Integer)
Other Columns : updateValue-(Integer)

Table Ball :
Columns : Primary-key primId(INteger)


My query so far :

update apple from apple a, ball b set updatevalue=1 where a.primId=b.primId;


I am getting a invalid query error, and I have no else clause yet. Any help would be nice. 
",1,-1,-1.0,"I am working on a Teradata system in which I have 2 tables, Apple as A &amp; Ball as B. Apple has 2 columns primId(integer) and updateValue(INteger). B only has primId(integer). What I am trying to do is, when A.primId=B.primId, then set updateValue=1 else to 0. 

Table Apple:
Columns : Primary-key primId(Integer)
Other Columns : updateValue-(Integer)

Table Ball :
Columns : Primary-key primId(INteger)


My query so far :

update apple from apple a, ball b set updatevalue=1 where a.primId=b.primId;


I am getting a invalid query error, and I have no else clause yet. Any help would be nice. 
",3
239,39510219,Tablesize is different although Table DDL and data is exactly same(Teradata),"I encountered very strange issue in teradata today.I created one table from another table using following syntax:

create table a.xyz as a.abc with data;


so obviously xyz will be created exactly same as abc(including column attributes). Also data will be same. However if I execute query to get size of that table using allspace or tablesize, new table is taking more size than original table? May I know why it should be the case? I checked skew factor as well for curiosity, surprisingly skew factor of new table was less(ideally it should be same because same PI and same data). abc having UPI and one column got column level compression as well, but of course both these attributes copied in new table as well.

May I know what is happening here? 
",-1,-1,-1.0,"I encountered very strange issue in teradata today.I created one table from another table using following syntax:

create table a.xyz as a.abc with data;


so obviously xyz will be created exactly same as abc(including column attributes). Also data will be same. However if I execute query to get size of that table using allspace or tablesize, new table is taking more size than original table? May I know why it should be the case? I checked skew factor as well for curiosity, surprisingly skew factor of new table was less(ideally it should be same because same PI and same data). abc having UPI and one column got column level compression as well, but of course both these attributes copied in new table as well.

May I know what is happening here? 
",3
240,39503419,Error handling in Teradata Stored Procedure,"I am trying to develop a stored procedure within TERADATA to handle and manage exceptions. 

The stored procedure should raise the error to the caller, which is an SSIS Package.

I am trying to illustrate this by creating a stored procedure for illustration only.   

I have these tables:

Table_A:

- ID           INT
- ITEM_NUM     INT
- DESC         VARCHAR(20)
- CREATE_DTTM  VARCHAR(2O)


Table_B:

- ID           INT
- ITEM_NUM     INT
- DESC         VARCHAR(20)
- CREATE_DTTM  VARCHAR(2O)


I have two tables that will be inserting data from two SELECT statements. 

REPLACE PROCEDURE csTest2()
SQL SECURITY OWNER
BEGIN
    DECLARE varErrorMessage char(256);
    DECLARE varSQLState char(5);
    DECLARE varReturnCode char(5);

    DECLARE varRollbackNeededInd char(1);           /* transaction mgt */
    SET varRollbackNeededInd = 'N';

    SET varReturnCode = '00000';
    SET varErrorMessage = '';

    BEGIN TRANSACTION;

    -- USING A SINGLE HANDLER WITH MULTIPLE STATEMENTS
    -- PLANING TO CHANGE ERROR MESSAGE IN EACH STATEMENT.
    ins6: BEGIN
        DECLARE EXIT HANDLER FOR SQLEXCEPTION 

        H99:Begin
            set varSQLState = SQLSTATE;
            set varErrorMessage= 'This message should not be displayed'; -- 
        end H99;

        -- IMAGINE THAT I AM GETTING THE VALUES AS INPUT PARAMERS IN THE PROCEDURE 
        INSERT INTO ""Table_A""
         (ID , ITEM_NUM, DESC, CREATE_DTTM)
         SELECT 1, '222', 'SOME DESC',CURRENT_TIMESTAMP;


         H98:Begin
             set varSQLState = SQLSTATE;
             set varErrorMessage= 'This message is displayed, ITEM_NUM invalid characters';
         end H98;

        -- NOW I AM DOING A SECOND INSERT TO table b WITH INVALID DATA
        -- THE VALUE FOR THE ITEM NUMBER CONTAINS ALPHANUMERICE CHARACTERS
        INSERT INTO ""Table_b""
         (ID , ITEM_NUM, DESC, CREATE_DTTM)
         SELECT 1, '333F', 'SOME DESC',CURRENT_TIMESTAMP;

    END ins6;

    EndTrans: BEGIN

        IF varSQLState &lt;&gt; '0' THEN
            SET varRollbackNeededInd = 'Y';
            SET varReturnCode = '9999';
        END IF;

        IF  varRollbackNeededInd = 'Y' THEN
            ROLLBACK;        -- ROLLBACK AND SEND ERROR TO CALLER
            SIGNAL SQLSTATE 'U0123' SET MESSAGE_TEXT = 'SQlState is - ' || varSQLSTATE || ' - and error is - ' || varErrorMessage;
        ELSE
            END TRANSACTION; -- COMMIT TRANSACTION

        END IF;
   END EndTrans;

END;


The problem that I am facing with the above stored procedure is that error message that I get is not the one that I am expecting. Since error is intentionally created in my second statement I am expecting to get: This message is displayed, ITEM_NUM invalid characters but I am getting This message should not be displayed

Now if I modify the PROCEDURE to have multiple handlers, one for each statement, I do get the correct error message, but now since I am intentionally generating the error in the first statement it does not terminate the procedure, it handles the error and sets the proper message but continues to process the next statement which I am not expecting to do this, so how can I terminate this procedure? 

REPLACE PROCEDURE csTest2()
SQL SECURITY OWNER
BEGIN

    DECLARE varErrorMessage char(256);
    DECLARE varSQLState char(5);
    DECLARE varReturnCode char(5);

    DECLARE varRollbackNeededInd char(1);           /* transaction mgt */
    SET varRollbackNeededInd = 'N';

    SET varReturnCode = '00000';
    SET varErrorMessage = '';

    BEGIN TRANSACTION;

    ins6: BEGIN
        DECLARE EXIT HANDLER FOR SQLEXCEPTION 

        H99:Begin
            set varSQLState = SQLSTATE;
            set varErrorMessage= 'Error is displayed in this case because ITEM_NUM';

        -- ERROR IS PRESENT IN THIS STATEMENT AND SHOULD TERMINATE THE PROCEDURE. 
        INSERT INTO ""Table_A""
         (ID , ITEM_NUM, DESC, CREATE_DTTM)
         SELECT 1, '222F', 'SOME DESC',CURRENT_TIMESTAMP;
    END ins6;

    ins7: BEGIN      
         H98:Begin
             set varSQLState = SQLSTATE;
             set varErrorMessage= 'no error is displayed in this case';
         end H98;

        -- NO ERROR IS EXPECTED, BUT IT SHOULD NOT REACH HERE SINCE WE HAD ERROR ON FIRST STATEMENT. 
        INSERT INTO ""Table_b""
         (ID , ITEM_NUM, DESC, CREATE_DTTM)
         SELECT 1, '333', 'SOME DESC',CURRENT_TIMESTAMP;

    END ins7;

    EndTrans: BEGIN

        IF varSQLState &lt;&gt; '0' THEN
            SET varRollbackNeededInd = 'Y';
            SET varReturnCode = '9999';
        END IF;

        IF  varRollbackNeededInd = 'Y' THEN
            ROLLBACK;        -- ROLLBACK AND SEND ERROR TO CALLER
            SIGNAL SQLSTATE 'U0123' SET MESSAGE_TEXT = 'SQlState is - ' || varSQLSTATE || ' - and error is - ' || varErrorMessage;
        ELSE
            END TRANSACTION; -- COMMIT TRANSACTION

        END IF;
   END EndTrans;

END;

",-1,-1,-1.0,"I am trying to develop a stored procedure within TERADATA to handle and manage exceptions. 

The stored procedure should raise the error to the caller, which is an SSIS Package.

I am trying to illustrate this by creating a stored procedure for illustration only.   

I have these tables:

Table_A:

- ID           INT
- ITEM_NUM     INT
- DESC         VARCHAR(20)
- CREATE_DTTM  VARCHAR(2O)


Table_B:

- ID           INT
- ITEM_NUM     INT
- DESC         VARCHAR(20)
- CREATE_DTTM  VARCHAR(2O)


I have two tables that will be inserting data from two SELECT statements. 

REPLACE PROCEDURE csTest2()
SQL SECURITY OWNER
BEGIN
    DECLARE varErrorMessage char(256);
    DECLARE varSQLState char(5);
    DECLARE varReturnCode char(5);

    DECLARE varRollbackNeededInd char(1);           /* transaction mgt */
    SET varRollbackNeededInd = 'N';

    SET varReturnCode = '00000';
    SET varErrorMessage = '';

    BEGIN TRANSACTION;

    -- USING A SINGLE HANDLER WITH MULTIPLE STATEMENTS
    -- PLANING TO CHANGE ERROR MESSAGE IN EACH STATEMENT.
    ins6: BEGIN
        DECLARE EXIT HANDLER FOR SQLEXCEPTION 

        H99:Begin
            set varSQLState = SQLSTATE;
            set varErrorMessage= 'This message should not be displayed'; -- 
        end H99;

        -- IMAGINE THAT I AM GETTING THE VALUES AS INPUT PARAMERS IN THE PROCEDURE 
        INSERT INTO ""Table_A""
         (ID , ITEM_NUM, DESC, CREATE_DTTM)
         SELECT 1, '222', 'SOME DESC',CURRENT_TIMESTAMP;


         H98:Begin
             set varSQLState = SQLSTATE;
             set varErrorMessage= 'This message is displayed, ITEM_NUM invalid characters';
         end H98;

        -- NOW I AM DOING A SECOND INSERT TO table b WITH INVALID DATA
        -- THE VALUE FOR THE ITEM NUMBER CONTAINS ALPHANUMERICE CHARACTERS
        INSERT INTO ""Table_b""
         (ID , ITEM_NUM, DESC, CREATE_DTTM)
         SELECT 1, '333F', 'SOME DESC',CURRENT_TIMESTAMP;

    END ins6;

    EndTrans: BEGIN

        IF varSQLState &lt;&gt; '0' THEN
            SET varRollbackNeededInd = 'Y';
            SET varReturnCode = '9999';
        END IF;

        IF  varRollbackNeededInd = 'Y' THEN
            ROLLBACK;        -- ROLLBACK AND SEND ERROR TO CALLER
            SIGNAL SQLSTATE 'U0123' SET MESSAGE_TEXT = 'SQlState is - ' || varSQLSTATE || ' - and error is - ' || varErrorMessage;
        ELSE
            END TRANSACTION; -- COMMIT TRANSACTION

        END IF;
   END EndTrans;

END;


The problem that I am facing with the above stored procedure is that error message that I get is not the one that I am expecting. Since error is intentionally created in my second statement I am expecting to get: This message is displayed, ITEM_NUM invalid characters but I am getting This message should not be displayed

Now if I modify the PROCEDURE to have multiple handlers, one for each statement, I do get the correct error message, but now since I am intentionally generating the error in the first statement it does not terminate the procedure, it handles the error and sets the proper message but continues to process the next statement which I am not expecting to do this, so how can I terminate this procedure? 

REPLACE PROCEDURE csTest2()
SQL SECURITY OWNER
BEGIN

    DECLARE varErrorMessage char(256);
    DECLARE varSQLState char(5);
    DECLARE varReturnCode char(5);

    DECLARE varRollbackNeededInd char(1);           /* transaction mgt */
    SET varRollbackNeededInd = 'N';

    SET varReturnCode = '00000';
    SET varErrorMessage = '';

    BEGIN TRANSACTION;

    ins6: BEGIN
        DECLARE EXIT HANDLER FOR SQLEXCEPTION 

        H99:Begin
            set varSQLState = SQLSTATE;
            set varErrorMessage= 'Error is displayed in this case because ITEM_NUM';

        -- ERROR IS PRESENT IN THIS STATEMENT AND SHOULD TERMINATE THE PROCEDURE. 
        INSERT INTO ""Table_A""
         (ID , ITEM_NUM, DESC, CREATE_DTTM)
         SELECT 1, '222F', 'SOME DESC',CURRENT_TIMESTAMP;
    END ins6;

    ins7: BEGIN      
         H98:Begin
             set varSQLState = SQLSTATE;
             set varErrorMessage= 'no error is displayed in this case';
         end H98;

        -- NO ERROR IS EXPECTED, BUT IT SHOULD NOT REACH HERE SINCE WE HAD ERROR ON FIRST STATEMENT. 
        INSERT INTO ""Table_b""
         (ID , ITEM_NUM, DESC, CREATE_DTTM)
         SELECT 1, '333', 'SOME DESC',CURRENT_TIMESTAMP;

    END ins7;

    EndTrans: BEGIN

        IF varSQLState &lt;&gt; '0' THEN
            SET varRollbackNeededInd = 'Y';
            SET varReturnCode = '9999';
        END IF;

        IF  varRollbackNeededInd = 'Y' THEN
            ROLLBACK;        -- ROLLBACK AND SEND ERROR TO CALLER
            SIGNAL SQLSTATE 'U0123' SET MESSAGE_TEXT = 'SQlState is - ' || varSQLSTATE || ' - and error is - ' || varErrorMessage;
        ELSE
            END TRANSACTION; -- COMMIT TRANSACTION

        END IF;
   END EndTrans;

END;

",3
241,38741235,"Scalikejdbc teradata ""Connection pool is not yet initialized.""","I'm trying to use ScalikeJdbc with Teradata but can't seem to get it to work. I have a configuration file:

application.conf

# JDBC settings
db.default.user=""user""
db.default.password=""pass""
# Connection Pool settings
db.default.poolInitialSize=10
db.default.poolMaxSize=20
db.default.connectionTimeoutMillis=1000

# Teradata
db.default.driver=""com.teradata.jdbc.TeraDriver""
db.default.url=""jdbc:teradata://url/database=db""


The code looks like this:

import scalikejdbc._
import scalikejdbc.config._


object DBObject {
  DBs.setupAll()

  case class Ad(id: Long, siteId: Int)
  object Ad extends SQLSyntaxSupport[Ad] {
    override val tableName = ""ad_table""

    def apply(rs: WrappedResultSet) = new Ad(rs.long(""id""), rs.int(""ad""))
  }

  ConnectionPool.borrow(""default"")
  val ad = Ad.syntax(""ad"")
  val ads = DB(ConnectionPool.borrow()) readOnly { implicit session =&gt;
    withSQL {
      select.from(Ad as ad).where.eq(ad.siteId, 3001).limit(10)
    }.map(rs =&gt; Ad(rs)).list.apply
  }
}


While running this example it throws an exception: Connection pool is not yet initialized.
What am I missing here?
",-1,-1,-1.0,"I'm trying to use ScalikeJdbc with Teradata but can't seem to get it to work. I have a configuration file:

application.conf

# JDBC settings
db.default.user=""user""
db.default.password=""pass""
# Connection Pool settings
db.default.poolInitialSize=10
db.default.poolMaxSize=20
db.default.connectionTimeoutMillis=1000

# Teradata
db.default.driver=""com.teradata.jdbc.TeraDriver""
db.default.url=""jdbc:teradata://url/database=db""


The code looks like this:

import scalikejdbc._
import scalikejdbc.config._


object DBObject {
  DBs.setupAll()

  case class Ad(id: Long, siteId: Int)
  object Ad extends SQLSyntaxSupport[Ad] {
    override val tableName = ""ad_table""

    def apply(rs: WrappedResultSet) = new Ad(rs.long(""id""), rs.int(""ad""))
  }

  ConnectionPool.borrow(""default"")
  val ad = Ad.syntax(""ad"")
  val ads = DB(ConnectionPool.borrow()) readOnly { implicit session =&gt;
    withSQL {
      select.from(Ad as ad).where.eq(ad.siteId, 3001).limit(10)
    }.map(rs =&gt; Ad(rs)).list.apply
  }
}


While running this example it throws an exception: Connection pool is not yet initialized.
What am I missing here?
",1
242,38686813,How to pull Spanish letters from teradata?,"Here is the connection string 

jdbc:teradata://DSN/LOGMECH=LDAP,tmode=TERA,charset=UTF8,RECONNECT_COUNT=11


I specified  UTF8 so everything should  work fine .

Test query 

SELECT 'test á é í ó ú ñ Ñ Á É Í Ó Ú test'  AS   test  
FROM tstTable2  




Output the result in Java 

resultSet.next(); 
String out = resultSet.getString (1); 
System.out.println(out); 


get the following result 

test    test  


So the Spanish character do not get printed 
Why?  



By the way . When I simply print without querying the   Teradata 

System.out.println(""test á é í ó ú ñ Ñ Á É Í Ó Ú test"");  


all characters print  fine , so I know it is the getString method that doesn't work 
",-1,-1,-1.0,"Here is the connection string 

jdbc:teradata://DSN/LOGMECH=LDAP,tmode=TERA,charset=UTF8,RECONNECT_COUNT=11


I specified  UTF8 so everything should  work fine .

Test query 

SELECT 'test á é í ó ú ñ Ñ Á É Í Ó Ú test'  AS   test  
FROM tstTable2  




Output the result in Java 

resultSet.next(); 
String out = resultSet.getString (1); 
System.out.println(out); 


get the following result 

test    test  


So the Spanish character do not get printed 
Why?  



By the way . When I simply print without querying the   Teradata 

System.out.println(""test á é í ó ú ñ Ñ Á É Í Ó Ú test"");  


all characters print  fine , so I know it is the getString method that doesn't work 
",1
243,42633918,Multiple value Parameters in SSRS connecting to Teradata returns error,"I am connecting to Teradata using SSRS and have multiple parameters, many of them that seek multiple values. 

The challenge is all these parameters may not be always filled, sometimes it could be blank. So I am using a logic as below - 

Select * from Table
Where 
(Param1 in (?) or ? = '')
AND 
(Param2 in (?) or ? = '')
AND 
(Param3 = ? OR ? = '')


This gives an error stating Something expected like a Precedes or Exists or In or contains .........., 

Anyone experienced similar issue? Please help.
",-1,-1,-1.0,"I am connecting to Teradata using SSRS and have multiple parameters, many of them that seek multiple values. 

The challenge is all these parameters may not be always filled, sometimes it could be blank. So I am using a logic as below - 

Select * from Table
Where 
(Param1 in (?) or ? = '')
AND 
(Param2 in (?) or ? = '')
AND 
(Param3 = ? OR ? = '')


This gives an error stating Something expected like a Precedes or Exists or In or contains .........., 

Anyone experienced similar issue? Please help.
",3
244,42847005,Teradata Python: teradata.api.InterfaceError,"I have a simple select query from a python script to select data from Teradata DB shown below,

session.execute(""select distinct v_column from a_table where a_column in (?)"",(value))

Value in this case is a tuple with values derived from other bunch of queries.

This query works just fine if I pass in the one value from the variable. For anything more than 1 - I had to change my query to contain more place holders like this (?,?).

I wanted to make this dynamic or generic place holder that can work irrespective. I tried few options and does not seem to make sense to me. Any better suggestions ?

Error with single placeholder with more than one value:
teradata.api.InterfaceError: ('PARAMS_MISMATCH', 'The number of supplied parameters (2) does not match the expected number of parameters (1).')
",1,-1,-1.0,"I have a simple select query from a python script to select data from Teradata DB shown below,

session.execute(""select distinct v_column from a_table where a_column in (?)"",(value))

Value in this case is a tuple with values derived from other bunch of queries.

This query works just fine if I pass in the one value from the variable. For anything more than 1 - I had to change my query to contain more place holders like this (?,?).

I wanted to make this dynamic or generic place holder that can work irrespective. I tried few options and does not seem to make sense to me. Any better suggestions ?

Error with single placeholder with more than one value:
teradata.api.InterfaceError: ('PARAMS_MISMATCH', 'The number of supplied parameters (2) does not match the expected number of parameters (1).')
",3
245,42948454,RODBC Teradata Copy Table,"I am using RODBC with R to connect to Teradata.

I am trying to copy a large table EXAMPLE (25GB) from the READ_ONLY database to the WORKdatabase. The two databases are under the same DB system so I only need one connection.

I have tried sqlQuery, sqlCopy and sqlCopyTablefunctions but do not succeed.

sqlQuery

EDIT: syntax error corrected as suggested by @dnoeth.

CREATE TABLE WORK.EXAMPLE AS (SELECT * FROM READ_ONLY.EXAMPLE) WITH DATA;


OR

CREATE TABLE WORK.EXAMPLE AS (SELECT * FROM READ_ONLY.EXAMPLE) WITH NO DATA;
INSERT INTO WORK.EXAMPLE SELECT * FROM READ_ONLY.EXAMPLE;


I let the latter method run for 15h but it did not complete the copy.

sqlCopy

sqlCopy(ch, 
    query='SELECT * FROM READ_ONLY.EXAMPLE',
    destination = 'WORK.EXAMPLE')

Error: cannot allocate vector of size 155.0 Mb


Does sqlCopy try to first copy the data to R's memory before creating the new table? If so, how can I bypass this step and work exclusively on the Teradata server? Also, the error persists even if use the option fast=F.

In case R's memory was the issue, I tried creating a smaller table of 1000 rows:

sqlCopy(ch, 
    query='SELECT * FROM READ_ONLY.EXAMPLE SAMPLE 1000',
    destination = 'WORK.EXAMPLE')

Error in sqlSave(destchannel, dataset, destination, verbose = verbose,  : 
  [RODBC] Failed exec in Update
22018 0 [Teradata][ODBC Teradata Driver] Data is not a numeric-literal.
In addition: Warning message:
In odbcUpdate(channel, query, mydata, coldata[m, ], test = test,  :
  character data '2017-03-20 12:08:25' truncated to 15 bytes in column 'ExtractionTS'


With this command a table was actually created but it only includes the column names without any rows.

sqlCopyTable

sqlCopyTable(ch, 
         srctable = 'READ_ONLY.EXAMPLE',
         desttable = 'WORK.EXAMPLE')

Error in if (as.character(keys[[4L]]) == colnames[i]) create &lt;- paste(create,  : 
  argument is of length zero

",-1,-1,-1.0,"I am using RODBC with R to connect to Teradata.

I am trying to copy a large table EXAMPLE (25GB) from the READ_ONLY database to the WORKdatabase. The two databases are under the same DB system so I only need one connection.

I have tried sqlQuery, sqlCopy and sqlCopyTablefunctions but do not succeed.

sqlQuery

EDIT: syntax error corrected as suggested by @dnoeth.

CREATE TABLE WORK.EXAMPLE AS (SELECT * FROM READ_ONLY.EXAMPLE) WITH DATA;


OR

CREATE TABLE WORK.EXAMPLE AS (SELECT * FROM READ_ONLY.EXAMPLE) WITH NO DATA;
INSERT INTO WORK.EXAMPLE SELECT * FROM READ_ONLY.EXAMPLE;


I let the latter method run for 15h but it did not complete the copy.

sqlCopy

sqlCopy(ch, 
    query='SELECT * FROM READ_ONLY.EXAMPLE',
    destination = 'WORK.EXAMPLE')

Error: cannot allocate vector of size 155.0 Mb


Does sqlCopy try to first copy the data to R's memory before creating the new table? If so, how can I bypass this step and work exclusively on the Teradata server? Also, the error persists even if use the option fast=F.

In case R's memory was the issue, I tried creating a smaller table of 1000 rows:

sqlCopy(ch, 
    query='SELECT * FROM READ_ONLY.EXAMPLE SAMPLE 1000',
    destination = 'WORK.EXAMPLE')

Error in sqlSave(destchannel, dataset, destination, verbose = verbose,  : 
  [RODBC] Failed exec in Update
22018 0 [Teradata][ODBC Teradata Driver] Data is not a numeric-literal.
In addition: Warning message:
In odbcUpdate(channel, query, mydata, coldata[m, ], test = test,  :
  character data '2017-03-20 12:08:25' truncated to 15 bytes in column 'ExtractionTS'


With this command a table was actually created but it only includes the column names without any rows.

sqlCopyTable

sqlCopyTable(ch, 
         srctable = 'READ_ONLY.EXAMPLE',
         desttable = 'WORK.EXAMPLE')

Error in if (as.character(keys[[4L]]) == colnames[i]) create &lt;- paste(create,  : 
  argument is of length zero

",3
246,42994085,not able to connect to the teradata aws instance via jdbc,"I am not able to connect to Teradata aws instance through either JDBC drivers in Java nor Teradata studio. however i am able to connect through putty and able to query the database in bteq.

whenn i am connecting through java code, it says connection time out and when i am trying through Teradata studio it says, ping failed(login time out).

AWS has Teradata Database Developer -singlenode -15-10-01-06-1.
Teradata studio version:16.0.2.201703141245

Exception that I am getting on Teradatastudio:

java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 16.00.00.23] [Error 1000] [SQLState 08S01] Login failure for Packet receive Sat Mar 25 01:33:12 IST 2017 socket orig=52.26.246.236 local=0.0.0.0/0.0.0.0:43787 remote=/XX.XX.XXX.XXX:XX keepalive=unavailable nodelay=unavailable receive=unavailable send=unavailable linger=unavailable traffic=unavailable concurrent=3 contimeout=10000 conwait=1000 connecttime=329 connecttotaltime=332 connectattempts=1 connectfailures=0 reconnectattempts=0 recoverable=false redrive=false failurecache={ec2-35-163-156-117.us-west-2.compute.amazonaws.com/35.163.156.117:1025=Wed Mar 22 14:32:40 IST 2017, /52.26.246.236:1025=Fri Mar 24 12:25:13 IST 2017, /172.31.10.129:22=Fri Mar 24 12:24:19 IST 2017, ec2-50-112-137-170.us-west-2.compute.amazonaws.com/50.112.137.170:1025=Thu Mar 23 04:27:23 IST 2017, /34.208.105.91:1025=Fri Mar 24 08:38:17 IST 2017, /172.31.10.129:1025=Fri Mar 24 12:23:19 IST 2017, /34.208.210.30:1025=Thu Mar 23 04:26:13 IST 2017} cid=17a34262 sess=0 java.net.SocketException: Connection reset   at java.net.SocketInputStream.read(Unknown Source)   at java.net.SocketInputStream.read(Unknown Source)   at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:734)   at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:768)   at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:104)   at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:209)   at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:36)   at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)   at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:179)   at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:169)   at com.teradata.jdbc.TeraDriver.doConnect(TeraDriver.java:235)   at com.teradata.jdbc.TeraDriver.connect(TeraDriver.java:161)   at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.makeConnection(TeradataJDBCConnection.java:280)   at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.createConnection(TeradataJDBCConnection.java:122)   at org.eclipse.datatools.connectivity.DriverConnectionBase.internalCreateConnection(DriverConnectionBase.java:105)   at org.eclipse.datatools.connectivity.DriverConnectionBase.open(DriverConnectionBase.java:54)   at org.eclipse.datatools.connectivity.drivers.jdbc.JDBCConnection.open(JDBCConnection.java:96)   at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataPingFactory.createConnection(TeradataPingFactory.java:36)   at org.eclipse.datatools.connectivity.internal.ConnectionFactoryProvider.createConnection(ConnectionFactoryProvider.java:83)   at org.eclipse.datatools.connectivity.internal.ConnectionProfile.createConnection(ConnectionProfile.java:359)   at org.eclipse.datatools.connectivity.ui.PingJob.createTestConnection(PingJob.java:76)   at org.eclipse.datatools.connectivity.ui.PingJob.run(PingJob.java:59)   at org.eclipse.core.internal.jobs.Worker.run(Worker.java:55)  
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeIoJDBCException(ErrorFactory.java:208)
    at com.teradata.jdbc.jdbc_4.util.ErrorAnalyzer.analyzeIoError(ErrorAnalyzer.java:59)
    at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:931)
    at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:768)
    at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:104)
    at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:209)
    at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:36)
    at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)
    at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:179)
    at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:169)
    at com.teradata.jdbc.TeraDriver.doConnect(TeraDriver.java:235)
    at com.teradata.jdbc.TeraDriver.connect(TeraDriver.java:161)
    at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.makeConnection(TeradataJDBCConnection.java:280)
    at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.createConnection(TeradataJDBCConnection.java:122)
    at org.eclipse.datatools.connectivity.DriverConnectionBase.internalCreateConnection(DriverConnectionBase.java:105)
    at org.eclipse.datatools.connectivity.DriverConnectionBase.open(DriverConnectionBase.java:54)
    at org.eclipse.datatools.connectivity.drivers.jdbc.JDBCConnection.open(JDBCConnection.java:96)
    at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataPingFactory.createConnection(TeradataPingFactory.java:36)
    at org.eclipse.datatools.connectivity.internal.ConnectionFactoryProvider.createConnection(ConnectionFactoryProvider.java:83)
    at org.eclipse.datatools.connectivity.internal.ConnectionProfile.createConnection(ConnectionProfile.java:359)
    at org.eclipse.datatools.connectivity.ui.PingJob.createTestConnection(PingJob.java:76)
    at org.eclipse.datatools.connectivity.ui.PingJob.run(PingJob.java:59)
    at org.eclipse.core.internal.jobs.Worker.run(Worker.java:55)
Caused by: java.net.SocketException: Connection reset
    at java.net.SocketInputStream.read(Unknown Source)
    at java.net.SocketInputStream.read(Unknown Source)
    at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:734)
    ... 20 more


please let me know what could be the gap here and let me know if you need any further details.

Thanks for your help in advance.
",-1,-1,-1.0,"I am not able to connect to Teradata aws instance through either JDBC drivers in Java nor Teradata studio. however i am able to connect through putty and able to query the database in bteq.

whenn i am connecting through java code, it says connection time out and when i am trying through Teradata studio it says, ping failed(login time out).

AWS has Teradata Database Developer -singlenode -15-10-01-06-1.
Teradata studio version:16.0.2.201703141245

Exception that I am getting on Teradatastudio:

java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 16.00.00.23] [Error 1000] [SQLState 08S01] Login failure for Packet receive Sat Mar 25 01:33:12 IST 2017 socket orig=52.26.246.236 local=0.0.0.0/0.0.0.0:43787 remote=/XX.XX.XXX.XXX:XX keepalive=unavailable nodelay=unavailable receive=unavailable send=unavailable linger=unavailable traffic=unavailable concurrent=3 contimeout=10000 conwait=1000 connecttime=329 connecttotaltime=332 connectattempts=1 connectfailures=0 reconnectattempts=0 recoverable=false redrive=false failurecache={ec2-35-163-156-117.us-west-2.compute.amazonaws.com/35.163.156.117:1025=Wed Mar 22 14:32:40 IST 2017, /52.26.246.236:1025=Fri Mar 24 12:25:13 IST 2017, /172.31.10.129:22=Fri Mar 24 12:24:19 IST 2017, ec2-50-112-137-170.us-west-2.compute.amazonaws.com/50.112.137.170:1025=Thu Mar 23 04:27:23 IST 2017, /34.208.105.91:1025=Fri Mar 24 08:38:17 IST 2017, /172.31.10.129:1025=Fri Mar 24 12:23:19 IST 2017, /34.208.210.30:1025=Thu Mar 23 04:26:13 IST 2017} cid=17a34262 sess=0 java.net.SocketException: Connection reset   at java.net.SocketInputStream.read(Unknown Source)   at java.net.SocketInputStream.read(Unknown Source)   at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:734)   at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:768)   at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:104)   at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:209)   at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:36)   at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)   at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:179)   at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:169)   at com.teradata.jdbc.TeraDriver.doConnect(TeraDriver.java:235)   at com.teradata.jdbc.TeraDriver.connect(TeraDriver.java:161)   at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.makeConnection(TeradataJDBCConnection.java:280)   at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.createConnection(TeradataJDBCConnection.java:122)   at org.eclipse.datatools.connectivity.DriverConnectionBase.internalCreateConnection(DriverConnectionBase.java:105)   at org.eclipse.datatools.connectivity.DriverConnectionBase.open(DriverConnectionBase.java:54)   at org.eclipse.datatools.connectivity.drivers.jdbc.JDBCConnection.open(JDBCConnection.java:96)   at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataPingFactory.createConnection(TeradataPingFactory.java:36)   at org.eclipse.datatools.connectivity.internal.ConnectionFactoryProvider.createConnection(ConnectionFactoryProvider.java:83)   at org.eclipse.datatools.connectivity.internal.ConnectionProfile.createConnection(ConnectionProfile.java:359)   at org.eclipse.datatools.connectivity.ui.PingJob.createTestConnection(PingJob.java:76)   at org.eclipse.datatools.connectivity.ui.PingJob.run(PingJob.java:59)   at org.eclipse.core.internal.jobs.Worker.run(Worker.java:55)  
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeIoJDBCException(ErrorFactory.java:208)
    at com.teradata.jdbc.jdbc_4.util.ErrorAnalyzer.analyzeIoError(ErrorAnalyzer.java:59)
    at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:931)
    at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:768)
    at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:104)
    at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:209)
    at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:36)
    at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)
    at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:179)
    at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:169)
    at com.teradata.jdbc.TeraDriver.doConnect(TeraDriver.java:235)
    at com.teradata.jdbc.TeraDriver.connect(TeraDriver.java:161)
    at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.makeConnection(TeradataJDBCConnection.java:280)
    at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.createConnection(TeradataJDBCConnection.java:122)
    at org.eclipse.datatools.connectivity.DriverConnectionBase.internalCreateConnection(DriverConnectionBase.java:105)
    at org.eclipse.datatools.connectivity.DriverConnectionBase.open(DriverConnectionBase.java:54)
    at org.eclipse.datatools.connectivity.drivers.jdbc.JDBCConnection.open(JDBCConnection.java:96)
    at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataPingFactory.createConnection(TeradataPingFactory.java:36)
    at org.eclipse.datatools.connectivity.internal.ConnectionFactoryProvider.createConnection(ConnectionFactoryProvider.java:83)
    at org.eclipse.datatools.connectivity.internal.ConnectionProfile.createConnection(ConnectionProfile.java:359)
    at org.eclipse.datatools.connectivity.ui.PingJob.createTestConnection(PingJob.java:76)
    at org.eclipse.datatools.connectivity.ui.PingJob.run(PingJob.java:59)
    at org.eclipse.core.internal.jobs.Worker.run(Worker.java:55)
Caused by: java.net.SocketException: Connection reset
    at java.net.SocketInputStream.read(Unknown Source)
    at java.net.SocketInputStream.read(Unknown Source)
    at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:734)
    ... 20 more


please let me know what could be the gap here and let me know if you need any further details.

Thanks for your help in advance.
",0
247,43055783,ODBC Error Connecting Teradata,"I have installed Teradata ODBC driver on Debian via alien.

Dependencies checked via ldd and are fufiled..

But when attempting to connect to the DB I am getting following errors (with Debug)

  user@server:/#isql -v testdsn username password
  4647:     find library=libodbc.so.1 [0]; searching
  4647:      search cache=/etc/ld.so.cache
  4647:       trying file=/usr/lib/x86_64-linux-gnu/libodbc.so.1
  4647:
  *** output omitted ***
  4647:
  4647:
  4647:     initialize program: isql
  4647:
  4647:
  4647:     transferring control: isql
  4647:
  4647:     find library=libnss_compat.so.2 [0]; searching
  4647:      search cache=/etc/ld.so.cache
  4647:       trying file=/lib/x86_64-linux-gnu/libnss_compat.so.2
  4647:
  *** output omitted ***
  4647:     calling init: /usr/lib/x86_64-linux-gnu/gconv/ISO8859-1.so
  4647:
  4647:     /usr/lib/x86_64-linux-gnu/gconv/ISO8859-1.so: error: symbol lookup error: undefined symbol: gconv_end (fatal)
  4647:     find library=libstdc++.so.6 [0]; searching
  4647:      search cache=/etc/ld.so.cache
  4647:       trying file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6
  *** output omitted ***
  4647:     calling init: /lib/x86_64-linux-gnu/libodbcinst.so
  4647:
  4647:
  4647:     calling init: /opt/teradata/client/ODBC_64/lib/tdata.so
  4647:
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: tdata_LTX_SQLDriverLoad (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: SQLDriverLoad (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: tdata_LTX_SQLDriverUnload (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: SQLDriverUnload (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: tdata_LTX_SQLAllocConnect (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: SQLAllocConnect (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: tdata_LTX_SQLAllocEnv (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: SQLAllocEnv (fatal)
  *** output omitted ***


Any idea how to get rid of those errors? It seems to be a root cause of failing connections

Architecture X64

OS Debian 7

Driver version 16.00

same behaviour 

OS CentOS 7

Driver version 16.00

Driver version 15.10
",-1,-1,-1.0,"I have installed Teradata ODBC driver on Debian via alien.

Dependencies checked via ldd and are fufiled..

But when attempting to connect to the DB I am getting following errors (with Debug)

  user@server:/#isql -v testdsn username password
  4647:     find library=libodbc.so.1 [0]; searching
  4647:      search cache=/etc/ld.so.cache
  4647:       trying file=/usr/lib/x86_64-linux-gnu/libodbc.so.1
  4647:
  *** output omitted ***
  4647:
  4647:
  4647:     initialize program: isql
  4647:
  4647:
  4647:     transferring control: isql
  4647:
  4647:     find library=libnss_compat.so.2 [0]; searching
  4647:      search cache=/etc/ld.so.cache
  4647:       trying file=/lib/x86_64-linux-gnu/libnss_compat.so.2
  4647:
  *** output omitted ***
  4647:     calling init: /usr/lib/x86_64-linux-gnu/gconv/ISO8859-1.so
  4647:
  4647:     /usr/lib/x86_64-linux-gnu/gconv/ISO8859-1.so: error: symbol lookup error: undefined symbol: gconv_end (fatal)
  4647:     find library=libstdc++.so.6 [0]; searching
  4647:      search cache=/etc/ld.so.cache
  4647:       trying file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6
  *** output omitted ***
  4647:     calling init: /lib/x86_64-linux-gnu/libodbcinst.so
  4647:
  4647:
  4647:     calling init: /opt/teradata/client/ODBC_64/lib/tdata.so
  4647:
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: tdata_LTX_SQLDriverLoad (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: SQLDriverLoad (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: tdata_LTX_SQLDriverUnload (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: SQLDriverUnload (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: tdata_LTX_SQLAllocConnect (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: SQLAllocConnect (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: tdata_LTX_SQLAllocEnv (fatal)
  4647:     /opt/teradata/client/ODBC_64/lib/tdata.so: error: symbol lookup error: undefined symbol: SQLAllocEnv (fatal)
  *** output omitted ***


Any idea how to get rid of those errors? It seems to be a root cause of failing connections

Architecture X64

OS Debian 7

Driver version 16.00

same behaviour 

OS CentOS 7

Driver version 16.00

Driver version 15.10
",1
248,43251446,Teradata adding Â in front of special characters,"I am having an issue where when fast loading data into Teradata Table is causing Â to appear in front of special characters when loading from .csv file.

CSV File

1|Hello, £5.00 has been debited from your account, thank you for your payment.|XXXX|XX|XXXX-XXX-XXX

Teradata Table

1|Hello, Â£5.00 has been debited from your account, thank you for your payment.|XXXX|XX|XXXX-XXX-XXX

Table Definition

CREATE MULTISET TABLE DATABASE1.TABLE1 ,NO FALLBACK ,
 NO BEFORE JOURNAL,
 NO AFTER JOURNAL,
 CHECKSUM = DEFAULT,
 DEFAULT MERGEBLOCKRATIO
 (
  FIELD1 VARCHAR(25) CHARACTER SET LATIN NOT CASESPECIFIC,
  FIELD2 VARCHAR(750) CHARACTER SET LATIN NOT CASESPECIFIC,
  FIELD3 VARCHAR(35) CHARACTER SET LATIN NOT CASESPECIFIC,
  FIELD4 VARCHAR(35) CHARACTER SET LATIN NOT CASESPECIFIC,
  FIELD5 VARCHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC
 ) PRIMARY INDEX ( FIELD1 );


Fastload Code

fastload &lt;&lt;-EOF
    .LOGON username/pass;
    DATABASE DATABASE1;

        SET RECORD VARTEXT ""|"";

        BEGIN LOADING TABLE1
            ERRORFILES TABLE1_ERR1,
                       TABLE1_ERR2;

        DEFINE
            FIELD1      (VARCHAR(25))
            FIELD2      (VARCHAR(750))
            FIELD3      (VARCHAR(35))
            FIELD4      (VARCHAR(35))
            FIELD5      (VARCHAR(50))


        FILE=${LOAD_FILE};

        SHOW;

        INSERT INTO DATABASE1.TABLE1
        (
            FIELD1 ,
            FIELD2 ,
            FIELD3 ,
            FIELD4 ,
            FIELD5 

        )
        VALUES
        (
            :FIELD1 ,
            :FIELD2 ,
            :FIELD3 ,
            :FIELD4 ,
            :FIELD5 
        );

    .END LOADING;
    .LOGOFF;
    .QUIT;
EOF


Does anyone know how to solve this, I am running this from Solaris 10 Fastload Utility v12.00.00.011
",-1,-1,-1.0,"I am having an issue where when fast loading data into Teradata Table is causing Â to appear in front of special characters when loading from .csv file.

CSV File

1|Hello, £5.00 has been debited from your account, thank you for your payment.|XXXX|XX|XXXX-XXX-XXX

Teradata Table

1|Hello, Â£5.00 has been debited from your account, thank you for your payment.|XXXX|XX|XXXX-XXX-XXX

Table Definition

CREATE MULTISET TABLE DATABASE1.TABLE1 ,NO FALLBACK ,
 NO BEFORE JOURNAL,
 NO AFTER JOURNAL,
 CHECKSUM = DEFAULT,
 DEFAULT MERGEBLOCKRATIO
 (
  FIELD1 VARCHAR(25) CHARACTER SET LATIN NOT CASESPECIFIC,
  FIELD2 VARCHAR(750) CHARACTER SET LATIN NOT CASESPECIFIC,
  FIELD3 VARCHAR(35) CHARACTER SET LATIN NOT CASESPECIFIC,
  FIELD4 VARCHAR(35) CHARACTER SET LATIN NOT CASESPECIFIC,
  FIELD5 VARCHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC
 ) PRIMARY INDEX ( FIELD1 );


Fastload Code

fastload &lt;&lt;-EOF
    .LOGON username/pass;
    DATABASE DATABASE1;

        SET RECORD VARTEXT ""|"";

        BEGIN LOADING TABLE1
            ERRORFILES TABLE1_ERR1,
                       TABLE1_ERR2;

        DEFINE
            FIELD1      (VARCHAR(25))
            FIELD2      (VARCHAR(750))
            FIELD3      (VARCHAR(35))
            FIELD4      (VARCHAR(35))
            FIELD5      (VARCHAR(50))


        FILE=${LOAD_FILE};

        SHOW;

        INSERT INTO DATABASE1.TABLE1
        (
            FIELD1 ,
            FIELD2 ,
            FIELD3 ,
            FIELD4 ,
            FIELD5 

        )
        VALUES
        (
            :FIELD1 ,
            :FIELD2 ,
            :FIELD3 ,
            :FIELD4 ,
            :FIELD5 
        );

    .END LOADING;
    .LOGOFF;
    .QUIT;
EOF


Does anyone know how to solve this, I am running this from Solaris 10 Fastload Utility v12.00.00.011
",3
249,43254110,sqoop import error from teradata to hive,"I am using below given sqoop command:

sqoop import 
    --libjars /usr/hdp/2.4.0.0-169/sqoop/lib,/usr/hdp/2.4.0.0-169/hive/lib 
    --connect jdbc:teradata://x/DATABASE=x 
    --connection-manager org.apache.sqoop.teradata.TeradataConnManager 
    --username ec 
    --password dc 
    --query ""select *  from hb where yr_nbr=2017""
    --hive-table schema.table 
    --num-mappers 1 
    --hive-import  
    --target-dir /user/hive/warehouse/GG


I'm getting this error:      

org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
    17/04/06 11:15:41 INFO mapreduce.Job:  map 100% reduce 0%
    17/04/06 11:15:41 INFO mapreduce.Job: Task Id : attempt_1491466460468_0029_m_000000_1, Status : FAILED
    Error: org.apache.hadoop.fs.FileAlreadyExistsException: /user/root/temp_111508/part-m-00000 for client 192.168.211.133 already exists

",-1,-1,-1.0,"I am using below given sqoop command:

sqoop import 
    --libjars /usr/hdp/2.4.0.0-169/sqoop/lib,/usr/hdp/2.4.0.0-169/hive/lib 
    --connect jdbc:teradata://x/DATABASE=x 
    --connection-manager org.apache.sqoop.teradata.TeradataConnManager 
    --username ec 
    --password dc 
    --query ""select *  from hb where yr_nbr=2017""
    --hive-table schema.table 
    --num-mappers 1 
    --hive-import  
    --target-dir /user/hive/warehouse/GG


I'm getting this error:      

org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
    17/04/06 11:15:41 INFO mapreduce.Job:  map 100% reduce 0%
    17/04/06 11:15:41 INFO mapreduce.Job: Task Id : attempt_1491466460468_0029_m_000000_1, Status : FAILED
    Error: org.apache.hadoop.fs.FileAlreadyExistsException: /user/root/temp_111508/part-m-00000 for client 192.168.211.133 already exists

",0
250,43283413,Bulkload option exporting data from R to Teradata using RODBC,"I have done many researches on how to upload a huge data in .txt through R to Teradata DB. I tried to use RODBC's sqlSave() but it did not work. I also followed some other similar questions posted such as:
Write from R to Teradata in 3.0 OR Export data frame to SQL server using RODBC package OR How to quickly export data from R to SQL Server.

However, since Teradata somehow is structured differently than MS SQL server, most of those options suggested are not applicable to my situation. 
I know that there is a TeradataR package available but it has not been updated since like 2-3 years ago. 

So here are my 2 main problems I am facing:
1. How to bulk load (all records at once) data in .txt format to Teradata using R if there is any way. (So far I only tried using SAS to do so, but I need to explore this in R) 
2. The data is big like 500+ MB so I cannot load it through R, I am sure there is a way to go around this but directly pull data from server. 

Here is what I tried according to one of posts but this was for MS SQL server:

toSQL = data.frame(...) #this doesn't work for me cause its too big.
write.table(toSQL,""C:\\export\\filename.txt"",quote=FALSE,sep="","",row.names=FALSE,col.names=FALSE,append=FALSE);

    sqlQuery(channel,""BULK
                INSERT Yada.dbo.yada
                FROM '\\\\&lt;server-that-SQL-server-can-see&gt;\\export\\filename.txt'
                WITH
                (
                FIELDTERMINATOR = ',',
                ROWTERMINATOR = '\\n'
                )"");


*Note: there is an option in Teradata to insert/import data but that is the same as writing millions of rows of Insert statements. 

Sorry that I do not have sample codes at this point since the package that I found wasn't the right one that I should use. 

Anyone has similar issues/problems like this? 

Thank you so much for your help in advance! 
",1,-1,-1.0,"I have done many researches on how to upload a huge data in .txt through R to Teradata DB. I tried to use RODBC's sqlSave() but it did not work. I also followed some other similar questions posted such as:
Write from R to Teradata in 3.0 OR Export data frame to SQL server using RODBC package OR How to quickly export data from R to SQL Server.

However, since Teradata somehow is structured differently than MS SQL server, most of those options suggested are not applicable to my situation. 
I know that there is a TeradataR package available but it has not been updated since like 2-3 years ago. 

So here are my 2 main problems I am facing:
1. How to bulk load (all records at once) data in .txt format to Teradata using R if there is any way. (So far I only tried using SAS to do so, but I need to explore this in R) 
2. The data is big like 500+ MB so I cannot load it through R, I am sure there is a way to go around this but directly pull data from server. 

Here is what I tried according to one of posts but this was for MS SQL server:

toSQL = data.frame(...) #this doesn't work for me cause its too big.
write.table(toSQL,""C:\\export\\filename.txt"",quote=FALSE,sep="","",row.names=FALSE,col.names=FALSE,append=FALSE);

    sqlQuery(channel,""BULK
                INSERT Yada.dbo.yada
                FROM '\\\\&lt;server-that-SQL-server-can-see&gt;\\export\\filename.txt'
                WITH
                (
                FIELDTERMINATOR = ',',
                ROWTERMINATOR = '\\n'
                )"");


*Note: there is an option in Teradata to insert/import data but that is the same as writing millions of rows of Insert statements. 

Sorry that I do not have sample codes at this point since the package that I found wasn't the right one that I should use. 

Anyone has similar issues/problems like this? 

Thank you so much for your help in advance! 
",3
251,43721311,Converting date set in 'DDMMYYYY'd format in Teradata,"I have an existing SAS program that reads in various macro date variables, and I am using these variables as one of my parameters in running queries in Teradata. 

One of the macro variables is formatted as DDMMYYYY, and I want to convert this to a format that can be read in Teradata; I tried the following query but it doesn't seem to work, can someone point out what's wrong?

ME_DT (Teradata date variable) = CAST(CAST(&amp;QEOP AS DATE FORMAT 'DDMMYYYY'd) AS DATE FORMAT 'YYYY-MM-DD')


where &amp;QEOP is a macro date variable.
",-1,1,-1.0,"I have an existing SAS program that reads in various macro date variables, and I am using these variables as one of my parameters in running queries in Teradata. 

One of the macro variables is formatted as DDMMYYYY, and I want to convert this to a format that can be read in Teradata; I tried the following query but it doesn't seem to work, can someone point out what's wrong?

ME_DT (Teradata date variable) = CAST(CAST(&amp;QEOP AS DATE FORMAT 'DDMMYYYY'd) AS DATE FORMAT 'YYYY-MM-DD')


where &amp;QEOP is a macro date variable.
",3
252,43732983,javax.naming.NameNotFoundException: While trying to lookup 'jdbc.teradata' didn't find subcontext 'jdbc'. Resolved ''; remaining name 'jdbc/teradata',"However I could open up the required remote port for Teradata, I am getting this error, is it because the required jdbc driver library is missing in the Weblogic in which I am deploying my Application.

Please advise, you can find complete error below. Because of this failure consecutive Spring Autowired Components are also failing.

Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'teradataDataSource' defined in ServletContext resource [/WEB-INF/async-db-config.xml]: Invocation of init method failed; nested exception is javax.naming.NameNotFoundException: While trying to lookup 'jdbc.teradata' didn't find subcontext 'jdbc'. Resolved ''; remaining name 'jdbc/teradata'
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:334)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1210)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)
    Truncated. see log file for complete stacktrace

",-1,-1,-1.0,"However I could open up the required remote port for Teradata, I am getting this error, is it because the required jdbc driver library is missing in the Weblogic in which I am deploying my Application.

Please advise, you can find complete error below. Because of this failure consecutive Spring Autowired Components are also failing.

Related cause: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'teradataDataSource' defined in ServletContext resource [/WEB-INF/async-db-config.xml]: Invocation of init method failed; nested exception is javax.naming.NameNotFoundException: While trying to lookup 'jdbc.teradata' didn't find subcontext 'jdbc'. Resolved ''; remaining name 'jdbc/teradata'
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:334)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1210)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)
    Truncated. see log file for complete stacktrace

",0
253,43937598,Import date from CSV File to teradata table,"I need to import data containing 2 dates from a CSV file to a teradata volatile table using BTEQ.
I use the following script :

CREATE MULTISET VOLATILE TABLE tab1,
NO FALLBACK ,      
NO BEFORE JOURNAL,      
NO AFTER JOURNAL,      
CHECKSUM = DEFAULT,      
DEFAULT MERGEBLOCKRATIO  (  
ent DECIMAL(3,0) NOT NULL, 
eng DECIMAL(7,0), 
dat_deb DATE, 
dat_fin DATE,  
ad_id DECIMAL(8,0),
id_lig DECIMAL(6,0),
Cd_a DECIMAL(7,0),
va_n DECIMAL(3,0),
satu DECIMAL(3,0)
)  
PRIMARY INDEX (ent, eng,ad_id,id_lig) 
ON COMMIT PRESERVE ROWS ;


then i import to the created table the following data :

.IMPORT VARTEXT ';' FILE=""C:\Desktop\fichiertest.csv""  , SKIP=1; 
.REPEAT *

.SET QUIET ON;

USING(
ent DECIMAL(3,0) NOT NULL, 
eng DECIMAL(7,0), 
dat_deb DATE, 
dat_fin DATE,  
ad_id DECIMAL(8,0),
id_lig DECIMAL(6,0),
cd_a DECIMAL(7,0),
va_n DECIMAL(3,0),
satu DECIMAL(3,0)
)

INSERT INTO tab1(
ent, 
eng, 
dat_deb , 
dat_fin ,  
ad_id, 
id_lig,
cd_a,
va_n,
satu 
) 
      VALUES (  
:ent, 
:eng, 
CAST(:dat_deb AS DATE), 
CAST(:dat_fin AS DATE),  
:ad_id, 
:id_lig,
:cd_a,
:va_n,
:satu  
      ); 


but I have a problem with the dates:  *** Failure 2665 Invalid date.

could you help me solve this error
",1,-1,-1.0,"I need to import data containing 2 dates from a CSV file to a teradata volatile table using BTEQ.
I use the following script :

CREATE MULTISET VOLATILE TABLE tab1,
NO FALLBACK ,      
NO BEFORE JOURNAL,      
NO AFTER JOURNAL,      
CHECKSUM = DEFAULT,      
DEFAULT MERGEBLOCKRATIO  (  
ent DECIMAL(3,0) NOT NULL, 
eng DECIMAL(7,0), 
dat_deb DATE, 
dat_fin DATE,  
ad_id DECIMAL(8,0),
id_lig DECIMAL(6,0),
Cd_a DECIMAL(7,0),
va_n DECIMAL(3,0),
satu DECIMAL(3,0)
)  
PRIMARY INDEX (ent, eng,ad_id,id_lig) 
ON COMMIT PRESERVE ROWS ;


then i import to the created table the following data :

.IMPORT VARTEXT ';' FILE=""C:\Desktop\fichiertest.csv""  , SKIP=1; 
.REPEAT *

.SET QUIET ON;

USING(
ent DECIMAL(3,0) NOT NULL, 
eng DECIMAL(7,0), 
dat_deb DATE, 
dat_fin DATE,  
ad_id DECIMAL(8,0),
id_lig DECIMAL(6,0),
cd_a DECIMAL(7,0),
va_n DECIMAL(3,0),
satu DECIMAL(3,0)
)

INSERT INTO tab1(
ent, 
eng, 
dat_deb , 
dat_fin ,  
ad_id, 
id_lig,
cd_a,
va_n,
satu 
) 
      VALUES (  
:ent, 
:eng, 
CAST(:dat_deb AS DATE), 
CAST(:dat_fin AS DATE),  
:ad_id, 
:id_lig,
:cd_a,
:va_n,
:satu  
      ); 


but I have a problem with the dates:  *** Failure 2665 Invalid date.

could you help me solve this error
",3
254,43967596,How to run teradata scripts in AWS EC2 Teradata Database developer,"I have launched Teradata Database Developer -Single Node--15 from Amazon AWS.
I have successfully logged into the instance from putty.
AWS instance is 12.xlarge

I am not able to write teradata scripts in the console. I am not sure how to create database or like what should be the next step after this? Right now I screen like this

ip-144-41-55-72:~ #
",-1,-1,-1.0,"I have launched Teradata Database Developer -Single Node--15 from Amazon AWS.
I have successfully logged into the instance from putty.
AWS instance is 12.xlarge

I am not able to write teradata scripts in the console. I am not sure how to create database or like what should be the next step after this? Right now I screen like this

ip-144-41-55-72:~ #
",1
255,44388727,teradata covalent td-menu-header,"I had been looking thru all the documentation available at https://teradata.github.io/covalent/ but I didn't found any documentation about the ""td-menu-header"" found in the Teradata Covalent Quickstart.

This css framework seems interesting but the lack of documentation -it would be great a ""API"" documentation or developper guide- is putting us in a difficult choice.
",-1,-1,-1.0,"I had been looking thru all the documentation available at https://teradata.github.io/covalent/ but I didn't found any documentation about the ""td-menu-header"" found in the Teradata Covalent Quickstart.

This css framework seems interesting but the lack of documentation -it would be great a ""API"" documentation or developper guide- is putting us in a difficult choice.
",2
256,44457020,"Cant connect to teradata sql server through python, works fine through teradata sql assistant","I have a problem connecting to a Teradata SQL Server through Python, even though it works fine through Teradata SQL Assistant

Options when connecting in Teradata SQL Assistant are:
Server: xx1,
Data Source Name: xx2,
Mechanism: LDAP,
User Name: *,
Password: *,
Session Character Set: ASCII,
Port: 1025

Im Trying to connect through Python using Pyodbc with the following connection string:
pyodbc.connect('Driver={Teradata}; DBCName=xx1; Uid=; Pwd=; Authentication=LDAP; Database=xx2')

I'm getting the error message: ""WSA E ConnRefused: The Teradata server is not accepting connections (10061) (SQLDriverConnect)"". This is even though I have no problems getting a connection using Teradata SQL Assistant.

Any ideas why this is happening?

Thanks
",-1,-1,-1.0,"I have a problem connecting to a Teradata SQL Server through Python, even though it works fine through Teradata SQL Assistant

Options when connecting in Teradata SQL Assistant are:
Server: xx1,
Data Source Name: xx2,
Mechanism: LDAP,
User Name: *,
Password: *,
Session Character Set: ASCII,
Port: 1025

Im Trying to connect through Python using Pyodbc with the following connection string:
pyodbc.connect('Driver={Teradata}; DBCName=xx1; Uid=; Pwd=; Authentication=LDAP; Database=xx2')

I'm getting the error message: ""WSA E ConnRefused: The Teradata server is not accepting connections (10061) (SQLDriverConnect)"". This is even though I have no problems getting a connection using Teradata SQL Assistant.

Any ideas why this is happening?

Thanks
",1
257,44522176,Failed to insert the data into Teradata using FastLoadUtil using java,"I'm trying to insert the data into teradata using preparedstatement of  java application using Teradata FastloadUtil. But I'm facing  the below exception :

Rolling back because of error: [Teradata JDBC Driver] [TeraJDBC 14.00.00.37] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""mydb.tablename"". Details of the failure can be found in the exception chain that is accessible with getNextException.


But, I'm able to insert the data into destined table of teradata using normal TeradataUtility but not fastload.

I have seen the similar exception in this link but with no clues in that site too.
",-1,-1,-1.0,"I'm trying to insert the data into teradata using preparedstatement of  java application using Teradata FastloadUtil. But I'm facing  the below exception :

Rolling back because of error: [Teradata JDBC Driver] [TeraJDBC 14.00.00.37] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""mydb.tablename"". Details of the failure can be found in the exception chain that is accessible with getNextException.


But, I'm able to insert the data into destined table of teradata using normal TeradataUtility but not fastload.

I have seen the similar exception in this link but with no clues in that site too.
",0
258,44560242,How to use preparedStatement with TeradataFastLoad Utility?,"Actually, I'm trying to insert the data into Teradata dynamically using the PreparedStatement using the following query:

INSERT INTO TABLE(id,name,date) VALUES(?,?,?)


By, using the above query, it is failing to load the data into Teradata.
Here, in the above query, I'm trying to give all parameters dynamically without any manipulation.
Suppose,if I try to manipulate any of the parameter,I'm able to load the data.

INSERT INTO TABLE(id,name,date) VALUES(?,?,cast(? as timestamp(0))


I have no idea, regarding why it is happenning like this?

Can anyone please help me out regarding this ...
",1,-1,-1.0,"Actually, I'm trying to insert the data into Teradata dynamically using the PreparedStatement using the following query:

INSERT INTO TABLE(id,name,date) VALUES(?,?,?)


By, using the above query, it is failing to load the data into Teradata.
Here, in the above query, I'm trying to give all parameters dynamically without any manipulation.
Suppose,if I try to manipulate any of the parameter,I'm able to load the data.

INSERT INTO TABLE(id,name,date) VALUES(?,?,cast(? as timestamp(0))


I have no idea, regarding why it is happenning like this?

Can anyone please help me out regarding this ...
",3
259,44760516,Case Specific Data in Teradata,"I want to retrieve data which is having casespecific columns.
For example :

sel
col1(CASESPECIFIC),
col2 (CASESPECIFIC),
col3 (CASESPECIFIC),
col4 (CASESPECIFIC),
count(*) as cntt
from DB.tablename  
group by 1,2,3,4


Here, I have selected all the columns that are case specific.

So the results will include it all. But I was wondering if there is any other way to retrieve case specific data when there are millions of records. The above query will not be efficient in the case of millions of records.

PS: I'm executing this query in Teradata 15.10.1.4
",-1,-1,-1.0,"I want to retrieve data which is having casespecific columns.
For example :

sel
col1(CASESPECIFIC),
col2 (CASESPECIFIC),
col3 (CASESPECIFIC),
col4 (CASESPECIFIC),
count(*) as cntt
from DB.tablename  
group by 1,2,3,4


Here, I have selected all the columns that are case specific.

So the results will include it all. But I was wondering if there is any other way to retrieve case specific data when there are millions of records. The above query will not be efficient in the case of millions of records.

PS: I'm executing this query in Teradata 15.10.1.4
",3
260,44961090,Drop/ Delete Database in Teradata,"Please help with direction on how to drop a database in Teradata.
When I run the command DROP DATABASE database_name, I get the error message:

*** Failure 3552 Cannot DROP databases with tables, journal tables, 
views, macros, or zones.
            Statement# 1, Info =0
*** Total elapsed time was 1 second.

",-1,-1,-1.0,"Please help with direction on how to drop a database in Teradata.
When I run the command DROP DATABASE database_name, I get the error message:

*** Failure 3552 Cannot DROP databases with tables, journal tables, 
views, macros, or zones.
            Statement# 1, Info =0
*** Total elapsed time was 1 second.

",3
261,45042923,Why does Teradata SQL Assistant stop color-coding text?,"I am using a very long .sql file (5,000 lines) in Teradata SQL Assistant. When I initially copy-pasted the text in, I got the usual font-colors (SELECT, FROM, etc show up in blue font, text strings in a pink/purple color, etc.)

However, when I saved and then re-opened this file directly, the font was all black. No colors at all. If I copy-paste the code into a new SQL Assistant query window, the color shows up again. But if I save that new query and then re-open it, I get just a solid black font.

Can anyone help me figure out what is going on here? It's not an absolute deal-breaker (I can still run the code), but it's definitely annoying to debug a wall of uniform, black font.

I can't post the code because (1) it's for work and (2) there's a ton of it. It's a long series of INSERT statements into a diagnostic table with the results of running SELECT on a trio of other tables for the purposes of looking for bad data. It does include some long SQL-code snippets as text (within quotes).
",1,-1,-1.0,"I am using a very long .sql file (5,000 lines) in Teradata SQL Assistant. When I initially copy-pasted the text in, I got the usual font-colors (SELECT, FROM, etc show up in blue font, text strings in a pink/purple color, etc.)

However, when I saved and then re-opened this file directly, the font was all black. No colors at all. If I copy-paste the code into a new SQL Assistant query window, the color shows up again. But if I save that new query and then re-open it, I get just a solid black font.

Can anyone help me figure out what is going on here? It's not an absolute deal-breaker (I can still run the code), but it's definitely annoying to debug a wall of uniform, black font.

I can't post the code because (1) it's for work and (2) there's a ton of it. It's a long series of INSERT statements into a diagnostic table with the results of running SELECT on a trio of other tables for the purposes of looking for bad data. It does include some long SQL-code snippets as text (within quotes).
",3
262,44728298,Teradata odbc driver for Mac OS sierra,"I am trying to connect to Teradata on my Tableau Desktop 9.2.7.

I am on Mac OS Sierra. It shows the following error when I try to connect to Teradata database

Detailed Error Message:

[iODBC][Driver Manager]dlopen(/Library/Application Support/teradata/client/ODBC/lib/tdata.dylib, 6): Library not loaded: libtdparse.dylib Referenced from: /Library/Application Support/teradata/client/ODBC/lib/tdata.dylib Reason: image not found [iODBC][Driver Manager]Specified driver could not be loaded Unable to connect to the server ""wm1"". Check that the server is running and that you have access privileges to the requested database.


I have already installed the required ODBC drivers from the Teradata website. I even tried installing the previous versions of the driver. I'm not able to get past this error message. Can someone guide me towards a fix?
",-1,-1,-1.0,"I am trying to connect to Teradata on my Tableau Desktop 9.2.7.

I am on Mac OS Sierra. It shows the following error when I try to connect to Teradata database

Detailed Error Message:

[iODBC][Driver Manager]dlopen(/Library/Application Support/teradata/client/ODBC/lib/tdata.dylib, 6): Library not loaded: libtdparse.dylib Referenced from: /Library/Application Support/teradata/client/ODBC/lib/tdata.dylib Reason: image not found [iODBC][Driver Manager]Specified driver could not be loaded Unable to connect to the server ""wm1"". Check that the server is running and that you have access privileges to the requested database.


I have already installed the required ODBC drivers from the Teradata website. I even tried installing the previous versions of the driver. I'm not able to get past this error message. Can someone guide me towards a fix?
",1
263,44606892,Connecting to Teradata using Spark JDBC,"I am trying to connect to extract data from Teradata using Spark JDBC. I have created a ""lib"" directory on the main parent directory and placed the external Teradata jars and ran the sbt package. In addition,I am also providing the ""--jars"" option on my spark-shell command to provide the jar. However, when I run the spark-shell, it does not seem to find the class

Exception in thread ""main"" java.lang.ClassNotFoundException: com.teradata.hadoop.tool.TeradataImportTool


However, when I do ""jar tvf"" on the jar file, I see the class. Somehow the Spark utility is unable to find the jar. Is there anything else I need to do so Spark could find it? Please help
",1,-1,-1.0,"I am trying to connect to extract data from Teradata using Spark JDBC. I have created a ""lib"" directory on the main parent directory and placed the external Teradata jars and ran the sbt package. In addition,I am also providing the ""--jars"" option on my spark-shell command to provide the jar. However, when I run the spark-shell, it does not seem to find the class

Exception in thread ""main"" java.lang.ClassNotFoundException: com.teradata.hadoop.tool.TeradataImportTool


However, when I do ""jar tvf"" on the jar file, I see the class. Somehow the Spark utility is unable to find the jar. Is there anything else I need to do so Spark could find it? Please help
",0
264,44587071,SQOOP issue with teradata connector powered by teradata and Avro,"I'm trying to use sqoop to import a table from Teradata to HDFS as a avrodatafile, but am having issues.

Everything is working ok when importing as textfile. However, when I add --as-avrodatafile to the end of my sqoop command, I get a NPE, i.e:

ERROR sqoop.Sqoop: Got exception running Sqoop     java.lang.NullPointerException
java.lang.NullPointerException
at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:763)
at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786)
at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289)
at org.apache.sqoop.manager.SqlManager.getColumnInfo(SqlManager.java:275)
    at org.apache.sqoop.manager.ConnManager.getColumnInfo(ConnManager.java:393)
    at org.apache.sqoop.orm.ClassWriter.getColumnInfo(ClassWriter.java:1854)
    at org.apache.sqoop.orm.AvroSchemaGenerator.generate(AvroSchemaGenerator.java:71)
    at org.apache.sqoop.orm.AvroSchemaGenerator.generate(AvroSchemaGenerator.java:65)
    at com.cloudera.connector.teradata.imports.BaseImportJob.configureInputFormat(BaseImportJob.java:165)
    at com.cloudera.connector.teradata.imports.TableImportJob.configureInputFormat(TableImportJob.java:32)
    at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:258)
    at com.cloudera.connector.teradata.TeradataManager.importTable(TeradataManager.java:273)
at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:507)
at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)
at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
at org.apache.sqoop.Sqoop.main(Sqoop.java:236)""


Not sure what is going wrong
",-1,-1,-1.0,"I'm trying to use sqoop to import a table from Teradata to HDFS as a avrodatafile, but am having issues.

Everything is working ok when importing as textfile. However, when I add --as-avrodatafile to the end of my sqoop command, I get a NPE, i.e:

ERROR sqoop.Sqoop: Got exception running Sqoop     java.lang.NullPointerException
java.lang.NullPointerException
at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:763)
at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786)
at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289)
at org.apache.sqoop.manager.SqlManager.getColumnInfo(SqlManager.java:275)
    at org.apache.sqoop.manager.ConnManager.getColumnInfo(ConnManager.java:393)
    at org.apache.sqoop.orm.ClassWriter.getColumnInfo(ClassWriter.java:1854)
    at org.apache.sqoop.orm.AvroSchemaGenerator.generate(AvroSchemaGenerator.java:71)
    at org.apache.sqoop.orm.AvroSchemaGenerator.generate(AvroSchemaGenerator.java:65)
    at com.cloudera.connector.teradata.imports.BaseImportJob.configureInputFormat(BaseImportJob.java:165)
    at com.cloudera.connector.teradata.imports.TableImportJob.configureInputFormat(TableImportJob.java:32)
    at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:258)
    at com.cloudera.connector.teradata.TeradataManager.importTable(TeradataManager.java:273)
at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:507)
at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)
at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
at org.apache.sqoop.Sqoop.main(Sqoop.java:236)""


Not sure what is going wrong
",0
265,44063944,Teradata C++ UDF,"I tried to create a function using object file and by using the following set of queries 

REPLACE FUNCTION db_1.append(inputString VARCHAR(512))
RETURNS VARCHAR(512)
LANGUAGE CPP
NO SQL
EXTERNAL NAME 'CO!tokenise!/root/Desktop/tokenise.o!F!append'
PARAMETER STYLE TD_GENERAL;


I had built gcc from source and created a soft link of gcc executable and g++ to the /usr/bin directory.After inserting the object file I got the following error inside teradata.

*** Failure 5600 Error creating UDF/XSP/UDM/UDT: no compiler available.
                Statement# 1, Info =0 
",-1,-1,-1.0,"I tried to create a function using object file and by using the following set of queries 

REPLACE FUNCTION db_1.append(inputString VARCHAR(512))
RETURNS VARCHAR(512)
LANGUAGE CPP
NO SQL
EXTERNAL NAME 'CO!tokenise!/root/Desktop/tokenise.o!F!append'
PARAMETER STYLE TD_GENERAL;


I had built gcc from source and created a soft link of gcc executable and g++ to the /usr/bin directory.After inserting the object file I got the following error inside teradata.

*** Failure 5600 Error creating UDF/XSP/UDM/UDT: no compiler available.
                Statement# 1, Info =0 
",3
266,43866309,"Class com.teradata.jdbc.TeraDriver not found (Python, jaydebeapi module)","I am trying to connect to teradata using jaydebeapi.

import jaydebeapi
conn = jaydebeapi.connect('com.teradata.jdbc.TeraDriver',
                           'jdbc:teradata://serverIP/charset=UTF8,DBS_PORT=1025',
                           {'user': 'xxx', 'password': 'xxx'},
[r'path_to_teradata_jdbc_driver/tdgssconfig.jar',r'path_to_teradata_jdbc_driver/terajdbc4.jar'])


When I run this script ($python ""Run SQL_Java.py"") I get the following error:


  Traceback (most recent call last):   File ""Run SQL_Java.py"", line 60,
  in 
      [r'path_to_teradata_jdbc_driver/tdgssconfig.jar',r'path_to_teradata_jdbc_driver/terajdbc4.jar'])
  File
  ""/Users/xxx/anaconda/lib/python2.7/site-packages/jaydebeapi/init.py"",
  line 381, in connect
      jconn = _jdbc_connect(jclassname, url, driver_args, jars, libs)   File
  ""/Users/xxx/anaconda/lib/python2.7/site-packages/jaydebeapi/init.py"",
  line 190, in _jdbc_connect_jpype
      jpype.JClass(jclassname)   File ""/Users/i.otenko/anaconda/lib/python2.7/site-packages/jpype/_jclass.py"",
  line 55, in JClass
      raise _RUNTIMEEXCEPTION.PYEXC(""Class %s not found"" % name) jpype._jexception.RuntimeExceptionPyRaisable:
  java.lang.RuntimeException: Class com.teradata.jdbc.TeraDriver not
  found


Am I not specifying path to JDBC drivers correctly? 
",-1,-1,-1.0,"I am trying to connect to teradata using jaydebeapi.

import jaydebeapi
conn = jaydebeapi.connect('com.teradata.jdbc.TeraDriver',
                           'jdbc:teradata://serverIP/charset=UTF8,DBS_PORT=1025',
                           {'user': 'xxx', 'password': 'xxx'},
[r'path_to_teradata_jdbc_driver/tdgssconfig.jar',r'path_to_teradata_jdbc_driver/terajdbc4.jar'])


When I run this script ($python ""Run SQL_Java.py"") I get the following error:


  Traceback (most recent call last):   File ""Run SQL_Java.py"", line 60,
  in 
      [r'path_to_teradata_jdbc_driver/tdgssconfig.jar',r'path_to_teradata_jdbc_driver/terajdbc4.jar'])
  File
  ""/Users/xxx/anaconda/lib/python2.7/site-packages/jaydebeapi/init.py"",
  line 381, in connect
      jconn = _jdbc_connect(jclassname, url, driver_args, jars, libs)   File
  ""/Users/xxx/anaconda/lib/python2.7/site-packages/jaydebeapi/init.py"",
  line 190, in _jdbc_connect_jpype
      jpype.JClass(jclassname)   File ""/Users/i.otenko/anaconda/lib/python2.7/site-packages/jpype/_jclass.py"",
  line 55, in JClass
      raise _RUNTIMEEXCEPTION.PYEXC(""Class %s not found"" % name) jpype._jexception.RuntimeExceptionPyRaisable:
  java.lang.RuntimeException: Class com.teradata.jdbc.TeraDriver not
  found


Am I not specifying path to JDBC drivers correctly? 
",1
267,43354690,Teradata SQL Reverse Parent Child Hierarchy,"I know how to build a hierarchy starting with the root node (i.e. where parent_id is null or something like that), but I can't find anything on how to build a hierarchy upward from the final child/edge node. I'd like to start with a child and build all the way back up to the top. Assume I don't know how many levels, or who the parent is, and we'll have to use SQL to figure it out.

Here is my base table:

old_entity_key,new_entity_key
1,2
2,3
3,4
4,5
5,6


Desired output:

new_entity_key,path
2,1/2
3,1/2/3
4,1/2/3/4
5,1/2/3/4/5
6,1/2/3/4/5/6


This is also acceptable:

new_entity_key,path
2,2/1
3,3/2/1
4,4/3/2/1
5,5/4/3/2/1
6,6/5/4/3/2/1


Here is the CTE I've started with:

with recursive history as (
    select
        old_entity_key,
        new_entity_key,
        cast(old_entity_key||'/'||new_entity_key as varchar(1000)) as path
    from table
    where new_entity_key not in (select old_entity_key from table)
        and cast(start_time as date) between current_date - interval '3' day and current_date
    union all
    select
        c.old_entity_key,
        c.new_entity_key,
        p.new_entity_key||'/'||c.path
    from history c
    join table p on p.new_entity_key = c.old_entity_key
)
select new_entity_key, old_entity_key, substr(path, 1, instr(path, '/') - 1) as original_entity_key, path
from history s;


The problem with the above query is that it runs forever. I think I've created an infinite loop. I've also tried using the below where filter in the bottom query of the union to try to find the root node, but Teradata gives me an error:

where p.new_entity_key in (select old_entity_key from table)


Any help would be greatly appreciated.
",-1,-1,-1.0,"I know how to build a hierarchy starting with the root node (i.e. where parent_id is null or something like that), but I can't find anything on how to build a hierarchy upward from the final child/edge node. I'd like to start with a child and build all the way back up to the top. Assume I don't know how many levels, or who the parent is, and we'll have to use SQL to figure it out.

Here is my base table:

old_entity_key,new_entity_key
1,2
2,3
3,4
4,5
5,6


Desired output:

new_entity_key,path
2,1/2
3,1/2/3
4,1/2/3/4
5,1/2/3/4/5
6,1/2/3/4/5/6


This is also acceptable:

new_entity_key,path
2,2/1
3,3/2/1
4,4/3/2/1
5,5/4/3/2/1
6,6/5/4/3/2/1


Here is the CTE I've started with:

with recursive history as (
    select
        old_entity_key,
        new_entity_key,
        cast(old_entity_key||'/'||new_entity_key as varchar(1000)) as path
    from table
    where new_entity_key not in (select old_entity_key from table)
        and cast(start_time as date) between current_date - interval '3' day and current_date
    union all
    select
        c.old_entity_key,
        c.new_entity_key,
        p.new_entity_key||'/'||c.path
    from history c
    join table p on p.new_entity_key = c.old_entity_key
)
select new_entity_key, old_entity_key, substr(path, 1, instr(path, '/') - 1) as original_entity_key, path
from history s;


The problem with the above query is that it runs forever. I think I've created an infinite loop. I've also tried using the below where filter in the bottom query of the union to try to find the root node, but Teradata gives me an error:

where p.new_entity_key in (select old_entity_key from table)


Any help would be greatly appreciated.
",3
268,43271711,Teradata : Delete n rows from a table,"I wanted to delete first n rows or last n rows from a table in Teradata. But I am not getting any correct query for doing this. Can anyone please tell me how I can do this?
",-1,-1,-1.0,"I wanted to delete first n rows or last n rows from a table in Teradata. But I am not getting any correct query for doing this. Can anyone please tell me how I can do this?
",3
269,43122331,Sqoop is failing to get data from teradata with java.IO exception,"Here is my sqoop import that I'm using to pull data from Teradata

     sqoop import -libjars jars --driver drivers --connect connection_url -m 1 --hive-overwrite --hive-import --hive-database hivedatabase --hive-table hivetable --target-dir '/user/hive/warehouse/database.db/table_name' --as-parquetfile --query ""select c1,c2,c3, to_char(SOURCE_ACTIVATION_DT,'YYYY-MM-DD HH24:MI:SS') as SOURCE_ACTIVATION_DT,to_char(SOURCE_DEACTIVATION_DT,'YYYY-MM-DD HH24:MI:SS') as SOURCE_DEACTIVATION_DT,to_char(EFF_DT,'YYYY-MM-DD HH24:MI:SS') as EFF_DT,to_char(EXP_DT,'YYYY-MM-DD HH24:MI:SS') as EXP_DT,to_char(SYS_UPDATE_DTM,'YYYY-MM-DD HH24:MI:SS') as SYS_UPDATE_DTM,to_char(SYS_LOAD_DTM,'YYYY-MM-DD HH24:MI:SS') as SYS_LOAD_DTM from source_schema.table_name WHERE to_char(SYS_UPDATE_DTM,'YYYY-MM-DD HH24:MI:SS')&gt; '2017-03-30 10:00:00' OR to_char(SYS_LOAD_DTM,'YYYY-MM-DD HH24:MI:SS') &gt; '2017-03-30 10:00:00' AND \$CONDITIONS""


Below is the error I'm getting, this was running fine for two days and started returning the below error recently.

17/03/29 20:07:53 INFO mapreduce.Job:  map 0% reduce 0%
17/03/29 20:56:46 INFO mapreduce.Job: Task Id : attempt_1487033963691_263120_m_000000_0, Status : FAILED
Error: java.io.IOException: SQLException in nextKeyValue
    at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)
    at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)
    at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
    at org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.10.00.14] [Error 1005] [SQLState HY000] Unexpected parcel kind received: 9
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:94)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:69)
    at com.teradata.jdbc.jdbc_4.statemachine.ReceiveRecordSubState.action(ReceiveRecordSubState.java:195)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:311)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:200)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:137)
    at com.teradata.jdbc.jdbc_4.statemachine.PreparedStatementController.run(PreparedStatementController.java:46)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.fetchRows(StatementController.java:360)
    at com.teradata.jdbc.jdbc_4.TDResultSet.goToRow(TDResultSet.java:374)
    at com.teradata.jdbc.jdbc_4.TDResultSet.next(TDResultSet.java:657)
    at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:237)
    ... 12 more


When i googled around I've seen people getting same errors for different errors, I know this is something related to the time i'm using in where clause, but not sure what exactly i have to change.

Thanks in advance...!!
",-1,-1,-1.0,"Here is my sqoop import that I'm using to pull data from Teradata

     sqoop import -libjars jars --driver drivers --connect connection_url -m 1 --hive-overwrite --hive-import --hive-database hivedatabase --hive-table hivetable --target-dir '/user/hive/warehouse/database.db/table_name' --as-parquetfile --query ""select c1,c2,c3, to_char(SOURCE_ACTIVATION_DT,'YYYY-MM-DD HH24:MI:SS') as SOURCE_ACTIVATION_DT,to_char(SOURCE_DEACTIVATION_DT,'YYYY-MM-DD HH24:MI:SS') as SOURCE_DEACTIVATION_DT,to_char(EFF_DT,'YYYY-MM-DD HH24:MI:SS') as EFF_DT,to_char(EXP_DT,'YYYY-MM-DD HH24:MI:SS') as EXP_DT,to_char(SYS_UPDATE_DTM,'YYYY-MM-DD HH24:MI:SS') as SYS_UPDATE_DTM,to_char(SYS_LOAD_DTM,'YYYY-MM-DD HH24:MI:SS') as SYS_LOAD_DTM from source_schema.table_name WHERE to_char(SYS_UPDATE_DTM,'YYYY-MM-DD HH24:MI:SS')&gt; '2017-03-30 10:00:00' OR to_char(SYS_LOAD_DTM,'YYYY-MM-DD HH24:MI:SS') &gt; '2017-03-30 10:00:00' AND \$CONDITIONS""


Below is the error I'm getting, this was running fine for two days and started returning the below error recently.

17/03/29 20:07:53 INFO mapreduce.Job:  map 0% reduce 0%
17/03/29 20:56:46 INFO mapreduce.Job: Task Id : attempt_1487033963691_263120_m_000000_0, Status : FAILED
Error: java.io.IOException: SQLException in nextKeyValue
    at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:277)
    at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:556)
    at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:80)
    at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:91)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
    at org.apache.sqoop.mapreduce.AutoProgressMapper.run(AutoProgressMapper.java:64)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.10.00.14] [Error 1005] [SQLState HY000] Unexpected parcel kind received: 9
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:94)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:69)
    at com.teradata.jdbc.jdbc_4.statemachine.ReceiveRecordSubState.action(ReceiveRecordSubState.java:195)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:311)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:200)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:137)
    at com.teradata.jdbc.jdbc_4.statemachine.PreparedStatementController.run(PreparedStatementController.java:46)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.fetchRows(StatementController.java:360)
    at com.teradata.jdbc.jdbc_4.TDResultSet.goToRow(TDResultSet.java:374)
    at com.teradata.jdbc.jdbc_4.TDResultSet.next(TDResultSet.java:657)
    at org.apache.sqoop.mapreduce.db.DBRecordReader.nextKeyValue(DBRecordReader.java:237)
    ... 12 more


When i googled around I've seen people getting same errors for different errors, I know this is something related to the time i'm using in where clause, but not sure what exactly i have to change.

Thanks in advance...!!
",0
270,45306260,Concatenate strings from multiple records in Teradata SQL,"I have a list of merchants that do business in different states.

Merch    State

A          NC

A          FL

B          CA

B          VA


Instead of returning four records I want to group by Merch but concatenate the strings of the states so that the output looks like

Merch      States

A           NC,FL

B           CA,VA


I'm having a lot of trouble translating the response in this answer for my issue
Optimal way to concatenate/aggregate strings

I also cannot get String_agg to work, I'm not sure it works in Teradata. https://learn.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql
",-1,-1,-1.0,"I have a list of merchants that do business in different states.

Merch    State

A          NC

A          FL

B          CA

B          VA


Instead of returning four records I want to group by Merch but concatenate the strings of the states so that the output looks like

Merch      States

A           NC,FL

B           CA,VA


I'm having a lot of trouble translating the response in this answer for my issue
Optimal way to concatenate/aggregate strings

I also cannot get String_agg to work, I'm not sure it works in Teradata. https://learn.microsoft.com/en-us/sql/t-sql/functions/string-agg-transact-sql
",3
271,45427886,Uploading sas data set to teradata-Error:the value applied to a Teradata 2 byte decimal was out of range,"Got an error when uploading a sas dataset into teradata.ERROR: On an insert, update, or index operation, the value applied to a Teradata 2 byte decimal was out of range. This was the proc contents of the table





LIBNAME VKTD TERADATA SERVER='XXXXX' USER=&amp;userid PWD=&amp;pwd DATABASE =""db"" MODE=TERADATA CONNECTION=GLOBAL;
/*Droping the table*/
PROC SQL NOERRORSTOP;
DROP TABLE VKTD.VK_FINAL;
QUIT;
PROC SQL;
 CREATE TABLE VKTD.VK_FINAL(TPT=YES FASTLOAD=YES TPT_MAX_SESSIONS=4 SET = YES DBCREATE_TABLE_OPTS='PRIMARY INDEX(column8,column6,column5)') AS
SELECT column1,column2, column3,column4,column5, column6,column7,column8, coloumn9 
FROM WORK.tablename;
QUIT;


Not sure which variable is causing the error and how to correct it.Column2 is decimal format 5.4
",-1,-1,-1.0,"Got an error when uploading a sas dataset into teradata.ERROR: On an insert, update, or index operation, the value applied to a Teradata 2 byte decimal was out of range. This was the proc contents of the table





LIBNAME VKTD TERADATA SERVER='XXXXX' USER=&amp;userid PWD=&amp;pwd DATABASE =""db"" MODE=TERADATA CONNECTION=GLOBAL;
/*Droping the table*/
PROC SQL NOERRORSTOP;
DROP TABLE VKTD.VK_FINAL;
QUIT;
PROC SQL;
 CREATE TABLE VKTD.VK_FINAL(TPT=YES FASTLOAD=YES TPT_MAX_SESSIONS=4 SET = YES DBCREATE_TABLE_OPTS='PRIMARY INDEX(column8,column6,column5)') AS
SELECT column1,column2, column3,column4,column5, column6,column7,column8, coloumn9 
FROM WORK.tablename;
QUIT;


Not sure which variable is causing the error and how to correct it.Column2 is decimal format 5.4
",3
272,45450618,Connect to Teradata Using Airflow JDBC Connection,"I'm trying to execute a SqlSensor task in Airflow using a connection to Teradata database. The connection is configured as follow:



I have provide in particular 2 driver paths separated by "", "" but I am not sure if it's the proper way to do it?


/home/airflow/java_sample/tdgssconfig.jar
/home/airflow/java_sample/terajdbc4.jar


When the DAG executes, it triggers the error message

[2017-08-02 02:32:45,162] {models.py:1342} INFO - Executing &lt;Task(SqlSensor): check_running_batch&gt; on 2017-08-02 02:32:12
[2017-08-02 02:32:45,179] {base_hook.py:67} INFO - Using connection to: jdbc:teradata://myservername.mycompanyname.org/database=MYDBNAME,TMODE=ANSI,CHARSET=UTF8
[2017-08-02 02:32:45,313] {sensors.py:109} INFO - Poking: SELECT BATCH_KEY FROM MYDBNAME.AUDIT_BATCH WHERE BATCH_OWNER='ARO_TEST' AND AUDIT_STATUS_KEY=1;
[2017-08-02 02:32:45,316] {base_hook.py:67} INFO - Using connection to: jdbc:teradata://myservername.mycompanyname.org/database=MYDBNAME,TMODE=ANSI,CHARSET=UTF8
[2017-08-02 02:32:45,497] {models.py:1417} ERROR - java.lang.RuntimeException: Class com.teradata.jdbc.TeraDriver not found


What am I doing wrong?
",-1,-1,-1.0,"I'm trying to execute a SqlSensor task in Airflow using a connection to Teradata database. The connection is configured as follow:



I have provide in particular 2 driver paths separated by "", "" but I am not sure if it's the proper way to do it?


/home/airflow/java_sample/tdgssconfig.jar
/home/airflow/java_sample/terajdbc4.jar


When the DAG executes, it triggers the error message

[2017-08-02 02:32:45,162] {models.py:1342} INFO - Executing &lt;Task(SqlSensor): check_running_batch&gt; on 2017-08-02 02:32:12
[2017-08-02 02:32:45,179] {base_hook.py:67} INFO - Using connection to: jdbc:teradata://myservername.mycompanyname.org/database=MYDBNAME,TMODE=ANSI,CHARSET=UTF8
[2017-08-02 02:32:45,313] {sensors.py:109} INFO - Poking: SELECT BATCH_KEY FROM MYDBNAME.AUDIT_BATCH WHERE BATCH_OWNER='ARO_TEST' AND AUDIT_STATUS_KEY=1;
[2017-08-02 02:32:45,316] {base_hook.py:67} INFO - Using connection to: jdbc:teradata://myservername.mycompanyname.org/database=MYDBNAME,TMODE=ANSI,CHARSET=UTF8
[2017-08-02 02:32:45,497] {models.py:1417} ERROR - java.lang.RuntimeException: Class com.teradata.jdbc.TeraDriver not found


What am I doing wrong?
",0
273,45552864,Have an issue in inserting the data into teradata from a hive table,"I have an issue in inserting the data into teradata from a hive table.

Error: Caused by: java.net.UnknownHostException : unknown error         at
java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)         at 
java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)         at
java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)         at
java.net.InetAddress.getAllByName0(InetAddress.java:1276)         at 
java.net.InetAddress.getAllByName(InetAddress.java:1192)         at 
java.net.InetAddress.getAllByName(InetAddress.java:1126)         at
com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$Lookup.doLookup(TDNetworkIOIF.java:223)         at
com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$Lookup.getAddresses(TDNetworkIOIF.java:261)         at 
com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.connectToHost(TDNetworkIOIF.java:381)         at 
com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.createSocketConnection(TDNetworkIOIF.java:156)


When I am trying to insert the data into the Teradata table which is read from a data frame is throwing an error
",-1,-1,-1.0,"I have an issue in inserting the data into teradata from a hive table.

Error: Caused by: java.net.UnknownHostException : unknown error         at
java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)         at 
java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)         at
java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)         at
java.net.InetAddress.getAllByName0(InetAddress.java:1276)         at 
java.net.InetAddress.getAllByName(InetAddress.java:1192)         at 
java.net.InetAddress.getAllByName(InetAddress.java:1126)         at
com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$Lookup.doLookup(TDNetworkIOIF.java:223)         at
com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$Lookup.getAddresses(TDNetworkIOIF.java:261)         at 
com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.connectToHost(TDNetworkIOIF.java:381)         at 
com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.createSocketConnection(TDNetworkIOIF.java:156)


When I am trying to insert the data into the Teradata table which is read from a data frame is throwing an error
",0
274,45570262,Spark export to Teradata,"I am trying to copy data from HDFS to Teradata through Spark. I am getting UnknownHostException when running thorugh the Spark-shell. I am getting teradata.main not found while running the same through spark submit(I have added teradata jars with the spark submit as well).
The Same Teradata connection URL and Credentials are working good when Sqooping.

I have added teradata jars to the executor and driver class path and also in sparkdefaults.conf as well.
Please find my Spark Teradata connection code as below,

val jdbcDF = sqlContext.load(""jdbc"", Map(""url"" -&gt; ""jdbc:teradata://teradataservername, user=***###, password=***###"",""dbtable"" -&gt; ""query"",""driver"" -&gt; ""com.teradata.jdbc.TeraDriver""))


Please find my excetion that I got while running from spark-shell as below,


  TERAJDBC4 ERROR [main]
  com.teradata.jdbc.jdk6.JDK6_SQL_Connection@219f9031 Connection to
  (teradata server), TMODE=TERA, username=###, password=### Sun Aug 06
  22:43:40 EDT 2017 socket orig=(teradata server), TMODE=TERA,
  username=###, password=### cid=742ff968 sess=0
  java.net.UnknownHostException: (teradata server), TMODE=TERA,
  username=###, password=###: unknown error  at
  java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)  at
  java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)  at
  java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
  at java.net.InetAddress.getAllByName0(InetAddress.java:1276)

",-1,-1,-1.0,"I am trying to copy data from HDFS to Teradata through Spark. I am getting UnknownHostException when running thorugh the Spark-shell. I am getting teradata.main not found while running the same through spark submit(I have added teradata jars with the spark submit as well).
The Same Teradata connection URL and Credentials are working good when Sqooping.

I have added teradata jars to the executor and driver class path and also in sparkdefaults.conf as well.
Please find my Spark Teradata connection code as below,

val jdbcDF = sqlContext.load(""jdbc"", Map(""url"" -&gt; ""jdbc:teradata://teradataservername, user=***###, password=***###"",""dbtable"" -&gt; ""query"",""driver"" -&gt; ""com.teradata.jdbc.TeraDriver""))


Please find my excetion that I got while running from spark-shell as below,


  TERAJDBC4 ERROR [main]
  com.teradata.jdbc.jdk6.JDK6_SQL_Connection@219f9031 Connection to
  (teradata server), TMODE=TERA, username=###, password=### Sun Aug 06
  22:43:40 EDT 2017 socket orig=(teradata server), TMODE=TERA,
  username=###, password=### cid=742ff968 sess=0
  java.net.UnknownHostException: (teradata server), TMODE=TERA,
  username=###, password=###: unknown error  at
  java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)  at
  java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)  at
  java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
  at java.net.InetAddress.getAllByName0(InetAddress.java:1276)

",0
275,45586473,Sqoop incremantal import fails for Teradata,"My platform: CDH 5.10 Quickstart VM and Teradata TDEXPRESS 16 VM version.
I am using Sqoop and the Teradata JDBC driver- 
tdgssconfig.jar, teradata-connector-1.5.3.jar,terajdbc4.jar 
My sqoop job is failing with the following errors. 

Any suggestions?

[cloudera@quickstart sqoop]$ sqoop job --create incjobtd1 -- import --connect jdbc:teradata://192.168.142.129/DATABASE=teradb  --driver com.teradata.jdbc.TeraDriver  --username dbc --password dbc   --table ACCT   --incremental lastmodified
       --check-column  ECF_XTRC_TS        --target-dir /user/cloudera/teradb/acct2 -m 1 --as-parquetfile  --append
    Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
    Please set $ACCUMULO_HOME to the root of your Accumulo installation.
    17/08/09 01:44:49 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0
    17/08/09 01:44:54 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
    [cloudera@quickstart sqoop]$ sqoop job --exec incjobtd1
    Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
    Please set $ACCUMULO_HOME to the root of your Accumulo installation.
    17/08/09 01:45:14 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0
    Enter password:
    17/08/09 01:45:25 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
    17/08/09 01:45:25 INFO manager.SqlManager: Using default fetchSize of 1000
    17/08/09 01:45:25 INFO tool.CodeGenTool: Beginning code generation
    17/08/09 01:45:25 INFO tool.CodeGenTool: Will generate java class as codegen_ACCT
    17/08/09 01:45:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM ACCT AS t WHERE 1=0
    17/08/09 01:45:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM ACCT AS t WHERE 1=0
    17/08/09 01:45:32 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
    Note: /tmp/sqoop-cloudera/compile/fc68a3f34abd80bde713cf1709dbc330/codegen_ACCT.java uses or overrides a deprecated API.
    Note: Recompile with -Xlint:deprecation for details.
    17/08/09 01:45:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/fc68a3f34abd80bde713cf1709dbc330/codegen_ACCT.jar
    17/08/09 01:46:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM ACCT AS t WHERE 1=0
    17/08/09 01:46:07 ERROR manager.SqlManager: SQL exception accessing current timestamp: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.00.00.23] [Error 3706] [SQLState 42000] Syntax error: expected something between '(' and ')'.
    java.sql.SQLException: [Teradata Database] [TeraJDBC 16.00.00.23] [Error 3706] [SQLState 42000] Syntax error: expected something between '(' and ')'.
            at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:309)
            at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:103)
            at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:311)
            at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:200)
            at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:137)
            at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:128)
            at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:389)
            at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:331)
            at com.teradata.jdbc.jdbc_4.TDStatement.doNonPrepExecuteQuery(TDStatement.java:319)
            at com.teradata.jdbc.jdbc_4.TDStatement.executeQuery(TDStatement.java:1121)
            at org.apache.sqoop.manager.SqlManager.getCurrentDbTimestamp(SqlManager.java:987)
            at org.apache.sqoop.tool.ImportTool.initIncrementalConstraints(ImportTool.java:328)
            at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:498)
            at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)
            at org.apache.sqoop.tool.JobTool.execJob(JobTool.java:213)
            at org.apache.sqoop.tool.JobTool.run(JobTool.java:268)
            at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
            at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
            at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
            at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
            at org.apache.sqoop.Sqoop.main(Sqoop.java:236)
    17/08/09 01:46:07 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Could not get current time from database
            at org.apache.sqoop.tool.ImportTool.initIncrementalConstraints(ImportTool.java:330)
            at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:498)
            at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)
            at org.apache.sqoop.tool.JobTool.execJob(JobTool.java:213)
            at org.apache.sqoop.tool.JobTool.run(JobTool.java:268)
            at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
            at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
            at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
            at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
            at org.apache.sqoop.Sqoop.main(Sqoop.java:236)

    [cloudera@quickstart sqoop]$

",-1,-1,-1.0,"My platform: CDH 5.10 Quickstart VM and Teradata TDEXPRESS 16 VM version.
I am using Sqoop and the Teradata JDBC driver- 
tdgssconfig.jar, teradata-connector-1.5.3.jar,terajdbc4.jar 
My sqoop job is failing with the following errors. 

Any suggestions?

[cloudera@quickstart sqoop]$ sqoop job --create incjobtd1 -- import --connect jdbc:teradata://192.168.142.129/DATABASE=teradb  --driver com.teradata.jdbc.TeraDriver  --username dbc --password dbc   --table ACCT   --incremental lastmodified
       --check-column  ECF_XTRC_TS        --target-dir /user/cloudera/teradb/acct2 -m 1 --as-parquetfile  --append
    Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
    Please set $ACCUMULO_HOME to the root of your Accumulo installation.
    17/08/09 01:44:49 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0
    17/08/09 01:44:54 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
    [cloudera@quickstart sqoop]$ sqoop job --exec incjobtd1
    Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
    Please set $ACCUMULO_HOME to the root of your Accumulo installation.
    17/08/09 01:45:14 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.10.0
    Enter password:
    17/08/09 01:45:25 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.
    17/08/09 01:45:25 INFO manager.SqlManager: Using default fetchSize of 1000
    17/08/09 01:45:25 INFO tool.CodeGenTool: Beginning code generation
    17/08/09 01:45:25 INFO tool.CodeGenTool: Will generate java class as codegen_ACCT
    17/08/09 01:45:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM ACCT AS t WHERE 1=0
    17/08/09 01:45:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM ACCT AS t WHERE 1=0
    17/08/09 01:45:32 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
    Note: /tmp/sqoop-cloudera/compile/fc68a3f34abd80bde713cf1709dbc330/codegen_ACCT.java uses or overrides a deprecated API.
    Note: Recompile with -Xlint:deprecation for details.
    17/08/09 01:45:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/fc68a3f34abd80bde713cf1709dbc330/codegen_ACCT.jar
    17/08/09 01:46:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM ACCT AS t WHERE 1=0
    17/08/09 01:46:07 ERROR manager.SqlManager: SQL exception accessing current timestamp: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.00.00.23] [Error 3706] [SQLState 42000] Syntax error: expected something between '(' and ')'.
    java.sql.SQLException: [Teradata Database] [TeraJDBC 16.00.00.23] [Error 3706] [SQLState 42000] Syntax error: expected something between '(' and ')'.
            at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:309)
            at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:103)
            at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:311)
            at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:200)
            at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:137)
            at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:128)
            at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:389)
            at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:331)
            at com.teradata.jdbc.jdbc_4.TDStatement.doNonPrepExecuteQuery(TDStatement.java:319)
            at com.teradata.jdbc.jdbc_4.TDStatement.executeQuery(TDStatement.java:1121)
            at org.apache.sqoop.manager.SqlManager.getCurrentDbTimestamp(SqlManager.java:987)
            at org.apache.sqoop.tool.ImportTool.initIncrementalConstraints(ImportTool.java:328)
            at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:498)
            at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)
            at org.apache.sqoop.tool.JobTool.execJob(JobTool.java:213)
            at org.apache.sqoop.tool.JobTool.run(JobTool.java:268)
            at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
            at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
            at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
            at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
            at org.apache.sqoop.Sqoop.main(Sqoop.java:236)
    17/08/09 01:46:07 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: Could not get current time from database
            at org.apache.sqoop.tool.ImportTool.initIncrementalConstraints(ImportTool.java:330)
            at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:498)
            at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)
            at org.apache.sqoop.tool.JobTool.execJob(JobTool.java:213)
            at org.apache.sqoop.tool.JobTool.run(JobTool.java:268)
            at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
            at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
            at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
            at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
            at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
            at org.apache.sqoop.Sqoop.main(Sqoop.java:236)

    [cloudera@quickstart sqoop]$

",0
276,45607268,SSIS - Teradata Attunity Connector and SQLServer 2016,"I have no problem running an SSIS package from VS2015 on my dev machine transferring from Teradata 15 to SQLServer 2016 using Attunity 4.0 and all of the following from TTU 16.10.01:

ODBC Driver

OLEDB Access Module

.Net Data Provider

TPT Base

TPT Stream
but when I deploy the package to the same SQLServer the execution fails with this error:


&quot;Data Flow Task:Error: The version of Teradata Source - [source name] is not compatible with this version of the DataFlow.&quot;
I have installed exactly the same Attunity &amp; TTU drivers on the server.
Have I missed setting up something on the SQL Server?
",-1,-1,-1.0,"I have no problem running an SSIS package from VS2015 on my dev machine transferring from Teradata 15 to SQLServer 2016 using Attunity 4.0 and all of the following from TTU 16.10.01:

ODBC Driver

OLEDB Access Module

.Net Data Provider

TPT Base

TPT Stream
but when I deploy the package to the same SQLServer the execution fails with this error:


&quot;Data Flow Task:Error: The version of Teradata Source - [source name] is not compatible with this version of the DataFlow.&quot;
I have installed exactly the same Attunity &amp; TTU drivers on the server.
Have I missed setting up something on the SQL Server?
",1
277,45970047,Create a linked table in Access from Teradata without DSN?,"Connection to Teradata

'Requires reference to ADO and ADOX  
Public adoCn As ADODB.Connection
Public adoCat As New ADOX.Catalog
Public adoTbl As New ADOX.Table

Function TD_Make_Linked_Table()

Dim varServer As String
Dim varDatabase As String
Dim varUser As String
Dim varPassword As String

varServer = ""Test""
varDatabase = ""Test_Test""
varUser = ""Test_User""
varPassword = ""Test_Password""

Set adoCn = New ADODB.Connection

'I have tried multiple connection strings

adoCn.ConnectionString = ""PROVIDER=MSDASQL;DRIVER={Teradata};"" &amp; _
                         ""DBCName="" &amp; varServer &amp; "";"" &amp; _
                         ""DefaultDatabase="" &amp; varDatabase &amp; "";"" &amp; _
                         ""UID="" &amp; varUser &amp; "";"" &amp; _
                         ""PWD="" &amp; varPassword &amp; "";""

adoCn.Open

Set adoCat = New ADOX.Catalog
Set adoCat.ActiveConnection = adoCn
Set adoTbl = New ADOX.Table

adoTbl.ParentCatalog = adoCat
adoTbl.Name = ""Test""

'I have tried multiple property combinations
'Causes error 3265 Item not found
'adoTbl.Properties(""?"") = adoCn
'adoTbl.Properties(""Jet OLEDB:Link Datasource"") = ""Test""
'adoTbl.Properties(""Jet OLEDB:Link Provider String"") = 
'adoTbl.Properties(""Jet OLEDB:Remote Table Name"") = ""LinkDatabaseTable""
'adoTbl.Properties(""Jet OLEDB:Create Link"") = True

'Causes 3251 provider is not capable of performing operation
'adoCat.Tables.Append adoTbl

adoCn.Close

   Set adoTbl = Nothing
   Set adoCat = Nothing
   Set adoCn = Nothing

End Function


I have validated the connection is working. I can query data by opening the connection and executing SQL. 

I am unable to create a linked table programmatically in Access using visual basic for applications with the created connection.

Has anyone been able to successfully create a linked dsn-less table from Access to Teradata? 
",-1,-1,-1.0,"Connection to Teradata

'Requires reference to ADO and ADOX  
Public adoCn As ADODB.Connection
Public adoCat As New ADOX.Catalog
Public adoTbl As New ADOX.Table

Function TD_Make_Linked_Table()

Dim varServer As String
Dim varDatabase As String
Dim varUser As String
Dim varPassword As String

varServer = ""Test""
varDatabase = ""Test_Test""
varUser = ""Test_User""
varPassword = ""Test_Password""

Set adoCn = New ADODB.Connection

'I have tried multiple connection strings

adoCn.ConnectionString = ""PROVIDER=MSDASQL;DRIVER={Teradata};"" &amp; _
                         ""DBCName="" &amp; varServer &amp; "";"" &amp; _
                         ""DefaultDatabase="" &amp; varDatabase &amp; "";"" &amp; _
                         ""UID="" &amp; varUser &amp; "";"" &amp; _
                         ""PWD="" &amp; varPassword &amp; "";""

adoCn.Open

Set adoCat = New ADOX.Catalog
Set adoCat.ActiveConnection = adoCn
Set adoTbl = New ADOX.Table

adoTbl.ParentCatalog = adoCat
adoTbl.Name = ""Test""

'I have tried multiple property combinations
'Causes error 3265 Item not found
'adoTbl.Properties(""?"") = adoCn
'adoTbl.Properties(""Jet OLEDB:Link Datasource"") = ""Test""
'adoTbl.Properties(""Jet OLEDB:Link Provider String"") = 
'adoTbl.Properties(""Jet OLEDB:Remote Table Name"") = ""LinkDatabaseTable""
'adoTbl.Properties(""Jet OLEDB:Create Link"") = True

'Causes 3251 provider is not capable of performing operation
'adoCat.Tables.Append adoTbl

adoCn.Close

   Set adoTbl = Nothing
   Set adoCat = Nothing
   Set adoCn = Nothing

End Function


I have validated the connection is working. I can query data by opening the connection and executing SQL. 

I am unable to create a linked table programmatically in Access using visual basic for applications with the created connection.

Has anyone been able to successfully create a linked dsn-less table from Access to Teradata? 
",0
278,45979429,Issue connecting R with Teradata,"I'm having issues connecting R with Teradata.

library(""RJDBC"")


drv = JDBC(""com.teradata.jdbc.TeraDriver"";""C:\\...\\terajdbc4.jar,
           C:\\...\\tdgssconfig.jar"")

conn = dbConnect(drv,""jdbc:teradata://dbserver"",""user1"",""pwd1"") 


When I try to run the connection part I get the following error message:

GSSException: Failure unspecified at GSS-API level (Mechanism level: UserFile parameter null)

....

Error in .jcall(drv@jdrv, ""Ljava/sql/Connection;"", ""connect"", as.character(url)[1],  : 
  java.lang.NullPointerException


Please note that I have already check that the .jar files are in the right path and that R has the proper rights to access them. I have looked at similar questions on the Teradata Community forum but that didn't help.
",-1,-1,-1.0,"I'm having issues connecting R with Teradata.

library(""RJDBC"")


drv = JDBC(""com.teradata.jdbc.TeraDriver"";""C:\\...\\terajdbc4.jar,
           C:\\...\\tdgssconfig.jar"")

conn = dbConnect(drv,""jdbc:teradata://dbserver"",""user1"",""pwd1"") 


When I try to run the connection part I get the following error message:

GSSException: Failure unspecified at GSS-API level (Mechanism level: UserFile parameter null)

....

Error in .jcall(drv@jdrv, ""Ljava/sql/Connection;"", ""connect"", as.character(url)[1],  : 
  java.lang.NullPointerException


Please note that I have already check that the .jar files are in the right path and that R has the proper rights to access them. I have looked at similar questions on the Teradata Community forum but that didn't help.
",1
279,46081237,Teradata SQL Get Rid of Duplicates with Specific Order,"I just started teradata SQL this week, so sorry if I don't phrase things correctly. I originally created a script in R that gets rid of duplicates within my table, but now I need to transfer this code into SQL. Here is some sample data:



I want to get rid of any D's in the DELETE column, partition by ID, order by STATUS, DATE, and AMOUNT (with actual dates and amounts before ?s). I want STATUS to go in this order: P, H, F, U, T. I want the first row that has STATUS, DATE, and AMOUNT filled out (with STATUS in order). Here is the example output data:



I'm really stuck on the order issue and the code I've written isn't producing any data at all (but no errors). 

SAMPLE CODE:

CREATE VOLATILE TABLE new_tble
AS
(SELECT * 
FROM table
QUALIFY row_number() OVER (partition BY ID ORDER BY ID, DATE, AMOUNT)=1
WHERE DELETE &lt;&gt; 'D'
)
with data;

",-1,-1,-1.0,"I just started teradata SQL this week, so sorry if I don't phrase things correctly. I originally created a script in R that gets rid of duplicates within my table, but now I need to transfer this code into SQL. Here is some sample data:



I want to get rid of any D's in the DELETE column, partition by ID, order by STATUS, DATE, and AMOUNT (with actual dates and amounts before ?s). I want STATUS to go in this order: P, H, F, U, T. I want the first row that has STATUS, DATE, and AMOUNT filled out (with STATUS in order). Here is the example output data:



I'm really stuck on the order issue and the code I've written isn't producing any data at all (but no errors). 

SAMPLE CODE:

CREATE VOLATILE TABLE new_tble
AS
(SELECT * 
FROM table
QUALIFY row_number() OVER (partition BY ID ORDER BY ID, DATE, AMOUNT)=1
WHERE DELETE &lt;&gt; 'D'
)
with data;

",3
280,46096923,Python - Teradata package cannot connect to ODBC after changing password,"I have used the teradata package for ~6 months now and have changed my password before without issue. After my most recent password change (via ODBC administrator), I am able to access the server via Teradata SQL Assistant. However, when I attempt to connect in python, I am having the following error:

Code:

import teradata

udaExec = teradata.UdaExec(appName=""HelloWorld"", version=""1.0"", logConsole=False)

udaExec.connect(method=""odbc"", dsn=""my_dsn_name_example"")


Error Message #1:

DatabaseError: [HY000] [Teradata][ODBC Teradata Driver] Loading the Teradata ICU Library Failed. Error is: 126, The driver returned invalid (or failed to return) SQL_DRIVER_ODBC_VER: 03.80



When I explicitly define my username and password in my udaExec.connect function, I get the same error.
When I enter an intentionally incorrect username and password into udaExec.connect, it gets the same error. Thus, I can conclude that the username and password are not being passed correctly.


I have tried deleting the connection in ODBC administrator, and get the following error code after running my code when the connection is deleted:

Error Message #2:

DatabaseError: [IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified


This demonstrates that the teradata package is in fact referencing this ODBC driver, but when I re-add the username and password, I get error message #1 again.

I have tried uninstalling and reinstalling the teradata package, but still receive the same error.
",-1,-1,-1.0,"I have used the teradata package for ~6 months now and have changed my password before without issue. After my most recent password change (via ODBC administrator), I am able to access the server via Teradata SQL Assistant. However, when I attempt to connect in python, I am having the following error:

Code:

import teradata

udaExec = teradata.UdaExec(appName=""HelloWorld"", version=""1.0"", logConsole=False)

udaExec.connect(method=""odbc"", dsn=""my_dsn_name_example"")


Error Message #1:

DatabaseError: [HY000] [Teradata][ODBC Teradata Driver] Loading the Teradata ICU Library Failed. Error is: 126, The driver returned invalid (or failed to return) SQL_DRIVER_ODBC_VER: 03.80



When I explicitly define my username and password in my udaExec.connect function, I get the same error.
When I enter an intentionally incorrect username and password into udaExec.connect, it gets the same error. Thus, I can conclude that the username and password are not being passed correctly.


I have tried deleting the connection in ODBC administrator, and get the following error code after running my code when the connection is deleted:

Error Message #2:

DatabaseError: [IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified


This demonstrates that the teradata package is in fact referencing this ODBC driver, but when I re-add the username and password, I get error message #1 again.

I have tried uninstalling and reinstalling the teradata package, but still receive the same error.
",1
281,46146971,Whitespaces coming in while exporting data from Teradata BTEQ file,"I have a BTEQ script which I'm running from shell (Ksh). Aim is to export the contents of a Teradata table to a .csv file. Problem is while exporting data too many white spaces are being introduced between columns. I have tried
 1. Trimming individual columns
 2. Using Cast to convert each column datatype in Char
but none of this seems to help.
BTEQ code looks something like this (I have used REPORT file since I need file headers.

.EXPORT REPORT FILE = exportfilepath.csv;
.SET SEPARATOR "","";
.SET TITLEDASHES OFF;
.set RECORDMODE OFF;
.set width 65531;
.SET ERRORLEVEL 3807 SEVERITY 0

select 
  trim('""' || trim(cast(col1 as char(256))) || '""') AS col1,
  trim('""' || trim(cast(col2 as char(256))) || '""') AS col2,
  trim(cast(col3 as INTEGER)) AS col3,
  trim(cast(col4 as char(6))) AS  col4,
  trim(col5) AS col5,
  trim(cast(col6 as decimal(18,2)) AS col6,
  trim(date) AS date
from table A;


Col1 and Col2 are having lot of white spaces between them.Any help as to how I can remove those white spaces. What else can I do in this case? I cannot decrease the char size since these are names with variable sizes.

I have added '""' here because col1 and col2 are names with comma in between them. Since the exported .csv file needs to phrased the format is not proper. 
",-1,-1,-1.0,"I have a BTEQ script which I'm running from shell (Ksh). Aim is to export the contents of a Teradata table to a .csv file. Problem is while exporting data too many white spaces are being introduced between columns. I have tried
 1. Trimming individual columns
 2. Using Cast to convert each column datatype in Char
but none of this seems to help.
BTEQ code looks something like this (I have used REPORT file since I need file headers.

.EXPORT REPORT FILE = exportfilepath.csv;
.SET SEPARATOR "","";
.SET TITLEDASHES OFF;
.set RECORDMODE OFF;
.set width 65531;
.SET ERRORLEVEL 3807 SEVERITY 0

select 
  trim('""' || trim(cast(col1 as char(256))) || '""') AS col1,
  trim('""' || trim(cast(col2 as char(256))) || '""') AS col2,
  trim(cast(col3 as INTEGER)) AS col3,
  trim(cast(col4 as char(6))) AS  col4,
  trim(col5) AS col5,
  trim(cast(col6 as decimal(18,2)) AS col6,
  trim(date) AS date
from table A;


Col1 and Col2 are having lot of white spaces between them.Any help as to how I can remove those white spaces. What else can I do in this case? I cannot decrease the char size since these are names with variable sizes.

I have added '""' here because col1 and col2 are names with comma in between them. Since the exported .csv file needs to phrased the format is not proper. 
",3
282,46160491,Optimize Teradata Query multiple table conditional join,"I am using Teradata to do the following.
Say I have the following Table1

col1  col2 col3 col4   col5
1     A    NULL  NULL  D
2     B    NULL  NULL  C    
3     A    B     NULL  D
4     A    B     C     D


and Table2

col1  col2 col3 col4
1     A    D    27
2     B    C    334    
3     A    B    434
4     B    D    100
5     C    D    200


I want to join Table1 and Table2 (will be 3 times) so that I can create a table like this

col1  col2 col3 col4   col5  col_val_1   col_val_2 col_val_3
1     A    NULL  NULL  D     27           NULL      NULL
2     B    NULL  NULL  C     334          NULL      NULL
3     A    B     NULL  D     434          100       NULL
4     A    B     C     D     434          334       200


I can create this table by the following code

select tab1.*, tab2_1.col4 as col_val_1, tab2_2.col4 as col_cal_2, tab2_3.col4 as col_val_3
from Table1 tab1
left outer join Table2 tab2_1
on tab2_1.col2 = tab1.col2
and tab2_1.col3 = coalesce(tab1.col3,tab1.col5) /* if col3 is Null then join on col5. I want to calculate pair wise value. If col3 is NULL, the pair is col2-col5.*/
left outer join Table2 tab2_2
on tab2_2.col2 = coalesce(tab1.col3,0)
and tab2_2.col3 = coalesce(tab1.col4, tab1.col5)
left outer join Table2 tab2_3
on tab2_3.col2 = coalesce(tab1.col4,0)
and tab2_3.col3 = tab1.col5


The Table1 data is such that if col3 is null col4 will be null. col2 and col5 are never null. So if col3 is null I will have col2-col5. If col3 is not null and col4 is null then I will have col2-col3, col3-col5. If nothing is null then I will have col2-col3, col3-col4, col4-col5.

This query runs for a small table and gives the desired output. However, this is a complex query. I ran EXPLAIN on this and the estimated runtime is in &gt;10^5 hours. I was wondering if there is a way. This query can be optimized.
",-1,1,-1.0,"I am using Teradata to do the following.
Say I have the following Table1

col1  col2 col3 col4   col5
1     A    NULL  NULL  D
2     B    NULL  NULL  C    
3     A    B     NULL  D
4     A    B     C     D


and Table2

col1  col2 col3 col4
1     A    D    27
2     B    C    334    
3     A    B    434
4     B    D    100
5     C    D    200


I want to join Table1 and Table2 (will be 3 times) so that I can create a table like this

col1  col2 col3 col4   col5  col_val_1   col_val_2 col_val_3
1     A    NULL  NULL  D     27           NULL      NULL
2     B    NULL  NULL  C     334          NULL      NULL
3     A    B     NULL  D     434          100       NULL
4     A    B     C     D     434          334       200


I can create this table by the following code

select tab1.*, tab2_1.col4 as col_val_1, tab2_2.col4 as col_cal_2, tab2_3.col4 as col_val_3
from Table1 tab1
left outer join Table2 tab2_1
on tab2_1.col2 = tab1.col2
and tab2_1.col3 = coalesce(tab1.col3,tab1.col5) /* if col3 is Null then join on col5. I want to calculate pair wise value. If col3 is NULL, the pair is col2-col5.*/
left outer join Table2 tab2_2
on tab2_2.col2 = coalesce(tab1.col3,0)
and tab2_2.col3 = coalesce(tab1.col4, tab1.col5)
left outer join Table2 tab2_3
on tab2_3.col2 = coalesce(tab1.col4,0)
and tab2_3.col3 = tab1.col5


The Table1 data is such that if col3 is null col4 will be null. col2 and col5 are never null. So if col3 is null I will have col2-col5. If col3 is not null and col4 is null then I will have col2-col3, col3-col5. If nothing is null then I will have col2-col3, col3-col4, col4-col5.

This query runs for a small table and gives the desired output. However, this is a complex query. I ran EXPLAIN on this and the estimated runtime is in &gt;10^5 hours. I was wondering if there is a way. This query can be optimized.
",3
283,46197598,Join produces duplicated key (column) in Teradata SQL,"In Teradata SQL I have two volatile tables that I would like to join. However, if I join them on colA, they both show up in the output. I just want this colA once in the output, not twice.

So I have:

Table1 with colA, colB, colC
Table2 with colA, colD, colE


My query:

SELECT * FROM Table1
JOIN Table2 ON Table1.colA = Table2.colA


gives me: colA, colB, colC, colA, colD, colE 

while I would like to have: colA, colB, colC, colD, colE

Anyone knows how to solve this?
",0,-1,-1.0,"In Teradata SQL I have two volatile tables that I would like to join. However, if I join them on colA, they both show up in the output. I just want this colA once in the output, not twice.

So I have:

Table1 with colA, colB, colC
Table2 with colA, colD, colE


My query:

SELECT * FROM Table1
JOIN Table2 ON Table1.colA = Table2.colA


gives me: colA, colB, colC, colA, colD, colE 

while I would like to have: colA, colB, colC, colD, colE

Anyone knows how to solve this?
",3
284,46210424,How to search a numeric column in teradata sql?,"I have a column in Teradata called updated with the format '01/02/2015'
Normally we can use a where updated like '%[value]%' to find a match. This does not seem to work with dates because it is a numeric column. 

How to use LIKE operator in Teradata SQL to get list of updates by year?

I would like to get all the records for a particular year. In my cas Like '%2015%' does not work. 
",-1,-1,-1.0,"I have a column in Teradata called updated with the format '01/02/2015'
Normally we can use a where updated like '%[value]%' to find a match. This does not seem to work with dates because it is a numeric column. 

How to use LIKE operator in Teradata SQL to get list of updates by year?

I would like to get all the records for a particular year. In my cas Like '%2015%' does not work. 
",3
285,46351511,SQL/Teradata: How to return specific values and their preceding rows?,"I have a SQL (Teradata) problem that I haven't been able to work through.  I know the answer is probably much simpler than I'm making it seem.  

I have a group of code like this:

ID      timestamp         location      event_type
1111    20160601-0112     Detroit       Event A
1111    20160602-0954     Brooklyn      Event B
1111    20160602-1123     Brooklyn      Event A
1112    20160912-1420     Minneapolis   Event B
1113    20161123-1742     New Orleans   Event A
1113    20161124-1841     New Orleans   Event A
1113    20161124-2100     New Orleans   Event B
1114    20170201-0959     Detroit       Event A
1114    20170201-2350     Detroit       Event A


Here are the conditions for what I need to return:

I want to return the FIRST event B per ID, and the nearest Event A that happened BEFORE that event B (based on timestamp).  Therefore, for the dataset above, I would get:

ID      timestamp         location      event_type
1111    20160601-0112     Detroit       Event A
1111    20160602-0954     Brooklyn      Event B
1113    20161124-1841     New Orleans   Event A
1113    20161124-2100     New Orleans   Event B


The third record for 1111 doesn't get returned because it happened after the Event B.  ID 1112 doesn't get returned because it doesn't have an Event A preceding it.  The first record of 1113 doesn't get returned because there is  closer event A (to the event B) after it.  1114 doesn't get returned because there is no Event B.  

I have been working on this for hours to the point where I'm no longer approaching it clearly...any help would be greatly appreciated!  
",1,-1,-1.0,"I have a SQL (Teradata) problem that I haven't been able to work through.  I know the answer is probably much simpler than I'm making it seem.  

I have a group of code like this:

ID      timestamp         location      event_type
1111    20160601-0112     Detroit       Event A
1111    20160602-0954     Brooklyn      Event B
1111    20160602-1123     Brooklyn      Event A
1112    20160912-1420     Minneapolis   Event B
1113    20161123-1742     New Orleans   Event A
1113    20161124-1841     New Orleans   Event A
1113    20161124-2100     New Orleans   Event B
1114    20170201-0959     Detroit       Event A
1114    20170201-2350     Detroit       Event A


Here are the conditions for what I need to return:

I want to return the FIRST event B per ID, and the nearest Event A that happened BEFORE that event B (based on timestamp).  Therefore, for the dataset above, I would get:

ID      timestamp         location      event_type
1111    20160601-0112     Detroit       Event A
1111    20160602-0954     Brooklyn      Event B
1113    20161124-1841     New Orleans   Event A
1113    20161124-2100     New Orleans   Event B


The third record for 1111 doesn't get returned because it happened after the Event B.  ID 1112 doesn't get returned because it doesn't have an Event A preceding it.  The first record of 1113 doesn't get returned because there is  closer event A (to the event B) after it.  1114 doesn't get returned because there is no Event B.  

I have been working on this for hours to the point where I'm no longer approaching it clearly...any help would be greatly appreciated!  
",3
286,46410799,How to write a SQL query in Teradata that returns count of transactions by store per month?,"I am trying to write a query in Teradata that will return the count of the number of transactions that occurred in each store by month. The main problem I am finding is that transaction dates are stored as dates (obviously) and I want to group them by month-year. Below is my attempt at the query.

SELECT
COUNT(txn_nbr),
str_nbr,
EXTRACT(MONTH FROM txn_dt) AS txn_month, EXTRACT (YEAR FROM txn_dt) AS txn_year

FROM tbl_name

WHERE str_nbr IN (xxxx, xxxx, xxxx)
AND fill_sold_dt BETWEEN '2016-12-31' AND '2017-08-31'
GROUP BY  4,3


The error message I'm getting is ""SELECT Failed. 3504: Selected non-aggregate must be part of the associated group."" Please, could anyone assist me in properly writing the query? I am not that experienced in SQL and I would greatly appreciate it. 
",-1,-1,-1.0,"I am trying to write a query in Teradata that will return the count of the number of transactions that occurred in each store by month. The main problem I am finding is that transaction dates are stored as dates (obviously) and I want to group them by month-year. Below is my attempt at the query.

SELECT
COUNT(txn_nbr),
str_nbr,
EXTRACT(MONTH FROM txn_dt) AS txn_month, EXTRACT (YEAR FROM txn_dt) AS txn_year

FROM tbl_name

WHERE str_nbr IN (xxxx, xxxx, xxxx)
AND fill_sold_dt BETWEEN '2016-12-31' AND '2017-08-31'
GROUP BY  4,3


The error message I'm getting is ""SELECT Failed. 3504: Selected non-aggregate must be part of the associated group."" Please, could anyone assist me in properly writing the query? I am not that experienced in SQL and I would greatly appreciate it. 
",3
287,46467775,Angular 4.0 Teradata ng-template with ngFor,"I'm using the td-data-table from the teradata/covalent framework but independet from the framework I'd like to use *ngFor in an  like this:

&lt;ng-template *ngFor=""let column of columns"" tdDataTableTemplate=""{{column.name}}"" let-value=""value""&gt;
    &lt;div&gt;
        &lt;span&gt;{{value.content}}&lt;/span&gt;
        &lt;br&gt;
        &lt;span *ngIf=""enabler""&gt;{{value.subContent}}&lt;/span&gt;
    &lt;/div&gt;
&lt;/ng-template&gt;


I've read that the syntax for using *ngFor with an  is different, so I've tried this:

&lt;ng-template ngFor let-column [ngForOf]=""columns"" tdDataTableTemplate=""{{column.name}}"" let-value=""value""&gt;
    &lt;div&gt;
        &lt;span&gt;{{value.content}}&lt;/span&gt;
        &lt;br&gt;
        &lt;span *ngIf=""enabler""&gt;{{value.subContent}}&lt;/span&gt;
    &lt;/div&gt;
&lt;/ng-template&gt;


Is it the framework which is preventing the code to work or have I written something wrong?
",-1,-1,-1.0,"I'm using the td-data-table from the teradata/covalent framework but independet from the framework I'd like to use *ngFor in an  like this:

&lt;ng-template *ngFor=""let column of columns"" tdDataTableTemplate=""{{column.name}}"" let-value=""value""&gt;
    &lt;div&gt;
        &lt;span&gt;{{value.content}}&lt;/span&gt;
        &lt;br&gt;
        &lt;span *ngIf=""enabler""&gt;{{value.subContent}}&lt;/span&gt;
    &lt;/div&gt;
&lt;/ng-template&gt;


I've read that the syntax for using *ngFor with an  is different, so I've tried this:

&lt;ng-template ngFor let-column [ngForOf]=""columns"" tdDataTableTemplate=""{{column.name}}"" let-value=""value""&gt;
    &lt;div&gt;
        &lt;span&gt;{{value.content}}&lt;/span&gt;
        &lt;br&gt;
        &lt;span *ngIf=""enabler""&gt;{{value.subContent}}&lt;/span&gt;
    &lt;/div&gt;
&lt;/ng-template&gt;


Is it the framework which is preventing the code to work or have I written something wrong?
",3
288,46521944,Cannot add teradata.client.provider.dll reference in vs2010,"I Try to add Teradata.client.provider.dll in a project in vs2017 .net2. and compile, I recieve an error: 
The primary reference ""Teradata.Client.Provider, Version=14.11.0.0, Culture=neutral, PublicKeyToken=76b417ee2e04956c, processorArchitecture=MSIL"" could not be resolved because it has an indirect dependency on the framework assembly ""System.Core, Version=3.5.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089"" which could not be resolved in the currently targeted framework. "".NETFramework,Version=v2.0"". To resolve this problem, either remove the reference ""Teradata.Client.Provider, Version=14.11.0.0, Culture=neutral, PublicKeyToken=76b417ee2e04956c, processorArchitecture=MSIL"" or retarget your application to a framework version which contains ""System.Core, Version=3.5.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089"".

Strangely in vs2005 .net2 There is no such problem .

dll version is 14.11.0.0
Its runtime version is v2.0.50727
",-1,-1,-1.0,"I Try to add Teradata.client.provider.dll in a project in vs2017 .net2. and compile, I recieve an error: 
The primary reference ""Teradata.Client.Provider, Version=14.11.0.0, Culture=neutral, PublicKeyToken=76b417ee2e04956c, processorArchitecture=MSIL"" could not be resolved because it has an indirect dependency on the framework assembly ""System.Core, Version=3.5.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089"" which could not be resolved in the currently targeted framework. "".NETFramework,Version=v2.0"". To resolve this problem, either remove the reference ""Teradata.Client.Provider, Version=14.11.0.0, Culture=neutral, PublicKeyToken=76b417ee2e04956c, processorArchitecture=MSIL"" or retarget your application to a framework version which contains ""System.Core, Version=3.5.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089"".

Strangely in vs2005 .net2 There is no such problem .

dll version is 14.11.0.0
Its runtime version is v2.0.50727
",1
289,46546838,"Teradata Studio Express: new install on Mac, not working","I just installed Teradata Express Studio (16.10.01) on MacOS Sierra 10.12.6, but when I try to open it I get the message 'An error has occurred'. The log file shows this (just first lines):

    !SESSION 2017-09-28 15:55:21.661 -----------------------------------------------
    eclipse.buildId=unknown
    java.version=9
java.vendor=Oracle Corporation
BootLoader constants: OS=macosx, ARCH=x86_64, WS=cocoa, NL=en_GB
Framework arguments:  -keyring /Users/KULMAK/.eclipse_keyring
Command-line arguments:  -os macosx -ws cocoa -arch x86_64 -keyring /Users/KULMAK/.eclipse_keyring

!ENTRY org.eclipse.equinox.ds 4 0 2017-09-28 15:55:26.460
!MESSAGE Exception occurred while creating new instance of component Component[
    name = ConnectionService
    activate = activate
    deactivate = deactivate
    modified = 
    configuration-policy = optional
    factory = null
    autoenable = true
    immediate = false
    implementation = com.teradata.datatools.connection.services.impl.ConnectionService
    state = Unsatisfied
    properties = 
    serviceFactory = false
    serviceInterface = [com.teradata.datatools.connection.services.interfaces.IConnectionService]
    references = null
    located in bundle = com.teradata.datatools.connection.services.definition_15.11.0.201708101349 [29]
] 
!STACK 0
java.lang.NoClassDefFoundError: java/sql/SQLException
    at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)
    at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3110)
    at java.base/java.lang.Class.getConstructor0(Class.java:3315)
    at java.base/java.lang.Class.newInstance(Class.java:530)
    at org.eclipse.equinox.internal.ds.model.ServiceComponent.createInstance(ServiceComponent.java:493)


Prior to that I installed not-the-latest version of Java JDK 8 (Java 1.8), for compatibility reasons with other pieces of software - not that it matters, even if I install JDK 9, the issues remain the same.    

Any ideas what could be causing issues?
Thanks for help!
",-1,-1,-1.0,"I just installed Teradata Express Studio (16.10.01) on MacOS Sierra 10.12.6, but when I try to open it I get the message 'An error has occurred'. The log file shows this (just first lines):

    !SESSION 2017-09-28 15:55:21.661 -----------------------------------------------
    eclipse.buildId=unknown
    java.version=9
java.vendor=Oracle Corporation
BootLoader constants: OS=macosx, ARCH=x86_64, WS=cocoa, NL=en_GB
Framework arguments:  -keyring /Users/KULMAK/.eclipse_keyring
Command-line arguments:  -os macosx -ws cocoa -arch x86_64 -keyring /Users/KULMAK/.eclipse_keyring

!ENTRY org.eclipse.equinox.ds 4 0 2017-09-28 15:55:26.460
!MESSAGE Exception occurred while creating new instance of component Component[
    name = ConnectionService
    activate = activate
    deactivate = deactivate
    modified = 
    configuration-policy = optional
    factory = null
    autoenable = true
    immediate = false
    implementation = com.teradata.datatools.connection.services.impl.ConnectionService
    state = Unsatisfied
    properties = 
    serviceFactory = false
    serviceInterface = [com.teradata.datatools.connection.services.interfaces.IConnectionService]
    references = null
    located in bundle = com.teradata.datatools.connection.services.definition_15.11.0.201708101349 [29]
] 
!STACK 0
java.lang.NoClassDefFoundError: java/sql/SQLException
    at java.base/java.lang.Class.getDeclaredConstructors0(Native Method)
    at java.base/java.lang.Class.privateGetDeclaredConstructors(Class.java:3110)
    at java.base/java.lang.Class.getConstructor0(Class.java:3315)
    at java.base/java.lang.Class.newInstance(Class.java:530)
    at org.eclipse.equinox.internal.ds.model.ServiceComponent.createInstance(ServiceComponent.java:493)


Prior to that I installed not-the-latest version of Java JDK 8 (Java 1.8), for compatibility reasons with other pieces of software - not that it matters, even if I install JDK 9, the issues remain the same.    

Any ideas what could be causing issues?
Thanks for help!
",0
290,46567750,Fetching all data in a row from teradata using unix,"I am trying to fetch all data in a row in teradata with unix

i am not getting all the data in a row. It is only EXPORTING half of the data into txt file in each row

I am using

bteq &lt;&lt;EOF
.logmech ldap
.logon IP/user_name,password
.EXPORT REPORT FILE=test.txt
    .SET RECORDMODE OFF;
    .SET WIDTH 65531;
.Set Titledashes off;

 select requesttext(title '') from abc.tb 


.export reset; 
.LOGOFF;
.EXIT;
EOF


I have used maximum width but still i am not able to fetch entire row it is only fetching half of the row.
My each row has huge data
",-1,-1,-1.0,"I am trying to fetch all data in a row in teradata with unix

i am not getting all the data in a row. It is only EXPORTING half of the data into txt file in each row

I am using

bteq &lt;&lt;EOF
.logmech ldap
.logon IP/user_name,password
.EXPORT REPORT FILE=test.txt
    .SET RECORDMODE OFF;
    .SET WIDTH 65531;
.Set Titledashes off;

 select requesttext(title '') from abc.tb 


.export reset; 
.LOGOFF;
.EXIT;
EOF


I have used maximum width but still i am not able to fetch entire row it is only fetching half of the row.
My each row has huge data
",3
291,46696602,How use Qualify row_number in teradata,"how are you?

I never used qualify row_number()/rank() and I have some questions.

I am trying this query in teradata:

select sit_site_id
     , count(distinct shp_shipment_id) 
from WHOWNER.BT_SHP_SHIPMENTS
group by sit_site_id
QUALIFY RANK() OVER (PARTITION BY sit_site_id 
                     ORDER BY count(distinct shp_shipment_id) ) = 3 


But the result is: 'No data available in table'.

I want to get the first 3 sit_site_id values with more shp_shipment_id.

Where is my error?

Thanks!
",-1,-1,-1.0,"how are you?

I never used qualify row_number()/rank() and I have some questions.

I am trying this query in teradata:

select sit_site_id
     , count(distinct shp_shipment_id) 
from WHOWNER.BT_SHP_SHIPMENTS
group by sit_site_id
QUALIFY RANK() OVER (PARTITION BY sit_site_id 
                     ORDER BY count(distinct shp_shipment_id) ) = 3 


But the result is: 'No data available in table'.

I want to get the first 3 sit_site_id values with more shp_shipment_id.

Where is my error?

Thanks!
",3
292,46795295,Select a variable to output from within stored procedure in Teradata,"I have a stored procedure I've started coding and need to return a value.  In SQL Server I could just do a SELECT of the variable to return it.  However this does not seem to work with Teradata and have not found a similar example on how to do this.  Here is my stored procedure:

REPLACE PROCEDURE sp_Get_MyValue()
BEGIN
DECLARE mytestvar  VARCHAR(40);
SELECT mycolumn INTO mytestvar FROM MyTable;
SELECT mytestvar;
END;


I get this error:


  STATEMENT 2: REPLACE  failed.  Failed [5526 : HY000] Stored Procedure
  is not created/replaced due to error(s).{Nested Failure Msg [5526 :
  HY000] SPL1045:E(L10), Invalid or missing INTO clause.}


I also tried adding an OUT variable to the procedure but that did not work either:

REPLACE PROCEDURE sp_Get_MyValue(mytestvarout VARCHAR(40))
BEGIN
DECLARE mytestvar  VARCHAR(40);
SELECT mycolumn INTO mytestvar FROM MyTable;
END;


With this error:


  Executed as Single statement.  Failed [5531 : HY000] Named-list is not
  supported for arguments of a procedure.  Elapsed time = 00:00:00.079

",-1,-1,-1.0,"I have a stored procedure I've started coding and need to return a value.  In SQL Server I could just do a SELECT of the variable to return it.  However this does not seem to work with Teradata and have not found a similar example on how to do this.  Here is my stored procedure:

REPLACE PROCEDURE sp_Get_MyValue()
BEGIN
DECLARE mytestvar  VARCHAR(40);
SELECT mycolumn INTO mytestvar FROM MyTable;
SELECT mytestvar;
END;


I get this error:


  STATEMENT 2: REPLACE  failed.  Failed [5526 : HY000] Stored Procedure
  is not created/replaced due to error(s).{Nested Failure Msg [5526 :
  HY000] SPL1045:E(L10), Invalid or missing INTO clause.}


I also tried adding an OUT variable to the procedure but that did not work either:

REPLACE PROCEDURE sp_Get_MyValue(mytestvarout VARCHAR(40))
BEGIN
DECLARE mytestvar  VARCHAR(40);
SELECT mycolumn INTO mytestvar FROM MyTable;
END;


With this error:


  Executed as Single statement.  Failed [5531 : HY000] Named-list is not
  supported for arguments of a procedure.  Elapsed time = 00:00:00.079

",3
293,46979222,Teradata String date to Date type change,"I have a Teradata query which I am running using SAS. Once of the Teradata fields I am trying to read has a series of digits which is in string format that basically refers to a date.
In the Teradata Field the value is 170919 which mean 2017-09-19.
I am unable to convert this value into a valid datetype.
Can you please help.

proc sql;
    connect to teradata (schema=&amp;terasilo user=&amp;terauser password=&amp;terapass tdpid=&amp;teradbase);
        create table COL_ASPECT_CALLS_2 as
            select * from connection to teradata(
                select 
                top 10 *
                from &amp;&amp;terasilo..DMI_COL_ASPECT_CALLS 
                where CAST(PROD_DATE_CH AS DATE FORMAT 'yymmdd')='2017-09-19'

                                          );
    disconnect from teradata;       
quit;

",1,-1,-1.0,"I have a Teradata query which I am running using SAS. Once of the Teradata fields I am trying to read has a series of digits which is in string format that basically refers to a date.
In the Teradata Field the value is 170919 which mean 2017-09-19.
I am unable to convert this value into a valid datetype.
Can you please help.

proc sql;
    connect to teradata (schema=&amp;terasilo user=&amp;terauser password=&amp;terapass tdpid=&amp;teradbase);
        create table COL_ASPECT_CALLS_2 as
            select * from connection to teradata(
                select 
                top 10 *
                from &amp;&amp;terasilo..DMI_COL_ASPECT_CALLS 
                where CAST(PROD_DATE_CH AS DATE FORMAT 'yymmdd')='2017-09-19'

                                          );
    disconnect from teradata;       
quit;

",3
294,46415600,Teradata : Error while converting Interval HOUR(2) TO MINUTE,"I have a Teradata table where in a column the value is integer number and I have to convert it to HOUR(2) TO MINUTE.

I have tried the below statement but getting error like Interval field overflow

CAST(CAST(col1 AS DECIMAL(18,0)) * INTERVAL '0000:01' MINUTE TO SECOND AS INTERVAL HOUR(2) TO MINUTE) from table1


col1 contains integer value like 5192 or 8734 etc.
",-1,-1,-1.0,"I have a Teradata table where in a column the value is integer number and I have to convert it to HOUR(2) TO MINUTE.

I have tried the below statement but getting error like Interval field overflow

CAST(CAST(col1 AS DECIMAL(18,0)) * INTERVAL '0000:01' MINUTE TO SECOND AS INTERVAL HOUR(2) TO MINUTE) from table1


col1 contains integer value like 5192 or 8734 etc.
",3
295,45487476,Unable to connect Teradata server with authentication mechanism as LDAP,"It would be a great help if anyone provides a VBA connection string to connect Excel with Teradata where Authentication mechanism = LDAP.

VBA code used before configuring Authentication mechanism = LDAP is given below. But the code doesn't work when my organisation moved our credentials to support only AUTHENTICATION MECHANISM = LDAP.
Please help!

Code:

Public cn As ADODB.Connection
Public rs As ADODB.Connection

Sub test_ter() 
    TDCONSTR1 = ""DSN &amp;_ =&lt;dsn_name&gt; ; uid = &lt;usr&gt;; PWD = &lt;PWD&gt;;"" 
    Set cn = new ADODB.Connection     
    cn.Open TDCONSTR1
    MsgBox(""Connection established"")
End Sub

",1,-1,-1.0,"It would be a great help if anyone provides a VBA connection string to connect Excel with Teradata where Authentication mechanism = LDAP.

VBA code used before configuring Authentication mechanism = LDAP is given below. But the code doesn't work when my organisation moved our credentials to support only AUTHENTICATION MECHANISM = LDAP.
Please help!

Code:

Public cn As ADODB.Connection
Public rs As ADODB.Connection

Sub test_ter() 
    TDCONSTR1 = ""DSN &amp;_ =&lt;dsn_name&gt; ; uid = &lt;usr&gt;; PWD = &lt;PWD&gt;;"" 
    Set cn = new ADODB.Connection     
    cn.Open TDCONSTR1
    MsgBox(""Connection established"")
End Sub

",1
296,45481974,"What defines a Teradata ""data definition statement""","The reason for my question is I have a SQL script which builds a result in a number of stages, using volatile tables (I don't have the permissions to create regular tables). I create my first volatile table to try and keep parts of the script quite modular, and then the second table uses the result set of the first, the third the second and then so on.

I'm then using Tableau to build a visualisation of my data, putting the create volatile table sections of the script in the Initial SQL portion, and then I'm just doing a select from the volatile table in my request. The problem is the Teradata ODBC client returns the error:


  3576   Data definition not valid unless solitary


I don't have this issue every time I try this, so I'm wondering what it is that's prompting this to occur, so I'm wondering, what counts as a ""data definition statement"" in Teradata. Sometimes I use this approach and it works fine, others do not, and I can't seem to figure it out.
",-1,-1,-1.0,"The reason for my question is I have a SQL script which builds a result in a number of stages, using volatile tables (I don't have the permissions to create regular tables). I create my first volatile table to try and keep parts of the script quite modular, and then the second table uses the result set of the first, the third the second and then so on.

I'm then using Tableau to build a visualisation of my data, putting the create volatile table sections of the script in the Initial SQL portion, and then I'm just doing a select from the volatile table in my request. The problem is the Teradata ODBC client returns the error:


  3576   Data definition not valid unless solitary


I don't have this issue every time I try this, so I'm wondering what it is that's prompting this to occur, so I'm wondering, what counts as a ""data definition statement"" in Teradata. Sometimes I use this approach and it works fine, others do not, and I can't seem to figure it out.
",3
297,47233040,ExceptionInInitializer Error while Reading Data from teradata table using Spark,"I am using the below code to read data from teradata but getting error

val jdbcDF = spark.read
  .format(""jdbc"")
  .option(""url"",s""jdbc:teradata://${TeradataDBHost}/database=${TeradataDBDatabase}"")
  .option(""dbtable"", TeradataDBDatabase+"".""+TeradataDBTable)
  .option(""driver"",""com.teradata.jdbc.TeraDriver"")
  .option(""user"", TeradataDBUsername)
  .option(""password"", TeradataDBPassword)
  .load()


Error Stack Trace

Exception in thread ""main"" java.lang.ExceptionInInitializerError
            at com.teradata.jdbc.jdbc.GenericTeraEncrypt.getGSSM(GenericTeraEncrypt.java:577)
            at com.teradata.jdbc.jdbc.GenericTeraEncrypt.&lt;init&gt;(GenericTeraEncrypt.java:116)
            at com.teradata.jdbc.jdbc.GenericTeradataConnection.&lt;init&gt;(GenericTeradataConnection.java:107)
            at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:186)
            at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:36)
            at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)

Caused by: java.lang.NullPointerException
        at com.teradata.tdgss.jtdgss.TdgssConfigApi.GetMechanisms(Unknown Source)
        at com.teradata.tdgss.jtdgss.TdgssManager.&lt;init&gt;(Unknown Source)
        at com.teradata.tdgss.jtdgss.TdgssManager.&lt;clinit&gt;(Unknown Source)

",-1,-1,-1.0,"I am using the below code to read data from teradata but getting error

val jdbcDF = spark.read
  .format(""jdbc"")
  .option(""url"",s""jdbc:teradata://${TeradataDBHost}/database=${TeradataDBDatabase}"")
  .option(""dbtable"", TeradataDBDatabase+"".""+TeradataDBTable)
  .option(""driver"",""com.teradata.jdbc.TeraDriver"")
  .option(""user"", TeradataDBUsername)
  .option(""password"", TeradataDBPassword)
  .load()


Error Stack Trace

Exception in thread ""main"" java.lang.ExceptionInInitializerError
            at com.teradata.jdbc.jdbc.GenericTeraEncrypt.getGSSM(GenericTeraEncrypt.java:577)
            at com.teradata.jdbc.jdbc.GenericTeraEncrypt.&lt;init&gt;(GenericTeraEncrypt.java:116)
            at com.teradata.jdbc.jdbc.GenericTeradataConnection.&lt;init&gt;(GenericTeradataConnection.java:107)
            at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:186)
            at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:36)
            at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)

Caused by: java.lang.NullPointerException
        at com.teradata.tdgss.jtdgss.TdgssConfigApi.GetMechanisms(Unknown Source)
        at com.teradata.tdgss.jtdgss.TdgssManager.&lt;init&gt;(Unknown Source)
        at com.teradata.tdgss.jtdgss.TdgssManager.&lt;clinit&gt;(Unknown Source)

",0
298,47372139,Teradata CMICAutoscalingGroup Error When Using Cloud Formation in AWS,"We are receiving a CMICAutoscalingGroup group error when using Cloud Formation to spin up a Dev/Test environment.  We have tried this multiple times and validated that:


The VPC/subnet are configured correctly (Can see both in and out; We all opened all TCP ports)
Setup Security Group with TCP (and at the end set all to open)
Made sure that SNS, AutoScaling and other permissions as assigned as full for the user creating the environment
We have tried creating a new VPC and different versions of the database and different regions


The error occurs about 1/2 hour to 45 minutes into the stack starting.  Below are events from the log.s

Errors:
The following resource(s) failed to create: [CMICAutoscalingGroup]. . Rollback requested by user.
Received 0 SUCCESS signal(s) out of 1. Unable to satisfy 100% MinSuccessfulInstancesPercent requirement
Failed to receive 1 resource signal(s) for the current batch. Each resource signal timeout is counted as a FAILURE.

Logs:


  ROLLBACK_COMPLETE AWS::CloudFormation::Stack  TeradataEcosystemTestdevEnterprise
  DELETE_COMPLETE   AWS::IAM::Role  DBMppRole
  DELETE_IN_PROGRESS    AWS::IAM::Role  DBMppRole
  DELETE_COMPLETE   AWS::EC2::SecurityGroup DBSecurityGroup
  DELETE_COMPLETE   AWS::EC2::SecurityGroup EcosystemSecurityGroup
  DELETE_COMPLETE   AWS::IAM::InstanceProfile   DBMppInstanceProfile
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroup DBSecurityGroup
  DELETE_IN_PROGRESS    AWS::IAM::InstanceProfile   DBMppInstanceProfile
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroup EcosystemSecurityGroup
  DELETE_COMPLETE   AWS::AutoScaling::LaunchConfiguration   DBLaunchConfig
  DELETE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   DBLaunchConfig
  DELETE_COMPLETE   AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  DELETE_COMPLETE   AWS::IAM::Role  CMICRole
  DELETE_COMPLETE   AWS::EC2::SecurityGroup CMICSecurityGroup
  DELETE_IN_PROGRESS    AWS::IAM::Role  CMICRole
  DELETE_COMPLETE   AWS::IAM::InstanceProfile   CMICInstanceProfile
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroup CMICSecurityGroup
  DELETE_IN_PROGRESS    AWS::IAM::InstanceProfile   CMICInstanceProfile
  DELETE_COMPLETE   AWS::AutoScaling::LaunchConfiguration   CMICLaunchConfig
  DELETE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   CMICLaunchConfig
  DELETE_COMPLETE   AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup
  DELETE_COMPLETE   AWS::EC2::SecurityGroup VPSecurityGroup
  DELETE_COMPLETE   AWS::EC2::SecurityGroupIngress  DBSecurityGroupSelfRule
  DELETE_COMPLETE   AWS::EC2::SecurityGroupIngress  CMICSecurityGroupSelfRule
  DELETE_COMPLETE   AWS::EC2::SecurityGroupIngress  EcosystemSecurityGroupSelfRule
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroup VPSecurityGroup
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  DBSecurityGroupSelfRule
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  EcosystemSecurityGroupSelfRule
  DELETE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  DELETE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup
  DELETE_IN_PROGRESS  AWS::EC2::SecurityGroupIngress  CMICSecurityGroupSelfRule
  ROLLBACK_IN_PROGRESS  AWS::CloudFormation::Stack  TeradataEcosystemTestdevEnterprise
  CREATE_FAILED AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup
  UPDATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup CREATE_COMPLETE  AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  CREATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  CREATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup
  CREATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup
  CREATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  CREATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  CREATE_COMPLETE   AWS::AutoScaling::LaunchConfiguration   CMICLaunchConfig
  CREATE_COMPLETE   AWS::AutoScaling::LaunchConfiguration   DBLaunchConfig
  CREATE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   CMICLaunchConfig
  CREATE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   DBLaunchConfig
  CREATE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   DBLaunchConfig
  CREATE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   CMICLaunchConfig
  CREATE_COMPLETE   AWS::IAM::InstanceProfile   DBMppInstanceProfile
  CREATE_COMPLETE   AWS::IAM::InstanceProfile   CMICInstanceProfile
  CREATE_IN_PROGRESS    AWS::IAM::InstanceProfile   DBMppInstanceProfile
  CREATE_IN_PROGRESS    AWS::IAM::InstanceProfile   DBMppInstanceProfile
  CREATE_IN_PROGRESS    AWS::IAM::InstanceProfile   CMICInstanceProfile
  CREATE_IN_PROGRESS    AWS::IAM::InstanceProfile   CMICInstanceProfile
  CREATE_COMPLETE   AWS::IAM::Role  DBMppRole
  CREATE_COMPLETE   AWS::IAM::Role  CMICRole
  CREATE_COMPLETE   AWS::EC2::SecurityGroupIngress  CMICSecurityGroupSelfRule
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  CMICSecurityGroupSelfRule
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  CMICSecurityGroupSelfRule
  CREATE_COMPLETE   AWS::EC2::SecurityGroupIngress  EcosystemSecurityGroupSelfRule
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  EcosystemSecurityGroupSelfRule
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  EcosystemSecurityGroupSelfRule
  CREATE_COMPLETE   AWS::EC2::SecurityGroup CMICSecurityGroup
  CREATE_COMPLETE   AWS::EC2::SecurityGroupIngress  DBSecurityGroupSelfRule
  CREATE_COMPLETE   AWS::EC2::SecurityGroup VPSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  DBSecurityGroupSelfRule
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  DBSecurityGroupSelfRule
  CREATE_COMPLETE   AWS::EC2::SecurityGroup DBSecurityGroup
  CREATE_COMPLETE   AWS::EC2::SecurityGroup EcosystemSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup EcosystemSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup VPSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup DBSecurityGroup
  CREATE_IN_PROGRESS    AWS::IAM::Role  DBMppRole
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup CMICSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup EcosystemSecurityGroup
  CREATE_IN_PROGRESS    AWS::IAM::Role  CMICRole
  CREATE_IN_PROGRESS    AWS::IAM::Role  DBMppRole
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup VPSecurityGroup
  CREATE_IN_PROGRESS    AWS::IAM::Role  CMICRole
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup CMICSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup DBSecurityGroup
  CREATE_IN_PROGRESS    AWS::CloudFormation::Stack  TeradataEcosystemTestdevEnterprise

",-1,-1,-1.0,"We are receiving a CMICAutoscalingGroup group error when using Cloud Formation to spin up a Dev/Test environment.  We have tried this multiple times and validated that:


The VPC/subnet are configured correctly (Can see both in and out; We all opened all TCP ports)
Setup Security Group with TCP (and at the end set all to open)
Made sure that SNS, AutoScaling and other permissions as assigned as full for the user creating the environment
We have tried creating a new VPC and different versions of the database and different regions


The error occurs about 1/2 hour to 45 minutes into the stack starting.  Below are events from the log.s

Errors:
The following resource(s) failed to create: [CMICAutoscalingGroup]. . Rollback requested by user.
Received 0 SUCCESS signal(s) out of 1. Unable to satisfy 100% MinSuccessfulInstancesPercent requirement
Failed to receive 1 resource signal(s) for the current batch. Each resource signal timeout is counted as a FAILURE.

Logs:


  ROLLBACK_COMPLETE AWS::CloudFormation::Stack  TeradataEcosystemTestdevEnterprise
  DELETE_COMPLETE   AWS::IAM::Role  DBMppRole
  DELETE_IN_PROGRESS    AWS::IAM::Role  DBMppRole
  DELETE_COMPLETE   AWS::EC2::SecurityGroup DBSecurityGroup
  DELETE_COMPLETE   AWS::EC2::SecurityGroup EcosystemSecurityGroup
  DELETE_COMPLETE   AWS::IAM::InstanceProfile   DBMppInstanceProfile
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroup DBSecurityGroup
  DELETE_IN_PROGRESS    AWS::IAM::InstanceProfile   DBMppInstanceProfile
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroup EcosystemSecurityGroup
  DELETE_COMPLETE   AWS::AutoScaling::LaunchConfiguration   DBLaunchConfig
  DELETE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   DBLaunchConfig
  DELETE_COMPLETE   AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  DELETE_COMPLETE   AWS::IAM::Role  CMICRole
  DELETE_COMPLETE   AWS::EC2::SecurityGroup CMICSecurityGroup
  DELETE_IN_PROGRESS    AWS::IAM::Role  CMICRole
  DELETE_COMPLETE   AWS::IAM::InstanceProfile   CMICInstanceProfile
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroup CMICSecurityGroup
  DELETE_IN_PROGRESS    AWS::IAM::InstanceProfile   CMICInstanceProfile
  DELETE_COMPLETE   AWS::AutoScaling::LaunchConfiguration   CMICLaunchConfig
  DELETE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   CMICLaunchConfig
  DELETE_COMPLETE   AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup
  DELETE_COMPLETE   AWS::EC2::SecurityGroup VPSecurityGroup
  DELETE_COMPLETE   AWS::EC2::SecurityGroupIngress  DBSecurityGroupSelfRule
  DELETE_COMPLETE   AWS::EC2::SecurityGroupIngress  CMICSecurityGroupSelfRule
  DELETE_COMPLETE   AWS::EC2::SecurityGroupIngress  EcosystemSecurityGroupSelfRule
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroup VPSecurityGroup
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  DBSecurityGroupSelfRule
  DELETE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  EcosystemSecurityGroupSelfRule
  DELETE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  DELETE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup
  DELETE_IN_PROGRESS  AWS::EC2::SecurityGroupIngress  CMICSecurityGroupSelfRule
  ROLLBACK_IN_PROGRESS  AWS::CloudFormation::Stack  TeradataEcosystemTestdevEnterprise
  CREATE_FAILED AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup
  UPDATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup CREATE_COMPLETE  AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  CREATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  CREATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup
  CREATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  CMICAutoscalingGroup
  CREATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  CREATE_IN_PROGRESS    AWS::AutoScaling::AutoScalingGroup  DBAutoscalingGroup
  CREATE_COMPLETE   AWS::AutoScaling::LaunchConfiguration   CMICLaunchConfig
  CREATE_COMPLETE   AWS::AutoScaling::LaunchConfiguration   DBLaunchConfig
  CREATE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   CMICLaunchConfig
  CREATE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   DBLaunchConfig
  CREATE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   DBLaunchConfig
  CREATE_IN_PROGRESS    AWS::AutoScaling::LaunchConfiguration   CMICLaunchConfig
  CREATE_COMPLETE   AWS::IAM::InstanceProfile   DBMppInstanceProfile
  CREATE_COMPLETE   AWS::IAM::InstanceProfile   CMICInstanceProfile
  CREATE_IN_PROGRESS    AWS::IAM::InstanceProfile   DBMppInstanceProfile
  CREATE_IN_PROGRESS    AWS::IAM::InstanceProfile   DBMppInstanceProfile
  CREATE_IN_PROGRESS    AWS::IAM::InstanceProfile   CMICInstanceProfile
  CREATE_IN_PROGRESS    AWS::IAM::InstanceProfile   CMICInstanceProfile
  CREATE_COMPLETE   AWS::IAM::Role  DBMppRole
  CREATE_COMPLETE   AWS::IAM::Role  CMICRole
  CREATE_COMPLETE   AWS::EC2::SecurityGroupIngress  CMICSecurityGroupSelfRule
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  CMICSecurityGroupSelfRule
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  CMICSecurityGroupSelfRule
  CREATE_COMPLETE   AWS::EC2::SecurityGroupIngress  EcosystemSecurityGroupSelfRule
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  EcosystemSecurityGroupSelfRule
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  EcosystemSecurityGroupSelfRule
  CREATE_COMPLETE   AWS::EC2::SecurityGroup CMICSecurityGroup
  CREATE_COMPLETE   AWS::EC2::SecurityGroupIngress  DBSecurityGroupSelfRule
  CREATE_COMPLETE   AWS::EC2::SecurityGroup VPSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  DBSecurityGroupSelfRule
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroupIngress  DBSecurityGroupSelfRule
  CREATE_COMPLETE   AWS::EC2::SecurityGroup DBSecurityGroup
  CREATE_COMPLETE   AWS::EC2::SecurityGroup EcosystemSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup EcosystemSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup VPSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup DBSecurityGroup
  CREATE_IN_PROGRESS    AWS::IAM::Role  DBMppRole
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup CMICSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup EcosystemSecurityGroup
  CREATE_IN_PROGRESS    AWS::IAM::Role  CMICRole
  CREATE_IN_PROGRESS    AWS::IAM::Role  DBMppRole
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup VPSecurityGroup
  CREATE_IN_PROGRESS    AWS::IAM::Role  CMICRole
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup CMICSecurityGroup
  CREATE_IN_PROGRESS    AWS::EC2::SecurityGroup DBSecurityGroup
  CREATE_IN_PROGRESS    AWS::CloudFormation::Stack  TeradataEcosystemTestdevEnterprise

",2
299,47410247,Issue with Casting string to date in Teradata,"I have a string column - COL1 in TABLE1 which is if string data type. This table is loaded by Informatica session ( data coming from mainframe) and the format of the COL1 is YYYY-MM-DD. Now I have to use TABLE1 as the source in my next mapping . In the SQL override query of second mapping i will be casting COL1 to date using the below query . 

SELECT
CAST(COL1 AS DATE FORMAT 'YYYY-MM-DD') AS CHK_DT FROM TABLE1


But when i try to execute this query in Teradata SQLA, just to check if it runs fine it gives me below error. 

SELECT Failed. 2666:  Invalid date supplied for COL1.


Can you please help me resolve this issue ? This is not the only date column which has issue, there are two more date columns . I guess the resolution is same for all three columns . 

P.S - Just to verify, I updated all rows of COL1 of TABLE1 as 2016-12-12 and ran the select statement, select worked fine . I then updated COL1 of all rows as 2016-13-12, it gave same error . If either of DD or MM is more than 12, it is giving me error 

Thanks
",-1,-1,-1.0,"I have a string column - COL1 in TABLE1 which is if string data type. This table is loaded by Informatica session ( data coming from mainframe) and the format of the COL1 is YYYY-MM-DD. Now I have to use TABLE1 as the source in my next mapping . In the SQL override query of second mapping i will be casting COL1 to date using the below query . 

SELECT
CAST(COL1 AS DATE FORMAT 'YYYY-MM-DD') AS CHK_DT FROM TABLE1


But when i try to execute this query in Teradata SQLA, just to check if it runs fine it gives me below error. 

SELECT Failed. 2666:  Invalid date supplied for COL1.


Can you please help me resolve this issue ? This is not the only date column which has issue, there are two more date columns . I guess the resolution is same for all three columns . 

P.S - Just to verify, I updated all rows of COL1 of TABLE1 as 2016-12-12 and ran the select statement, select worked fine . I then updated COL1 of all rows as 2016-13-12, it gave same error . If either of DD or MM is more than 12, it is giving me error 

Thanks
",3
300,47440175,Teradata character to date conversion,"I have a string associated with date in ‘Teradata’ tables

Var1=09OCT2017-EMRT


I need to extract the date from the above string in ‘mm/dd/yyyy’ format

I tried the following

 Cast(cast(substr(var1,1,9) as char(20)) as date format    ‘mm/dd/yyyy’) as date


I am getting error as ‘invalid date supplied for var1’

I would appreciate your help
",-1,-1,-1.0,"I have a string associated with date in ‘Teradata’ tables

Var1=09OCT2017-EMRT


I need to extract the date from the above string in ‘mm/dd/yyyy’ format

I tried the following

 Cast(cast(substr(var1,1,9) as char(20)) as date format    ‘mm/dd/yyyy’) as date


I am getting error as ‘invalid date supplied for var1’

I would appreciate your help
",3
301,47655321,Unable to connect to Teradata through VBA macro with mechanism as LDAP,"Tried with the following code to connect to Teradata and it is working fine 

  Worksheets(""BO"").range(""A:C"").Clearcontents
  conn.Open ""Driver={Teradata};"" &amp; _
  ""DBCName="" &amp; TDServer &amp; "";"" &amp; _
  ""Database="" &amp; TDDb &amp; "";"" &amp; _
  ""Uid="" &amp; TDUname &amp; "";"" &amp; _
  ""Pwd="" &amp; TDPword &amp; """"


Unable to connect to ""LDAP"" mechanism and it is throwing Userid/password is invalid even after including the ""Authentication"" as LDAP in the above code
",1,-1,-1.0,"Tried with the following code to connect to Teradata and it is working fine 

  Worksheets(""BO"").range(""A:C"").Clearcontents
  conn.Open ""Driver={Teradata};"" &amp; _
  ""DBCName="" &amp; TDServer &amp; "";"" &amp; _
  ""Database="" &amp; TDDb &amp; "";"" &amp; _
  ""Uid="" &amp; TDUname &amp; "";"" &amp; _
  ""Pwd="" &amp; TDPword &amp; """"


Unable to connect to ""LDAP"" mechanism and it is throwing Userid/password is invalid even after including the ""Authentication"" as LDAP in the above code
",1
302,47874419,RDBMS code 6706: The string contains an untranslatable character in teradata,"I got the following error while inserting data into teradata DB using data stage job, I can not see data file and all the fields in #table are varchar, below is my code:

insert into #table
(
 column1,
 column2
)
values
(
column1_value,
column2_value
);


Error:

RDBMS code 6706: The string contains an untranslatable character

",-1,-1,-1.0,"I got the following error while inserting data into teradata DB using data stage job, I can not see data file and all the fields in #table are varchar, below is my code:

insert into #table
(
 column1,
 column2
)
values
(
column1_value,
column2_value
);


Error:

RDBMS code 6706: The string contains an untranslatable character

",3
303,47888667,Teradata Temporary Table,"I need help with creating temporary tables in Teradata. Below I am creating a global temporary table  TOPTABLE3. Then I am inserting 10 rows into the table. However, then when I query the table, I only get 0 rows returned which shows that the records were not inserted into the temporary table. Thanks in advance. 

CREATE global temporary table  TOPTABLE3 (
TABLE1  VARCHAR(20))


-- CREATE TABLE completed. 0 rows processed. Elapsed Time = 00:00:01

INSERT INTO TOPTABLE3 
SEL          top 10 TABLE1  
FROM    schema.table 
where       table.column1 = 'D'  
and  table.column2 = CAST('01/01/2017' AS DATE FORMAT 'MM/DD/YYYY')    


-- INSERT completed. 10 rows processed. Elapsed Time = 00:00:06 Output directed to Answer window

SELECT  *
FROM     My_ID.TOPTABLE3


-- SELECT completed. 0 rows returned. Elapsed Time = 00:00:02
",1,-1,-1.0,"I need help with creating temporary tables in Teradata. Below I am creating a global temporary table  TOPTABLE3. Then I am inserting 10 rows into the table. However, then when I query the table, I only get 0 rows returned which shows that the records were not inserted into the temporary table. Thanks in advance. 

CREATE global temporary table  TOPTABLE3 (
TABLE1  VARCHAR(20))


-- CREATE TABLE completed. 0 rows processed. Elapsed Time = 00:00:01

INSERT INTO TOPTABLE3 
SEL          top 10 TABLE1  
FROM    schema.table 
where       table.column1 = 'D'  
and  table.column2 = CAST('01/01/2017' AS DATE FORMAT 'MM/DD/YYYY')    


-- INSERT completed. 10 rows processed. Elapsed Time = 00:00:06 Output directed to Answer window

SELECT  *
FROM     My_ID.TOPTABLE3


-- SELECT completed. 0 rows returned. Elapsed Time = 00:00:02
",3
304,47940816,Teradata - Adding preceding zero and casting as varchar(50),"I have the following query:

SELECT
  s.cola, s.colb, t.colc, t.cold, u.cole, u.colf, u.colg, u.colh, u.coli, u.colj, u.colk, u.coll 
FROM table1 s
INNER JOIN table2 t
  ON s.colb = t.colc
INNER JOIN table3 u
  ON u.colm = CAST(t.cold AS varchar(50))
WHERE cast(s.cola as date) between date '2017-11-06' and date '2017-11-10'
ORDER BY 3


Here, in the last joining condition, u.colm is of type varchar(50) and t.cold is of type decimal(10, 0). I cannot cast u.colm as decimal(10, 0) because some of the outdated values in that column are not purely numbers. Now, if I run the query as shown above, it will return a blank table because the common values in u.colm has a preceding/leading zero while t.cold doesn't have that zero. I tried the following:

1)  on u.colm = '0' + cast(t.cold as varchar(50))

This gave the error: 
[Teradata Database] [2620] The format or data contains a bad character. 

2) on u.colm = right('0000000000' + cast(t.cold as varchar(50)), 50)

This gave the error:
[Teradata Database] [9881] Function 'TD_RIGHT' called with an invalid number or type of parameters

The question, answers and comments in link1 will provide some more context. I am pretty new to Teradata and don't have much idea on how to resolve this issue. Kindly help.
",-1,-1,-1.0,"I have the following query:

SELECT
  s.cola, s.colb, t.colc, t.cold, u.cole, u.colf, u.colg, u.colh, u.coli, u.colj, u.colk, u.coll 
FROM table1 s
INNER JOIN table2 t
  ON s.colb = t.colc
INNER JOIN table3 u
  ON u.colm = CAST(t.cold AS varchar(50))
WHERE cast(s.cola as date) between date '2017-11-06' and date '2017-11-10'
ORDER BY 3


Here, in the last joining condition, u.colm is of type varchar(50) and t.cold is of type decimal(10, 0). I cannot cast u.colm as decimal(10, 0) because some of the outdated values in that column are not purely numbers. Now, if I run the query as shown above, it will return a blank table because the common values in u.colm has a preceding/leading zero while t.cold doesn't have that zero. I tried the following:

1)  on u.colm = '0' + cast(t.cold as varchar(50))

This gave the error: 
[Teradata Database] [2620] The format or data contains a bad character. 

2) on u.colm = right('0000000000' + cast(t.cold as varchar(50)), 50)

This gave the error:
[Teradata Database] [9881] Function 'TD_RIGHT' called with an invalid number or type of parameters

The question, answers and comments in link1 will provide some more context. I am pretty new to Teradata and don't have much idea on how to resolve this issue. Kindly help.
",3
305,47948466,REGR_SLOPE in Teradata SQL Query Returning 0 Slope,"I am a relative newbie with Teradata SQL and have run into this strange (I think strange) situation. I am trying to run a regression (REGR_SLOPE) on sensor data. I am gathering sensor readings for a single day, each day is 80 observations which is confirmed by the COUNT in the outer SELECT. My query is:

 SELECT
  d.meter_id,
  REGR_SLOPE(d.reading_measure, d.x_axis) AS slope,
  COUNT(d.x_axis) AS xcount,
  COUNT(d.reading_measure) AS read_count
  FROM
  (
  SELECT
      meter_id,
      reading_measure,
      row_number() OVER (ORDER BY Reading_Dttm) AS x_axis
  FROM data_mart.v_meter_reading
  WHERE Reading_Start_Dt = '2017-12-12'

  AND Meter_Id IN (11932101, 11419827, 11385229, 11643466)

  AND Channel_Num = 5
  ) d
  GROUP BY 1


When I use the ""IN"" clause in the subquery to specify Meter_Id, I get slope values, but when I take it out (to run over all meters) all the slopes are 0 (zero).  I would simply like to run a line through a day's worth of observations (80).

I'm using Teradata v15.0.

What am I missing / doing wrong?
",-1,-1,-1.0,"I am a relative newbie with Teradata SQL and have run into this strange (I think strange) situation. I am trying to run a regression (REGR_SLOPE) on sensor data. I am gathering sensor readings for a single day, each day is 80 observations which is confirmed by the COUNT in the outer SELECT. My query is:

 SELECT
  d.meter_id,
  REGR_SLOPE(d.reading_measure, d.x_axis) AS slope,
  COUNT(d.x_axis) AS xcount,
  COUNT(d.reading_measure) AS read_count
  FROM
  (
  SELECT
      meter_id,
      reading_measure,
      row_number() OVER (ORDER BY Reading_Dttm) AS x_axis
  FROM data_mart.v_meter_reading
  WHERE Reading_Start_Dt = '2017-12-12'

  AND Meter_Id IN (11932101, 11419827, 11385229, 11643466)

  AND Channel_Num = 5
  ) d
  GROUP BY 1


When I use the ""IN"" clause in the subquery to specify Meter_Id, I get slope values, but when I take it out (to run over all meters) all the slopes are 0 (zero).  I would simply like to run a line through a day's worth of observations (80).

I'm using Teradata v15.0.

What am I missing / doing wrong?
",3
306,47983128,Teradata - selecting between two columns based on whether it starts with a number or not,"I have the a query which looks similar to:

SELECT
  s.cola, s.colb, t.colc, t.cold, u.cole, u.colf, u.colg, u.colh, u.coli, u.colj, u.colk, u.coll 
FROM table1 s
INNER JOIN table2 t
  ON s.colb = t.colc
INNER JOIN table3 u
  ON u.colm = t.cold
WHERE cast(s.cola as date) between date '2017-11-06' and date '2017-11-10'
ORDER BY 3


I need to add a new column, called col_new, which is to be filled by either u.colm or u.coln. This column will have values from u.colm if that column starts with a number. Otherwise it will have values from u.coln. It is known that either u.coln or u.colm starts with a number, for each entry in table u.

I tried the following query to test if entries starting with a number can be identified or not:

SELECT CASE WHEN ISNUMERIC(SUBSTRING(LTRIM(colm), 1, 1)) = 1
        THEN 'yes'
        ELSE 'no'
      END AS col_new
FROM table_u


It returned the error:  Syntax error: expected something between '(' and the 'substring' keyword.

Kindly suggest a solution. 

Edit:
Exact Error:

[Teradata Database] [3706] Syntax error: expected something between '(' and the 'substring' keyword.

",-1,-1,-1.0,"I have the a query which looks similar to:

SELECT
  s.cola, s.colb, t.colc, t.cold, u.cole, u.colf, u.colg, u.colh, u.coli, u.colj, u.colk, u.coll 
FROM table1 s
INNER JOIN table2 t
  ON s.colb = t.colc
INNER JOIN table3 u
  ON u.colm = t.cold
WHERE cast(s.cola as date) between date '2017-11-06' and date '2017-11-10'
ORDER BY 3


I need to add a new column, called col_new, which is to be filled by either u.colm or u.coln. This column will have values from u.colm if that column starts with a number. Otherwise it will have values from u.coln. It is known that either u.coln or u.colm starts with a number, for each entry in table u.

I tried the following query to test if entries starting with a number can be identified or not:

SELECT CASE WHEN ISNUMERIC(SUBSTRING(LTRIM(colm), 1, 1)) = 1
        THEN 'yes'
        ELSE 'no'
      END AS col_new
FROM table_u


It returned the error:  Syntax error: expected something between '(' and the 'substring' keyword.

Kindly suggest a solution. 

Edit:
Exact Error:

[Teradata Database] [3706] Syntax error: expected something between '(' and the 'substring' keyword.

",3
307,48227003,Lunch TDCH to Load to load data from Hive parquet table to Teradata,"I need to load data from Hive tables which stored as parquet files to Teradata Database using TDCH(Teradata connector for Hadoop). I use TDCH 1.5.3 and CDH 5.8.3. and Hive 1.1.0

I try to start TDCH usign hadoop jar command and getting the Error:


  java.lang.ClassNotFoundException:
  org.apache.parquet.hadoop.util.ContextUtil


Is anybody have any idea why it's happened?
",-1,-1,-1.0,"I need to load data from Hive tables which stored as parquet files to Teradata Database using TDCH(Teradata connector for Hadoop). I use TDCH 1.5.3 and CDH 5.8.3. and Hive 1.1.0

I try to start TDCH usign hadoop jar command and getting the Error:


  java.lang.ClassNotFoundException:
  org.apache.parquet.hadoop.util.ContextUtil


Is anybody have any idea why it's happened?
",0
308,48467945,Recordset not opening when exporting data from Excel to Teradata,"I am trying to export a table from Excel into a DB using Teradata.  I know I have a connection to the DB, but the recordset is not open and I am getting ERROR 3704 ""Operation is not allowed when the object is closed. Here is my code.

    Dim FullQry As String
    Dim qry1 As String
    Dim qry2 As String
    Dim qry3 As String
    Dim qry4 As String
    Dim wb As Workbook, nWB As Workbook
    Dim oWS As Worksheet, oExWS As Worksheet
    Dim y As Long, z As Long
    Dim aRange As Range, bRange As Range
    Dim aData() As Variant

    Application.StatusBar = ""Pulling actuals data from TeraData""
    DoEvents

    Set wb = ThisWorkbook
    Set oWS = wb.Sheets(""LastRanSchedule"")
' Set data range/array
    With oWS
        y = .Cells(2, 1).End(xlDown).Row
        z = .Cells(1, 1).End(xlToRight).Column
        Set aRange = .Range(.Cells(2, 1), .Cells(y, z))
        aData = aRange
    End With

'DECLARE VARIABLES FOR CONNECTION
    Dim cn As ADODB.Connection
    Set cn = New ADODB.Connection
    cn.ConnectionTimeout = 120
    cn.CommandTimeout = 120

'DECLARE VARIABLES FOR RECORDSET
    Dim RS As ADODB.Recordset
    Set RS = New Recordset

'DECLARE VARIABLES FOR COMMAND 
    Dim cmdSQLData As ADODB.Command
    Set cmdSQLData = New ADODB.Command

'Connect to Teradata
    On Error GoTo errhndlr
    cn.Open ""Data Source = EDTDPAP1; Database= PROD_TECHOPS_LMP_SNBX_DB.AAL_Tableau_TestData; Persist Security info=True; User ID="" &amp; UserID &amp; ""; Password="" &amp; UserPassword &amp; ""; Session Mode=System Default;""
    On Error GoTo 0
    Set cmdSQLData.ActiveConnection = cn
    RS.CursorType = adOpenKeyset
    'RS.LockType = adLockOptimistic
    RS.CursorLocation = adUseClient

'Export Data
Dim val As Variant
Debug.Print cn.State
Debug.Print RS.State
For i = 1 To y - 1
   RS.AddNew
    For j = 1 To z
        val = aData(i, j)
        If IsEmpty(val) Then
        Else
            RS.Fields(j) = val
        End If
    Next j
Next i
RS.UpdateBatch


Stepping through the code, the error pops up on RS.AddNew.  My debug.print codes confirm that my connection is open but the recordset is closed.  I have ran out of ideas and could really use some suggestions.  Thanks.
",-1,-1,-1.0,"I am trying to export a table from Excel into a DB using Teradata.  I know I have a connection to the DB, but the recordset is not open and I am getting ERROR 3704 ""Operation is not allowed when the object is closed. Here is my code.

    Dim FullQry As String
    Dim qry1 As String
    Dim qry2 As String
    Dim qry3 As String
    Dim qry4 As String
    Dim wb As Workbook, nWB As Workbook
    Dim oWS As Worksheet, oExWS As Worksheet
    Dim y As Long, z As Long
    Dim aRange As Range, bRange As Range
    Dim aData() As Variant

    Application.StatusBar = ""Pulling actuals data from TeraData""
    DoEvents

    Set wb = ThisWorkbook
    Set oWS = wb.Sheets(""LastRanSchedule"")
' Set data range/array
    With oWS
        y = .Cells(2, 1).End(xlDown).Row
        z = .Cells(1, 1).End(xlToRight).Column
        Set aRange = .Range(.Cells(2, 1), .Cells(y, z))
        aData = aRange
    End With

'DECLARE VARIABLES FOR CONNECTION
    Dim cn As ADODB.Connection
    Set cn = New ADODB.Connection
    cn.ConnectionTimeout = 120
    cn.CommandTimeout = 120

'DECLARE VARIABLES FOR RECORDSET
    Dim RS As ADODB.Recordset
    Set RS = New Recordset

'DECLARE VARIABLES FOR COMMAND 
    Dim cmdSQLData As ADODB.Command
    Set cmdSQLData = New ADODB.Command

'Connect to Teradata
    On Error GoTo errhndlr
    cn.Open ""Data Source = EDTDPAP1; Database= PROD_TECHOPS_LMP_SNBX_DB.AAL_Tableau_TestData; Persist Security info=True; User ID="" &amp; UserID &amp; ""; Password="" &amp; UserPassword &amp; ""; Session Mode=System Default;""
    On Error GoTo 0
    Set cmdSQLData.ActiveConnection = cn
    RS.CursorType = adOpenKeyset
    'RS.LockType = adLockOptimistic
    RS.CursorLocation = adUseClient

'Export Data
Dim val As Variant
Debug.Print cn.State
Debug.Print RS.State
For i = 1 To y - 1
   RS.AddNew
    For j = 1 To z
        val = aData(i, j)
        If IsEmpty(val) Then
        Else
            RS.Fields(j) = val
        End If
    Next j
Next i
RS.UpdateBatch


Stepping through the code, the error pops up on RS.AddNew.  My debug.print codes confirm that my connection is open but the recordset is closed.  I have ran out of ideas and could really use some suggestions.  Thanks.
",1
309,48503886,Stored procedure is trying to call to the database inside the database it is created in Teradata,"I created a procedure in teradata 

REPLACE PROCEDURE ABC.SHOWTABLE (my_table VARCHAR(30),my_database VARCHAR(30)) 
BEGIN

DECLARE sqlstr VARCHAR(500);
SET sqlstr = 'SHOW TABLE ' || my_database || 
            '.' || my_table  ;
EXECUTE IMMEDIATE sqlstr;
END;



call ABC.SHOWTABLE (XYZ,MYTABLE);


It is showing error
Column/Parameter 'ABC.SHOWTABLE.XYZ' does not exist. 
Elapsed time = 00:00:00.011 

why it is looking inside procedure
",-1,-1,-1.0,"I created a procedure in teradata 

REPLACE PROCEDURE ABC.SHOWTABLE (my_table VARCHAR(30),my_database VARCHAR(30)) 
BEGIN

DECLARE sqlstr VARCHAR(500);
SET sqlstr = 'SHOW TABLE ' || my_database || 
            '.' || my_table  ;
EXECUTE IMMEDIATE sqlstr;
END;



call ABC.SHOWTABLE (XYZ,MYTABLE);


It is showing error
Column/Parameter 'ABC.SHOWTABLE.XYZ' does not exist. 
Elapsed time = 00:00:00.011 

why it is looking inside procedure
",3
310,48547051,"Teradata Express Studio 16.2, filtering schemas that are not accessible","For Teradata Studio Express 16.2, How do 
I filter out all schemas I do not have access to? 

My user account only has access to 2 database schemas but it is showing way more than that as you can see from the image below (Even shows all the user's db): 


",-1,-1,-1.0,"For Teradata Studio Express 16.2, How do 
I filter out all schemas I do not have access to? 

My user account only has access to 2 database schemas but it is showing way more than that as you can see from the image below (Even shows all the user's db): 


",3
311,47927795,Teradata - Comparing Varchar to decimal,"I am very new to Teradata and SQL in general. I need to create a table by combining data from three tables. I was able to successfully join two of them. I am not able to write the joining condition for the third table properly. Here is the code:

select s.cola, s.colb, 
t.colc, t.cold,
u.cole, u.colf, u.colg, u.colh, u.coli, u.colj, u.colk, u.coll
from table1 s 
inner join table2 t
on s.colb = t.colc
inner join table3 u
on t.cold = cast(u.colm as decimal)
order by 3
where substr(cast(s.cola as varchar(10)),6,2) = 11 and substr(cast(s.cola as varchar(10)),1,4) = 2017 and substr(cast(s.cola as varchar(10)),9,2) between 06 and 10


The error I am getting is:

[Teradata Database] [2620] The format or data contains a bad character.

I think the problem is with the line: on t.cold = cast(u.colm as decimal). The u.colm is of type VARCHAR(50) while t.cold is of type DECIMAL(10, 0). I believe I have casted it properly. Please help.Thanks in advance.
",-1,-1,-1.0,"I am very new to Teradata and SQL in general. I need to create a table by combining data from three tables. I was able to successfully join two of them. I am not able to write the joining condition for the third table properly. Here is the code:

select s.cola, s.colb, 
t.colc, t.cold,
u.cole, u.colf, u.colg, u.colh, u.coli, u.colj, u.colk, u.coll
from table1 s 
inner join table2 t
on s.colb = t.colc
inner join table3 u
on t.cold = cast(u.colm as decimal)
order by 3
where substr(cast(s.cola as varchar(10)),6,2) = 11 and substr(cast(s.cola as varchar(10)),1,4) = 2017 and substr(cast(s.cola as varchar(10)),9,2) between 06 and 10


The error I am getting is:

[Teradata Database] [2620] The format or data contains a bad character.

I think the problem is with the line: on t.cold = cast(u.colm as decimal). The u.colm is of type VARCHAR(50) while t.cold is of type DECIMAL(10, 0). I believe I have casted it properly. Please help.Thanks in advance.
",3
312,48668273,How can I run multiple Teradata SQL statements using a single string in C#?,"I know I can run them with one ExecuteNonQuery() per statement, but I am being asked to run a large text file with many statements.  A user who understands SQL (not a programmer) will update the file as the queries change in the future.

A typical file has a lot of CREATE VOLATILE TABLE statements with some INSERTs, DELETEs, and UPDATEs interspersed.  If I try this as I have below, I get [Teradata Database] [3932] Only an ET or null statement is legal after a DDL Statement.  Adding an ET; after either or both statements still returns the same error.

using (TdConnection tdConnection = new TdConnection())
{
    TdConnectionStringBuilder conn = new TdConnectionStringBuilder()
    {
        DataSource = ""XXXXX"",
        UserId = ""ID"",
        Password = ""PW"",
        AuthenticationMechanism = ""LDAP"",
        ConnectionPooling = false
    };

    tdConnection.ConnectionString = conn.ConnectionString;
    tdConnection.Open();

    TdCommand command = new TdCommand(@""
        CREATE VOLATILE TABLE ABC AS
        (
        ...
        )
        WITH DATA PRIMARY INDEX(ABC_ID)
        ON COMMIT PRESERVE ROWS;

        DELETE FROM ABC;
    "", tdConnection)
    {
        CommandTimeout = 0
    };

    command.ExecuteNonQuery();
}


I see that ""Multi-statement"" is valid in the documentation, but I can't seem to understand why it's not working in this case.  It does work if I run it from Teradata SQL Assistant rather than code.

I also understand that this is not the ideal way to implement this.  I know I can load the statements individually or use stored procedures, but I'm not being given a choice in the design in this case.
",-1,-1,-1.0,"I know I can run them with one ExecuteNonQuery() per statement, but I am being asked to run a large text file with many statements.  A user who understands SQL (not a programmer) will update the file as the queries change in the future.

A typical file has a lot of CREATE VOLATILE TABLE statements with some INSERTs, DELETEs, and UPDATEs interspersed.  If I try this as I have below, I get [Teradata Database] [3932] Only an ET or null statement is legal after a DDL Statement.  Adding an ET; after either or both statements still returns the same error.

using (TdConnection tdConnection = new TdConnection())
{
    TdConnectionStringBuilder conn = new TdConnectionStringBuilder()
    {
        DataSource = ""XXXXX"",
        UserId = ""ID"",
        Password = ""PW"",
        AuthenticationMechanism = ""LDAP"",
        ConnectionPooling = false
    };

    tdConnection.ConnectionString = conn.ConnectionString;
    tdConnection.Open();

    TdCommand command = new TdCommand(@""
        CREATE VOLATILE TABLE ABC AS
        (
        ...
        )
        WITH DATA PRIMARY INDEX(ABC_ID)
        ON COMMIT PRESERVE ROWS;

        DELETE FROM ABC;
    "", tdConnection)
    {
        CommandTimeout = 0
    };

    command.ExecuteNonQuery();
}


I see that ""Multi-statement"" is valid in the documentation, but I can't seem to understand why it's not working in this case.  It does work if I run it from Teradata SQL Assistant rather than code.

I also understand that this is not the ideal way to implement this.  I know I can load the statements individually or use stored procedures, but I'm not being given a choice in the design in this case.
",3
313,48676270,Teradata Fast export connection in Informatica misbehaving,"We are using Teradata Fastexport connection in Informatica to export data from few tables by joining them in Source Qualifier query and write to a CSV file. We have around 180 columns to pull, recently we added 2 new columns to this flow and we found that data for few of the records looks junk. We figured out those records and tried to run the SQ query only for those records, to our surprise the columns which were junk earlier now throwing us expected data. 

Is there any fast export limitation to export the columns ? or any properties should be increased at Informatica level.

We are clueless on this issue, please help.
",1,-1,-1.0,"We are using Teradata Fastexport connection in Informatica to export data from few tables by joining them in Source Qualifier query and write to a CSV file. We have around 180 columns to pull, recently we added 2 new columns to this flow and we found that data for few of the records looks junk. We figured out those records and tried to run the SQ query only for those records, to our surprise the columns which were junk earlier now throwing us expected data. 

Is there any fast export limitation to export the columns ? or any properties should be increased at Informatica level.

We are clueless on this issue, please help.
",3
314,48691646,Issue while loading an avro dataset into Teradata with spark-streaming,"I am trying to load a dataset of avro files into a Teradata table through spark streaming (jdbc). The configuration is properly set and the load succeeds to certain extent (I can validate rows of data have been inserted into the table), but halfways through I start having exceptions and the load fails. The stacktrace is below. Any inkling as to what might causing this ?

18/02/08 17:27:42 ERROR executor.Executor: Exception in task 2.0 in stage 0.0 (TID 0)
java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 16.20.00.02] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""database"".""table"". Details of the failure can be found in the exception chain that is accessible with getNextException.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:149)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:133)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2389)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:592)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
    at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
    at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:99)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 16.20.00.02] [Error 1147] [SQLState HY000] The next failure(s) in the exception chain occurred while beginning FastLoad of database table ""database"".""table""
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.beginFastLoad(FastLoadManagerPreparedStatement.java:966)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2210)

",-1,-1,-1.0,"I am trying to load a dataset of avro files into a Teradata table through spark streaming (jdbc). The configuration is properly set and the load succeeds to certain extent (I can validate rows of data have been inserted into the table), but halfways through I start having exceptions and the load fails. The stacktrace is below. Any inkling as to what might causing this ?

18/02/08 17:27:42 ERROR executor.Executor: Exception in task 2.0 in stage 0.0 (TID 0)
java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 16.20.00.02] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""database"".""table"". Details of the failure can be found in the exception chain that is accessible with getNextException.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:149)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:133)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2389)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:592)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
    at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
    at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:99)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 16.20.00.02] [Error 1147] [SQLState HY000] The next failure(s) in the exception chain occurred while beginning FastLoad of database table ""database"".""table""
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.beginFastLoad(FastLoadManagerPreparedStatement.java:966)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2210)

",0
315,48791092,Teradata - Remove whitespace within a varchar,"I have a table with varchar columns that have whitespace between the text (e.g: S O C I A L). I've tried using the following to remove the whitespace:


regexp_replace([column], '[[:space:]]{2,}', '')
regexp_replace([column], '[\s]', '', 1, 0, 'c')
regexp_replace([column], ' ', '')
oreplace([column], '[\s]', '')
oreplace([column], ' ', '')


But so far nothing has worked, so I'm not really sure what to do since Teradata SQL doesn't have the usual replace function.
",-1,-1,-1.0,"I have a table with varchar columns that have whitespace between the text (e.g: S O C I A L). I've tried using the following to remove the whitespace:


regexp_replace([column], '[[:space:]]{2,}', '')
regexp_replace([column], '[\s]', '', 1, 0, 'c')
regexp_replace([column], ' ', '')
oreplace([column], '[\s]', '')
oreplace([column], ' ', '')


But so far nothing has worked, so I'm not really sure what to do since Teradata SQL doesn't have the usual replace function.
",3
316,48803209,How to copy column names from Teradata,"I want to export the column names into excel sheet by running the query in Teradata. I used ctrl+c  but it didnt work. Thanks in advance.
",1,-1,-1.0,"I want to export the column names into excel sheet by running the query in Teradata. I used ctrl+c  but it didnt work. Thanks in advance.
",3
317,49041501,"Timeout Error Connecting to Teradata Server From SSIS using Attunity Driver - ""[WSock32 DLL] 10060 WSA E TimedOut""","I'm facing an issue connecting to Teradata from SSIS using Attunity connector. My environment details as follows:


Windows 2016
SQL Server 2016 (Enterprise Edition)
VS 2015
Attunity Driver 5.0
Teradata drivers installed - 15.10.05


When I run the package from the server (both via Agent as well as from BIDS) following is the error I'm getting:


  ""Error: There was an error trying to establish an Open Database
  Connectivity (ODBC) connection with the database server. SqlState =
  HYT00 Message = [WSock32 DLL] 10060 WSA E TimedOut: No response
  received when attempting to connect to the Teradata server""


Is there any server level configuration or network settings, I need to enable? Any help on this is much appreciated. Thanks in advance!!
",-1,-1,-1.0,"I'm facing an issue connecting to Teradata from SSIS using Attunity connector. My environment details as follows:


Windows 2016
SQL Server 2016 (Enterprise Edition)
VS 2015
Attunity Driver 5.0
Teradata drivers installed - 15.10.05


When I run the package from the server (both via Agent as well as from BIDS) following is the error I'm getting:


  ""Error: There was an error trying to establish an Open Database
  Connectivity (ODBC) connection with the database server. SqlState =
  HYT00 Message = [WSock32 DLL] 10060 WSA E TimedOut: No response
  received when attempting to connect to the Teradata server""


Is there any server level configuration or network settings, I need to enable? Any help on this is much appreciated. Thanks in advance!!
",1
318,49067373,Teradata Create a timestamp from a date and time,"Hi,

I am trying to create a bespoke time stamp in teradata. The end result should be last saturday at 14:30. Below is my attempt but im getting an error i was wondering if anyone could help

SELECT CAST(CAST(DATE - ((DATE - DATE '0001-01-07') MOD 7) -1 AS FORMAT 'DD/MM/YYYY') AS CHAR(12)) + ('04:00:00' - TIME '00:00:00' HOUR TO SECOND) ts



  Invalid Operation for date or time


Thank you for your time
",-1,-1,-1.0,"Hi,

I am trying to create a bespoke time stamp in teradata. The end result should be last saturday at 14:30. Below is my attempt but im getting an error i was wondering if anyone could help

SELECT CAST(CAST(DATE - ((DATE - DATE '0001-01-07') MOD 7) -1 AS FORMAT 'DD/MM/YYYY') AS CHAR(12)) + ('04:00:00' - TIME '00:00:00' HOUR TO SECOND) ts



  Invalid Operation for date or time


Thank you for your time
",3
319,49182313,How to deal with a date stored as a string in an int field in Teradata?,"I've got a table in Teradata that stores a date in an 8 character INT field in the following form ""YYYYMMDD"", so for today it would store ""20180308"". If I try to CAST it as a date like this:

CAST(date_field AS DATE FORMAT 'YYYY-MM-DD')

It transforms the date to some future date in the year 3450 or something.

I think it's an error that this data isn't either stored as a date object. Is there anyway to overcome this glitch? I don't have access to change this unfortunately.

Thanks
",0,-1,-1.0,"I've got a table in Teradata that stores a date in an 8 character INT field in the following form ""YYYYMMDD"", so for today it would store ""20180308"". If I try to CAST it as a date like this:

CAST(date_field AS DATE FORMAT 'YYYY-MM-DD')

It transforms the date to some future date in the year 3450 or something.

I think it's an error that this data isn't either stored as a date object. Is there anyway to overcome this glitch? I don't have access to change this unfortunately.

Thanks
",3
320,49246274,Using ODBC in SAS to Access Teradata,"I am thinking of using SAS and would like to know:

Is it possible to use ODBC to access a Teradata database in SAS?  

I know that accessing MS Access using ODBC in SAS is possible, and accessing Teradata with Excel VBA using ODBC is possible, but I can not find anything for SAS with Teradata and ODBC.
",-1,-1,-1.0,"I am thinking of using SAS and would like to know:

Is it possible to use ODBC to access a Teradata database in SAS?  

I know that accessing MS Access using ODBC in SAS is possible, and accessing Teradata with Excel VBA using ODBC is possible, but I can not find anything for SAS with Teradata and ODBC.
",3
321,49442751,Teradata 16 Issues,"We recently upgraded our Teradata version to 16 and since then we are facing few spool space issues. Before the query was consuming less then 200GB and now it consumes more than 1.5TB. We then started optimizing the query and we modified a part of the query 

LEFT JOIN (SELECT A.XX, C.YY  FROM TABLE1  A
INNER JOIN TABLE2 B ON  A.DD = B.EE
INNER JOIN TABLE3 C ON  C.FF = B.GG
GROUP BY 1,2) 


to

LEFT JOIN (SELECT A.XX, C.YY  FROM TABLE1  A
INNER JOIN TABLE2 B ON  A.DD = B.EE
INNER JOIN (SELECT FF, YY FROM TABLE3 GROUP BY 1,2) C 
ON  C.FF = B.GG
GROUP BY 1,2)  


After modification the query runs within 200GB. Could someone please explain as why we face such issues after upgrade? Thanks in advance.
",0,-1,-1.0,"We recently upgraded our Teradata version to 16 and since then we are facing few spool space issues. Before the query was consuming less then 200GB and now it consumes more than 1.5TB. We then started optimizing the query and we modified a part of the query 

LEFT JOIN (SELECT A.XX, C.YY  FROM TABLE1  A
INNER JOIN TABLE2 B ON  A.DD = B.EE
INNER JOIN TABLE3 C ON  C.FF = B.GG
GROUP BY 1,2) 


to

LEFT JOIN (SELECT A.XX, C.YY  FROM TABLE1  A
INNER JOIN TABLE2 B ON  A.DD = B.EE
INNER JOIN (SELECT FF, YY FROM TABLE3 GROUP BY 1,2) C 
ON  C.FF = B.GG
GROUP BY 1,2)  


After modification the query runs within 200GB. Could someone please explain as why we face such issues after upgrade? Thanks in advance.
",3
322,49451731,How to use Teradata Python module to login into environment using LDAP?,"I'm writing a script to run a series of queries from a database and do some analysis on the data returned using the Teradata Python Module. 

The environment I want to access uses LDAP authentication. Does anyone have any guidance on what I need to do to my script to access that environment? 

I'm currently getting the error: 

teradata.api.DatabaseError: (8017, '[28000] [Teradata][ODBC Teradata Driver][Teradata Database] The UserId, Password or Account is invalid. ')


However I can log into Teradata with my credentials so I'm not sure what I'm messing up on my UdaExec files...
",-1,-1,-1.0,"I'm writing a script to run a series of queries from a database and do some analysis on the data returned using the Teradata Python Module. 

The environment I want to access uses LDAP authentication. Does anyone have any guidance on what I need to do to my script to access that environment? 

I'm currently getting the error: 

teradata.api.DatabaseError: (8017, '[28000] [Teradata][ODBC Teradata Driver][Teradata Database] The UserId, Password or Account is invalid. ')


However I can log into Teradata with my credentials so I'm not sure what I'm messing up on my UdaExec files...
",1
323,49521541,Upload Multiple Rows to Teradata via VBA ODBC Connection,"I have the following VBA code that I use for select statements for Teradata that works.



Sub Item_Review()

Dim strsql As String

strsql = Worksheets(""SQL"").Range(""b3"")

    With ActiveWorkbook.Connections(""Item_Review"").ODBCConnection
    .BackgroundQuery = True
    Debug.Print strsql
    CommandText = SplitMeUp(strsql)

    End With

    ActiveWorkbook.Connections(""Item_Review"").Refresh

End Sub


However, I tried changing the strsql from a select statement to an insert into statement.  When I run the VBA nothing happens. I hit the button and I do not get any update notification nor do I get an error notification.  When I view the table in Teradata, I can see that it has not been updated.  Any thoughts?

Sub Item_Add()

Dim strsql As String

strsql = ""insert into Table_1  (Item_id, NOTE, date, Work_STATUS) values ('Item_1','Testing ','2014-01-01','Worked');""

    With ActiveWorkbook.Connections(""Item_Add"").ODBCConnection
    .BackgroundQuery = True
    Debug.Print strsql
    CommandText = SplitMeUp(strsql)

    End With

    ActiveWorkbook.Connections(""Item_Add"").Refresh

End Sub

",-1,-1,-1.0,"I have the following VBA code that I use for select statements for Teradata that works.



Sub Item_Review()

Dim strsql As String

strsql = Worksheets(""SQL"").Range(""b3"")

    With ActiveWorkbook.Connections(""Item_Review"").ODBCConnection
    .BackgroundQuery = True
    Debug.Print strsql
    CommandText = SplitMeUp(strsql)

    End With

    ActiveWorkbook.Connections(""Item_Review"").Refresh

End Sub


However, I tried changing the strsql from a select statement to an insert into statement.  When I run the VBA nothing happens. I hit the button and I do not get any update notification nor do I get an error notification.  When I view the table in Teradata, I can see that it has not been updated.  Any thoughts?

Sub Item_Add()

Dim strsql As String

strsql = ""insert into Table_1  (Item_id, NOTE, date, Work_STATUS) values ('Item_1','Testing ','2014-01-01','Worked');""

    With ActiveWorkbook.Connections(""Item_Add"").ODBCConnection
    .BackgroundQuery = True
    Debug.Print strsql
    CommandText = SplitMeUp(strsql)

    End With

    ActiveWorkbook.Connections(""Item_Add"").Refresh

End Sub

",3
324,49522027,How to connect to Teradata using SoapUI? What are the configuration details for SoapUI-Teradata connection?,"I want to add a test step in Soap UI Test Case, wherein I can connect to Teradata database and fire a query to validate. 
However, I am facing issues while setting up the configuration for the connection.

In the configuration details, I have provided as below:


  Driver: com.teradata.jdbc.TeraDriver
  
  Connection String: jdbc:teradata:


Below is the error thrown on TEST CONNECTION:


  Can't get the Connection for specified properties; java.sql.SQLException: 
     [Teradata JDBC Driver] [TeraJDBC 15.10.00.35] [Error 1032] [SQLState HY000] 
     Single Sign-On NOT supported for Mechanism TD2.


Note: 
1. I have NOT added any properties(Name/Value).
2. SoapUI 5.3.0. (Free) version. 
3. Teradata drivers(terajdbc4, tdjssconfig jars) are added in ../SmartBear/SoapUI-5.3.0/bin/ext/
",-1,-1,-1.0,"I want to add a test step in Soap UI Test Case, wherein I can connect to Teradata database and fire a query to validate. 
However, I am facing issues while setting up the configuration for the connection.

In the configuration details, I have provided as below:


  Driver: com.teradata.jdbc.TeraDriver
  
  Connection String: jdbc:teradata:


Below is the error thrown on TEST CONNECTION:


  Can't get the Connection for specified properties; java.sql.SQLException: 
     [Teradata JDBC Driver] [TeraJDBC 15.10.00.35] [Error 1032] [SQLState HY000] 
     Single Sign-On NOT supported for Mechanism TD2.


Note: 
1. I have NOT added any properties(Name/Value).
2. SoapUI 5.3.0. (Free) version. 
3. Teradata drivers(terajdbc4, tdjssconfig jars) are added in ../SmartBear/SoapUI-5.3.0/bin/ext/
",1
325,49610780,Avoid Deadlocks Using Teradata and SQLAlchemy,"I am using sqlalchemy and sqlalchemy-teradata to query on my database. Every select statement generated by sqlalchemy would generate a table lock which impacts other concurrent users who cannot perform any insert, update or delete operation on the table while the previous user is reading it. To avoid such table locks, I need to add the following string before the SELECT statement:

LOCK ROW FOR ACCESS
SELECT * FROM DATABASE.TABLE;


How can I override sqlalchemy select statements to add this string?
Note that I have also raised the issue on GitHub here but did not find any solution so far: https://github.com/Teradata/sqlalchemy-teradata/issues/39

[Update]

When connecting to Teradata, I have added the statement provided below by dnoeth:

from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base, DeferredReflection
from sqlalchemy.orm import scoped_session, sessionmaker
[...]
engine = create_engine('teradata://' + user + ':' + password + '@' + host + ':22/' + database)
db_session = scoped_session(sessionmaker(autocommit=False, autoflush=False, bind=engine))
db_session.execute('SET SESSION CHARACTERISTICS AS TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;')  # To avoid locking tables when doing select on tables

Base = declarative_base(cls=DeferredReflection)
Base.query = db_session.query_property()


But now I have the following error message when SQLAlchemy tries to read Teradata meta data:


  sqlalchemy.exc.DatabaseError: (teradata.api.DatabaseError) (3932,
  '[25000] [Teradata][ODBC Teradata Driver][Teradata Database] Only an
  ET or null statement is legal after a DDL Statement. ') [SQL: 'SELECT
  columnname, columntype, columnlength, chartype, decimaltotaldigits,
  decimalfractionaldigits, columnformat, nullable, defaultvalue,
  idcoltype \nFROM dbc.ColumnsV \nWHERE DatabaseName=? AND TableName=?']
  [parameters: ('dev_migration_tool', 'migration_object_type')]

",-1,-1,-1.0,"I am using sqlalchemy and sqlalchemy-teradata to query on my database. Every select statement generated by sqlalchemy would generate a table lock which impacts other concurrent users who cannot perform any insert, update or delete operation on the table while the previous user is reading it. To avoid such table locks, I need to add the following string before the SELECT statement:

LOCK ROW FOR ACCESS
SELECT * FROM DATABASE.TABLE;


How can I override sqlalchemy select statements to add this string?
Note that I have also raised the issue on GitHub here but did not find any solution so far: https://github.com/Teradata/sqlalchemy-teradata/issues/39

[Update]

When connecting to Teradata, I have added the statement provided below by dnoeth:

from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base, DeferredReflection
from sqlalchemy.orm import scoped_session, sessionmaker
[...]
engine = create_engine('teradata://' + user + ':' + password + '@' + host + ':22/' + database)
db_session = scoped_session(sessionmaker(autocommit=False, autoflush=False, bind=engine))
db_session.execute('SET SESSION CHARACTERISTICS AS TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;')  # To avoid locking tables when doing select on tables

Base = declarative_base(cls=DeferredReflection)
Base.query = db_session.query_property()


But now I have the following error message when SQLAlchemy tries to read Teradata meta data:


  sqlalchemy.exc.DatabaseError: (teradata.api.DatabaseError) (3932,
  '[25000] [Teradata][ODBC Teradata Driver][Teradata Database] Only an
  ET or null statement is legal after a DDL Statement. ') [SQL: 'SELECT
  columnname, columntype, columnlength, chartype, decimaltotaldigits,
  decimalfractionaldigits, columnformat, nullable, defaultvalue,
  idcoltype \nFROM dbc.ColumnsV \nWHERE DatabaseName=? AND TableName=?']
  [parameters: ('dev_migration_tool', 'migration_object_type')]

",1
326,49661232,How to use Volatile Tables in Python Teradata module PyTD,"Does anyone know how to use volatile tables in the PyTd Teradata ODBC module? I can run single queries and get the results to return, but if I try to build a query with volatile tables, I keep getting an error. Any help? I'm fairly new to python.

I'm using 

udaExec = td.UdaExec (appName=""dpull"", version=""1.0"",logConsole=False)
with udaExec.connect(method=""ODBC"",dsn=""tdata"", username=""un"", 
password=""pw"") as session:

query = """"""""
create volatile table vol_table
,no fallback, no before journal, no after journal as 
    (
    select a, b, c
    from my_table1
    )
with data primary index (a)
on commit preserve rows

Select vt.a, vt.b, vt.c, t2.a
from vol_table vt
inner join table2 t2
on vt.anything = t2.anything
""""""""
df = pd.read_sql(query,session)


Error:

 `InterfaceError: ('SQL_INVALID_HANDLE', 'Invalid handle passed to SQLGetDiagRecW.')`

",-1,-1,-1.0,"Does anyone know how to use volatile tables in the PyTd Teradata ODBC module? I can run single queries and get the results to return, but if I try to build a query with volatile tables, I keep getting an error. Any help? I'm fairly new to python.

I'm using 

udaExec = td.UdaExec (appName=""dpull"", version=""1.0"",logConsole=False)
with udaExec.connect(method=""ODBC"",dsn=""tdata"", username=""un"", 
password=""pw"") as session:

query = """"""""
create volatile table vol_table
,no fallback, no before journal, no after journal as 
    (
    select a, b, c
    from my_table1
    )
with data primary index (a)
on commit preserve rows

Select vt.a, vt.b, vt.c, t2.a
from vol_table vt
inner join table2 t2
on vt.anything = t2.anything
""""""""
df = pd.read_sql(query,session)


Error:

 `InterfaceError: ('SQL_INVALID_HANDLE', 'Invalid handle passed to SQLGetDiagRecW.')`

",3
327,49419342,How to get the count of records returned by teradata sql griup by query,"I have a condition where I need to check number of rows returned by select query in Teradata.

My query looks like below 

select 
PDate ,Risk ,BName ,BNumber ,
ONumber ,OnNumber ,ID_CD ,Entity ,
AU ,RType, count (*) 
from Load_one.import_test
group by 1,2,3,4,5,6,7,8,9,10
having count (*) &gt; 1;


So I would like to know the count of rows it returns. I tried something like below

Select Count(*)
From    ( select 
PDate ,Risk ,BName ,BNumber ,
ONumber ,OnNumber ,ID_CD ,Entity ,
AU ,RType, count (*) 
from Load_one.import_test
group by 1,2,3,4,5,6,7,8,9,10
having count (*) &gt; 1;
)  as temp


It returning an error: select failed 3707 expected something like an 'EXCEPT' keyword or an 'UNION' keyword or a 'MINUS' keyword between an integer and ;

Please help me.
",-1,-1,-1.0,"I have a condition where I need to check number of rows returned by select query in Teradata.

My query looks like below 

select 
PDate ,Risk ,BName ,BNumber ,
ONumber ,OnNumber ,ID_CD ,Entity ,
AU ,RType, count (*) 
from Load_one.import_test
group by 1,2,3,4,5,6,7,8,9,10
having count (*) &gt; 1;


So I would like to know the count of rows it returns. I tried something like below

Select Count(*)
From    ( select 
PDate ,Risk ,BName ,BNumber ,
ONumber ,OnNumber ,ID_CD ,Entity ,
AU ,RType, count (*) 
from Load_one.import_test
group by 1,2,3,4,5,6,7,8,9,10
having count (*) &gt; 1;
)  as temp


It returning an error: select failed 3707 expected something like an 'EXCEPT' keyword or an 'UNION' keyword or a 'MINUS' keyword between an integer and ;

Please help me.
",3
328,49861161,"Python exe giving error in connecting to Teradata, however works fine while running from spyder","My python program works fine while running from Spyder.
When i convert that into an exe file using pyinstaller and run it, I get the following error

sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:teradata

I figured out that below line of code is creating problem in the exe while working fine in spyder as a .py file.

engine = sqlalchemy.create_engine('teradata://user_id:password/dbname?driver= Teradata')

This plugin is available with Spyder. Is it possible that it is not available in pyinstaller? Does any one encountered this issue before?
",-1,-1,-1.0,"My python program works fine while running from Spyder.
When i convert that into an exe file using pyinstaller and run it, I get the following error

sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:teradata

I figured out that below line of code is creating problem in the exe while working fine in spyder as a .py file.

engine = sqlalchemy.create_engine('teradata://user_id:password/dbname?driver= Teradata')

This plugin is available with Spyder. Is it possible that it is not available in pyinstaller? Does any one encountered this issue before?
",1
329,49974195,Calculate Teradata Query Execution Time,"I'm trying to compute Teradata queries execution time for a specific Teradata user. I currently have the following query inspired from there:
Calculating-the-actual-query-run-time

Query:

SELECT date
,a.username
,a.errorcode
,SUBSTR(b.sqltextinfo, 1, 15000)
,a.starttime
,a.firstresptime
,a.firststeptime
,((a.firstresptime - a.starttime) HOUR(4) TO SECOND(2)) AS elapsedtime
,((a.firstresptime - a.firststeptime) HOUR(4) TO SECOND(2)) AS executiontime
,elapsedtime - executiontime AS delaytime
FROM dbc.QryLogV a
INNER JOIN dbc.QryLogSqlV b ON a.procid = b.procid AND a.queryid = b.queryid 
WHERE a.Username = 'xxx';


Sadly it's triggering the error:


  Executed as Single statement.  Failed [7453 : HY000] Interval field overflow. 

",-1,-1,-1.0,"I'm trying to compute Teradata queries execution time for a specific Teradata user. I currently have the following query inspired from there:
Calculating-the-actual-query-run-time

Query:

SELECT date
,a.username
,a.errorcode
,SUBSTR(b.sqltextinfo, 1, 15000)
,a.starttime
,a.firstresptime
,a.firststeptime
,((a.firstresptime - a.starttime) HOUR(4) TO SECOND(2)) AS elapsedtime
,((a.firstresptime - a.firststeptime) HOUR(4) TO SECOND(2)) AS executiontime
,elapsedtime - executiontime AS delaytime
FROM dbc.QryLogV a
INNER JOIN dbc.QryLogSqlV b ON a.procid = b.procid AND a.queryid = b.queryid 
WHERE a.Username = 'xxx';


Sadly it's triggering the error:


  Executed as Single statement.  Failed [7453 : HY000] Interval field overflow. 

",3
330,50024626,SQLAlchemy Unicode Error - Querying Teradata database,"I'm trying to use Python's SQLAlchemy library to query a Teradata database. I was able to create the engine okay using the following code.

from sqlalchemy import create_engine

td_engine = create_engine('teradata://' + 'usrname' + ':' + 'pswrd' + '@' + 'myOdbcDataSource' + ':22/?charset=UTF8')

But when I try to use the engine, I get the following error.


  ValueError: character U+590048 is not in range [U+0000; U+10ffff]


This error occurs using all the functions that interact with the database that I tried. For example, I get this error when I try to execute the following.

sqlStr = 'select top 1000 * from myTable;'
result = td_engine.execute(sqlStr)

As another example, I get the same error when I try to execute the following.

td_engine.table_names('mySchema')

The log right before the error indicates a connection to the database using the ODBC driver is being made so I wonder if this has something to do with the way I configured the ODBC driver. Below is my odbc.ini file located at /Library/ODBC/ on my Mac.  

[ODBC Data Sources]
myodbca       = MySQL ODBC 5.3 ANSI Driver
myodbc        = MySQL ODBC 5.3 Unicode Driver
myOdbcDataSource        = Teradata Database ODBC Driver 16.20

[myOdbcDataSource]
Driver                = /Library/Application Support/teradata/client/16.20/lib/tdataodbc_sbu.dylib
DBCName               = myUrl
DefaultDatabase       = myDb
UserName              = usrname
Password              = pswrd
CharacterSet          = UTF8


Does anyone have any ideas on how to fix this Unicode error and get the SQL Alchemy Teradata engine working? Thank you in advance.
",-1,-1,-1.0,"I'm trying to use Python's SQLAlchemy library to query a Teradata database. I was able to create the engine okay using the following code.

from sqlalchemy import create_engine

td_engine = create_engine('teradata://' + 'usrname' + ':' + 'pswrd' + '@' + 'myOdbcDataSource' + ':22/?charset=UTF8')

But when I try to use the engine, I get the following error.


  ValueError: character U+590048 is not in range [U+0000; U+10ffff]


This error occurs using all the functions that interact with the database that I tried. For example, I get this error when I try to execute the following.

sqlStr = 'select top 1000 * from myTable;'
result = td_engine.execute(sqlStr)

As another example, I get the same error when I try to execute the following.

td_engine.table_names('mySchema')

The log right before the error indicates a connection to the database using the ODBC driver is being made so I wonder if this has something to do with the way I configured the ODBC driver. Below is my odbc.ini file located at /Library/ODBC/ on my Mac.  

[ODBC Data Sources]
myodbca       = MySQL ODBC 5.3 ANSI Driver
myodbc        = MySQL ODBC 5.3 Unicode Driver
myOdbcDataSource        = Teradata Database ODBC Driver 16.20

[myOdbcDataSource]
Driver                = /Library/Application Support/teradata/client/16.20/lib/tdataodbc_sbu.dylib
DBCName               = myUrl
DefaultDatabase       = myDb
UserName              = usrname
Password              = pswrd
CharacterSet          = UTF8


Does anyone have any ideas on how to fix this Unicode error and get the SQL Alchemy Teradata engine working? Thank you in advance.
",1
331,50218823,Teradata gets division wrong?,"I have a coworker that doesn't want to include null rows in a percentile rank. The default Teradata function seems to just treat null as the lowest number in the set, so I decided to do the math manually. I started using the below query to test out my equation

drop table tmp;

create multiset volatile table tmp (
  num byteint
) primary index (num)
  on commit preserve rows
;

insert into tmp
values (1)
;insert into tmp
values (2)
;insert into tmp
values (1)
;insert into tmp
values (4)
;insert into tmp
values (null)
;insert into tmp
values (4)
;insert into tmp
values (null)
;insert into tmp
values (2)
;insert into tmp
values (9)
;insert into tmp
values (null)
;insert into tmp
values (10)
;insert into tmp
values (10)
;insert into tmp
values (11)
;

select
  num,
  case
    when num is null then 0
    else cast(dense_rank() over (partition by case when num is not null then 1 else 2 end order by num) as number)
  end as str_rnk,
  q.nn,
  str_rnk/q.nn as pct_rnk
from tmp
cross join (
    select cast(count(num) as number) as nn from tmp
) q
order by num
;


So what I expect to see in result set is this:

num   str_rnk  nn  pct_rnk
null        0  10        0
null        0  10        0
null        0  10        0
   1        1  10      0.1
   1        1  10      0.1
   2        2  10      0.2
   2        2  10      0.2
   4        3  10      0.3
   4        3  10      0.3
   9        4  10      0.4
  10        5  10      0.5
  10        5  10      0.5


But I'm getting a result that looks like it did a regular rank instead of a dense_rank, like this:

num   str_rnk  nn  pct_rnk
null        0  10        0
null        0  10        0
null        0  10        0
   1        1  10      0.1
   1        1  10      0.1
   2        2  10      0.3
   2        2  10      0.3
   4        3  10      0.5
   4        3  10      0.5
   9        4  10      0.7
  10        5  10      0.8
  10        5  10      0.8


I know I could set the rank in a subquery and It would calculate the way I expect it to, but why isn't it doing it the way I have it now?
",-1,-1,-1.0,"I have a coworker that doesn't want to include null rows in a percentile rank. The default Teradata function seems to just treat null as the lowest number in the set, so I decided to do the math manually. I started using the below query to test out my equation

drop table tmp;

create multiset volatile table tmp (
  num byteint
) primary index (num)
  on commit preserve rows
;

insert into tmp
values (1)
;insert into tmp
values (2)
;insert into tmp
values (1)
;insert into tmp
values (4)
;insert into tmp
values (null)
;insert into tmp
values (4)
;insert into tmp
values (null)
;insert into tmp
values (2)
;insert into tmp
values (9)
;insert into tmp
values (null)
;insert into tmp
values (10)
;insert into tmp
values (10)
;insert into tmp
values (11)
;

select
  num,
  case
    when num is null then 0
    else cast(dense_rank() over (partition by case when num is not null then 1 else 2 end order by num) as number)
  end as str_rnk,
  q.nn,
  str_rnk/q.nn as pct_rnk
from tmp
cross join (
    select cast(count(num) as number) as nn from tmp
) q
order by num
;


So what I expect to see in result set is this:

num   str_rnk  nn  pct_rnk
null        0  10        0
null        0  10        0
null        0  10        0
   1        1  10      0.1
   1        1  10      0.1
   2        2  10      0.2
   2        2  10      0.2
   4        3  10      0.3
   4        3  10      0.3
   9        4  10      0.4
  10        5  10      0.5
  10        5  10      0.5


But I'm getting a result that looks like it did a regular rank instead of a dense_rank, like this:

num   str_rnk  nn  pct_rnk
null        0  10        0
null        0  10        0
null        0  10        0
   1        1  10      0.1
   1        1  10      0.1
   2        2  10      0.3
   2        2  10      0.3
   4        3  10      0.5
   4        3  10      0.5
   9        4  10      0.7
  10        5  10      0.8
  10        5  10      0.8


I know I could set the rank in a subquery and It would calculate the way I expect it to, but why isn't it doing it the way I have it now?
",3
332,50378155,Teradata Select where date = previous workday/weekday,"Using Teradata 15.1x

I'm trying to select data for the previous weekday. I can get the previous day using 

cast(cast(cast(a.date_dim_ck as varchar(13)) as date format 'yyyymmdd') as date format 'mm-dd-yyyy') = current_date - 1


but when I try to do a case when to look back 3 days on Monday instead of 1 (to get Friday's data, see below), I get ""[3707] Syntax error, expected something like an 'END' keyword between ')' and '='.""

where case 
when td_day_of_week(current_date)&lt;&gt;2 
then
cast(cast(cast(a.date_dim_ck as varchar(13)) as date format 'yyyymmdd') as date format 'mm-dd-yyyy') = current_date - 1
else
cast(cast(cast(a.date_dim_ck as varchar(13)) as date format 'yyyymmdd') as date format 'mm-dd-yyyy') = current_date - 3


What am I doing wrong? 
",-1,-1,-1.0,"Using Teradata 15.1x

I'm trying to select data for the previous weekday. I can get the previous day using 

cast(cast(cast(a.date_dim_ck as varchar(13)) as date format 'yyyymmdd') as date format 'mm-dd-yyyy') = current_date - 1


but when I try to do a case when to look back 3 days on Monday instead of 1 (to get Friday's data, see below), I get ""[3707] Syntax error, expected something like an 'END' keyword between ')' and '='.""

where case 
when td_day_of_week(current_date)&lt;&gt;2 
then
cast(cast(cast(a.date_dim_ck as varchar(13)) as date format 'yyyymmdd') as date format 'mm-dd-yyyy') = current_date - 1
else
cast(cast(cast(a.date_dim_ck as varchar(13)) as date format 'yyyymmdd') as date format 'mm-dd-yyyy') = current_date - 3


What am I doing wrong? 
",3
333,50410309,how to trim trailing spaces in teradata table columns,"i want to trim trailing spaces for teradata table columns,

i do it like this,

trim(trailing from dictionary_managed_databases.dbname),


or use trim directly,

trim(dictionary_managed_databases.dbname),


but the result shows:


seems the trim do not work, 
not sure how to do it in teradata,
",-1,-1,-1.0,"i want to trim trailing spaces for teradata table columns,

i do it like this,

trim(trailing from dictionary_managed_databases.dbname),


or use trim directly,

trim(dictionary_managed_databases.dbname),


but the result shows:


seems the trim do not work, 
not sure how to do it in teradata,
",3
334,50453098,Can't use CONCAT in Teradata from SQLWorkbenchJ,"I have a script that used to run just fine from Teradata SQL Assistant. I am unable to get the exact same script to run from SQL Workbench/J. I have isolated the problem to one specific line. Here's the query:

SELECT
    variable1 as name1,
    variable2 as name2,
    CONCAT(TRIM(variable3), ':', trim(variable4)) as name3
    variable4 as name4,
FROM
    table1
WHERE
    variable4 between '2017-01-01' AND '2017-01-31';


The problem is the CONCAT line. If I comment that line out, code runs fine. If I leave that line in, I get the unhelpful message:

[Teradata Database] [TeraJDBC 16.10.00.07] [Error 3706] [SQLState 42000] 
Syntax error: expected something between '(' and the 'TRIM' keyword. [SQL State=42000, 
DB Errorcode=3706]
1 statement failed.


I say unhelpful, because that makes it sound like a syntax error, but this isn't a syntax error as far as Teradata is concerned. The exact same code ran fine on Windows Teradata SQL Assistant. But since OSX Teradata SQL Assistant is a dumpster fire, I have to try and run the script in SQL Workbench J.

Help, please? 
",-1,-1,-1.0,"I have a script that used to run just fine from Teradata SQL Assistant. I am unable to get the exact same script to run from SQL Workbench/J. I have isolated the problem to one specific line. Here's the query:

SELECT
    variable1 as name1,
    variable2 as name2,
    CONCAT(TRIM(variable3), ':', trim(variable4)) as name3
    variable4 as name4,
FROM
    table1
WHERE
    variable4 between '2017-01-01' AND '2017-01-31';


The problem is the CONCAT line. If I comment that line out, code runs fine. If I leave that line in, I get the unhelpful message:

[Teradata Database] [TeraJDBC 16.10.00.07] [Error 3706] [SQLState 42000] 
Syntax error: expected something between '(' and the 'TRIM' keyword. [SQL State=42000, 
DB Errorcode=3706]
1 statement failed.


I say unhelpful, because that makes it sound like a syntax error, but this isn't a syntax error as far as Teradata is concerned. The exact same code ran fine on Windows Teradata SQL Assistant. But since OSX Teradata SQL Assistant is a dumpster fire, I have to try and run the script in SQL Workbench J.

Help, please? 
",3
335,50633716,Trouble connecting to Teradata with teradata python module,"I have installed ODBC and done pip install teradata, but 

After trying to connect to teradata session, I get this error:

teradata.api.DatabaseError: (0, u'[HY000] [Teradata][ODBC Teradata Driver] Could not find security entry point')


Here is what my python script looks like:

 import teradata


udaExec = teradata.UdaExec(appName = 'table1', version='1.0', logConsole = False)

session = udaExec.connect(method = ""odbc"", system = ""db1"", username = ""user1"", password = ""pass1"", driver = ""Teradata"")


Any idea of what I am doing wrong? Thanks!
",-1,-1,-1.0,"I have installed ODBC and done pip install teradata, but 

After trying to connect to teradata session, I get this error:

teradata.api.DatabaseError: (0, u'[HY000] [Teradata][ODBC Teradata Driver] Could not find security entry point')


Here is what my python script looks like:

 import teradata


udaExec = teradata.UdaExec(appName = 'table1', version='1.0', logConsole = False)

session = udaExec.connect(method = ""odbc"", system = ""db1"", username = ""user1"", password = ""pass1"", driver = ""Teradata"")


Any idea of what I am doing wrong? Thanks!
",1
336,50640731,Teradata Error 2665: Invalid Date,"I am issuing the following SELECT on SQLA (Teradata version: 15.10.01.11):

select cast('2018-05-31' as date format 'yyyy-mm-dd') - interval '6' month;


And I am getting: SELECT Failed. 2665: Invalid Date.
Any help would be much appreciated.

Br,

Shardul
",1,-1,-1.0,"I am issuing the following SELECT on SQLA (Teradata version: 15.10.01.11):

select cast('2018-05-31' as date format 'yyyy-mm-dd') - interval '6' month;


And I am getting: SELECT Failed. 2665: Invalid Date.
Any help would be much appreciated.

Br,

Shardul
",3
337,50748443,Issue with Inserting records through Python into Teradata table,"I have a project where I am inputting a Terdata DB table name as a parameter, executing a SQL statement that provides aggregates on the table (min, max etc) for every column,  and returns that information where I then put it in a dataframe. 
What I would like to do is then take the rows that are in the dataframe (1 row per column_name in the table) and insert the results into another DB table 'Data Analysis' where the results will be stored.  

def main():
    def func_1(cfg_tbl):
        udaExec = teradata.UdaExec(appName=""DataAnalysis"", version=""1.0"", logConsole=False)
        main_query = """"""
        SELECT 'SELECT '''
        || TRIM(ColumnName)
        || ''', COUNT(DISTINCT ""' || ColumnName || '"") AS DISTINCT_COUNT,'
        || ' COUNT(1) - COUNT(""' || ColumnName || '"") AS NULL_COUNT,'
        || ' MAX(""' || ColumnName || '"") AS MAX_COL_VALUE,'
        || ' MIN(""' || ColumnName || '"") AS MIN_COL_VALUE,'
        || CASE WHEN ColumnType IN ('I', 'D', 'F', 'I1', 'I2', 'I8', 'N', 'DA', 'TS') THEN ' MAX(LENGTH(TO_CHAR(""' || ColumnName || '"")))'
                WHEN ColumnType IN ('CF', 'CV', 'CO') THEN ' MAX(LENGTH(""' || ColumnName || '""))'
                ELSE NULL END || ' AS MAX_COLUMN_LENGTH,'
        || CASE WHEN ColumnType IN ('I', 'D', 'F', 'I1', 'I2', 'I8', 'N', 'DA', 'TS') THEN ' MIN(LENGTH(TO_CHAR(""' || ColumnName || '"")))'
                WHEN ColumnType IN ('CF', 'CV', 'CO') THEN ' MIN(LENGTH(""' || ColumnName || '""))'
                ELSE NULL END || ' AS MIN_COLUMN_LENGTH,'
        || ' COUNT(1) AS TABLE_COUNT,'
        || ' ''%s_%s'' AS TABLE_NM,'
        || ' ''%s'' AS SOURCE_TYPE'
        || ' FROM ' || TRIM(DatabaseName) || '.' || TRIM(TableName) || ';' AS COL
        FROM DBC.ColumnsV A
        WHERE DatabaseName = 'XXX'
        AND TableName = '%s_%s'
        """""" % (cfg_pre, cfg_tbl, cfg_src, cfg_pre, cfg_tbl)
        #  connect to Teradata, execute above sql and subsequent SELECT statements
        session = udaExec.connect(method=""odbc"", dsn=""XXXX"", username=""XXX"", password=""XXX"")
        pd.set_option('max_colwidth', 500)
        df = pd.read_sql(main_query, session)
        sql_execute = list(df.values.flatten())  
        col = ['COLUMN_NAME', 'DISTINCT_COUNT', 'NULL_COUNT', 'MAX_COL_VALUE', 'MIN_COL_VALUE',
        'MAX_COL_LENGTH', 'MIN_COL_LENGTH', 'TABLE_CNT', 'TABLE_NM', 'DATA_SOURCE']

        jdf = pd.DataFrame(columns=col)
        for script in sql_execute:
            jdf = pd.read_sql(script, session)
            print(jdf)
            session.execute(""""""INSERT INTO DATA_ANALYSIS(COLUMN_NM, DISTINCT_COUNT, NULL_COUNT, MAX_COL_VALUE, MIN_COL_VALUE,
                                            MAX_COL_LENGTH, MIN_COL_LENGTH, TABLE_CNT, TABLE_NM, DATA_SOURCE) 
                                            VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"""""", (jdf[0], jdf[1], jdf[2], jdf[3], 
                                                                                     jdf[4], jdf[5], jdf[6], jdf[7], jdf[8], jdf[9]))


The problem I am facing is with the INSERT code at the bottom.   I am getting a KeyError: 0, I know I must be doing something incorrectly when I am inserting. Any ideas?

The exact error is:

    Traceback (most recent call last):
  File ""C:\Users\xxx\AppData\Local\Continuum\Anaconda3\lib\site-packages\pandas\core\indexes\base.py"", line 2442, in get_loc
    return self._engine.get_loc(key)
  File ""pandas\_libs\index.pyx"", line 132, in pandas._libs.index.IndexEngine.get_loc (pandas\_libs\index.c:5280)
  File ""pandas\_libs\index.pyx"", line 154, in pandas._libs.index.IndexEngine.get_loc (pandas\_libs\index.c:5126)
  File ""pandas\_libs\hashtable_class_helper.pxi"", line 1210, in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas\_libs\hashtable.c:20523)
  File ""pandas\_libs\hashtable_class_helper.pxi"", line 1218, in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas\_libs\hashtable.c:20477)
KeyError: 0

",-1,-1,-1.0,"I have a project where I am inputting a Terdata DB table name as a parameter, executing a SQL statement that provides aggregates on the table (min, max etc) for every column,  and returns that information where I then put it in a dataframe. 
What I would like to do is then take the rows that are in the dataframe (1 row per column_name in the table) and insert the results into another DB table 'Data Analysis' where the results will be stored.  

def main():
    def func_1(cfg_tbl):
        udaExec = teradata.UdaExec(appName=""DataAnalysis"", version=""1.0"", logConsole=False)
        main_query = """"""
        SELECT 'SELECT '''
        || TRIM(ColumnName)
        || ''', COUNT(DISTINCT ""' || ColumnName || '"") AS DISTINCT_COUNT,'
        || ' COUNT(1) - COUNT(""' || ColumnName || '"") AS NULL_COUNT,'
        || ' MAX(""' || ColumnName || '"") AS MAX_COL_VALUE,'
        || ' MIN(""' || ColumnName || '"") AS MIN_COL_VALUE,'
        || CASE WHEN ColumnType IN ('I', 'D', 'F', 'I1', 'I2', 'I8', 'N', 'DA', 'TS') THEN ' MAX(LENGTH(TO_CHAR(""' || ColumnName || '"")))'
                WHEN ColumnType IN ('CF', 'CV', 'CO') THEN ' MAX(LENGTH(""' || ColumnName || '""))'
                ELSE NULL END || ' AS MAX_COLUMN_LENGTH,'
        || CASE WHEN ColumnType IN ('I', 'D', 'F', 'I1', 'I2', 'I8', 'N', 'DA', 'TS') THEN ' MIN(LENGTH(TO_CHAR(""' || ColumnName || '"")))'
                WHEN ColumnType IN ('CF', 'CV', 'CO') THEN ' MIN(LENGTH(""' || ColumnName || '""))'
                ELSE NULL END || ' AS MIN_COLUMN_LENGTH,'
        || ' COUNT(1) AS TABLE_COUNT,'
        || ' ''%s_%s'' AS TABLE_NM,'
        || ' ''%s'' AS SOURCE_TYPE'
        || ' FROM ' || TRIM(DatabaseName) || '.' || TRIM(TableName) || ';' AS COL
        FROM DBC.ColumnsV A
        WHERE DatabaseName = 'XXX'
        AND TableName = '%s_%s'
        """""" % (cfg_pre, cfg_tbl, cfg_src, cfg_pre, cfg_tbl)
        #  connect to Teradata, execute above sql and subsequent SELECT statements
        session = udaExec.connect(method=""odbc"", dsn=""XXXX"", username=""XXX"", password=""XXX"")
        pd.set_option('max_colwidth', 500)
        df = pd.read_sql(main_query, session)
        sql_execute = list(df.values.flatten())  
        col = ['COLUMN_NAME', 'DISTINCT_COUNT', 'NULL_COUNT', 'MAX_COL_VALUE', 'MIN_COL_VALUE',
        'MAX_COL_LENGTH', 'MIN_COL_LENGTH', 'TABLE_CNT', 'TABLE_NM', 'DATA_SOURCE']

        jdf = pd.DataFrame(columns=col)
        for script in sql_execute:
            jdf = pd.read_sql(script, session)
            print(jdf)
            session.execute(""""""INSERT INTO DATA_ANALYSIS(COLUMN_NM, DISTINCT_COUNT, NULL_COUNT, MAX_COL_VALUE, MIN_COL_VALUE,
                                            MAX_COL_LENGTH, MIN_COL_LENGTH, TABLE_CNT, TABLE_NM, DATA_SOURCE) 
                                            VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"""""", (jdf[0], jdf[1], jdf[2], jdf[3], 
                                                                                     jdf[4], jdf[5], jdf[6], jdf[7], jdf[8], jdf[9]))


The problem I am facing is with the INSERT code at the bottom.   I am getting a KeyError: 0, I know I must be doing something incorrectly when I am inserting. Any ideas?

The exact error is:

    Traceback (most recent call last):
  File ""C:\Users\xxx\AppData\Local\Continuum\Anaconda3\lib\site-packages\pandas\core\indexes\base.py"", line 2442, in get_loc
    return self._engine.get_loc(key)
  File ""pandas\_libs\index.pyx"", line 132, in pandas._libs.index.IndexEngine.get_loc (pandas\_libs\index.c:5280)
  File ""pandas\_libs\index.pyx"", line 154, in pandas._libs.index.IndexEngine.get_loc (pandas\_libs\index.c:5126)
  File ""pandas\_libs\hashtable_class_helper.pxi"", line 1210, in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas\_libs\hashtable.c:20523)
  File ""pandas\_libs\hashtable_class_helper.pxi"", line 1218, in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas\_libs\hashtable.c:20477)
KeyError: 0

",1
338,50825475,kafka-connect-jdbc Teradata sink connector error,"I am testing the kafka-connect-jdbc sink connector for teradata. I am getting the below error while running the connector. Is the create table option is not supported by teradata jdbc driver?

[2018-06-12 10:55:54,172] INFO product:Teradata schema:null catalog: -- table:TEST_KAFKA_BULK is absent (io.confluent.connect.jdbc.sink.DbMetadataQueries:60)
[2018-06-12 10:55:54,174] ERROR Task JDBCTDSinkBulk-0 threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerSinkTask:449)
java.lang.UnsupportedOperationException
        at io.confluent.connect.jdbc.sink.dialect.GenericDialect.getCreateQuery(GenericDialect.java:34)
        at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:90)
        at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:63)
        at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:78)
        at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
        at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:69)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:429)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:250)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:179)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:148)
        at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:139)
        at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:182)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2018-06-12 10:55:54,177] ERROR Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerSinkTask:450)
[2018-06-12 10:55:54,177] ERROR Task JDBCTDSinkBulk-0 threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:141)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
        at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:451)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:250)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:179)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:148)
        at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:139)
        at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:182)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2018-06-12 10:55:54,178] ERROR Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:142)

",-1,-1,-1.0,"I am testing the kafka-connect-jdbc sink connector for teradata. I am getting the below error while running the connector. Is the create table option is not supported by teradata jdbc driver?

[2018-06-12 10:55:54,172] INFO product:Teradata schema:null catalog: -- table:TEST_KAFKA_BULK is absent (io.confluent.connect.jdbc.sink.DbMetadataQueries:60)
[2018-06-12 10:55:54,174] ERROR Task JDBCTDSinkBulk-0 threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerSinkTask:449)
java.lang.UnsupportedOperationException
        at io.confluent.connect.jdbc.sink.dialect.GenericDialect.getCreateQuery(GenericDialect.java:34)
        at io.confluent.connect.jdbc.sink.DbStructure.create(DbStructure.java:90)
        at io.confluent.connect.jdbc.sink.DbStructure.createOrAmendIfNecessary(DbStructure.java:63)
        at io.confluent.connect.jdbc.sink.BufferedRecords.add(BufferedRecords.java:78)
        at io.confluent.connect.jdbc.sink.JdbcDbWriter.write(JdbcDbWriter.java:66)
        at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:69)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:429)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:250)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:179)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:148)
        at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:139)
        at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:182)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2018-06-12 10:55:54,177] ERROR Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerSinkTask:450)
[2018-06-12 10:55:54,177] ERROR Task JDBCTDSinkBulk-0 threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:141)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
        at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:451)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:250)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:179)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:148)
        at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:139)
        at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:182)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2018-06-12 10:55:54,178] ERROR Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:142)

",0
339,50957634,Syntax error while fetching table list from teradata using python,"For learning purpose, I am trying to fetch table list from Teradata
in Python with the following code:

import jaydebeapi
import pandas as pd

try:
   conn = jaydebeapi.connect(jclassname='com.teradata.jdbc.TeraDriver',
                                      url=""jdbc:teradata://10.10.10.10"",
                                      driver_args=['@user','@pss'],
                                      jars=['/mnt/TERADATA/tdgssconfig.jar','/mnt/TERADATA/terajdbc4.jar'])

   print(""Connection was successful"")
except Exception as e:
        print(e)

 #df = pd.read_sql_query('SELECT Databasename,TableName FROM dbc.tables WHERE tablekind = \'T\'',conn)
  dfr = pd.DataFrame(df)
  print(dfr.head(3))


The above works fine but when I try to fetch the table list based on a Database name then things are not working:

 df = pd.read_sql_query('SELECT Databasename,TableName FROM dbc.tables WHERE tablekind = \'T\' and DatabaseName ='SALES'',conn)
 dfr = pd.DataFrame(df)
 print(dfr.head(3))


getting:

Error: Invalid Syntax


It has been couple of hours trying to figure it out but not able to get through it. Its very stupid to ask this but please advice where am I going wrong.

Ref: https://forgetcode.com/Teradata/1433-To-get-all-the-tables

",-1,-1,-1.0,"For learning purpose, I am trying to fetch table list from Teradata
in Python with the following code:

import jaydebeapi
import pandas as pd

try:
   conn = jaydebeapi.connect(jclassname='com.teradata.jdbc.TeraDriver',
                                      url=""jdbc:teradata://10.10.10.10"",
                                      driver_args=['@user','@pss'],
                                      jars=['/mnt/TERADATA/tdgssconfig.jar','/mnt/TERADATA/terajdbc4.jar'])

   print(""Connection was successful"")
except Exception as e:
        print(e)

 #df = pd.read_sql_query('SELECT Databasename,TableName FROM dbc.tables WHERE tablekind = \'T\'',conn)
  dfr = pd.DataFrame(df)
  print(dfr.head(3))


The above works fine but when I try to fetch the table list based on a Database name then things are not working:

 df = pd.read_sql_query('SELECT Databasename,TableName FROM dbc.tables WHERE tablekind = \'T\' and DatabaseName ='SALES'',conn)
 dfr = pd.DataFrame(df)
 print(dfr.head(3))


getting:

Error: Invalid Syntax


It has been couple of hours trying to figure it out but not able to get through it. Its very stupid to ask this but please advice where am I going wrong.

Ref: https://forgetcode.com/Teradata/1433-To-get-all-the-tables

",1
340,51051745,Teradata - Pandas TypeError: 'NoneType' object is not iterable,"I am trying to run the following code to create a Teradata table using the teradata python library:

import teradata
import pandas as pd

udaExec = teradata.UdaExec (appName=""Hello"", version=""1.0"",
    logConsole=False)
session = udaExec.connect(method=""odbc"", system=""tdprod"",
    username=""xxx"", password=""xxx"");

sqlStr = ""CREATE SET TABLE \""TEST123\"" \
(col1 INTEGER) PRIMARY INDEX (col1);""

result = pd.read_sql(sqlStr, self.session)


I am receiving the following error:

File ""..\pandas\io\sql.py"", line 1436, in read_query
columns = [col_desc[0] for col_desc in cursor.description]

TypeError: 'NoneType' object is not iterable


Any idea on how to solve this? 
",-1,-1,-1.0,"I am trying to run the following code to create a Teradata table using the teradata python library:

import teradata
import pandas as pd

udaExec = teradata.UdaExec (appName=""Hello"", version=""1.0"",
    logConsole=False)
session = udaExec.connect(method=""odbc"", system=""tdprod"",
    username=""xxx"", password=""xxx"");

sqlStr = ""CREATE SET TABLE \""TEST123\"" \
(col1 INTEGER) PRIMARY INDEX (col1);""

result = pd.read_sql(sqlStr, self.session)


I am receiving the following error:

File ""..\pandas\io\sql.py"", line 1436, in read_query
columns = [col_desc[0] for col_desc in cursor.description]

TypeError: 'NoneType' object is not iterable


Any idea on how to solve this? 
",1
341,50719596,Teradata/SQL : How to compare dates in consecutive rows using teradata SQL and write case statement on the result,"I need to compare a date (end dt) in 1st row with the date (strt dt) in secord row, if it matches I need to pick up strt dt of 2nd row.
if it dosent match I need to pick up strt dt of 1st row. There can be multiple rows for one subscription and channel package.
I am finding difficulty in writting a query in teradata to get the desired output.

Below are the cases and expected results. 

Case 1          

Subscription    Channel package Start dt    End dt
11111112    Sports  7/3/2015    11/28/2015
11111112    Sports  4/1/2016    11/23/2016
11111112    Sports  11/23/2016  12/17/2017


Require Output          

Subscription    Channel package Start dt    End dt
11111112    Sports  4/1/2016    12/17/2017


Case 2          

Subscription    Channel package Start dt    End dt
11111112    Sports  7/3/2015    11/28/2015
11111112    Sports  11/28/2015  4/1/2016
11111112    Sports  4/1/2016    11/23/2016
11111112    Sports  11/23/2016  12/17/2017


Require Output          

Subscription    Channel package Start dt    End dt
11111112    Sports  7/3/2015    12/17/2017


I tried it using 

MIN(Start dt) OVER (PARTITION BY Subscription   , Channel package   
                    ORDER BY Start dt ROWS BETWEEN 1 FOLLOWING  AND 1 FOLLOWING
                   ) AS NXT Start dt


But couldn't go further as i m facing some issues while using qualify. 
",-1,-1,-1.0,"I need to compare a date (end dt) in 1st row with the date (strt dt) in secord row, if it matches I need to pick up strt dt of 2nd row.
if it dosent match I need to pick up strt dt of 1st row. There can be multiple rows for one subscription and channel package.
I am finding difficulty in writting a query in teradata to get the desired output.

Below are the cases and expected results. 

Case 1          

Subscription    Channel package Start dt    End dt
11111112    Sports  7/3/2015    11/28/2015
11111112    Sports  4/1/2016    11/23/2016
11111112    Sports  11/23/2016  12/17/2017


Require Output          

Subscription    Channel package Start dt    End dt
11111112    Sports  4/1/2016    12/17/2017


Case 2          

Subscription    Channel package Start dt    End dt
11111112    Sports  7/3/2015    11/28/2015
11111112    Sports  11/28/2015  4/1/2016
11111112    Sports  4/1/2016    11/23/2016
11111112    Sports  11/23/2016  12/17/2017


Require Output          

Subscription    Channel package Start dt    End dt
11111112    Sports  7/3/2015    12/17/2017


I tried it using 

MIN(Start dt) OVER (PARTITION BY Subscription   , Channel package   
                    ORDER BY Start dt ROWS BETWEEN 1 FOLLOWING  AND 1 FOLLOWING
                   ) AS NXT Start dt


But couldn't go further as i m facing some issues while using qualify. 
",3
342,50692788,"can not start teradata database using ""tpa start""","when i try to logon bteq, i get the following error,

*** Warning: RDBMS CRASHED OR SESSIONS RESET.  RECOVERY IN PROGRESS.


and then i try to restart the database,

pdestate -a


status:

PDE state: DOWN/HARDSTOP


start:

/etc/init.d/tpa start


result:

Teradata Database Initiator service is starting...
Teradata Database Initiator service started successfully.


then check the status again:

PDE state: DOWN/HARDSTOP


i tried many times, but still can not start database, don't know the reason, 
",-1,-1,-1.0,"when i try to logon bteq, i get the following error,

*** Warning: RDBMS CRASHED OR SESSIONS RESET.  RECOVERY IN PROGRESS.


and then i try to restart the database,

pdestate -a


status:

PDE state: DOWN/HARDSTOP


start:

/etc/init.d/tpa start


result:

Teradata Database Initiator service is starting...
Teradata Database Initiator service started successfully.


then check the status again:

PDE state: DOWN/HARDSTOP


i tried many times, but still can not start database, don't know the reason, 
",1
343,50684215,Column foo not found tablename - Teradata,"I'm using Teradata Express Studio with this query:

SELECT column1
FROM table1
INNER JOIN table2
ON table1.column2 = table2.column3
WHERE table2.column4 IN ""foo"";


I'm getting the following error:

Executed as Single statement.  Failed [5628 : HY000] Column foo not found in table2 or table1.


I am trying to search for an entry where column4's value is foo, but it seems to think that foo is the column name, can anyone assist me with this?
",-1,-1,-1.0,"I'm using Teradata Express Studio with this query:

SELECT column1
FROM table1
INNER JOIN table2
ON table1.column2 = table2.column3
WHERE table2.column4 IN ""foo"";


I'm getting the following error:

Executed as Single statement.  Failed [5628 : HY000] Column foo not found in table2 or table1.


I am trying to search for an entry where column4's value is foo, but it seems to think that foo is the column name, can anyone assist me with this?
",3
344,50627049,Execute Teradata UDF Triggers Data Type does not match a Defined Type name,"I have the following Teradata UDF:

REPLACE FUNCTION MIGRATION_TOOL.GET_OBJECT_TYPE(OBJECT_KIND VARCHAR(2))
RETURNS CHARACTER(20)
CONTAINS SQL
SPECIFIC MIGRATION_TOOL.GET_OBJECT_TYPE
COLLATION INVOKER
INLINE TYPE 1
RETURN CASE OBJECT_KIND
    WHEN 'O' THEN 'Table' --1
    WHEN 'T' THEN 'Table' --1
    WHEN 'V' THEN 'View'  --2
    WHEN 'M' THEN 'Macro' --3
    WHEN 'G' THEN 'Trigger'  --4
    WHEN 'P' THEN 'Stored Procedure'  --5
    WHEN 'I' THEN 'Join Index'  --6
    WHEN 'F' THEN 'Function'  --7
    WHEN 'FK' THEN 'Foreign Key' --0
    ELSE 'ND'
END;


And I have the following query using the UDF:

SELECT A.DATABASE_ID,
B.TABLENAME AS OBJECT_NAME,
GET_OBJECT_TYPE(B.TABLEKIND) AS OBJECT_TYPE,
MAX(CASE WHEN IDCOLTYPE IS NULL THEN 0 ELSE 1 END) AS FLAG_IDENTITY_COLUMN
FROM MIGRATION_TOOL.VW_TERADATA_DATABASE A
INNER JOIN DBC.TABLESV B ON A.DATABASE_NAME = B.DATABASENAME
LEFT JOIN DBC.COLUMNSV C ON B.DATABASENAME = C.DATABASENAME AND B.TABLENAME = C.TABLENAME
WHERE B.TABLENAME NOT LIKE ALL ('BKP%')
GROUP BY 1, 2, 3


The query fails with the following error message:


  Executed as Single statement.  Failed [3706 : 42000] Syntax error: Data Type  ""TABLEKIND"" does not match a Defined Type name. 
  Elapsed time = 00:00:00.283 
  STATEMENT 1: Select Statement failed. 


I checked the data type of TableKind and it is CHAR(1) so it should be accepted by the UDF which accepts VARCHAR(2). Any idea about the cause the issue?
",-1,-1,-1.0,"I have the following Teradata UDF:

REPLACE FUNCTION MIGRATION_TOOL.GET_OBJECT_TYPE(OBJECT_KIND VARCHAR(2))
RETURNS CHARACTER(20)
CONTAINS SQL
SPECIFIC MIGRATION_TOOL.GET_OBJECT_TYPE
COLLATION INVOKER
INLINE TYPE 1
RETURN CASE OBJECT_KIND
    WHEN 'O' THEN 'Table' --1
    WHEN 'T' THEN 'Table' --1
    WHEN 'V' THEN 'View'  --2
    WHEN 'M' THEN 'Macro' --3
    WHEN 'G' THEN 'Trigger'  --4
    WHEN 'P' THEN 'Stored Procedure'  --5
    WHEN 'I' THEN 'Join Index'  --6
    WHEN 'F' THEN 'Function'  --7
    WHEN 'FK' THEN 'Foreign Key' --0
    ELSE 'ND'
END;


And I have the following query using the UDF:

SELECT A.DATABASE_ID,
B.TABLENAME AS OBJECT_NAME,
GET_OBJECT_TYPE(B.TABLEKIND) AS OBJECT_TYPE,
MAX(CASE WHEN IDCOLTYPE IS NULL THEN 0 ELSE 1 END) AS FLAG_IDENTITY_COLUMN
FROM MIGRATION_TOOL.VW_TERADATA_DATABASE A
INNER JOIN DBC.TABLESV B ON A.DATABASE_NAME = B.DATABASENAME
LEFT JOIN DBC.COLUMNSV C ON B.DATABASENAME = C.DATABASENAME AND B.TABLENAME = C.TABLENAME
WHERE B.TABLENAME NOT LIKE ALL ('BKP%')
GROUP BY 1, 2, 3


The query fails with the following error message:


  Executed as Single statement.  Failed [3706 : 42000] Syntax error: Data Type  ""TABLEKIND"" does not match a Defined Type name. 
  Elapsed time = 00:00:00.283 
  STATEMENT 1: Select Statement failed. 


I checked the data type of TableKind and it is CHAR(1) so it should be accepted by the UDF which accepts VARCHAR(2). Any idea about the cause the issue?
",3
345,51539166,"Trying to parse a JSON key in Teradata, but the query fails","I have a field called JSON_VALUE is my database. I'm trying the get the value for one of the keys in the JSON called book. It exists in every record and is unique.

The query I created, with help of Teradata manual:

SELECT JSONGETVALUE(NEW JSON(JSON_VALUE, LATIN), '$.book' AS VARCHAR) FROM BOOK_TABLE


This fails with an error: The string contains an untranslatable character.

I tried checking whether my JSON is valid with JSON_CHECK, but it showed OK for each record.

Am I doing something wrong or missing anything?

Thanks
",-1,-1,-1.0,"I have a field called JSON_VALUE is my database. I'm trying the get the value for one of the keys in the JSON called book. It exists in every record and is unique.

The query I created, with help of Teradata manual:

SELECT JSONGETVALUE(NEW JSON(JSON_VALUE, LATIN), '$.book' AS VARCHAR) FROM BOOK_TABLE


This fails with an error: The string contains an untranslatable character.

I tried checking whether my JSON is valid with JSON_CHECK, but it showed OK for each record.

Am I doing something wrong or missing anything?

Thanks
",3
346,51592545,Teradata.Net.Data.Provider not support in Pivotal Cloud Foundry(PCF) environment,"Teradata.Net.Data.Provider methods working as per expected in local environment. But after deploying it on PCF its throwing error while trying to interact with Tera data through Teradat.Net.Data.Provider methods.
",-1,-1,-1.0,"Teradata.Net.Data.Provider methods working as per expected in local environment. But after deploying it on PCF its throwing error while trying to interact with Tera data through Teradat.Net.Data.Provider methods.
",1
347,51616243,Recursive script in Teradata,"I have a table in Teradata, consisting of five main columns below:

Old   New   Type_of_Old    Type_of_New    Change_Dt
----------------------------------------------------
A     B         0             1            Date 1
B     C         1             1            Date 2
C     D         1             0            Date 3
D     B         0             1            Date 4
B     E         1             1            Date 5


I want to bind old - new values starting from the first elements whose type_of_old = 0 and cut the bound when it reaches to the last type_of_old = 1. At the end i want to have this data:

Fırst    Next       Change Dt
-------------------------------
A        B          Date 1
A        C          Date 2
A        D          Date 3
D        B          Date 4
D        E          Date 5


The problem is, Element B is next element of both A and D in different change dates. So when i use WITH RECURSIVE script it binds next elements of B, with both A end D starting elements.

So far, i wrote this script but i couldn't manage to cut the bound when it reaches to type_of_old = 1

CREATE MULTISET TABLE TEST.TABLE_A ( OLD_ELM VARCHAR(1), NEW_ELM VARCHAR(1), 
TYPE_OF_OLD BYTEINT , TYPE_OF_NEW BYTEINT, CHANGE_DT DATE)

INSERT INTO TEST.TABLE_A ('A','B',0, 1, DATE '2018-01-01');
INSERT INTO TEST.TABLE_A ('B','C',1, 1, DATE '2018-02-01');
INSERT INTO TEST.TABLE_A ('C','D',1, 0, DATE '2018-03-01');
INSERT INTO TEST.TABLE_A ('D','B',0, 1, DATE '2018-04-01');
INSERT INTO TEST.TABLE_A ('B','E',1, 1, DATE '2018-05-01');

WITH RECURSIVE ELEMENT_PATH ( FIRST_ELM, NEXT_ELM, CHANGE_DT ) AS 
( SELECT OLD_ELM, NEW_ELM, CHANGE_DT
   FROM TEST.TABLE_A
  WHERE TYPE_OF_OLD = 0
    AND TYPE_OF_NEW = 1

  UNION ALL

 SELECT P.FIRST_ELM, A.NEW_ELM, A.CHANGE_DT
   FROM ELEMENT_PATH P
   JOIN TEST.TABLE_A A ON A.OLD_ELM = P.NEXT_ELM
  WHERE A.CHANGE_DT &gt; P.CHANGE_DT) 
 SELECT * FROM ELEMENT_PATH


Data from this script is as follows:

FIRST_ELM   NEXT_ELM    CHANGE_DT
   A           B        01.01.2018
   A           C        01.02.2018
   A           D        01.03.2018
   D           B        01.04.2018
   D           E        01.05.2018
   A           B        01.04.2018 (Wrong)
   A           E        01.05.2018 (Wrong)
   A           E        01.05.2018 (Wrong)


How can i fix it?

Thanks in advance,

Regards
",-1,-1,-1.0,"I have a table in Teradata, consisting of five main columns below:

Old   New   Type_of_Old    Type_of_New    Change_Dt
----------------------------------------------------
A     B         0             1            Date 1
B     C         1             1            Date 2
C     D         1             0            Date 3
D     B         0             1            Date 4
B     E         1             1            Date 5


I want to bind old - new values starting from the first elements whose type_of_old = 0 and cut the bound when it reaches to the last type_of_old = 1. At the end i want to have this data:

Fırst    Next       Change Dt
-------------------------------
A        B          Date 1
A        C          Date 2
A        D          Date 3
D        B          Date 4
D        E          Date 5


The problem is, Element B is next element of both A and D in different change dates. So when i use WITH RECURSIVE script it binds next elements of B, with both A end D starting elements.

So far, i wrote this script but i couldn't manage to cut the bound when it reaches to type_of_old = 1

CREATE MULTISET TABLE TEST.TABLE_A ( OLD_ELM VARCHAR(1), NEW_ELM VARCHAR(1), 
TYPE_OF_OLD BYTEINT , TYPE_OF_NEW BYTEINT, CHANGE_DT DATE)

INSERT INTO TEST.TABLE_A ('A','B',0, 1, DATE '2018-01-01');
INSERT INTO TEST.TABLE_A ('B','C',1, 1, DATE '2018-02-01');
INSERT INTO TEST.TABLE_A ('C','D',1, 0, DATE '2018-03-01');
INSERT INTO TEST.TABLE_A ('D','B',0, 1, DATE '2018-04-01');
INSERT INTO TEST.TABLE_A ('B','E',1, 1, DATE '2018-05-01');

WITH RECURSIVE ELEMENT_PATH ( FIRST_ELM, NEXT_ELM, CHANGE_DT ) AS 
( SELECT OLD_ELM, NEW_ELM, CHANGE_DT
   FROM TEST.TABLE_A
  WHERE TYPE_OF_OLD = 0
    AND TYPE_OF_NEW = 1

  UNION ALL

 SELECT P.FIRST_ELM, A.NEW_ELM, A.CHANGE_DT
   FROM ELEMENT_PATH P
   JOIN TEST.TABLE_A A ON A.OLD_ELM = P.NEXT_ELM
  WHERE A.CHANGE_DT &gt; P.CHANGE_DT) 
 SELECT * FROM ELEMENT_PATH


Data from this script is as follows:

FIRST_ELM   NEXT_ELM    CHANGE_DT
   A           B        01.01.2018
   A           C        01.02.2018
   A           D        01.03.2018
   D           B        01.04.2018
   D           E        01.05.2018
   A           B        01.04.2018 (Wrong)
   A           E        01.05.2018 (Wrong)
   A           E        01.05.2018 (Wrong)


How can i fix it?

Thanks in advance,

Regards
",3
348,51746519,MOD operator in Teradata SQL Assistent 15,"can't understand why my old sripts didn't work. Problem in MOD operator. 

SELECT MOD(20,2)

Error:
Query failed. 3706: Syntax error ...

Can't understand why MOD operator doesnt work any more.

Teradata SQL Assistent 15.10
",-1,-1,-1.0,"can't understand why my old sripts didn't work. Problem in MOD operator. 

SELECT MOD(20,2)

Error:
Query failed. 3706: Syntax error ...

Can't understand why MOD operator doesnt work any more.

Teradata SQL Assistent 15.10
",3
349,51971239,Connecting R and Teradata using JDBC,"Connect R and Teradata using JDBC

I've followed the steps above and run into an error after the first step:


  Error in .jfindClass(as.character(driverClass)[1]) :
  java.lang.ClassNotFoundException


Yes, I have the jdbc driver downloaded and the files stored in the directory listed. rJava and RJDBC packages installed and loaded too.

EDIT: The following block of code:

drv = JDBC(""com.teradata.jdbc.TeraDriver"", c(""\\Users\\bo762818\\Downloads\\TeraJDBC__indep_indep.16.20.00.08\\tdgssconfig.jar"",
                                         ""\\Users\\bo762818\\Downloads\\TeraJDBC__indep_indep.16.20.00.08\\terajdbc4.jar""))


yields the below error:


  Error in .jfindClass(as.character(driverClass)[1]) :
  java.lang.ClassNotFoundException

",-1,-1,-1.0,"Connect R and Teradata using JDBC

I've followed the steps above and run into an error after the first step:


  Error in .jfindClass(as.character(driverClass)[1]) :
  java.lang.ClassNotFoundException


Yes, I have the jdbc driver downloaded and the files stored in the directory listed. rJava and RJDBC packages installed and loaded too.

EDIT: The following block of code:

drv = JDBC(""com.teradata.jdbc.TeraDriver"", c(""\\Users\\bo762818\\Downloads\\TeraJDBC__indep_indep.16.20.00.08\\tdgssconfig.jar"",
                                         ""\\Users\\bo762818\\Downloads\\TeraJDBC__indep_indep.16.20.00.08\\terajdbc4.jar""))


yields the below error:


  Error in .jfindClass(as.character(driverClass)[1]) :
  java.lang.ClassNotFoundException

",1
350,51997479,connect teradata DB using sqlalchemy,"I am trying to connect to Teradata using SqlAlchemy.

Below is the code:

from sqlalchemy import create_engine

username = 'userName'
password = 'pass#word'
host = 'hostname'
query = 'sel count(*) from databaseName.tableName;'

link = 'teradata://'+ username +':'+ password +'@'+host+'/'+'?driver=/teradata/client/15.10/odbc_64/lib/tdata.so'

td_engine = create_engine(link)
result = td_engine.execute(query)


Error:

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2074, in execute
    connection = self.contextual_connect(close_with_result=True)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2123, in contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2162, in _wrap_pool_connect
    e, dialect, self)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1476, in _handle_dbapi_exception_noconnection
    exc_info
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/util/compat.py"", line 203, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2158, in _wrap_pool_connect
    return fn()
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 413, in connect
    return _ConnectionFairy._checkout(self, self._threadconns)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 788, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 532, in checkout
    rec = pool._do_get()
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 1096, in _do_get
    c = self._create_connection()
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 350, in _create_connection
    return _ConnectionRecord(self)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 477, in __init__
    self.__connect(first_connect_check=True)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 671, in __connect
    connection = pool._invoke_creator(self)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py"", line 106, in connect
    return dialect.connect(*cargs, **cparams)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 410, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File ""/ftp/lib/python2.7/site-packages/teradata/tdodbc.py"", line 427, in __init__
    connectParams[""DRIVER""] = determineDriver(dbType, driver)
  File ""/ftp/lib/python2.7/site-packages/teradata/tdodbc.py"", line 381, in determineDriver
    "" Available drivers: {}"".format(driver, "","".join(drivers)))
sqlalchemy.exc.InterfaceError: (teradata.api.InterfaceError) ('DRIVER_NOT_FOUND', ""No driver found with name '/teradata/client/15.10/odbc_64/lib/tdata.so'.  Available drivers: composite70 ,composite70_2x ,composite70_x64 ,"") (Background on this error at: http://sqlalche.me/e/rvf5)


Please suggest what could be the issue. I gave actual path of Teradata driver, i even tried changing driver to /teradata/client/15.10/odbc_64/lib and Teradata but still error is same, it is not able to find the Driver. 
",-1,-1,-1.0,"I am trying to connect to Teradata using SqlAlchemy.

Below is the code:

from sqlalchemy import create_engine

username = 'userName'
password = 'pass#word'
host = 'hostname'
query = 'sel count(*) from databaseName.tableName;'

link = 'teradata://'+ username +':'+ password +'@'+host+'/'+'?driver=/teradata/client/15.10/odbc_64/lib/tdata.so'

td_engine = create_engine(link)
result = td_engine.execute(query)


Error:

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2074, in execute
    connection = self.contextual_connect(close_with_result=True)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2123, in contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2162, in _wrap_pool_connect
    e, dialect, self)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1476, in _handle_dbapi_exception_noconnection
    exc_info
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/util/compat.py"", line 203, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 2158, in _wrap_pool_connect
    return fn()
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 413, in connect
    return _ConnectionFairy._checkout(self, self._threadconns)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 788, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 532, in checkout
    rec = pool._do_get()
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 1096, in _do_get
    c = self._create_connection()
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 350, in _create_connection
    return _ConnectionRecord(self)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 477, in __init__
    self.__connect(first_connect_check=True)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/pool.py"", line 671, in __connect
    connection = pool._invoke_creator(self)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py"", line 106, in connect
    return dialect.connect(*cargs, **cparams)
  File ""/ftp/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 410, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File ""/ftp/lib/python2.7/site-packages/teradata/tdodbc.py"", line 427, in __init__
    connectParams[""DRIVER""] = determineDriver(dbType, driver)
  File ""/ftp/lib/python2.7/site-packages/teradata/tdodbc.py"", line 381, in determineDriver
    "" Available drivers: {}"".format(driver, "","".join(drivers)))
sqlalchemy.exc.InterfaceError: (teradata.api.InterfaceError) ('DRIVER_NOT_FOUND', ""No driver found with name '/teradata/client/15.10/odbc_64/lib/tdata.so'.  Available drivers: composite70 ,composite70_2x ,composite70_x64 ,"") (Background on this error at: http://sqlalche.me/e/rvf5)


Please suggest what could be the issue. I gave actual path of Teradata driver, i even tried changing driver to /teradata/client/15.10/odbc_64/lib and Teradata but still error is same, it is not able to find the Driver. 
",1
351,52090787,Use for loop in Teradata SQL,"I would like to do this in teradata SQL/ MACRO or PROCEDURE :

CREATE MACRO insertloop ( val1 VARCHAR( 1000)) AS
(

 sublist_i  = ' SELECT sublist from table3  '

 FOR sublist_i in sublist :
   INSERT INTO table5 
      SELECT t.id, t.address, sum(t.amount)
      FROM table2 AS t
      WHERE 
              t.id in sublist_i
         AND  t.address = :val1
      GROUP BY t.id t.address
);


Explanation:

table3 contains list of id (by block of 1000 id)
   (12, 546, 999)
   (45,789)
   (970, 990, 123)


Main reason :

table2 is very huge (1 billion record).

A full join requires too much memory, we need
to create a table table3 containing disjoint list of id
and iterate on this list.

But, am not sure how to correct this MACRO to be make correct.
",1,-1,-1.0,"I would like to do this in teradata SQL/ MACRO or PROCEDURE :

CREATE MACRO insertloop ( val1 VARCHAR( 1000)) AS
(

 sublist_i  = ' SELECT sublist from table3  '

 FOR sublist_i in sublist :
   INSERT INTO table5 
      SELECT t.id, t.address, sum(t.amount)
      FROM table2 AS t
      WHERE 
              t.id in sublist_i
         AND  t.address = :val1
      GROUP BY t.id t.address
);


Explanation:

table3 contains list of id (by block of 1000 id)
   (12, 546, 999)
   (45,789)
   (970, 990, 123)


Main reason :

table2 is very huge (1 billion record).

A full join requires too much memory, we need
to create a table table3 containing disjoint list of id
and iterate on this list.

But, am not sure how to correct this MACRO to be make correct.
",3
352,52259038,AWS EMR - Sqoop import using Cloudera Teradata connector failed to load data in AVRO format,"I have installed cloudera sqoop teradata connector on EMR cluster and tried to import data in avro format. But my sqoop job is failing with below error. I need your help on this issue. I am using Sqoop 1.4.6 version and Teradata driver 14.0 version.By referring some blogs copied below avro jars to /usr/lib/sqoop/lib/


avro-mapred-1.7.7-hadoop2.jar 
avro-1.7.7.jar



  18/09/10 13:31:31 INFO common.ConnectorPlugin: load plugins in jar:file:/usr/lib/SQOOP_TERADATA_CONNECTOR-1.5c5/sqoop-connector-teradata-1.5c5.jar!/teradata.connector.plugins.xml
  Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.sqoop.orm.AvroSchemaGenerator.generate()Lorg/apache/avro/Schema;
          at com.cloudera.connector.teradata.imports.BaseImportJob.configureInputFormat(BaseImportJob.java:165)
          at com.cloudera.connector.teradata.imports.TableImportJob.configureInputFormat(TableImportJob.java:32)
          at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:258)
          at com.cloudera.connector.teradata.TeradataManager.importTable(TeradataManager.java:274)
          at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:497)
          at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605)
          at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
          at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
          at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
          at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
          at org.apache.sqoop.Sqoop.main(Sqoop.java:236)

",-1,-1,-1.0,"I have installed cloudera sqoop teradata connector on EMR cluster and tried to import data in avro format. But my sqoop job is failing with below error. I need your help on this issue. I am using Sqoop 1.4.6 version and Teradata driver 14.0 version.By referring some blogs copied below avro jars to /usr/lib/sqoop/lib/


avro-mapred-1.7.7-hadoop2.jar 
avro-1.7.7.jar



  18/09/10 13:31:31 INFO common.ConnectorPlugin: load plugins in jar:file:/usr/lib/SQOOP_TERADATA_CONNECTOR-1.5c5/sqoop-connector-teradata-1.5c5.jar!/teradata.connector.plugins.xml
  Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.sqoop.orm.AvroSchemaGenerator.generate()Lorg/apache/avro/Schema;
          at com.cloudera.connector.teradata.imports.BaseImportJob.configureInputFormat(BaseImportJob.java:165)
          at com.cloudera.connector.teradata.imports.TableImportJob.configureInputFormat(TableImportJob.java:32)
          at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:258)
          at com.cloudera.connector.teradata.TeradataManager.importTable(TeradataManager.java:274)
          at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:497)
          at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605)
          at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
          at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
          at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
          at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
          at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)
          at org.apache.sqoop.Sqoop.main(Sqoop.java:236)

",0
353,52386291,How to setup ODBC connection via command line for Teradata driver?,"I am trying to setup a Teradata ODBC User DSN via below command line code.

%WINDIR%\System32\odbcconf.exe CONFIGDSN ""Teradata"" ""DSN=Test|Description=Test|SERVER=ServerName|Trusted_Connection=Yes|Database=Controltub|UID=&lt;LoginId&gt;""
%WINDIR%\SysWOW64\odbcconf.exe CONFIGDSN ""Teradata"" ""DSN=Test|Description=Test|SERVER=ServerName|Trusted_Connection=Tes|Database=Controltub|UID=&lt;LoginId&gt;""


This code works fine for ""SQL Server"" driver. However when I try to make changes accordingly for Teradata ODBC creation, it fails without any error.
I get blank in text fields for server name/IP field &amp; user id in ODBC window.
",-1,-1,-1.0,"I am trying to setup a Teradata ODBC User DSN via below command line code.

%WINDIR%\System32\odbcconf.exe CONFIGDSN ""Teradata"" ""DSN=Test|Description=Test|SERVER=ServerName|Trusted_Connection=Yes|Database=Controltub|UID=&lt;LoginId&gt;""
%WINDIR%\SysWOW64\odbcconf.exe CONFIGDSN ""Teradata"" ""DSN=Test|Description=Test|SERVER=ServerName|Trusted_Connection=Tes|Database=Controltub|UID=&lt;LoginId&gt;""


This code works fine for ""SQL Server"" driver. However when I try to make changes accordingly for Teradata ODBC creation, it fails without any error.
I get blank in text fields for server name/IP field &amp; user id in ODBC window.
",1
354,52490955,Calculated column syntax when using a group by function Teradata,"I'm trying to include a column calculated as a % of OTYPE.
IE
Order type | Status | volume of orders at each status | % of all orders at this status

SELECT 
T.OTYPE,
STATUS_CD,
COUNT(STATUS_CD) AS STATVOL,
(STATVOL / COUNT(ROW_ID)) * 100

FROM Database.S_ORDER O

LEFT JOIN /* Finding definitions for status codes &amp; attaching */
(
SELECT 
ROW_ID AS TYPEJOIN,
""NAME"" AS OTYPE
FROM database.S_ORDER_TYPE
) T

ON T.TYPEJOIN = ORDER_TYPE_ID


GROUP BY (T.OTYPE, STATUS_CD)
/*Excludes pending and pending online orders */
WHERE  CAST(CREATED AS DATE) =  '2018/09/21'  AND  STATUS_CD &lt;&gt; 'Pending' 
AND STATUS_CD &lt;&gt; 'Pending-Online'
ORDER BY T.OTYPE, STATUS_CD DESC

OTYPE   STATUS_CD                    STATVOL TOTALPERC
Add New Service Provisioning         2,740   100
Add New Service In-transit           13      100
Add New Service Error - Provisioning 568     100
Add New Service Error - Integration  1       100
Add New Service Complete             14,387  100


Current output just puts 100 at every line, need it to be a % of total orders

Could anyone help out a Teradata &amp; SQL student?
The complication making this difficult is my understanding of the group by and count syntax is tenuous. It took some fiddling to get it displayed as I have it, I'm not sure how to introduce a calculated column within this combo.

Thanks in advance
",-1,the,-1.0,"I'm trying to include a column calculated as a % of OTYPE.
IE
Order type | Status | volume of orders at each status | % of all orders at this status

SELECT 
T.OTYPE,
STATUS_CD,
COUNT(STATUS_CD) AS STATVOL,
(STATVOL / COUNT(ROW_ID)) * 100

FROM Database.S_ORDER O

LEFT JOIN /* Finding definitions for status codes &amp; attaching */
(
SELECT 
ROW_ID AS TYPEJOIN,
""NAME"" AS OTYPE
FROM database.S_ORDER_TYPE
) T

ON T.TYPEJOIN = ORDER_TYPE_ID


GROUP BY (T.OTYPE, STATUS_CD)
/*Excludes pending and pending online orders */
WHERE  CAST(CREATED AS DATE) =  '2018/09/21'  AND  STATUS_CD &lt;&gt; 'Pending' 
AND STATUS_CD &lt;&gt; 'Pending-Online'
ORDER BY T.OTYPE, STATUS_CD DESC

OTYPE   STATUS_CD                    STATVOL TOTALPERC
Add New Service Provisioning         2,740   100
Add New Service In-transit           13      100
Add New Service Error - Provisioning 568     100
Add New Service Error - Integration  1       100
Add New Service Complete             14,387  100


Current output just puts 100 at every line, need it to be a % of total orders

Could anyone help out a Teradata &amp; SQL student?
The complication making this difficult is my understanding of the group by and count syntax is tenuous. It took some fiddling to get it displayed as I have it, I'm not sure how to introduce a calculated column within this combo.

Thanks in advance
",3
355,52605557,Teradata ODBC connection issue: No version information available (required by ./tdxodbc64) on ubuntu,"We are facing the following issue while connecting to teradata server. Driver v16.10 and ubuntu v14.04.


  /opt/teradata/client/16.10/bin$ ./tdxodbc64 -c SQLDriverConnect -t -S 'SERVER_IP' -u 'USERNAME' -p 'PASSWORD'
  
  ./tdxodbc64: /usr/lib/x86_64-linux-gnu/libodbcinst.so: no version
  information available (required by ./tdxodbc64) 
  
  ./tdxodbc64: /usr/lib/x86_64-linux-gnu/libodbc.so: no version
  information available (required by ./tdxodbc64)
  
  Segmentation fault (core dumped)

",0,-1,-1.0,"We are facing the following issue while connecting to teradata server. Driver v16.10 and ubuntu v14.04.


  /opt/teradata/client/16.10/bin$ ./tdxodbc64 -c SQLDriverConnect -t -S 'SERVER_IP' -u 'USERNAME' -p 'PASSWORD'
  
  ./tdxodbc64: /usr/lib/x86_64-linux-gnu/libodbcinst.so: no version
  information available (required by ./tdxodbc64) 
  
  ./tdxodbc64: /usr/lib/x86_64-linux-gnu/libodbc.so: no version
  information available (required by ./tdxodbc64)
  
  Segmentation fault (core dumped)

",1
356,52742761,Teradata Query Using Dapper C#,"I am trying to dynamically query a Teradata database using Dapper but am having some issues. Here is the code:

// model variable is the parameter passed in with search information
using (IDbConnection con = new TdConnection(connection.GetConnectionString()))
{
    var builder = new SqlBuilder();
    var selector = builder.AddTemplate($""SELECT * FROM Temp_Table /**where**/"");

    if (model.Id != 0)
    {
        builder.Where(""Id = ?"", new { model.Id });
    }

    if (!string.IsNullOrEmpty(model.Employee_Id))
    {
        builder.Where(""Employee_Id = ?"", new { model.Employee_Id });
    }

    var data= con.Query&lt;TableModel&gt;(selector.RawSql, model).ToList();
    return data;
}


The error I am getting is:


  [Teradata Database] [3939] There is a mismatch between the number of
  parameters specified and the number of parameters required.


I have used very similar code to query DB2 which worked just fine; what do I need to do differently with Teradata?
",-1,-1,-1.0,"I am trying to dynamically query a Teradata database using Dapper but am having some issues. Here is the code:

// model variable is the parameter passed in with search information
using (IDbConnection con = new TdConnection(connection.GetConnectionString()))
{
    var builder = new SqlBuilder();
    var selector = builder.AddTemplate($""SELECT * FROM Temp_Table /**where**/"");

    if (model.Id != 0)
    {
        builder.Where(""Id = ?"", new { model.Id });
    }

    if (!string.IsNullOrEmpty(model.Employee_Id))
    {
        builder.Where(""Employee_Id = ?"", new { model.Employee_Id });
    }

    var data= con.Query&lt;TableModel&gt;(selector.RawSql, model).ToList();
    return data;
}


The error I am getting is:


  [Teradata Database] [3939] There is a mismatch between the number of
  parameters specified and the number of parameters required.


I have used very similar code to query DB2 which worked just fine; what do I need to do differently with Teradata?
",3
357,52746000,Teradata datatypes to hive datatypes incompatibility,"how to map Teradata data types to hive datatypes, for ex: in TD DATE FORMAT 'YYYY-MM-DD' COMPRESS (DATE '2013-10-09',DATE '2014-02-25'),. in hive we can use it as DATE.  but whenever we load data to hive table it show as NULL... please help me in this regard... Thank you
",1,-1,-1.0,"how to map Teradata data types to hive datatypes, for ex: in TD DATE FORMAT 'YYYY-MM-DD' COMPRESS (DATE '2013-10-09',DATE '2014-02-25'),. in hive we can use it as DATE.  but whenever we load data to hive table it show as NULL... please help me in this regard... Thank you
",3
358,52847985,Importing a pandas dataframe into Teradata database,"I am attempting to import an Excel file into a new table in a Teradata database, using SQLAlchemy and pandas.
I am using the pandas to_sql function. I load the Excel file with pandas and save it as a dataframe named df. I then use df.to_sql and load it into the Teradata database.
When using the code:
df.to_sql('rt_test4', con=td_engine, schema='db_sandbox')
I am prompted with the error:
DatabaseError: (teradata.api.DatabaseError) (3534, '[42S11] [Teradata][ODBC Teradata Driver][Teradata Database] Another index already exists, using the same columns and the same ordering. ') [SQL: 'CREATE INDEX ix_db_sandbox_rt_test4_index (&quot;index&quot;) ON db_sandbox.rt_test4']
When I try this and use Teradata SQL Assistant to see if the table exists, I am prompted with selecting txt or unicode for each column name, and to pick a folder directory. A prompt titled LOB information pops open and I have to select if it's UTF or unicode, and a file directory. Then it loads and all the column titles populate, but they are left as empty fields. Looking for some direction here, I feel I've been spinning my wheels on this.
",-1,-1,-1.0,"I am attempting to import an Excel file into a new table in a Teradata database, using SQLAlchemy and pandas.
I am using the pandas to_sql function. I load the Excel file with pandas and save it as a dataframe named df. I then use df.to_sql and load it into the Teradata database.
When using the code:
df.to_sql('rt_test4', con=td_engine, schema='db_sandbox')
I am prompted with the error:
DatabaseError: (teradata.api.DatabaseError) (3534, '[42S11] [Teradata][ODBC Teradata Driver][Teradata Database] Another index already exists, using the same columns and the same ordering. ') [SQL: 'CREATE INDEX ix_db_sandbox_rt_test4_index (&quot;index&quot;) ON db_sandbox.rt_test4']
When I try this and use Teradata SQL Assistant to see if the table exists, I am prompted with selecting txt or unicode for each column name, and to pick a folder directory. A prompt titled LOB information pops open and I have to select if it's UTF or unicode, and a file directory. Then it loads and all the column titles populate, but they are left as empty fields. Looking for some direction here, I feel I've been spinning my wheels on this.
",1
359,53017503,Using Case-When in Teradata,"Simple problem but driving me crazy for a while. Have following table in Teradata:

A       B
112211  
113311  56
226144  61
996688  66
005400  
771277 


For the blank fields in col-B need to pull the numbers from col-A like:

SELECT A
CASE 'B'
WHEN 'B' IS NULL THEN SUBSTR('A',3,4)
END AS 'B'
FROM TABLE_T1;


Referred some documentation but not getting what's going wrong in the above query. This query will work on MySQL but why not in Teradata.

Expected O/P:

A       B
112211  22
113311  56
226144  61
996688  66
005400  54
771277  12

",-1,-1,-1.0,"Simple problem but driving me crazy for a while. Have following table in Teradata:

A       B
112211  
113311  56
226144  61
996688  66
005400  
771277 


For the blank fields in col-B need to pull the numbers from col-A like:

SELECT A
CASE 'B'
WHEN 'B' IS NULL THEN SUBSTR('A',3,4)
END AS 'B'
FROM TABLE_T1;


Referred some documentation but not getting what's going wrong in the above query. This query will work on MySQL but why not in Teradata.

Expected O/P:

A       B
112211  22
113311  56
226144  61
996688  66
005400  54
771277  12

",3
360,53049911,Oracle syntax Vs teradata Syntax,"Let's say I have 2 columns Start date and end date both are timestamp.

I want to subtract both (Start date - end date ) as date diff

But it's throwing me error. What should be the correct syntax.

Note:- I am doing this in teradata
",-1,-1,-1.0,"Let's say I have 2 columns Start date and end date both are timestamp.

I want to subtract both (Start date - end date ) as date diff

But it's throwing me error. What should be the correct syntax.

Note:- I am doing this in teradata
",3
361,53166385,Teradata Trigger Syntax (referencing similar rows for conditional insert),"I have a TeraData table that looks something like this:

Name;Year;Amount
1. Bob;2018;20
2. Bob;2022;14
3. Joe;2019;40
4. Ben;2017;12


The PK is Name and Year. I have a trigger in place that prevents a user from editing a row's Year to a lesser number. i.e. changing row 3 from 2019 to 2018. 

That trigger is below:

Replace TRIGGER xyz.Month_Update
AFTER UPDATE OF Month ON xyz.table
REFERENCING OLD ROW as OldRow NEW ROW  as NewRow
 FOR EACH ROW
  WHEN NewRow.Year &lt; OldRow.Year
abort;


Now I would like to do something similar for insert.
I would like to prevent a user from inserting a new row into the table
if..


There already exists a row for the same person in the table and
That row(s) has a greater year than the one the user is attempting to enter


i.e. User can't enter Joe;2017;19 but user can enter Joe;2020;19

There are a few obvious problems with the trigger below but it shows the general idea:

Replace TRIGGER xyz.Month_Update
AFTER INSERT ON xyz.table
REFERENCING NEW ROW  as NewRow
 FOR EACH ROW
  WHEN NewRow.Year &lt; (select max(year) from xyz.table as t1 where t1.name = NewRow.name group by t1.name)
abort;


I'm new to triggers in general and teradata documentation appears porous.. any suggestions are greatly appreciated. 
",-1,-1,-1.0,"I have a TeraData table that looks something like this:

Name;Year;Amount
1. Bob;2018;20
2. Bob;2022;14
3. Joe;2019;40
4. Ben;2017;12


The PK is Name and Year. I have a trigger in place that prevents a user from editing a row's Year to a lesser number. i.e. changing row 3 from 2019 to 2018. 

That trigger is below:

Replace TRIGGER xyz.Month_Update
AFTER UPDATE OF Month ON xyz.table
REFERENCING OLD ROW as OldRow NEW ROW  as NewRow
 FOR EACH ROW
  WHEN NewRow.Year &lt; OldRow.Year
abort;


Now I would like to do something similar for insert.
I would like to prevent a user from inserting a new row into the table
if..


There already exists a row for the same person in the table and
That row(s) has a greater year than the one the user is attempting to enter


i.e. User can't enter Joe;2017;19 but user can enter Joe;2020;19

There are a few obvious problems with the trigger below but it shows the general idea:

Replace TRIGGER xyz.Month_Update
AFTER INSERT ON xyz.table
REFERENCING NEW ROW  as NewRow
 FOR EACH ROW
  WHEN NewRow.Year &lt; (select max(year) from xyz.table as t1 where t1.name = NewRow.name group by t1.name)
abort;


I'm new to triggers in general and teradata documentation appears porous.. any suggestions are greatly appreciated. 
",3
362,52569660,Teradata not responding in Windows Server 2012,"I am trying to install Teradata connection in a virtual machine of Windows Server 2012.
I tried multiple times but I still got the timed out error: No response received when attempting to connect the Teradata error. 

I tried connecting through SQL assistant and python teradata module. Below is what I have tried based on resolutions found on web search and none of them works
1. Close the firewall
2. Change the odbcLibPath (in Python)
3. Reinstall .NET framework
4. Extend the time awaiting for response
5. Seeking resolutions around odbc.ini and odbcinst.ini but for Windows specifically, I can not find any resolution.

I have also not be able to find much difference between the configuration on the virtual machine and that on my local desktop which works well. 

I felt I have run out of possible solutions at the moment. Appreciate any help in this.
",1,-1,-1.0,"I am trying to install Teradata connection in a virtual machine of Windows Server 2012.
I tried multiple times but I still got the timed out error: No response received when attempting to connect the Teradata error. 

I tried connecting through SQL assistant and python teradata module. Below is what I have tried based on resolutions found on web search and none of them works
1. Close the firewall
2. Change the odbcLibPath (in Python)
3. Reinstall .NET framework
4. Extend the time awaiting for response
5. Seeking resolutions around odbc.ini and odbcinst.ini but for Windows specifically, I can not find any resolution.

I have also not be able to find much difference between the configuration on the virtual machine and that on my local desktop which works well. 

I felt I have run out of possible solutions at the moment. Appreciate any help in this.
",1
363,53222075,Teradata SQL assistant by default works in ANSI mode,"Once for no reason Teradata sql assistant has started to work in ANSI mode, as result help view ..., show view, sel stopped to work:




After connection creating new sheets by ctrl+N - creates normal sheet, everything work well there:


However reconnecting breaks everything, so one more time is needeed to create working sheets.

Session mode is Teradata:


Versions:



It doesnt show any my connection lines:

How to fix this?

UPD1:

Log (deleted old and unsuccessful connections):
*******************************************************
20.08.2018 9:58:38
SQLA Version: 16.10.0.2
Driver Version: Teradata.Net 16.10.0.0
Teradata.Client.Provider.TdException (0x80004005): [.NET Data Provider for Teradata] [100002] Cannot create connection within the time specified.
   в Teradata.Client.Provider.WpTcpTransport.&lt;&gt;c.&lt;WaitOnSockets&gt;b__52_0()
   в Teradata.Client.Provider.UtlStopwatchWrapper.GetTimeRemaining(Int32 timeout, Action timeoutAction)
   в Teradata.Client.Provider.WpTcpTransport.WaitOnSockets(List`1 sockets, UtlStopwatchWrapper timer, Int32 timeout, Int32 lccTimeout, TdErrorCollection errors)
   в Teradata.Client.Provider.WpTcpTransport.Connect(UtlStopwatchWrapper timer, Int32 timeout)
   в Teradata.Client.Provider.WpSession.OpenTransport(UtlStopwatchWrapper timer, Int32 connectionTimeout)
   в Teradata.Client.Provider.WpSession.Open(Int32 connectionTimeout, String password)
   в Teradata.Client.Provider.ExeContext`3.Open(Int32 timeout, String password)
   в Teradata.Client.Provider.Connection.Open(UtlConnectionString connectionString, UInt32 timeout)
   в Teradata.Client.Provider.ConnectionFactory.GetConnection(Object owningObject, UtlConnectionString connStr)
   в Teradata.Client.Provider.TdConnection.Open()
   в Teradata.SQLA.TdConnectInfo.GetSession() в v:\cm.client.ttu16.x\tdcli\qman\sqla\TdConnectInfo.vb:строка 202
*******************************************************
28.08.2018 14:38:05
SQLA Version: 16.10.0.2
Driver Version: Teradata.Net 16.10.0.0
System.Exception: Exception handled in FpSpread.WndProc ---&gt; System.NullReferenceException:  
   в FarPoint.Win.Spread.SpreadView.a(Int32 A_0, MouseEventArgs A_1)
   в FarPoint.Win.Spread.SpreadView.g(MouseEventArgs A_0)
   в FarPoint.Win.Spread.FpSpread.OnMouseDown(MouseEventArgs e)
   в System.Windows.Forms.Control.WmMouseDown(Message&amp; m, MouseButtons button, Int32 clicks)
   в System.Windows.Forms.Control.WndProc(Message&amp; m)
   в FarPoint.Win.Spread.FpSpread.WndProc(Message&amp; m)
   --- Конец трассировки внутреннего стека исключений ---
   в FarPoint.Win.Spread.FpSpread.WndProc(Message&amp; m)
   в System.Windows.Forms.Control.ControlNativeWindow.OnMessage(Message&amp; m)
   в System.Windows.Forms.Control.ControlNativeWindow.WndProc(Message&amp; m)
   в System.Windows.Forms.NativeWindow.Callback(IntPtr hWnd, Int32 msg, IntPtr wparam, IntPtr lparam)
*******************************************************
13.09.2018 10:03:19
SQLA Version: 16.10.0.2
System.InvalidOperationException: Cant call Invoke or BeginInvoke on a control until the window handle has been completed.
   в System.Windows.Forms.Control.MarshaledInvoke(Control caller, Delegate method, Object[] args, Boolean synchronous)
   в System.Windows.Forms.Control.BeginInvoke(Delegate method, Object[] args)
   в System.Windows.Forms.Control.BeginInvoke(Delegate method)
   в Teradata.SQLA.RunQry.ExecuteStatement() в v:\cm.client.ttu16.x\tdcli\qman\sqla\RunQry.vb:строка 645
   в Teradata.SQLA.RunQry._Lambda$__3(Object a0) в v:\cm.client.ttu16.x\tdcli\qman\sqla\RunQry.vb:строка 357
   в System.Threading.QueueUserWorkItemCallback.WaitCallback_Context(Object state)
   в System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
   в System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
   в System.Threading.QueueUserWorkItemCallback.System.Threading.IThreadPoolWorkItem.ExecuteWorkItem()
   в System.Threading.ThreadPoolWorkQueue.Dispatch()
   в System.Threading._ThreadPoolWaitCallback.PerformWaitCallback()
*******************************************************
23.10.2018 18:08:44
SQLA Version: 16.10.0.2
Driver Version: Teradata.Net 16.10.0.0
Teradata.Client.Provider.TdException (0x80004005): [.NET Data Provider for Teradata] [100015] Total size of all parcels is greater than the max message size.
   в Teradata.Client.Provider.Request.VerifyRequestDoesNotExceedMaxBuffer(TeraTypeBase[][] parameters)
   в Teradata.Client.Provider.Request.ExecuteStartRequest(String commandText, TeraTypeBase[][] parameters, ExecutionMode executionMode, Boolean asynchronous, Boolean isTrustedRequest)
   в Teradata.Client.Provider.TdCommand.ExecuteRequest(CommandBehavior cmdBehavior, Boolean asynchronousCall, Boolean useStatementIndependence)
   в Teradata.Client.Provider.TdCommand.ExecuteReader(CommandBehavior behavior)
   в Teradata.Client.Provider.TdCommand.ExecuteDbDataReader(CommandBehavior behavior)
   в System.Data.Common.DbCommand.ExecuteReader(CommandBehavior behavior)
   в Teradata.SQLA.TdConnectInfo.Execute(DbCommand cmd, String keyword, DbDataReader&amp; rdr, Int64&amp; rowCnt, Int32&amp; errPos) в v:\cm.client.ttu16.x\tdcli\qman\sqla\TdConnectInfo.vb:line 169
*******************************************************
07.11.2018 12:21:21
SQLA Version: 16.10.0.2
Driver Version: Teradata.Net 16.10.0.0
Teradata.Client.Provider.TdException (0x80004005): [.NET Data Provider for Teradata] [100038] Command did not complete within the time specified (timeout).
[Teradata Database] [3110] The transaction was aborted by the user.
[Socket Transport] [115003] The receive operation timed out. ---&gt; System.Net.Sockets.SocketException (0x80004005)
   в System.Net.Sockets.Socket.Receive(Byte[] buffer, Int32 offset, Int32 size, SocketFlags socketFlags)
   в Teradata.Client.Provider.WpTcpTransport.ReadLanHeader(Buffer buffer, Int32 timeout, Int32 readBytes)
   в Teradata.Client.Provider.WpTcpTransport.ReadLanHeader(Buffer buffer, Int32 timeout, Int32 readBytes)
   в Teradata.Client.Provider.WpTcpTransport.Receive(Buffer buffer, Int32 timeout)
   в Teradata.Client.Provider.WpSession.Receive(Buffer buffer, Int32 timeout)
   в Teradata.Client.Provider.WpMessageManager.Receive(Int32 timeout)
   в Teradata.Client.Provider.WpStartRequestManager.ReceiveStartMessage()
   в Teradata.Client.Provider.WpStartRequestManager.Action(ManagerActions step)
   в Teradata.Client.Provider.WpStartRequestManager.RedriveAction(ManagerActions step)
   в Teradata.Client.Provider.WpStartRequestManager.Action()
   в Teradata.Client.Provider.Request.ExecuteStartRequest(String commandText, TeraTypeBase[][] parameters, ExecutionMode executionMode, Boolean asynchronous, Boolean isTrustedRequest)
   в Teradata.Client.Provider.TdCommand.ExecuteRequest(CommandBehavior cmdBehavior, Boolean asynchronousCall, Boolean useStatementIndependence)
   в Teradata.Client.Provider.TdCommand.ExecuteNonQuery()
   в Teradata.Client.Provider.TdConnection.ChangeDatabase(String databaseName)
   в Teradata.Client.Provider.TdConnection.ExecuteInitializationStmts()
   в Teradata.Client.Provider.TdConnection.Open()
   в Teradata.SQLA.TdConnectInfo.GetSession() в v:\cm.client.ttu16.x\tdcli\qman\sqla\TdConnectInfo.vb:line 202
*******************************************************

",1,-1,-1.0,"Once for no reason Teradata sql assistant has started to work in ANSI mode, as result help view ..., show view, sel stopped to work:




After connection creating new sheets by ctrl+N - creates normal sheet, everything work well there:


However reconnecting breaks everything, so one more time is needeed to create working sheets.

Session mode is Teradata:


Versions:



It doesnt show any my connection lines:

How to fix this?

UPD1:

Log (deleted old and unsuccessful connections):
*******************************************************
20.08.2018 9:58:38
SQLA Version: 16.10.0.2
Driver Version: Teradata.Net 16.10.0.0
Teradata.Client.Provider.TdException (0x80004005): [.NET Data Provider for Teradata] [100002] Cannot create connection within the time specified.
   в Teradata.Client.Provider.WpTcpTransport.&lt;&gt;c.&lt;WaitOnSockets&gt;b__52_0()
   в Teradata.Client.Provider.UtlStopwatchWrapper.GetTimeRemaining(Int32 timeout, Action timeoutAction)
   в Teradata.Client.Provider.WpTcpTransport.WaitOnSockets(List`1 sockets, UtlStopwatchWrapper timer, Int32 timeout, Int32 lccTimeout, TdErrorCollection errors)
   в Teradata.Client.Provider.WpTcpTransport.Connect(UtlStopwatchWrapper timer, Int32 timeout)
   в Teradata.Client.Provider.WpSession.OpenTransport(UtlStopwatchWrapper timer, Int32 connectionTimeout)
   в Teradata.Client.Provider.WpSession.Open(Int32 connectionTimeout, String password)
   в Teradata.Client.Provider.ExeContext`3.Open(Int32 timeout, String password)
   в Teradata.Client.Provider.Connection.Open(UtlConnectionString connectionString, UInt32 timeout)
   в Teradata.Client.Provider.ConnectionFactory.GetConnection(Object owningObject, UtlConnectionString connStr)
   в Teradata.Client.Provider.TdConnection.Open()
   в Teradata.SQLA.TdConnectInfo.GetSession() в v:\cm.client.ttu16.x\tdcli\qman\sqla\TdConnectInfo.vb:строка 202
*******************************************************
28.08.2018 14:38:05
SQLA Version: 16.10.0.2
Driver Version: Teradata.Net 16.10.0.0
System.Exception: Exception handled in FpSpread.WndProc ---&gt; System.NullReferenceException:  
   в FarPoint.Win.Spread.SpreadView.a(Int32 A_0, MouseEventArgs A_1)
   в FarPoint.Win.Spread.SpreadView.g(MouseEventArgs A_0)
   в FarPoint.Win.Spread.FpSpread.OnMouseDown(MouseEventArgs e)
   в System.Windows.Forms.Control.WmMouseDown(Message&amp; m, MouseButtons button, Int32 clicks)
   в System.Windows.Forms.Control.WndProc(Message&amp; m)
   в FarPoint.Win.Spread.FpSpread.WndProc(Message&amp; m)
   --- Конец трассировки внутреннего стека исключений ---
   в FarPoint.Win.Spread.FpSpread.WndProc(Message&amp; m)
   в System.Windows.Forms.Control.ControlNativeWindow.OnMessage(Message&amp; m)
   в System.Windows.Forms.Control.ControlNativeWindow.WndProc(Message&amp; m)
   в System.Windows.Forms.NativeWindow.Callback(IntPtr hWnd, Int32 msg, IntPtr wparam, IntPtr lparam)
*******************************************************
13.09.2018 10:03:19
SQLA Version: 16.10.0.2
System.InvalidOperationException: Cant call Invoke or BeginInvoke on a control until the window handle has been completed.
   в System.Windows.Forms.Control.MarshaledInvoke(Control caller, Delegate method, Object[] args, Boolean synchronous)
   в System.Windows.Forms.Control.BeginInvoke(Delegate method, Object[] args)
   в System.Windows.Forms.Control.BeginInvoke(Delegate method)
   в Teradata.SQLA.RunQry.ExecuteStatement() в v:\cm.client.ttu16.x\tdcli\qman\sqla\RunQry.vb:строка 645
   в Teradata.SQLA.RunQry._Lambda$__3(Object a0) в v:\cm.client.ttu16.x\tdcli\qman\sqla\RunQry.vb:строка 357
   в System.Threading.QueueUserWorkItemCallback.WaitCallback_Context(Object state)
   в System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
   в System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
   в System.Threading.QueueUserWorkItemCallback.System.Threading.IThreadPoolWorkItem.ExecuteWorkItem()
   в System.Threading.ThreadPoolWorkQueue.Dispatch()
   в System.Threading._ThreadPoolWaitCallback.PerformWaitCallback()
*******************************************************
23.10.2018 18:08:44
SQLA Version: 16.10.0.2
Driver Version: Teradata.Net 16.10.0.0
Teradata.Client.Provider.TdException (0x80004005): [.NET Data Provider for Teradata] [100015] Total size of all parcels is greater than the max message size.
   в Teradata.Client.Provider.Request.VerifyRequestDoesNotExceedMaxBuffer(TeraTypeBase[][] parameters)
   в Teradata.Client.Provider.Request.ExecuteStartRequest(String commandText, TeraTypeBase[][] parameters, ExecutionMode executionMode, Boolean asynchronous, Boolean isTrustedRequest)
   в Teradata.Client.Provider.TdCommand.ExecuteRequest(CommandBehavior cmdBehavior, Boolean asynchronousCall, Boolean useStatementIndependence)
   в Teradata.Client.Provider.TdCommand.ExecuteReader(CommandBehavior behavior)
   в Teradata.Client.Provider.TdCommand.ExecuteDbDataReader(CommandBehavior behavior)
   в System.Data.Common.DbCommand.ExecuteReader(CommandBehavior behavior)
   в Teradata.SQLA.TdConnectInfo.Execute(DbCommand cmd, String keyword, DbDataReader&amp; rdr, Int64&amp; rowCnt, Int32&amp; errPos) в v:\cm.client.ttu16.x\tdcli\qman\sqla\TdConnectInfo.vb:line 169
*******************************************************
07.11.2018 12:21:21
SQLA Version: 16.10.0.2
Driver Version: Teradata.Net 16.10.0.0
Teradata.Client.Provider.TdException (0x80004005): [.NET Data Provider for Teradata] [100038] Command did not complete within the time specified (timeout).
[Teradata Database] [3110] The transaction was aborted by the user.
[Socket Transport] [115003] The receive operation timed out. ---&gt; System.Net.Sockets.SocketException (0x80004005)
   в System.Net.Sockets.Socket.Receive(Byte[] buffer, Int32 offset, Int32 size, SocketFlags socketFlags)
   в Teradata.Client.Provider.WpTcpTransport.ReadLanHeader(Buffer buffer, Int32 timeout, Int32 readBytes)
   в Teradata.Client.Provider.WpTcpTransport.ReadLanHeader(Buffer buffer, Int32 timeout, Int32 readBytes)
   в Teradata.Client.Provider.WpTcpTransport.Receive(Buffer buffer, Int32 timeout)
   в Teradata.Client.Provider.WpSession.Receive(Buffer buffer, Int32 timeout)
   в Teradata.Client.Provider.WpMessageManager.Receive(Int32 timeout)
   в Teradata.Client.Provider.WpStartRequestManager.ReceiveStartMessage()
   в Teradata.Client.Provider.WpStartRequestManager.Action(ManagerActions step)
   в Teradata.Client.Provider.WpStartRequestManager.RedriveAction(ManagerActions step)
   в Teradata.Client.Provider.WpStartRequestManager.Action()
   в Teradata.Client.Provider.Request.ExecuteStartRequest(String commandText, TeraTypeBase[][] parameters, ExecutionMode executionMode, Boolean asynchronous, Boolean isTrustedRequest)
   в Teradata.Client.Provider.TdCommand.ExecuteRequest(CommandBehavior cmdBehavior, Boolean asynchronousCall, Boolean useStatementIndependence)
   в Teradata.Client.Provider.TdCommand.ExecuteNonQuery()
   в Teradata.Client.Provider.TdConnection.ChangeDatabase(String databaseName)
   в Teradata.Client.Provider.TdConnection.ExecuteInitializationStmts()
   в Teradata.Client.Provider.TdConnection.Open()
   в Teradata.SQLA.TdConnectInfo.GetSession() в v:\cm.client.ttu16.x\tdcli\qman\sqla\TdConnectInfo.vb:line 202
*******************************************************

",0
364,53296031,Teradata SQL - how to get everything before hyphen?,"The question is how to get all data before hyphen using SQL in Teradata? 

The pattern is like this : 123ABC-456. I need only 123ABC.

This regex expression:

SELECT RegExp_Replace('123ABC-456',  '\w[^-]*$')


returns ""123ABC-"" with hyphen for some reason

and this ""^[^-]*[^ -]"" - returns ""-456"" instead of ""123ABC""

Please any help? 
",1,-1,-1.0,"The question is how to get all data before hyphen using SQL in Teradata? 

The pattern is like this : 123ABC-456. I need only 123ABC.

This regex expression:

SELECT RegExp_Replace('123ABC-456',  '\w[^-]*$')


returns ""123ABC-"" with hyphen for some reason

and this ""^[^-]*[^ -]"" - returns ""-456"" instead of ""123ABC""

Please any help? 
",3
365,53315479,How to create a/multiple stored procedure in Teradata by running script file,"For the below stored procedure in Teradara DB, I am able to crate it when running it as a sql statement. But when I am trying to run it through SQl script file, it's not getting created and throws error messgae:

SQL Error [5526] [HY000]: [Teradata Database] [TeraJDBC 15.00.00.33] [Error 5526] [SQLState HY000] Stored Procedure is not created/replaced due to error(s).

CREATE PROCEDURE SCHEMA.SELECT_PROCEDURE(IN ROLL_ INTEGER,OUT MARKS_ 
FLOAT)
DYNAMIC RESULT SETS 1
BEGIN
    SELECT MARKS INTO MARKS_ FROM SCHEMA.TEST WHERE ROLL = :ROLL_;
END;


I have also searched the error on internet, but didn't get much help.
Help here, please !
",-1,-1,-1.0,"For the below stored procedure in Teradara DB, I am able to crate it when running it as a sql statement. But when I am trying to run it through SQl script file, it's not getting created and throws error messgae:

SQL Error [5526] [HY000]: [Teradata Database] [TeraJDBC 15.00.00.33] [Error 5526] [SQLState HY000] Stored Procedure is not created/replaced due to error(s).

CREATE PROCEDURE SCHEMA.SELECT_PROCEDURE(IN ROLL_ INTEGER,OUT MARKS_ 
FLOAT)
DYNAMIC RESULT SETS 1
BEGIN
    SELECT MARKS INTO MARKS_ FROM SCHEMA.TEST WHERE ROLL = :ROLL_;
END;


I have also searched the error on internet, but didn't get much help.
Help here, please !
",3
366,53406939,Teradata pypyodbc SQL_INVALID_HANDLE,"While connecting pypyodbc with teradata i am getting SQL_INVALID_HANDLE

OS - Red Hat  4.4.7-17

Python Version - 2.6.6

I am not having administrative privilages to install the pypyodbc so i have downloaded the module, set pythonpath and imported it.

Done the required exports to point the python module to find the correct libodbc.so

export LD_LIBRARY_PATH=/opt/teradata/client/15.10/odbc_64/lib/
export ODBCINST=/opt/teradata/client/ODBC_64/odbcinst.ini
export ODBCINI=/opt/teradata/client/ODBC_64/odbc.ini


used the below code to create a db connection

pypyodbc.connect(""Driver={Teradata};DBCNAME=&lt;DB&gt;;UID=&lt;UNAME&gt;;PWD=&lt;PASS&gt;"")


The odbc connection string i have used is correct. I have tested using the below approach

/opt/teradata/client/15.10/bin/tdxodbc64 -C ""Driver={Teradata};DBCNAME=&lt;DB&gt;;UID=&lt;UNAME&gt;;PWD=&lt;PASS&gt;""


**Note:**I am not using teradata OOTB python module because it works with a higher version of python which i cannot install.

Error

  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""&lt;LOCAL_PATH&gt;/pypyodbc.py"", line 2421, in __init__
    AllocateEnv()
  File ""&lt;LOCAL_PATH&gt;/pypyodbc.py"", line 1000, in AllocateEnv
    check_success(SQL_NULL_HANDLE, ret)
  File ""&lt;LOCAL_PATH&gt;/pypyodbc.py"", line 990, in check_success
    ctrl_err(SQL_HANDLE_ENV, ODBC_obj, ret, False)
  File ""&lt;LOCAL_PATH&gt;/pypyodbc.py"", line 970, in ctrl_err
    raise ProgrammingError('', 'SQL_INVALID_HANDLE')
pypyodbc.ProgrammingError: ('', 'SQL_INVALID_HANDLE')


any guidance on the matter is helpful
",1,-1,-1.0,"While connecting pypyodbc with teradata i am getting SQL_INVALID_HANDLE

OS - Red Hat  4.4.7-17

Python Version - 2.6.6

I am not having administrative privilages to install the pypyodbc so i have downloaded the module, set pythonpath and imported it.

Done the required exports to point the python module to find the correct libodbc.so

export LD_LIBRARY_PATH=/opt/teradata/client/15.10/odbc_64/lib/
export ODBCINST=/opt/teradata/client/ODBC_64/odbcinst.ini
export ODBCINI=/opt/teradata/client/ODBC_64/odbc.ini


used the below code to create a db connection

pypyodbc.connect(""Driver={Teradata};DBCNAME=&lt;DB&gt;;UID=&lt;UNAME&gt;;PWD=&lt;PASS&gt;"")


The odbc connection string i have used is correct. I have tested using the below approach

/opt/teradata/client/15.10/bin/tdxodbc64 -C ""Driver={Teradata};DBCNAME=&lt;DB&gt;;UID=&lt;UNAME&gt;;PWD=&lt;PASS&gt;""


**Note:**I am not using teradata OOTB python module because it works with a higher version of python which i cannot install.

Error

  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""&lt;LOCAL_PATH&gt;/pypyodbc.py"", line 2421, in __init__
    AllocateEnv()
  File ""&lt;LOCAL_PATH&gt;/pypyodbc.py"", line 1000, in AllocateEnv
    check_success(SQL_NULL_HANDLE, ret)
  File ""&lt;LOCAL_PATH&gt;/pypyodbc.py"", line 990, in check_success
    ctrl_err(SQL_HANDLE_ENV, ODBC_obj, ret, False)
  File ""&lt;LOCAL_PATH&gt;/pypyodbc.py"", line 970, in ctrl_err
    raise ProgrammingError('', 'SQL_INVALID_HANDLE')
pypyodbc.ProgrammingError: ('', 'SQL_INVALID_HANDLE')


any guidance on the matter is helpful
",1
367,53672172,Pyspark -- java.lang.NullPointerException (When using jdbc reading from Teradata to a dataframe),"Below is the Pyspark code to load data from EDW (Teradata) to HDFS(Hadoop system) using the JDBC driver:

from pyspark.sql import *
from pyspark.sql.types import *

q = """"""(select columns
    from TD.Table1 as a
    inner join TD.Table2 as b 
      on a.col=b.col
    where b.col = date '2017-09-30'
    ) foo""""""

df = spark.read.format('jdbc') \
.option('url','jdbc:teradata://teradata-dns-sysa.ab.xyz.com') \
.option('driver','com.teradata.jdbc.TeraDriver') \
.option('user','username') \
.option('password','####') \
.option('tmode','tera') \
.option('dbtable',q) \ 
.option('type','fastload') \
.load()

df.show(20,False)


This throws the error below:

Py4JJavaError: An error occurred while calling o64.showString.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NullPointerException
    at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:210)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:108)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2854)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2154)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2154)
    at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2838)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2837)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2154)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:2367)
    at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:280)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:214)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:210)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:108)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    ... 1 more


Basically when I trying to do any actions on this df, like save as a hive table to hive, and save as an orc file to hdfs. It throws out the same error. 

I did a df.printSchema and I found out that maybe the reason is that the dataset contains null values for (nullable = false) columns

So, I also tried to create a new Dataframe from the previous one, specifying the wanted schema:

df2 = spark.createDataFrame(df.rdd,schema)


Still got the same NPE error.
Any idea how to solve this one? Thank you!
",-1,-1,-1.0,"Below is the Pyspark code to load data from EDW (Teradata) to HDFS(Hadoop system) using the JDBC driver:

from pyspark.sql import *
from pyspark.sql.types import *

q = """"""(select columns
    from TD.Table1 as a
    inner join TD.Table2 as b 
      on a.col=b.col
    where b.col = date '2017-09-30'
    ) foo""""""

df = spark.read.format('jdbc') \
.option('url','jdbc:teradata://teradata-dns-sysa.ab.xyz.com') \
.option('driver','com.teradata.jdbc.TeraDriver') \
.option('user','username') \
.option('password','####') \
.option('tmode','tera') \
.option('dbtable',q) \ 
.option('type','fastload') \
.load()

df.show(20,False)


This throws the error below:

Py4JJavaError: An error occurred while calling o64.showString.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NullPointerException
    at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:210)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:108)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2854)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2154)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2154)
    at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2838)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2837)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2154)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:2367)
    at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:280)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:214)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:210)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:108)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    ... 1 more


Basically when I trying to do any actions on this df, like save as a hive table to hive, and save as an orc file to hdfs. It throws out the same error. 

I did a df.printSchema and I found out that maybe the reason is that the dataset contains null values for (nullable = false) columns

So, I also tried to create a new Dataframe from the previous one, specifying the wanted schema:

df2 = spark.createDataFrame(df.rdd,schema)


Still got the same NPE error.
Any idea how to solve this one? Thank you!
",0
368,53721907,TDWALLETERROR(543): Teradata Wallet error. The helper process is already being traced,"When I start my jobs using fast export they sometimes end with an error: 

TDWALLETERROR(543): Teradata Wallet error. The helper process is already being traced


When I restart them, they work. 
I'm using saved-key protection scheme.

Can someone explain to me why is that error occuring and how to fix it?
",-1,-1,-1.0,"When I start my jobs using fast export they sometimes end with an error: 

TDWALLETERROR(543): Teradata Wallet error. The helper process is already being traced


When I restart them, they work. 
I'm using saved-key protection scheme.

Can someone explain to me why is that error occuring and how to fix it?
",3
369,53768089,Teradata create table from UNION of multi tables error: (3707),"I am new to Teradata SQL. I am trying to write a query as below

Create table abc as
(
   with datasets as
   (
    select * from tableA
  UNION
    select * from tableB
)  );


I am getting an error:


  3707

",-1,-1,-1.0,"I am new to Teradata SQL. I am trying to write a query as below

Create table abc as
(
   with datasets as
   (
    select * from tableA
  UNION
    select * from tableB
)  );


I am getting an error:


  3707

",3
370,53814918,Volatile Table in Stored Procedure - Teradata,"I create a stored procedure as follows:

replace PROCEDURE mydb.sp_Insert_Values ( 
   IN  lastExecDate timestamp 
) 
SQL SECURITY CREATOR 
BEGIN 

CREATE MULTISET VOLATILE TABLE vt_ref_table_1
(
    Ref_Id integer,
    Ref_Unit_Type varchar(50)
) ON COMMIT PRESERVE ROWS;

insert into mydb.vt_ref_table_1
select Ref_Id, Ref_Unit_Type
from mydb.ref_table_1;  

INSERT INTO mydb.Time_Series_Table
select
  t1.TD_TIMECODE
, t1.Time_Series_Meas
, t1.Time_Series_Curve_Type_CD 
, t1.Ref_Unit_Type
, t1.Ref_Id 
, t1.created_on
from
        (
            select 
            meas_ts as TD_TIMECODE 
            , ref_table_1.Ref_Id as  Ref_Id
            , meas as Time_Series_Meas,
            , Time_Series_Curve_Type_CD
            , Ref_Unit_Type
            , current_timestamp as created_on
            from mydb_stg.Time_Series_Table_Stg as stg
            left join mydb.ref_table_1 as ref_table_1
            on ref_table_1.Ref_Id = stg.Ref_Id
            where stg.created_on &gt;= :lastExecDate
        ) as t1
        left join mydb.Time_Series_Table as t2
        on t1.TD_TIMECODE=t2.TD_TIMECODE
        and t1.Time_Series_Curve_Type_CD = t2.Time_Series_Curve_Type_CD
        and t1.Ref_Id=t2.Ref_Id
        where t2.Ref_Id is null
;   
END;


It compiles but when I call it this error is thrown:


  Only a COMMIT WORK or null statement is legal after a DDL Statement.


I know the error is related to the volatile table but I don't know how to correct it.

Why I need the volatile table:

The reference table has a row-level-security constraint. If I use it directly I get another error:


  A multi-table operation is executed and the tables do not have the
  same security constraints.


Teradata Version: 16.20

Mode: ANSI
",-1,-1,-1.0,"I create a stored procedure as follows:

replace PROCEDURE mydb.sp_Insert_Values ( 
   IN  lastExecDate timestamp 
) 
SQL SECURITY CREATOR 
BEGIN 

CREATE MULTISET VOLATILE TABLE vt_ref_table_1
(
    Ref_Id integer,
    Ref_Unit_Type varchar(50)
) ON COMMIT PRESERVE ROWS;

insert into mydb.vt_ref_table_1
select Ref_Id, Ref_Unit_Type
from mydb.ref_table_1;  

INSERT INTO mydb.Time_Series_Table
select
  t1.TD_TIMECODE
, t1.Time_Series_Meas
, t1.Time_Series_Curve_Type_CD 
, t1.Ref_Unit_Type
, t1.Ref_Id 
, t1.created_on
from
        (
            select 
            meas_ts as TD_TIMECODE 
            , ref_table_1.Ref_Id as  Ref_Id
            , meas as Time_Series_Meas,
            , Time_Series_Curve_Type_CD
            , Ref_Unit_Type
            , current_timestamp as created_on
            from mydb_stg.Time_Series_Table_Stg as stg
            left join mydb.ref_table_1 as ref_table_1
            on ref_table_1.Ref_Id = stg.Ref_Id
            where stg.created_on &gt;= :lastExecDate
        ) as t1
        left join mydb.Time_Series_Table as t2
        on t1.TD_TIMECODE=t2.TD_TIMECODE
        and t1.Time_Series_Curve_Type_CD = t2.Time_Series_Curve_Type_CD
        and t1.Ref_Id=t2.Ref_Id
        where t2.Ref_Id is null
;   
END;


It compiles but when I call it this error is thrown:


  Only a COMMIT WORK or null statement is legal after a DDL Statement.


I know the error is related to the volatile table but I don't know how to correct it.

Why I need the volatile table:

The reference table has a row-level-security constraint. If I use it directly I get another error:


  A multi-table operation is executed and the tables do not have the
  same security constraints.


Teradata Version: 16.20

Mode: ANSI
",3
371,53815252,What’s best approach to load data source as teradata to destination as Postgres table using Nifi,"I am new to Nifi so could you help me to create a 
Nifi flow to copy the source teradata table to 
Target as Postgres table.

The intial process I started is copying the same 
Db as source and target (oracle).

ListDatbaseTables —> GenerateTableFetch
—> ExcuteSQL —> ConvertRecord —>
PutDatabaseRecord

How do I define the flow for teradata to Postgres 
When I am trying to pull the data from Postgres 
Getting error as “connection attempt failed 
Due to poolable connection factory”
",-1,-1,-1.0,"I am new to Nifi so could you help me to create a 
Nifi flow to copy the source teradata table to 
Target as Postgres table.

The intial process I started is copying the same 
Db as source and target (oracle).

ListDatbaseTables —> GenerateTableFetch
—> ExcuteSQL —> ConvertRecord —>
PutDatabaseRecord

How do I define the flow for teradata to Postgres 
When I am trying to pull the data from Postgres 
Getting error as “connection attempt failed 
Due to poolable connection factory”
",3
372,53436163,Teradata and MySQL behave different in OVER() and Partition By () Clauses,"I want to understand why the same query is producing different result in Teradata and My SQL.
I am trying to write a query for running total and each DB is giving me different solutions. 

Below is the code:

CREATE TABLE runn_tot (p_id int, p_name varchar(10), price decimal(5,2));

insert into runn_tot values (1,'p1',34);
insert into runn_tot values (2,'p1',56);
insert into runn_tot values (3,'p1',65);
insert into runn_tot values (4,'p1',12);
insert into runn_tot values (5,'p1',34);
insert into runn_tot values (6,'p1',78);
insert into runn_tot values (7,'p1',23);
insert into runn_tot values (8,'p1',55);
insert into runn_tot values (9,'p1',34);
insert into runn_tot values (10,'p1',66);


The query which i'm using in Both MySQL and Teradata

select p_id, p_name, SUM(price) OVER ( partition by p_name order by p_id)  Running_Total
from runn_tot;


Results from MySQL:

+------+--------+---------------+
| p_id | p_name | Running_Total |
+------+--------+---------------+
|    1 | p1     |         34.00 |
|    2 | p1     |         90.00 |
|    3 | p1     |        155.00 |
|    4 | p1     |        167.00 |
|    5 | p1     |        201.00 |
|    6 | p1     |        279.00 |
|    7 | p1     |        302.00 |
|    8 | p1     |        357.00 |
|    9 | p1     |        391.00 |
|   10 | p1     |        457.00 |
+------+--------+---------------+


Results from Teradata:

1   p1  457.00
2   p1  457.00
3   p1  457.00
4   p1  457.00
5   p1  457.00
6   p1  457.00
7   p1  457.00
8   p1  457.00
9   p1  457.00
10  p1  457.00


I am trying to understand why MySQL is able to get the correct running total and teradata is not doing the window function correctly.
",1,-1,-1.0,"I want to understand why the same query is producing different result in Teradata and My SQL.
I am trying to write a query for running total and each DB is giving me different solutions. 

Below is the code:

CREATE TABLE runn_tot (p_id int, p_name varchar(10), price decimal(5,2));

insert into runn_tot values (1,'p1',34);
insert into runn_tot values (2,'p1',56);
insert into runn_tot values (3,'p1',65);
insert into runn_tot values (4,'p1',12);
insert into runn_tot values (5,'p1',34);
insert into runn_tot values (6,'p1',78);
insert into runn_tot values (7,'p1',23);
insert into runn_tot values (8,'p1',55);
insert into runn_tot values (9,'p1',34);
insert into runn_tot values (10,'p1',66);


The query which i'm using in Both MySQL and Teradata

select p_id, p_name, SUM(price) OVER ( partition by p_name order by p_id)  Running_Total
from runn_tot;


Results from MySQL:

+------+--------+---------------+
| p_id | p_name | Running_Total |
+------+--------+---------------+
|    1 | p1     |         34.00 |
|    2 | p1     |         90.00 |
|    3 | p1     |        155.00 |
|    4 | p1     |        167.00 |
|    5 | p1     |        201.00 |
|    6 | p1     |        279.00 |
|    7 | p1     |        302.00 |
|    8 | p1     |        357.00 |
|    9 | p1     |        391.00 |
|   10 | p1     |        457.00 |
+------+--------+---------------+


Results from Teradata:

1   p1  457.00
2   p1  457.00
3   p1  457.00
4   p1  457.00
5   p1  457.00
6   p1  457.00
7   p1  457.00
8   p1  457.00
9   p1  457.00
10  p1  457.00


I am trying to understand why MySQL is able to get the correct running total and teradata is not doing the window function correctly.
",3
373,53433513,login to bteq in solaris one liner,"can any one please help me with the command to login to teradata in solaris using bteq

general command to login is

.LOGON $TDPID/$username,$password;


but this is not working in solaris. it says below


  *** Error:  Invalid logon!


teradata version installed -13.10.00.02
",-1,-1,-1.0,"can any one please help me with the command to login to teradata in solaris using bteq

general command to login is

.LOGON $TDPID/$username,$password;


but this is not working in solaris. it says below


  *** Error:  Invalid logon!


teradata version installed -13.10.00.02
",1
374,53934895,I am trying to store in HDFS as parquet file from teradata with help of TDCH jar 1.6 version,"I am trying to store in HDFS as parquet file  from teradata with help of TDCH jar

I am getting connection exception : plugin ""hdfs-parquet"" not found

How can i resolve the issue?
",-1,-1,-1.0,"I am trying to store in HDFS as parquet file  from teradata with help of TDCH jar

I am getting connection exception : plugin ""hdfs-parquet"" not found

How can i resolve the issue?
",0
375,53935728,sqlalchemy - connect to teradata,"Sorry in advance, I am very new to Python. I am trying to connect to a Teradata database via python sqlalchemy, however I can't successfully establish a connection. 

The error message I receive is ""NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:teradata"".

Here is what I have so far:

import sqlalchemy

from sqlalchemy import create_engine
user = 'myusername'
pasw= 'mypassword'
host = 'hostname.com'
db = 'PRD_DB_SCHEMA'
engine_str = (""teradata://""+user+"":""+pasw+""@""+host+"":22/""+db+""?authentication=LDAP"")
td_engine = create_engine(engine_str)
conn = td_engine.connect()

",-1,-1,-1.0,"Sorry in advance, I am very new to Python. I am trying to connect to a Teradata database via python sqlalchemy, however I can't successfully establish a connection. 

The error message I receive is ""NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:teradata"".

Here is what I have so far:

import sqlalchemy

from sqlalchemy import create_engine
user = 'myusername'
pasw= 'mypassword'
host = 'hostname.com'
db = 'PRD_DB_SCHEMA'
engine_str = (""teradata://""+user+"":""+pasw+""@""+host+"":22/""+db+""?authentication=LDAP"")
td_engine = create_engine(engine_str)
conn = td_engine.connect()

",1
376,54048595,sqoop export to Teradata gives com.teradata.connector.common.exception.ConnectorException: Malformed \uxxxx encoding,"I am trying to export data from HDFS to Teradata using sqoop. I have created a table in Teradata and tried to import a sample text file with some sample data. Here is my sqoop export command

sqoop export --connect jdbc:teradata://xxx.xxx.xxx.xx/Database=XXXXXXX,CHARSET=UTF8 \
--username User_name  \
--password pwd \
--export-dir /user/User/test_td_export/ \
--table HDP_TD_EXPORT_TEST \
--input-fields-terminated-by ',' \
--input-escaped-by '\' \
--input-enclosed-by '\""' \
--input-optionally-enclosed-by '\""' \
--mapreduce-job-name td_export_test          

I am able to do a sqoop eval to the same table to get the count successfully but while exporting data, I am getting the exception.

19/01/04 20:48:26 ERROR tool.ExportTool: Encountered IOException running export job:
com.teradata.connector.common.exception.ConnectorException: Malformed \uxxxx encoding

This is the first time I have tried to export to teradata. I have exported data to Oracle and didn't see any such issues. Any help is greatly appreciated. Thanks
",-1,-1,-1.0,"I am trying to export data from HDFS to Teradata using sqoop. I have created a table in Teradata and tried to import a sample text file with some sample data. Here is my sqoop export command

sqoop export --connect jdbc:teradata://xxx.xxx.xxx.xx/Database=XXXXXXX,CHARSET=UTF8 \
--username User_name  \
--password pwd \
--export-dir /user/User/test_td_export/ \
--table HDP_TD_EXPORT_TEST \
--input-fields-terminated-by ',' \
--input-escaped-by '\' \
--input-enclosed-by '\""' \
--input-optionally-enclosed-by '\""' \
--mapreduce-job-name td_export_test          

I am able to do a sqoop eval to the same table to get the count successfully but while exporting data, I am getting the exception.

19/01/04 20:48:26 ERROR tool.ExportTool: Encountered IOException running export job:
com.teradata.connector.common.exception.ConnectorException: Malformed \uxxxx encoding

This is the first time I have tried to export to teradata. I have exported data to Oracle and didn't see any such issues. Any help is greatly appreciated. Thanks
",3
377,54106763,How to insert Null values from file to teradata table using python. Its inserting as NaN in place of Null Values in table,"Not able to insert null values into teradata table using python.

I have excel file of 5000 records.One column has all null values.I want to insert to teradata table as-is using python script.But after insertion in place of Null its inserted NaN values.

import pandas as pd
import teradata
dataset=pd.read_excel(""Filepath"",skiprows=[0])
host,username,password = 'hostname','username', 'password'
#Make a connection
udaExec = teradata.UdaExec (appName=""test"", version=""1.0"", logConsole=False)

session = udaExec.connect(driver='Teradata',method=""odbc"", system=host,username=username, password=password);
for row in range(0,dataset.shape[0]):
   col=list(df.values[row])
   session.execute(""""""INSERT INTO PRSNA_STG(PRSNA_ID,BIRTH_DT,DEATH_DT,GNDR_TYPE_CD,ETHCTY_TYPE_CD,NTLTY_CD,MARITAL_CD)VALUES(?,CAST(? AS DATE FORMAT 'MM/DD/YYYY'),?,?,?,?,?)"""""",(col[0],'/'.join([str(i).zfill(2) for i in col[1].split('/')]),col[2],col[3],col[4],col[5],col[6]))


col[2] is the one which am speaking about.The above code is working.But DEATH_DT values in teradata are showing as NaN
",-1,-1,-1.0,"Not able to insert null values into teradata table using python.

I have excel file of 5000 records.One column has all null values.I want to insert to teradata table as-is using python script.But after insertion in place of Null its inserted NaN values.

import pandas as pd
import teradata
dataset=pd.read_excel(""Filepath"",skiprows=[0])
host,username,password = 'hostname','username', 'password'
#Make a connection
udaExec = teradata.UdaExec (appName=""test"", version=""1.0"", logConsole=False)

session = udaExec.connect(driver='Teradata',method=""odbc"", system=host,username=username, password=password);
for row in range(0,dataset.shape[0]):
   col=list(df.values[row])
   session.execute(""""""INSERT INTO PRSNA_STG(PRSNA_ID,BIRTH_DT,DEATH_DT,GNDR_TYPE_CD,ETHCTY_TYPE_CD,NTLTY_CD,MARITAL_CD)VALUES(?,CAST(? AS DATE FORMAT 'MM/DD/YYYY'),?,?,?,?,?)"""""",(col[0],'/'.join([str(i).zfill(2) for i in col[1].split('/')]),col[2],col[3],col[4],col[5],col[6]))


col[2] is the one which am speaking about.The above code is working.But DEATH_DT values in teradata are showing as NaN
",1
378,54137966,Is there a way to store pandas dataframe to a Teradata table,"I have created a pandas data-frame 'df' and I am trying to store it in a 'table' using Teradata-SQL assistant.

Connection string - 

conn = pyodbc.connect(
         ""DRIVER=Teradata;DBCNAME=tdprod;Authentication=LDAP;UID="" + username + "";PWD="" + password + "";QUIETMODE=YES"",
        autocommit=True, unicode_results=True)

cursor = conn.cursor().execute(sql)


Tried using:  df.to_sql('table', con =conn)

This doesn't work.

Is there an easier way to store a dataframe into a table.

Any help is appreciated.

Thanks.

Traceback (most recent call last):

 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2158, in _wrap_pool_connect
return fn()
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 410, in connect
return _ConnectionFairy._checkout(self, self._threadconns)
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 788, in _checkout
fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 529, in checkout
rec = pool._do_get()
  File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 1096, in _do_get
c = self._create_connection()
  File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 347, in _create_connection
return _ConnectionRecord(self)
  File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 474, in __init__
self.__connect(first_connect_check=True)
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 671, in __connect
connection = pool._invoke_creator(self)
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\engine\strategies.py"", line 106, in connect
 return dialect.connect(*cargs, **cparams)
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 412, in connect
return self.dbapi.connect(*cargs, **cparams)
  File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\teradata\tdodbc.py"", line 454, in __init__
checkStatus(rc, hDbc=self.hDbc, method=""SQLDriverConnectW"")
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\teradata\tdodbc.py"", line 231, in checkStatus
raise DatabaseError(i[2], u""[{}] {}"".format(i[0], msg), i[0])
teradata.api.DatabaseError: (8017, '[28000] [Teradata][ODBC Teradata Driver][Teradata Database] The UserId, Password or Account is invalid. , [Teradata][ODBC Teradata Driver][Teradata Database] The UserId, Password or Account is invalid. ')

",1,-1,-1.0,"I have created a pandas data-frame 'df' and I am trying to store it in a 'table' using Teradata-SQL assistant.

Connection string - 

conn = pyodbc.connect(
         ""DRIVER=Teradata;DBCNAME=tdprod;Authentication=LDAP;UID="" + username + "";PWD="" + password + "";QUIETMODE=YES"",
        autocommit=True, unicode_results=True)

cursor = conn.cursor().execute(sql)


Tried using:  df.to_sql('table', con =conn)

This doesn't work.

Is there an easier way to store a dataframe into a table.

Any help is appreciated.

Thanks.

Traceback (most recent call last):

 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\engine\base.py"", line 2158, in _wrap_pool_connect
return fn()
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 410, in connect
return _ConnectionFairy._checkout(self, self._threadconns)
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 788, in _checkout
fairy = _ConnectionRecord.checkout(pool)
  File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 529, in checkout
rec = pool._do_get()
  File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 1096, in _do_get
c = self._create_connection()
  File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 347, in _create_connection
return _ConnectionRecord(self)
  File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 474, in __init__
self.__connect(first_connect_check=True)
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\pool.py"", line 671, in __connect
connection = pool._invoke_creator(self)
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\engine\strategies.py"", line 106, in connect
 return dialect.connect(*cargs, **cparams)
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\sqlalchemy\engine\default.py"", line 412, in connect
return self.dbapi.connect(*cargs, **cparams)
  File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\teradata\tdodbc.py"", line 454, in __init__
checkStatus(rc, hDbc=self.hDbc, method=""SQLDriverConnectW"")
 File ""C:\Users\tripata\PycharmProjects\NLP\venv\lib\site-packages\teradata\tdodbc.py"", line 231, in checkStatus
raise DatabaseError(i[2], u""[{}] {}"".format(i[0], msg), i[0])
teradata.api.DatabaseError: (8017, '[28000] [Teradata][ODBC Teradata Driver][Teradata Database] The UserId, Password or Account is invalid. , [Teradata][ODBC Teradata Driver][Teradata Database] The UserId, Password or Account is invalid. ')

",1
379,54251443,Getting issue in XmlAgg fuction in teradata,"I am getting below issue  when using 'XmlAgg'function in teradata.

Code:-

,Trim(Trailing '|' FROM ( XmlAgg(EDW1.sr_part_tracking_desc || '|'   ORDER BY EDW1.req_line_id) (VARCHAR(4000)) ) )


Issue:

*** Failure 7548 13 The target type is not large enough to accomodate the results of the cast.

Please help, it is very critical for me. 

tried

substr(Trim(Trailing '|' FROM ( XmlAgg(EDW1.sr_part_tracking_desc || '|'   ORDER BY EDW1.req_line_id) (VARCHAR(4000)) ) ),1,4000) 


but same issue.

Trim(Trailing '|' FROM ( XmlAgg(EDW1.sr_part_tracking_desc || '|'   ORDER BY EDW1.req_line_id) (VARCHAR(4000)) ) )


Issue: *** Failure 7548 13 The target type is not large enough to accomodate the results of the cast.
",-1,-1,-1.0,"I am getting below issue  when using 'XmlAgg'function in teradata.

Code:-

,Trim(Trailing '|' FROM ( XmlAgg(EDW1.sr_part_tracking_desc || '|'   ORDER BY EDW1.req_line_id) (VARCHAR(4000)) ) )


Issue:

*** Failure 7548 13 The target type is not large enough to accomodate the results of the cast.

Please help, it is very critical for me. 

tried

substr(Trim(Trailing '|' FROM ( XmlAgg(EDW1.sr_part_tracking_desc || '|'   ORDER BY EDW1.req_line_id) (VARCHAR(4000)) ) ),1,4000) 


but same issue.

Trim(Trailing '|' FROM ( XmlAgg(EDW1.sr_part_tracking_desc || '|'   ORDER BY EDW1.req_line_id) (VARCHAR(4000)) ) )


Issue: *** Failure 7548 13 The target type is not large enough to accomodate the results of the cast.
",3
380,54413720,Creating a new column in Teradata using CASE statement,"I am trying to create a new column in a table in Teradata using a CASE Statement

I am trying a create a new column called ID_Number_Mod from a CASE Statement based off an original column  called ID_Number from the table. Below is my code

alter table Table_A
add  (case
when char_length(cast(cast(ID_Number as bigint) as varchar(50)))=12 then cast('999000' as char(6)) || cast(cast( substr(cast(cast(ID_Number as bigint) as char(12)), 4, 12) as bigint) as char(15))
else ID_Number end) as ID_Number_Mod float;


I am getting the below syntax error


  ""expected something like a name or a Unicode delimited identifier
  between '(' and the 'case' keyword

",-1,-1,-1.0,"I am trying to create a new column in a table in Teradata using a CASE Statement

I am trying a create a new column called ID_Number_Mod from a CASE Statement based off an original column  called ID_Number from the table. Below is my code

alter table Table_A
add  (case
when char_length(cast(cast(ID_Number as bigint) as varchar(50)))=12 then cast('999000' as char(6)) || cast(cast( substr(cast(cast(ID_Number as bigint) as char(12)), 4, 12) as bigint) as char(15))
else ID_Number end) as ID_Number_Mod float;


I am getting the below syntax error


  ""expected something like a name or a Unicode delimited identifier
  between '(' and the 'case' keyword

",3
381,54656883,Turn Multiple Rows into 1 Column with Multiple Fields in Teradata SQL,"I have a table that looks like this in Teradata SQL Assistant:

Software Name   Employee
Word            Bob
Excel           Bob
Word            Kim
Excel           Kim
PowerPoint      Bob


I want to create it so that for each software, there is only 1 row and each employee is stored as a binary 1 or 0 based on whether they have the software or not. The Final Table should look like this:

Software Name    Bob   Kim
Word             1      1
Excel            1      1
PowerPoint       1      0


I'm using Teradata 15.10 and I haven't been able to figure it out how to do this because Teradata 15.10 does not support the PIVOT function.   
",-1,1,-1.0,"I have a table that looks like this in Teradata SQL Assistant:

Software Name   Employee
Word            Bob
Excel           Bob
Word            Kim
Excel           Kim
PowerPoint      Bob


I want to create it so that for each software, there is only 1 row and each employee is stored as a binary 1 or 0 based on whether they have the software or not. The Final Table should look like this:

Software Name    Bob   Kim
Word             1      1
Excel            1      1
PowerPoint       1      0


I'm using Teradata 15.10 and I haven't been able to figure it out how to do this because Teradata 15.10 does not support the PIVOT function.   
",3
382,54699361,Format Python timestamp for Teradata DB table,"I am working with a Teradata table that has a timestamp column: TIMESTAMP(6) with data that looks like this:

2/14/2019 13:09:51.210000


Currently I have a Python time variable that I want to send into the Teradata table via SQL, that looks like below:

from datetime import datetime

time = datetime.now().strftime(""%m/%d/%Y %H:%M:%S"")

02/14/2019 13:23:24


How can I reformat that to insert correctly?  It is error'ing out with:

teradata.api.DatabaseError: (6760, '[22008] [Teradata][ODBC Teradata Driver][Teradata Database](-6760)Invalid timestamp.')


I tried using the same format the Teradata timestamp column uses:

time = datetime.now().strftime(""%mm/%dd/%YYYY %HH24:%MI:%SS"")


Same error message

Thanks
",-1,-1,-1.0,"I am working with a Teradata table that has a timestamp column: TIMESTAMP(6) with data that looks like this:

2/14/2019 13:09:51.210000


Currently I have a Python time variable that I want to send into the Teradata table via SQL, that looks like below:

from datetime import datetime

time = datetime.now().strftime(""%m/%d/%Y %H:%M:%S"")

02/14/2019 13:23:24


How can I reformat that to insert correctly?  It is error'ing out with:

teradata.api.DatabaseError: (6760, '[22008] [Teradata][ODBC Teradata Driver][Teradata Database](-6760)Invalid timestamp.')


I tried using the same format the Teradata timestamp column uses:

time = datetime.now().strftime(""%mm/%dd/%YYYY %HH24:%MI:%SS"")


Same error message

Thanks
",3
383,54748104,Create volatile table and query against it in one request (using teradata sql assistant),"Is it possible to construct a volatile table and query against that table in a single request while using Teradata SQL assistant?

Doing something like:

create volatile table table_a as 
(select .....
)
with data on commit preserve rows;

 ;select * from table_a;


I receive the error:

 Query Failed. 3932:  Only an ET or null statement is legal after a DDL Statement. 

",-1,-1,-1.0,"Is it possible to construct a volatile table and query against that table in a single request while using Teradata SQL assistant?

Doing something like:

create volatile table table_a as 
(select .....
)
with data on commit preserve rows;

 ;select * from table_a;


I receive the error:

 Query Failed. 3932:  Only an ET or null statement is legal after a DDL Statement. 

",3
384,54836516,Trouble with text of very long Teradata query,"I am using the following function to perform a Teradata query from EXCEL 2007:

    Function Get_Query_Results(Rng As Range, Location As String, var As String, UID As String, PWD As String) As Long
    On Error GoTo TroubleWithTeradata
        Rng.Select
        With ActiveSheet.ListObjects.Add(SourceType:=0, Source:=""ODBC;DSN=Server12;UID="" &amp; UID &amp; "";PWD="" &amp; PWD &amp; "";"", Destination:=Rng).QueryTable
            .CommandText = var
            .RowNumbers = False
            .FillAdjacentFormulas = False
            .PreserveFormatting = True
            .RefreshOnFileOpen = False
            .BackgroundQuery = True
            .RefreshStyle = xlInsertDeleteCells
            .SavePassword = False
            .SaveData = True
            .AdjustColumnWidth = True
            .RefreshPeriod = 0
            .PreserveColumnInfo = True
            .Refresh BackgroundQuery:=False
        End With
        Get_Query_Results = LastInCol(Columns(Location)) - 1
        Exit Function
    TroubleWithTeradata:
        Get_Query_Results = -1
    End Function


This query puts its results in a region whose ""North West"" corner is some cell specified by Rng, and determines how many records were returned by getting the row number of the last record in column number Location, which is what the function LastInCol (not listed here) returns. If the query doesn't fail but returns no records (i.e., only a header row), the number of records returned is 0. If the function does indeed fail, the number of records returned is -1.

The text of the Teradata query itself is contained in string var. And herein lies the problem. I have been using this function successfully for years. But  now I need to build a new query that makes the variable var exceed the EXCEL VBA limit of 32767 characters.

I really am not sure what limits are being exceeded here. Certainly that of the length of the variable var, but there's also what the QueryTable parameter .CommandText can contain.

How can I get around these limits?
",-1,1,-1.0,"I am using the following function to perform a Teradata query from EXCEL 2007:

    Function Get_Query_Results(Rng As Range, Location As String, var As String, UID As String, PWD As String) As Long
    On Error GoTo TroubleWithTeradata
        Rng.Select
        With ActiveSheet.ListObjects.Add(SourceType:=0, Source:=""ODBC;DSN=Server12;UID="" &amp; UID &amp; "";PWD="" &amp; PWD &amp; "";"", Destination:=Rng).QueryTable
            .CommandText = var
            .RowNumbers = False
            .FillAdjacentFormulas = False
            .PreserveFormatting = True
            .RefreshOnFileOpen = False
            .BackgroundQuery = True
            .RefreshStyle = xlInsertDeleteCells
            .SavePassword = False
            .SaveData = True
            .AdjustColumnWidth = True
            .RefreshPeriod = 0
            .PreserveColumnInfo = True
            .Refresh BackgroundQuery:=False
        End With
        Get_Query_Results = LastInCol(Columns(Location)) - 1
        Exit Function
    TroubleWithTeradata:
        Get_Query_Results = -1
    End Function


This query puts its results in a region whose ""North West"" corner is some cell specified by Rng, and determines how many records were returned by getting the row number of the last record in column number Location, which is what the function LastInCol (not listed here) returns. If the query doesn't fail but returns no records (i.e., only a header row), the number of records returned is 0. If the function does indeed fail, the number of records returned is -1.

The text of the Teradata query itself is contained in string var. And herein lies the problem. I have been using this function successfully for years. But  now I need to build a new query that makes the variable var exceed the EXCEL VBA limit of 32767 characters.

I really am not sure what limits are being exceeded here. Certainly that of the length of the variable var, but there's also what the QueryTable parameter .CommandText can contain.

How can I get around these limits?
",3
385,55012283,Retrieving results quicker from a large table in Teradata,"I'm working on an analytics project that requires me to pull some data from a very large table in Teradata. This is the query I'm using:

select TransactionNumber
from my_table
where TransactionDate between date '2017-01-01' and date '2017-12-31'
and ItemNumber in (99276);


Even though I'm filtering my_table for all of 2017, there are still almost 900 million rows that result from this query and the query takes a little over 30 seconds to run. Because of the nature of my project, I would like it to run in about 5 seconds or fewer, but given the size of the table, I'm not even sure if it's possible. Here's what shows up when I use 'explain' if it helps:

1) First, we lock DBTables.my_table in view
DB.my_table for access.
2) Next, we do an all-AMPs RETRIEVE step from 365 partitions of
DBTables.my_table in view DB.my_table with a
condition of (""(DBTables.my_table in view
DB.my_table.TransactionDate &lt;= DATE '2017-12-31') AND
((DBTables.my_table in view
DB.my_table.TransactionDate &gt;= DATE '2017-01-01') AND
(DBTables.my_table in view
DB.my_table.ItemNumber = 99276 ))"") into Spool
1 (group_amps), which is built locally on the AMPs. The size of
Spool 1 is estimated with no confidence to be 617,535,066 rows (
14,203,306,518 bytes). The estimated time for this step is 2
minutes and 48 seconds.
3) Finally, we send out an END TRANSACTION step to all AMPs involved
in processing the request.
-&gt; The contents of Spool 1 are sent back to the user as the result of
statement 1. The total estimated time is 2 minutes and 48 seconds.


Admittedly, I'm not all too familiar with optimizing queries and I only have read access to the database as I'm not a DBA.
",1,-1,-1.0,"I'm working on an analytics project that requires me to pull some data from a very large table in Teradata. This is the query I'm using:

select TransactionNumber
from my_table
where TransactionDate between date '2017-01-01' and date '2017-12-31'
and ItemNumber in (99276);


Even though I'm filtering my_table for all of 2017, there are still almost 900 million rows that result from this query and the query takes a little over 30 seconds to run. Because of the nature of my project, I would like it to run in about 5 seconds or fewer, but given the size of the table, I'm not even sure if it's possible. Here's what shows up when I use 'explain' if it helps:

1) First, we lock DBTables.my_table in view
DB.my_table for access.
2) Next, we do an all-AMPs RETRIEVE step from 365 partitions of
DBTables.my_table in view DB.my_table with a
condition of (""(DBTables.my_table in view
DB.my_table.TransactionDate &lt;= DATE '2017-12-31') AND
((DBTables.my_table in view
DB.my_table.TransactionDate &gt;= DATE '2017-01-01') AND
(DBTables.my_table in view
DB.my_table.ItemNumber = 99276 ))"") into Spool
1 (group_amps), which is built locally on the AMPs. The size of
Spool 1 is estimated with no confidence to be 617,535,066 rows (
14,203,306,518 bytes). The estimated time for this step is 2
minutes and 48 seconds.
3) Finally, we send out an END TRANSACTION step to all AMPs involved
in processing the request.
-&gt; The contents of Spool 1 are sent back to the user as the result of
statement 1. The total estimated time is 2 minutes and 48 seconds.


Admittedly, I'm not all too familiar with optimizing queries and I only have read access to the database as I'm not a DBA.
",3
386,55021354,Sqoop import from Teradata - No more room in database,"I am new to Big data, when I am using Sqoop commands to import data from teradata into my Hadoop cluster I am encountering a ""No more room in database"" error
I am doing the following:

1.The data I am trying to pull into my Hadoop cluster is a view table
2.The I have used the following sqoop command

 sqoop import --connect ""jdbc:teradata://xxx.xxx.xxx.xxx/DATABASE=XY"" \
    -- username user1 \
    -- password xyc
    -- query ""
    SELECT * FROM TABLE1 WHERE .... AND \$CONDITIONS \
    "" \
    --split-by ITEM_1 \
    --delete-target-dir \
    --target-dir /user/home/folder1 \
    --as-avrodatafile;


I know that the default mappers is 4 since I do not have a primary key for my view, I am using split-by.

Using --num-mappers 1, works but takes a long time for to port over roughly 36GB of data, hence I wanted to increase the num-mappers to 4 or more, however, I am getting the ""no more room"" error. Does anyone know what's happening?
",-1,-1,-1.0,"I am new to Big data, when I am using Sqoop commands to import data from teradata into my Hadoop cluster I am encountering a ""No more room in database"" error
I am doing the following:

1.The data I am trying to pull into my Hadoop cluster is a view table
2.The I have used the following sqoop command

 sqoop import --connect ""jdbc:teradata://xxx.xxx.xxx.xxx/DATABASE=XY"" \
    -- username user1 \
    -- password xyc
    -- query ""
    SELECT * FROM TABLE1 WHERE .... AND \$CONDITIONS \
    "" \
    --split-by ITEM_1 \
    --delete-target-dir \
    --target-dir /user/home/folder1 \
    --as-avrodatafile;


I know that the default mappers is 4 since I do not have a primary key for my view, I am using split-by.

Using --num-mappers 1, works but takes a long time for to port over roughly 36GB of data, hence I wanted to increase the num-mappers to 4 or more, however, I am getting the ""no more room"" error. Does anyone know what's happening?
",3
387,55131340,Teradata Compression Tools,"Any recommend multi value compression tools for Teradata? (Except from Atana Suite and Prise Tools. Their trial versions do not show output or enough test to get convinced that I should aquire this)
",0,-1,-1.0,"Any recommend multi value compression tools for Teradata? (Except from Atana Suite and Prise Tools. Their trial versions do not show output or enough test to get convinced that I should aquire this)
",3
388,55137165,Error while loading data in Teradata using proc append (SAS EG 14.1),"I am getting below error while appending data in a Teradata table from SAS

ERROR: Teradata connection: No more room in database TINYDB. Correct error and
restart as an APPEND process with option TPT_RESTART=YES. Since no checkpoints were
taken, if the previous run used FIRSTOBS=n, use the same value in the restart.

I don't know why i am getting this error for one table, because i am able to append other Teradata tables.

I am using simple proc append

proc append data=table1 base=table2 (MULTILOAD=YES TPT=YES) force;
run;


Please suggest why its giving above error just for one table, while appending in other Teradata tables is working fine.

Thanks

#

Adding
Just one explanation, if i remove (MULTILOAD=YES TPT=YES) from the Proc Append Statement, then it will work, but will take huge amount of time
",-1,-1,-1.0,"I am getting below error while appending data in a Teradata table from SAS

ERROR: Teradata connection: No more room in database TINYDB. Correct error and
restart as an APPEND process with option TPT_RESTART=YES. Since no checkpoints were
taken, if the previous run used FIRSTOBS=n, use the same value in the restart.

I don't know why i am getting this error for one table, because i am able to append other Teradata tables.

I am using simple proc append

proc append data=table1 base=table2 (MULTILOAD=YES TPT=YES) force;
run;


Please suggest why its giving above error just for one table, while appending in other Teradata tables is working fine.

Thanks

#

Adding
Just one explanation, if i remove (MULTILOAD=YES TPT=YES) from the Proc Append Statement, then it will work, but will take huge amount of time
",3
389,55217925,Teradata Current year and year-1,"How to get the dynamic years in the Query for where condition, i need to fetch data for 2017,2018,2019, currently i am hard coding them ( where FSC_YR in (2017,2018,2019) instead i need in a dynamic way. How to do it in teradata.
I tried extract(year from current_date)-2,extract(year from current_date)-1,extract(year from current_date)-3). I am getting error too many expression.
",-1,-1,-1.0,"How to get the dynamic years in the Query for where condition, i need to fetch data for 2017,2018,2019, currently i am hard coding them ( where FSC_YR in (2017,2018,2019) instead i need in a dynamic way. How to do it in teradata.
I tried extract(year from current_date)-2,extract(year from current_date)-1,extract(year from current_date)-3). I am getting error too many expression.
",3
390,55227892,getting query from teradata to pyspark,"I am trying to run a query in teradata using pyspark, I am able pull the whole table using this function but I am getting a error when I try to run the query.
can guys check and tell where I am going wrong.

def from_rdbms(spark,user, password, driver, jdbc_url, p_type, p_table, p_query, p_partition, p_numpartitions, p_lower=1, p_upper=1, p_fetchsize=1000):
    df_ret = None
    dbProperties = {
            ""user"": user,
            ""password"": password,
            ""driver"": driver
            }
    jdbcUrl = jdbc_url
    dbProperties[""fetchsize""] = str(p_fetchsize)
    dbPropertiesExtended = dbProperties
    if p_type == ""Table"":
        query = p_table
    else:
        query =  p_query 
    if p_partition == '':
        df_ret = spark.read.jdbc(url=jdbcUrl , table=query , properties=dbProperties)
    else:
        dbPropertiesExtended[""partition""] = str(p_partition)
        dbPropertiesExtended[""lower""] = str(p_lower)
        dbPropertiesExtended[""upper""] = str(p_upper)
        dbPropertiesExtended[""numpartitions""] = str(p_numpartitions)
        df_ret = spark.read.jdbc(url=jdbcUrl, table=query , properties=dbPropertiesExtended)
    return df_ret


running the fuction

query1=""select count(*) as c from ""+db_name+"".""+table_name + "" t1""

count_td = from_rdbms(spark,user_name, password,driver=""com.teradata.jdbc.TeraDriver"" , jdbc_url= source_url, p_type=""Query"", p_table="""", p_query=query1, p_partition="""", p_numpartitions="""", p_lower=1, p_upper=1, p_fetchsize=1000)


error that I am getting is:

 java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.08] [Error 3707] [SQLState 42000] Syntax error, expected something like a name or a Unicode delimited identifier or an 'UDFCALLNAME' keyword or '(' between the 'FROM' keyword and the 'select' keyword.
        at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:309)
        at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:103)
        at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:311)
        at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:200)
        at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:137)
        at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:128)
        at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:389)
        at com.teradata.jdbc.jdbc_4.TDStatement.prepareRequest(TDStatement.java:576)
        at com.teradata.jdbc.jdbc_4.TDPreparedStatement.&lt;init&gt;(TDPreparedStatement.java:131)
        at com.teradata.jdbc.jdk6.JDK6_SQL_PreparedStatement.&lt;init&gt;(JDK6_SQL_PreparedStatement.java:30)
        at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.constructPreparedStatement(JDK6_SQL_Connection.java:82)
        at com.teradata.jdbc.jdbc_4.TDSession.prepareStatement(TDSession.java:1337)
        at com.teradata.jdbc.jdbc_4.TDSession.prepareStatement(TDSession.java:1381)
        at com.teradata.jdbc.jdbc_4.TDSession.prepareStatement(TDSession.java:1367)
        at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:60)
        at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.&lt;init&gt;(JDBCRelation.scala:114)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:52)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:309)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)
        at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:193)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:280)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:214)
        at java.lang.Thread.run(Thread.java:745)

",-1,-1,-1.0,"I am trying to run a query in teradata using pyspark, I am able pull the whole table using this function but I am getting a error when I try to run the query.
can guys check and tell where I am going wrong.

def from_rdbms(spark,user, password, driver, jdbc_url, p_type, p_table, p_query, p_partition, p_numpartitions, p_lower=1, p_upper=1, p_fetchsize=1000):
    df_ret = None
    dbProperties = {
            ""user"": user,
            ""password"": password,
            ""driver"": driver
            }
    jdbcUrl = jdbc_url
    dbProperties[""fetchsize""] = str(p_fetchsize)
    dbPropertiesExtended = dbProperties
    if p_type == ""Table"":
        query = p_table
    else:
        query =  p_query 
    if p_partition == '':
        df_ret = spark.read.jdbc(url=jdbcUrl , table=query , properties=dbProperties)
    else:
        dbPropertiesExtended[""partition""] = str(p_partition)
        dbPropertiesExtended[""lower""] = str(p_lower)
        dbPropertiesExtended[""upper""] = str(p_upper)
        dbPropertiesExtended[""numpartitions""] = str(p_numpartitions)
        df_ret = spark.read.jdbc(url=jdbcUrl, table=query , properties=dbPropertiesExtended)
    return df_ret


running the fuction

query1=""select count(*) as c from ""+db_name+"".""+table_name + "" t1""

count_td = from_rdbms(spark,user_name, password,driver=""com.teradata.jdbc.TeraDriver"" , jdbc_url= source_url, p_type=""Query"", p_table="""", p_query=query1, p_partition="""", p_numpartitions="""", p_lower=1, p_upper=1, p_fetchsize=1000)


error that I am getting is:

 java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.08] [Error 3707] [SQLState 42000] Syntax error, expected something like a name or a Unicode delimited identifier or an 'UDFCALLNAME' keyword or '(' between the 'FROM' keyword and the 'select' keyword.
        at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:309)
        at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:103)
        at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:311)
        at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:200)
        at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:137)
        at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:128)
        at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:389)
        at com.teradata.jdbc.jdbc_4.TDStatement.prepareRequest(TDStatement.java:576)
        at com.teradata.jdbc.jdbc_4.TDPreparedStatement.&lt;init&gt;(TDPreparedStatement.java:131)
        at com.teradata.jdbc.jdk6.JDK6_SQL_PreparedStatement.&lt;init&gt;(JDK6_SQL_PreparedStatement.java:30)
        at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.constructPreparedStatement(JDK6_SQL_Connection.java:82)
        at com.teradata.jdbc.jdbc_4.TDSession.prepareStatement(TDSession.java:1337)
        at com.teradata.jdbc.jdbc_4.TDSession.prepareStatement(TDSession.java:1381)
        at com.teradata.jdbc.jdbc_4.TDSession.prepareStatement(TDSession.java:1367)
        at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:60)
        at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.&lt;init&gt;(JDBCRelation.scala:114)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:52)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:309)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)
        at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:193)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:280)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:214)
        at java.lang.Thread.run(Thread.java:745)

",0
391,54995782,How to use 'where clause' for Timestamp teradata where the data is in format 'YYYY-MM-DD HH:MI:SS.s'?,"I'm willing to filter a table based on a column which has the data in format 'YYYY-MM-DD HH:MI:SS.s' as described in the question. Since this is not default timestamp format in teradata,so I tried to typecast. 

So far I have tried following things :

where column_name &gt; ${VAL1}(Timestamp(1),Format'YYYY-MM-DDbHH:MI:SS.s(1)') and column_name &lt; ${VAL2}(Timestamp(1),Format'YYYY-MM-DDbHH:MI:SS.s(1)')

where column_name &gt; ${VAL1}(Timestamp(0),Format'YYYY-MM-DDbHH:MI:SS.s(1)') and column_name &lt; ${VAL2}(Timestamp(0),Format'YYYY-MM-DDbHH:MI:SS.s(1)')

where column_name &gt; ${Val1}(Timestamp(1),Format'YYYY-MM-DDbHH:MI:SS.s(1)bt') and column_name &lt; ${Val2}(Timestamp(1),Format'YYYY-MM-DDbHH:MI:SS.s(1)bt')


  But everytime the error is same ""Syntax error,expected something like
  an 'OR' keyword or ')' between an integer and the integer '13'.


I believe the problem is in passing the format to teradata. Any help appreciated.

P.S: VAL1 and VAL2 are bash variables.
",-1,-1,-1.0,"I'm willing to filter a table based on a column which has the data in format 'YYYY-MM-DD HH:MI:SS.s' as described in the question. Since this is not default timestamp format in teradata,so I tried to typecast. 

So far I have tried following things :

where column_name &gt; ${VAL1}(Timestamp(1),Format'YYYY-MM-DDbHH:MI:SS.s(1)') and column_name &lt; ${VAL2}(Timestamp(1),Format'YYYY-MM-DDbHH:MI:SS.s(1)')

where column_name &gt; ${VAL1}(Timestamp(0),Format'YYYY-MM-DDbHH:MI:SS.s(1)') and column_name &lt; ${VAL2}(Timestamp(0),Format'YYYY-MM-DDbHH:MI:SS.s(1)')

where column_name &gt; ${Val1}(Timestamp(1),Format'YYYY-MM-DDbHH:MI:SS.s(1)bt') and column_name &lt; ${Val2}(Timestamp(1),Format'YYYY-MM-DDbHH:MI:SS.s(1)bt')


  But everytime the error is same ""Syntax error,expected something like
  an 'OR' keyword or ')' between an integer and the integer '13'.


I believe the problem is in passing the format to teradata. Any help appreciated.

P.S: VAL1 and VAL2 are bash variables.
",3
392,54937499,Python teradata Uda Exec ODBC connection issue,"I am using (Anaconda 3) Python 3.6.3 and have installed the Teradata's python module from https://pypi.python.org/pypi/teradata

I've also created a ODBC data source on my system and am able to use it to login successfully (using Teradata SQL Assistant) to a Teradata system (on a different server). Driver version is 13.00.00.09

I've written a small test code which is failing with ODBC connection issue:

 import Teradata

 import pandas as pd

 import sys
 print(""attempting TD connection"")
 udaExec = teradata.UdaExec(appName=""just_td_test"",       version=""1.0"", logConsole=False)
 #
 with udaExec.connect(method=""odbc"",system=""abc"", username=""aaaaa"",password=""xxxxx"", driver=""Teradata"") as connect:
     print(""connection done. querying now..."")
     query = ""select top 10 tablename from dbc.tables;""
     df = pd.read_sql(query,connect)
     print(df.head())
 connect.close()    


Error messages:

attempting TD connection
Traceback (most recent call last):
File ""td.py"", line 16, in &lt;module&gt;
with udaExec.connect(method=""odbc"",system=""abc"", username=""aaaaa"",password=""xxxxx"", driver=""Teradata"") as connect:
File ""C:\teradata-15.10.0.21.tar\teradata-15.10.0.21\teradata\udaexec.py"", line 183, in connect
**args))
File ""C:\teradata-15.10.0.21.tar\teradata-15.10.0.21\teradata\tdodbc.py"", line 450, in __init__
SQL_NTS, None, 0, None, 0)
OSError: exception: access violation writing 0x0000000000000078
Exception ignored in: &lt;bound method OdbcConnection.__del__ of OdbcConnection(sessionno=0)&gt;
Traceback (most recent call last):
File ""C:\teradata-15.10.0.21.tar\teradata-15.10.0.21\teradata\tdodbc.py"", line 538, in __del__
self.close()
File ""C:\teradata-15.10.0.21.tar\teradata-15.10.0.21\teradata\tdodbc.py"", line 513, in close
 connections.remove(self)
 ValueError: list.remove(x): x not in list


Its clear that error is with ODBC connection but the error message is not clear. Teradata ODBC driver version is 13 while pypi provides the Teradata python module of version 15. Is this the reason for the error ? 
",-1,-1,-1.0,"I am using (Anaconda 3) Python 3.6.3 and have installed the Teradata's python module from https://pypi.python.org/pypi/teradata

I've also created a ODBC data source on my system and am able to use it to login successfully (using Teradata SQL Assistant) to a Teradata system (on a different server). Driver version is 13.00.00.09

I've written a small test code which is failing with ODBC connection issue:

 import Teradata

 import pandas as pd

 import sys
 print(""attempting TD connection"")
 udaExec = teradata.UdaExec(appName=""just_td_test"",       version=""1.0"", logConsole=False)
 #
 with udaExec.connect(method=""odbc"",system=""abc"", username=""aaaaa"",password=""xxxxx"", driver=""Teradata"") as connect:
     print(""connection done. querying now..."")
     query = ""select top 10 tablename from dbc.tables;""
     df = pd.read_sql(query,connect)
     print(df.head())
 connect.close()    


Error messages:

attempting TD connection
Traceback (most recent call last):
File ""td.py"", line 16, in &lt;module&gt;
with udaExec.connect(method=""odbc"",system=""abc"", username=""aaaaa"",password=""xxxxx"", driver=""Teradata"") as connect:
File ""C:\teradata-15.10.0.21.tar\teradata-15.10.0.21\teradata\udaexec.py"", line 183, in connect
**args))
File ""C:\teradata-15.10.0.21.tar\teradata-15.10.0.21\teradata\tdodbc.py"", line 450, in __init__
SQL_NTS, None, 0, None, 0)
OSError: exception: access violation writing 0x0000000000000078
Exception ignored in: &lt;bound method OdbcConnection.__del__ of OdbcConnection(sessionno=0)&gt;
Traceback (most recent call last):
File ""C:\teradata-15.10.0.21.tar\teradata-15.10.0.21\teradata\tdodbc.py"", line 538, in __del__
self.close()
File ""C:\teradata-15.10.0.21.tar\teradata-15.10.0.21\teradata\tdodbc.py"", line 513, in close
 connections.remove(self)
 ValueError: list.remove(x): x not in list


Its clear that error is with ODBC connection but the error message is not clear. Teradata ODBC driver version is 13 while pypi provides the Teradata python module of version 15. Is this the reason for the error ? 
",1
393,55269816,Efficiently find last date in a table - Teradata SQL,"Say I have a rather large table in a Teradata database, ""Sales"" that has a daily record for every sale and I want to write a SQL statement that limits this to the latest date only. This will not always be the previous day, for example, if it was a Monday the latest date would be the previous Friday. 

I know I can get the results by the following:

SELECT s.*
FROM Sales s
JOIN (
      SELECT MAX(SalesDate) as SalesDate 
      FROM Sales
) sd 
ON s.SalesDate=sd.SalesDt


I am not knowledgable on how it would process the subquery and since Sales is a large table would there be a more efficient way to do this given there is not another table I could use?
",1,1,-1.0,"Say I have a rather large table in a Teradata database, ""Sales"" that has a daily record for every sale and I want to write a SQL statement that limits this to the latest date only. This will not always be the previous day, for example, if it was a Monday the latest date would be the previous Friday. 

I know I can get the results by the following:

SELECT s.*
FROM Sales s
JOIN (
      SELECT MAX(SalesDate) as SalesDate 
      FROM Sales
) sd 
ON s.SalesDate=sd.SalesDt


I am not knowledgable on how it would process the subquery and since Sales is a large table would there be a more efficient way to do this given there is not another table I could use?
",3
394,55291089,How to print the results of a teradata SHOW statement to a text file so that DDL is executable,"I would like to write a Teradata bteq script that exports all the DDL for my tables by capturing the output of a show command.  I thought this would be as simple as exporting the results from a show command executed from BTEQ to a file, and that approach worked (see script below) fine on tables without column compression.  

The issue is that when there are columns with a large number of compressed values the output wraps and sometimes breaks the line in the middle of compressed values list (e.g. for 'BUSINESS' you get 'BUS and then next line starts with INESS'.  The fact that the compressed values word wrap means that I can't run the exported DDL.  

At first I thought that setting the width to a super large number would prevent the word wrapping, but I can't set the width to a large enough value to handle how wide the compressed values string is.  This means that my only option is to run a show statement in Teradata SQL Assistant and copy and paste that one at a time to create my create table DDL script.  I have to think there is a better way to capture the existing DDL in a manner where the exported DDL is executable?

#!/bin/ksh
# -------------------------------------------------------------------------
# Environment Variables
LDAP_IND="".logmech LDAP""
TD_UID=uXXXXX
TD_PWD='pwXXXX'
TD_TDPID=TD1
OUT_SCRIPT=./DDL.txt
echo ""-----------------------------------""
echo ""TD_UID =&lt;${TD_UID}&gt;""
echo ""-----------------------------------""

###########################################################################
bteq&lt;&lt;EOBTQ
${LDAP_IND}
.logon ${TD_TDPID}/${TD_UID},${TD_PWD}
.IF ERRORCODE&gt;0 THEN .GOTO ABEND
--#  START  SQL ###########################################################

.set width 999
.export report file=${OUT_SCRIPT}
.set defaults
.set format off
.set foldline on 1,2,3,4
.set sidetitles off
--  IS WIDTH MAX ENOUGH TO ALWAYS ACCOMODATE ALL COMPRESSED VALUES?
.set width 65531

-- Export DDL for Tables Using Show Commands
show sel * from DB_PRODUCTS.t_product_ref;

show sel * from DB_PRODUCTS.t_acct;

--#  FINISH SQL ###########################################################
.LABEL GOODEND;
.QUIT;
.LABEL ABEND;
.QUIT ERRORCODE;
EOBTQ

",-1,-1,-1.0,"I would like to write a Teradata bteq script that exports all the DDL for my tables by capturing the output of a show command.  I thought this would be as simple as exporting the results from a show command executed from BTEQ to a file, and that approach worked (see script below) fine on tables without column compression.  

The issue is that when there are columns with a large number of compressed values the output wraps and sometimes breaks the line in the middle of compressed values list (e.g. for 'BUSINESS' you get 'BUS and then next line starts with INESS'.  The fact that the compressed values word wrap means that I can't run the exported DDL.  

At first I thought that setting the width to a super large number would prevent the word wrapping, but I can't set the width to a large enough value to handle how wide the compressed values string is.  This means that my only option is to run a show statement in Teradata SQL Assistant and copy and paste that one at a time to create my create table DDL script.  I have to think there is a better way to capture the existing DDL in a manner where the exported DDL is executable?

#!/bin/ksh
# -------------------------------------------------------------------------
# Environment Variables
LDAP_IND="".logmech LDAP""
TD_UID=uXXXXX
TD_PWD='pwXXXX'
TD_TDPID=TD1
OUT_SCRIPT=./DDL.txt
echo ""-----------------------------------""
echo ""TD_UID =&lt;${TD_UID}&gt;""
echo ""-----------------------------------""

###########################################################################
bteq&lt;&lt;EOBTQ
${LDAP_IND}
.logon ${TD_TDPID}/${TD_UID},${TD_PWD}
.IF ERRORCODE&gt;0 THEN .GOTO ABEND
--#  START  SQL ###########################################################

.set width 999
.export report file=${OUT_SCRIPT}
.set defaults
.set format off
.set foldline on 1,2,3,4
.set sidetitles off
--  IS WIDTH MAX ENOUGH TO ALWAYS ACCOMODATE ALL COMPRESSED VALUES?
.set width 65531

-- Export DDL for Tables Using Show Commands
show sel * from DB_PRODUCTS.t_product_ref;

show sel * from DB_PRODUCTS.t_acct;

--#  FINISH SQL ###########################################################
.LABEL GOODEND;
.QUIT;
.LABEL ABEND;
.QUIT ERRORCODE;
EOBTQ

",3
395,55411725,Unable to establish connection with data source with Teradata. Missing settings: {[DBCName]},"I'am installed Teradata driver on CentOS as described in official README. 
After installation drivers is located in /opt/teradata/client

/ODBC_32/
/ODBC_64/


Each folder has subfolders and files

include
lib
locale
odbc.ini
odbcinst.ini


Using ODBC_64 driver, I'am copied contents from odbc.ini and odbcinst.ini into /etc/odbc.ini and /etc/odbcinst.ini files respectively.

Now in /etc/odbc.ini

[ODBC]
InstallDir=/opt/teradata/client/ODBC_64
Trace=no
Pooling=yes

[ODBC Data Sources]
Teradata_ODBC_DSN=Teradata Database ODBC Driver 16.20

[Teradata_ODBC_DSN]
Description=Teradata Database ODBC Driver 16.20
Driver=/opt/teradata/client/ODBC_64/lib/tdataodbc_sb64.so
DBCName=My_Teradata_Server_IP
UID=
PWD=


in /etc/odbcinst.ini

[ODBC Drivers]
Teradata Database ODBC Driver 16.20=Installed

[Teradata Database ODBC Driver 16.20]
Description=Teradata Database ODBC Driver 16.20
Driver=/opt/teradata/client/ODBC_64/lib/tdataodbc_sb64.so
CPTimeout=60


But, when I'am tested connection with command isql -v Teradata_ODBC_DSN it show me an error [08001][unixODBC][Teradata][ODBC] (10380) Unable to establish connection with data source. Missing settings: {[DBCName]}

DBCName option is filled as you can see above. Why such error is appear?
",-1,1,-1.0,"I'am installed Teradata driver on CentOS as described in official README. 
After installation drivers is located in /opt/teradata/client

/ODBC_32/
/ODBC_64/


Each folder has subfolders and files

include
lib
locale
odbc.ini
odbcinst.ini


Using ODBC_64 driver, I'am copied contents from odbc.ini and odbcinst.ini into /etc/odbc.ini and /etc/odbcinst.ini files respectively.

Now in /etc/odbc.ini

[ODBC]
InstallDir=/opt/teradata/client/ODBC_64
Trace=no
Pooling=yes

[ODBC Data Sources]
Teradata_ODBC_DSN=Teradata Database ODBC Driver 16.20

[Teradata_ODBC_DSN]
Description=Teradata Database ODBC Driver 16.20
Driver=/opt/teradata/client/ODBC_64/lib/tdataodbc_sb64.so
DBCName=My_Teradata_Server_IP
UID=
PWD=


in /etc/odbcinst.ini

[ODBC Drivers]
Teradata Database ODBC Driver 16.20=Installed

[Teradata Database ODBC Driver 16.20]
Description=Teradata Database ODBC Driver 16.20
Driver=/opt/teradata/client/ODBC_64/lib/tdataodbc_sb64.so
CPTimeout=60


But, when I'am tested connection with command isql -v Teradata_ODBC_DSN it show me an error [08001][unixODBC][Teradata][ODBC] (10380) Unable to establish connection with data source. Missing settings: {[DBCName]}

DBCName option is filled as you can see above. Why such error is appear?
",1
396,55505507,Conversion from Teradata to Snowflake SQL,"I am having difficulty in converting a Partition code from Teradata to Snowflake.The Partition code has reset function in it. May i know how to Snowflake

SUM(1) OVER (PARTITION BY acct_id ORDER BY  snap_dt RESET WHEN reset_flag = 1 ROWS UNBOUNDED PRECEDING)

",0,-1,-1.0,"I am having difficulty in converting a Partition code from Teradata to Snowflake.The Partition code has reset function in it. May i know how to Snowflake

SUM(1) OVER (PARTITION BY acct_id ORDER BY  snap_dt RESET WHEN reset_flag = 1 ROWS UNBOUNDED PRECEDING)

",3
397,55634733,Can the pyODBC module drop tables in teradata,"I'm creating a Python program to execute multiple pieces of SQL code against a Teradata box. I'm currently having an issue with the pyODBC code not dropping a table when the table exists. There's a simple if/else statement that governs this, but it doesn't seem to be executing the code at all.

I've tried redoing the cursor pointing at the Teradata box and taking the code out of the if/else statement, but unfortunately, neither of these things seem to have worked. The table still exists.

def saveYourTable(Source, UserName, Password):
    print(""UPLOADING DATA TO TERADATA TABLE"")
    eiwconnection = py.connect('DSN='+CONN+
                               ';UID='+ID+';PWD='+PW)
    eiwcursor = eiwconnection.cursor()

    vSQL = """"""
    Select count(*) from dbc.tables where
    databasename = 'DB' and TableName = 'Table'
    """"""

    Counter = int()
    eiwcursor.execute(vSQL)
    Counter = eiwcursor.fetchall()
    Counter = [int(l[0]) for l in Counter]
    print(Counter)

    if 0 in Counter:
        print(""THE TABLE DOES NOT EXIST - MOVING ON"")
    else:
        print(""DROPPING THE OLD TABLE"")
        vSQL = """"""
        Drop Table DB.Table
        """"""
        eiwcursor.execute(vSQL)


executor = saveYourTable(CONN, ID, PW)


The expectation is that this code, when executed, will delete the table, if it exists on the box. Unfortunately, that does not seem to be happening with this current bit of code. Any help or recommendations would be appreciated.

Thanks!
",-1,-1,-1.0,"I'm creating a Python program to execute multiple pieces of SQL code against a Teradata box. I'm currently having an issue with the pyODBC code not dropping a table when the table exists. There's a simple if/else statement that governs this, but it doesn't seem to be executing the code at all.

I've tried redoing the cursor pointing at the Teradata box and taking the code out of the if/else statement, but unfortunately, neither of these things seem to have worked. The table still exists.

def saveYourTable(Source, UserName, Password):
    print(""UPLOADING DATA TO TERADATA TABLE"")
    eiwconnection = py.connect('DSN='+CONN+
                               ';UID='+ID+';PWD='+PW)
    eiwcursor = eiwconnection.cursor()

    vSQL = """"""
    Select count(*) from dbc.tables where
    databasename = 'DB' and TableName = 'Table'
    """"""

    Counter = int()
    eiwcursor.execute(vSQL)
    Counter = eiwcursor.fetchall()
    Counter = [int(l[0]) for l in Counter]
    print(Counter)

    if 0 in Counter:
        print(""THE TABLE DOES NOT EXIST - MOVING ON"")
    else:
        print(""DROPPING THE OLD TABLE"")
        vSQL = """"""
        Drop Table DB.Table
        """"""
        eiwcursor.execute(vSQL)


executor = saveYourTable(CONN, ID, PW)


The expectation is that this code, when executed, will delete the table, if it exists on the box. Unfortunately, that does not seem to be happening with this current bit of code. Any help or recommendations would be appreciated.

Thanks!
",3
398,55706206,Answer Query in Teradata,"Query: 2. List the top 5 shoppers (number) who spent the most on Martin Luther King Jr. Day in 2000. (Hint: Use SELECT TOP 5….)

SELECT TOP 5 Member_Dimension.Member_Key AS ""Top 5 Members"", Total_Scan_Amount As ""Total Scan Amount""
FROM Item_Scan_Fact, Member_Dimension, Date_Dimension
WHERE Item_Scan_Fact.Member_Key = Member_Dimension.Member_Key
  AND Item_Scan_Fact.Transaction_Date_Key = Date_Dimension.Date_Key
  AND Date_Dimension.Year_Number = 2000
  AND Date_Dimension.Day_Description LIKE 'Martin';


My code doesn't return any values, just an empty table. I think the source of the error is the Day_Description, but I am not sure. I believe this because when I remove the Day_Description = under the WHERE clause, the code does return a table of 5 values. 

This is on teradata under the UA_SAMSCLUB_STAR. Thank you so much!
",-1,-1,-1.0,"Query: 2. List the top 5 shoppers (number) who spent the most on Martin Luther King Jr. Day in 2000. (Hint: Use SELECT TOP 5….)

SELECT TOP 5 Member_Dimension.Member_Key AS ""Top 5 Members"", Total_Scan_Amount As ""Total Scan Amount""
FROM Item_Scan_Fact, Member_Dimension, Date_Dimension
WHERE Item_Scan_Fact.Member_Key = Member_Dimension.Member_Key
  AND Item_Scan_Fact.Transaction_Date_Key = Date_Dimension.Date_Key
  AND Date_Dimension.Year_Number = 2000
  AND Date_Dimension.Day_Description LIKE 'Martin';


My code doesn't return any values, just an empty table. I think the source of the error is the Day_Description, but I am not sure. I believe this because when I remove the Day_Description = under the WHERE clause, the code does return a table of 5 values. 

This is on teradata under the UA_SAMSCLUB_STAR. Thank you so much!
",3
399,55730676,How does this teradata timestamp with timezone example make sense?,"In the teradata documentation it says:

&quot;Suppose an installation is in the PST time zone and it is New Years Eve, 1998-12-31 20:30 local time.
The system TIMESTAMP WITH TIME ZONE for the indicated time is ' 1999-01-01 04:30-08:00 ' internally.&quot;

This does not mesh with my understanding. I figure it ought to be '1999-01-01 04:30+00:00' internally because it should be stored in UTC.
Or, it can be stored as a the local time with a -8 offset, but this example seems to mix the two. Perhaps I am misunderstanding the text?
",-1,-1,-1.0,"In the teradata documentation it says:

&quot;Suppose an installation is in the PST time zone and it is New Years Eve, 1998-12-31 20:30 local time.
The system TIMESTAMP WITH TIME ZONE for the indicated time is ' 1999-01-01 04:30-08:00 ' internally.&quot;

This does not mesh with my understanding. I figure it ought to be '1999-01-01 04:30+00:00' internally because it should be stored in UTC.
Or, it can be stored as a the local time with a -8 offset, but this example seems to mix the two. Perhaps I am misunderstanding the text?
",3
400,55872430,"Why does this error occur in abinitio while loading to Teradata? ""Input port closed while active (unread data remains)""","I am loading in Ab Initio from input file to Teradata table via PT_api_utility using Output Table component. It has been running fine since long but today it threw error: 

Error from Component 'TARGET_TABLE__table_.tload', Partition 0
[B1]

Internal error: Input port ""in"" closed while active (unread data remains).
  Flows into this port: Flow_2 (flow from Inter Load File to TARGET_TABLE).000

After analyzing we found around 80% of the records were inserted into table, but rest 20% were not loaded, rejected to reject file and gave above error. 
Every time we run this again manually via another test graph from start or in same graph from checkpoint, we still get same error at the same number of records: 

Internal error: Input port ""in"" closed while active (unread data remains).

What is going wrong in this process and how to correct it?
",-1,-1,-1.0,"I am loading in Ab Initio from input file to Teradata table via PT_api_utility using Output Table component. It has been running fine since long but today it threw error: 

Error from Component 'TARGET_TABLE__table_.tload', Partition 0
[B1]

Internal error: Input port ""in"" closed while active (unread data remains).
  Flows into this port: Flow_2 (flow from Inter Load File to TARGET_TABLE).000

After analyzing we found around 80% of the records were inserted into table, but rest 20% were not loaded, rejected to reject file and gave above error. 
Every time we run this again manually via another test graph from start or in same graph from checkpoint, we still get same error at the same number of records: 

Internal error: Input port ""in"" closed while active (unread data remains).

What is going wrong in this process and how to correct it?
",3
401,55908800,Pull 1 column from Teradata LDAP and store in array/table to use in script,"My PowerShell skills are weak/rusty.  Seeking a little help.

Overall, I want to do the following:


Pull a static list of Active Directory group names from Teradata using LDAP.

SELECT GroupName FROM DB1.AdGroupList

Store those entries in a table in memory to reference.
Use Get-ADGroupMember to get members of each of those groups.
Put that group name and AD data into a 2nd Teradata table.

""Insert into DB1.AdGroupMembers
(GroupName, Name, GivenName, SurName, SamAccountName)""



The first problem is getting the Teradata data from LDAP and storing it.
There is very little PowerShell help for Teradata.
",1,-1,-1.0,"My PowerShell skills are weak/rusty.  Seeking a little help.

Overall, I want to do the following:


Pull a static list of Active Directory group names from Teradata using LDAP.

SELECT GroupName FROM DB1.AdGroupList

Store those entries in a table in memory to reference.
Use Get-ADGroupMember to get members of each of those groups.
Put that group name and AD data into a 2nd Teradata table.

""Insert into DB1.AdGroupMembers
(GroupName, Name, GivenName, SurName, SamAccountName)""



The first problem is getting the Teradata data from LDAP and storing it.
There is very little PowerShell help for Teradata.
",3
402,55928924,Converting Teradata timestamp to PDT,"for converting the timestamp to PST we use a query like 

select start_ts AT 'America Pacific' from table_name;


How to specify the time zone for PDT. I was not able to find it in teradata docs
",-1,-1,-1.0,"for converting the timestamp to PST we use a query like 

select start_ts AT 'America Pacific' from table_name;


How to specify the time zone for PDT. I was not able to find it in teradata docs
",3
403,56026489,Expression.Error when dynamically passing in Teradata SQL query (with column aliases) to ODBC query,"I have a macro that prompts me for a SQL query (unless it was called by another Sub, in which case it uses the argument that was passed into its optional string parameter as the query) and then executes the query against my Teradata SQL database. 

It works fine, unless there's a column alias containing a space in the query.

Example query:
SELECT 2 + 2 AS ""Query Result""; 

Error: 

Run-time error '1004':

[Expression.Error] The name 'Source' wasn't recognized. Make sure it's spelled correctly.


The line of code which I believe is the culprit is as follows (my apologies for the readability-- I recorded the macro, modified it just enough to get it to work somewhat dynamically and then haven't touched it since).

ActiveWorkbook.Queries.Add Name:=queryName, formula:= _
""let"" &amp; Chr(13) &amp; """" &amp; Chr(10) &amp; ""    Source = Odbc.Query(""""dsn=my-server-name"""", "" &amp; Chr(34) &amp; code &amp; Chr(34) &amp; "")"" &amp; Chr(13) &amp; """" &amp; Chr(10) &amp; ""in"" &amp; Chr(13) &amp; """" &amp; Chr(10) &amp; ""    Source""


I assume it has to do with the fact that the example query above has double quotes for the alias, which are confusing the syntax when trying to be interpolated. Any help would be greatly appreciated!
",1,1,-1.0,"I have a macro that prompts me for a SQL query (unless it was called by another Sub, in which case it uses the argument that was passed into its optional string parameter as the query) and then executes the query against my Teradata SQL database. 

It works fine, unless there's a column alias containing a space in the query.

Example query:
SELECT 2 + 2 AS ""Query Result""; 

Error: 

Run-time error '1004':

[Expression.Error] The name 'Source' wasn't recognized. Make sure it's spelled correctly.


The line of code which I believe is the culprit is as follows (my apologies for the readability-- I recorded the macro, modified it just enough to get it to work somewhat dynamically and then haven't touched it since).

ActiveWorkbook.Queries.Add Name:=queryName, formula:= _
""let"" &amp; Chr(13) &amp; """" &amp; Chr(10) &amp; ""    Source = Odbc.Query(""""dsn=my-server-name"""", "" &amp; Chr(34) &amp; code &amp; Chr(34) &amp; "")"" &amp; Chr(13) &amp; """" &amp; Chr(10) &amp; ""in"" &amp; Chr(13) &amp; """" &amp; Chr(10) &amp; ""    Source""


I assume it has to do with the fact that the example query above has double quotes for the alias, which are confusing the syntax when trying to be interpolated. Any help would be greatly appreciated!
",3
404,56036512,Common Table Expressions (CTE) Inside Teradata Macro?,"I want to insert data into a table using a common table expression.
If I just write my statement like this and execute it inside the teradata sql assistant it works:

INSERT INTO DB.Table(
...,
...,
)
WITH cte AS (
Select 
a,
b,
....,
from
.....
)
SELECT a, b from cte
UNION 
SELECT ... from cte


I've created a macro with the same sql code. When I try to execute the macro, I get the error message: ""EXECUTE Failed. 3706: All expressions in a derived table must have an explicit name"". Are there any restrictions on using common table expressions inside a teradata macro? 
",-1,-1,-1.0,"I want to insert data into a table using a common table expression.
If I just write my statement like this and execute it inside the teradata sql assistant it works:

INSERT INTO DB.Table(
...,
...,
)
WITH cte AS (
Select 
a,
b,
....,
from
.....
)
SELECT a, b from cte
UNION 
SELECT ... from cte


I've created a macro with the same sql code. When I try to execute the macro, I get the error message: ""EXECUTE Failed. 3706: All expressions in a derived table must have an explicit name"". Are there any restrictions on using common table expressions inside a teradata macro? 
",3
405,56066529,Writing a Teradata With Statement in RODBC in R,"I have connected Teradata to my R session with RODBC.

Typically I use data &lt;- sqlQuery(conn, ""SELECT statement"") however when I put the following WITH statement in place of the SELECT statement, there is an error.

data &lt;- sqlQuery(conn, 
""WITH zzz as (SELECT statement1),
yyy as (SELECT statement2)

SELECT statement3""

",-1,1,-1.0,"I have connected Teradata to my R session with RODBC.

Typically I use data &lt;- sqlQuery(conn, ""SELECT statement"") however when I put the following WITH statement in place of the SELECT statement, there is an error.

data &lt;- sqlQuery(conn, 
""WITH zzz as (SELECT statement1),
yyy as (SELECT statement2)

SELECT statement3""

",3
406,56129612,Writing Pandas DataFrame into Teradata database table using Jaydebeapi giving error,"I am trying to write pandas dataframe into Teradata database using Jaydebeapi. I am able to read data from database easily, but facing problem while writing.

Code:

import jaydebeapi
import pandas as pd
case_detail_server ='server_url'
server_name= 'servername'
user ='xyz'
password ='****'
jars= ['/anaconda3/lib/tdgssconfig.jar','/anaconda3/lib/terajdbc4.jar']
#jars= ['/anaconda3/lib/terajdbc4.jar']
jclassname_case_detail='com.teradata.jdbc.TeraDriver'
teradata_url='jdbc:teradata://server_url/DBS_PORT=1025,TMODE=ANSI,CHARSET=UTF8'
conn = jaydebeapi.connect('com.teradata.jdbc.TeraDriver',teradata_url,{'user': user, 'password': password},jars)
curs = conn.cursor()


Method 1:
Pandastable to be written => df_out

for row in df_out.head().iterrows():
    curs.execute(""INSERT INTO TD_table(column1, column2) VALUES(?,?)"", row)


Error 1

Error
---------------------------------------------------------------------------
Error
                                    Traceback (most recent call last)
&lt;ipython-input-390-8067f51cb5e3&gt; in &lt;module&gt;
      1 for row in df_out.head().iterrows():
----&gt; 2     curs.execute(""INSERT INTO CTD_table(column1, column2) VALUES(?,?)"", row)
/anaconda3/lib/python3.6/site-packages/jaydebeapi/__init__.py in execute(self, operation, parameters)
    492     def execute(self, operation, parameters=None):
    493         if self._connection._closed:
--&gt; 494             raise Error()
    495         if not parameters:
    496             parameters = ()
Error: 


Method 2:

resultset=[]
for column1,column2 in df_out:
    try:
        resultset.append((column1,column2))
    except AttributeError:
        pass
curs.executemany(""INSERT INTO CTD_table(column1, column2) VALUES (?,?)"",resultset)


Error 2

ValueError: too many values to unpack (expected 2)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-392-71473191f932&gt; in &lt;module&gt;
      1 resultset=[]
----&gt; 2 for column1,column2 in df_out:
      3     try:
      4         resultset.append((column1,column2))
      5     except AttributeError:
ValueError: too many values to unpack (expected 2)```

",-1,-1,-1.0,"I am trying to write pandas dataframe into Teradata database using Jaydebeapi. I am able to read data from database easily, but facing problem while writing.

Code:

import jaydebeapi
import pandas as pd
case_detail_server ='server_url'
server_name= 'servername'
user ='xyz'
password ='****'
jars= ['/anaconda3/lib/tdgssconfig.jar','/anaconda3/lib/terajdbc4.jar']
#jars= ['/anaconda3/lib/terajdbc4.jar']
jclassname_case_detail='com.teradata.jdbc.TeraDriver'
teradata_url='jdbc:teradata://server_url/DBS_PORT=1025,TMODE=ANSI,CHARSET=UTF8'
conn = jaydebeapi.connect('com.teradata.jdbc.TeraDriver',teradata_url,{'user': user, 'password': password},jars)
curs = conn.cursor()


Method 1:
Pandastable to be written => df_out

for row in df_out.head().iterrows():
    curs.execute(""INSERT INTO TD_table(column1, column2) VALUES(?,?)"", row)


Error 1

Error
---------------------------------------------------------------------------
Error
                                    Traceback (most recent call last)
&lt;ipython-input-390-8067f51cb5e3&gt; in &lt;module&gt;
      1 for row in df_out.head().iterrows():
----&gt; 2     curs.execute(""INSERT INTO CTD_table(column1, column2) VALUES(?,?)"", row)
/anaconda3/lib/python3.6/site-packages/jaydebeapi/__init__.py in execute(self, operation, parameters)
    492     def execute(self, operation, parameters=None):
    493         if self._connection._closed:
--&gt; 494             raise Error()
    495         if not parameters:
    496             parameters = ()
Error: 


Method 2:

resultset=[]
for column1,column2 in df_out:
    try:
        resultset.append((column1,column2))
    except AttributeError:
        pass
curs.executemany(""INSERT INTO CTD_table(column1, column2) VALUES (?,?)"",resultset)


Error 2

ValueError: too many values to unpack (expected 2)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-392-71473191f932&gt; in &lt;module&gt;
      1 resultset=[]
----&gt; 2 for column1,column2 in df_out:
      3     try:
      4         resultset.append((column1,column2))
      5     except AttributeError:
ValueError: too many values to unpack (expected 2)```

",1
407,56155455,How to compute a rolling average by rank (not row) in Teradata?,"I have the following table in Teradata:

 ranked | data_val 
-------- ----------
 1      | 100
 2      |  30
 2      |  20
 2      |  70


I want the following table, where avg_val is the rolling average of data_val values ordered by increasingly ascending ranked values:

 ranked | avg_val 
------ ---------
 1      | 100
 2      |  55




I try using:

SELECT 
    ranked
  , AVERAGE(data)val) OVER (
      PARTITION BY NULL 
      ORDER BY ranked ASC
      ROWS UNBOUNDED PRECEDING
      ) AS avg_val
  FROM tbl
;


but I get this:

 ranked | avg_val 
------ ---------
 1      | 100
 2      |  65
 2      |  50
 2      |  55


which is not what I want.

How do I return the desired output?
",-1,-1,-1.0,"I have the following table in Teradata:

 ranked | data_val 
-------- ----------
 1      | 100
 2      |  30
 2      |  20
 2      |  70


I want the following table, where avg_val is the rolling average of data_val values ordered by increasingly ascending ranked values:

 ranked | avg_val 
------ ---------
 1      | 100
 2      |  55




I try using:

SELECT 
    ranked
  , AVERAGE(data)val) OVER (
      PARTITION BY NULL 
      ORDER BY ranked ASC
      ROWS UNBOUNDED PRECEDING
      ) AS avg_val
  FROM tbl
;


but I get this:

 ranked | avg_val 
------ ---------
 1      | 100
 2      |  65
 2      |  50
 2      |  55


which is not what I want.

How do I return the desired output?
",3
408,56178003,Teradata stored procedure with python,"I'm trying to call a Teradata Stored procedure in python. But it is giving the following error. 

    cursor = session.cursor()
    cursor.callproc(""usr.RESULTSET"", (teradata.InOutParam(""today""),
                                     teradata.OutParam(""p2"")))
    output = cursor.fetchone()
    print(output)


Error

raise DatabaseError(i[2], u""[{}] {}"".format(i[0], msg), i[0])
teradata.api.DatabaseError: (6, '[HY000] [Teradata][ODBC Teradata Driver] (6) Internal Error (Exception).')
INFO:teradata.udaexec: UdaExec exiting. (2019-05-17 10:02:13,350; udaexec.py:68)

",-1,-1,-1.0,"I'm trying to call a Teradata Stored procedure in python. But it is giving the following error. 

    cursor = session.cursor()
    cursor.callproc(""usr.RESULTSET"", (teradata.InOutParam(""today""),
                                     teradata.OutParam(""p2"")))
    output = cursor.fetchone()
    print(output)


Error

raise DatabaseError(i[2], u""[{}] {}"".format(i[0], msg), i[0])
teradata.api.DatabaseError: (6, '[HY000] [Teradata][ODBC Teradata Driver] (6) Internal Error (Exception).')
INFO:teradata.udaexec: UdaExec exiting. (2019-05-17 10:02:13,350; udaexec.py:68)

",1
409,56281073,Python teradata Connection exception - UdaExec attribute,"having a issue connecting to teradata from python 

installed teradata module that was missing earlier.

import teradata
import pandas as pd
import pyodbc


udaExec = teradata.UdaExec (appName=""test"", version=""1.0"",logConsole=False)
with udaExec.connect(method=""odbc"",system=""xxxxxx.abc.com"", username=""xxxxx"",password=""xxxxxxx"", driver=""DRIVERNAME"") as connect:
        query=""SELECT METHOD_NM,EVENT,CHARACTERISTIC from R_VIEWS.EVENT_HIST WHERE EXCEPTION_EVENT_TS  = Date -1 AND exception_nm IN ('ABFSDC');""

        df = pd.read_sql(query,connect)
        print(df.head())



 udaExec = teradata.UdaExec (appName=""test"", version=""1.0"", logConsole=False)


AttributeError: module 'teradata' has no attribute 'UdaExec'
",-1,-1,-1.0,"having a issue connecting to teradata from python 

installed teradata module that was missing earlier.

import teradata
import pandas as pd
import pyodbc


udaExec = teradata.UdaExec (appName=""test"", version=""1.0"",logConsole=False)
with udaExec.connect(method=""odbc"",system=""xxxxxx.abc.com"", username=""xxxxx"",password=""xxxxxxx"", driver=""DRIVERNAME"") as connect:
        query=""SELECT METHOD_NM,EVENT,CHARACTERISTIC from R_VIEWS.EVENT_HIST WHERE EXCEPTION_EVENT_TS  = Date -1 AND exception_nm IN ('ABFSDC');""

        df = pd.read_sql(query,connect)
        print(df.head())



 udaExec = teradata.UdaExec (appName=""test"", version=""1.0"", logConsole=False)


AttributeError: module 'teradata' has no attribute 'UdaExec'
",1
410,56356294,Partition a table in teradata using SAS DI studio 4.902,"For ETL operations we use SAS DI studio and then finally the tables are loaded in Teradata. DDL is dynamically generated in SAS DI Studio for the tables created. But when we want to customize the DDL to include partitions it throws an error. Can anyone suggest a workaround?
Note: We cannot create the DDL in Teradata first and then register the table in SAS DI Studio to be using it.

In the table properties->Options->advanced and write custom SQL in create table option it works fine for UNIQUE PRIMARY INDEX(NOTI_DT)
But when we try 

UNIQUE PRIMARY INDEX(NOTI_DT)
PARTITION BY RANGE_N(NOTI__DT BETWEEN DATE'1950-01-01'AND DATE'2022-12-31' EACH INTERVAL '1' MONTH)


it throws an error:

",-1,-1,-1.0,"For ETL operations we use SAS DI studio and then finally the tables are loaded in Teradata. DDL is dynamically generated in SAS DI Studio for the tables created. But when we want to customize the DDL to include partitions it throws an error. Can anyone suggest a workaround?
Note: We cannot create the DDL in Teradata first and then register the table in SAS DI Studio to be using it.

In the table properties->Options->advanced and write custom SQL in create table option it works fine for UNIQUE PRIMARY INDEX(NOTI_DT)
But when we try 

UNIQUE PRIMARY INDEX(NOTI_DT)
PARTITION BY RANGE_N(NOTI__DT BETWEEN DATE'1950-01-01'AND DATE'2022-12-31' EACH INTERVAL '1' MONTH)


it throws an error:

",3
411,56544771,Connect to teradata using DBI Package in R,"Can you anyone help me with how to connect to Teradata using DBI ODBC Package?

I use the code below, 

  con &lt;- dbConnect( drv = dbDriver('Teradata'),
                   server=prodServer,
                   DBCName=prodDatabaseName,
                   uid=username,
                   pwd=password,
                   MechanismName = TD2)


but it throws the following error:

Error: Couldn't find driver Teradata. Looked in:
* global namespace
* in package called Teradata
* in package called RTeradata

",-1,-1,-1.0,"Can you anyone help me with how to connect to Teradata using DBI ODBC Package?

I use the code below, 

  con &lt;- dbConnect( drv = dbDriver('Teradata'),
                   server=prodServer,
                   DBCName=prodDatabaseName,
                   uid=username,
                   pwd=password,
                   MechanismName = TD2)


but it throws the following error:

Error: Couldn't find driver Teradata. Looked in:
* global namespace
* in package called Teradata
* in package called RTeradata

",1
412,56568911,How to connect to Teradata Server using sqlalchemy and DSN less connection,"I can't figure out the right ODBC string I need to pass to the create engine statement.
This works
import pyodbc
import pandas as pd

cnxn=pyodbc.connect('DRIVER=/opt/teradata/client/ODBC_64/lib/tdata.so;DBCName=Server;UID=UN;PWD=PW;Database=myDB')

query = &quot;select top 10 * from TABLE&quot;
df = pd.read_sql(query,cnxn) 

This does not work
import urllib
import sqlalchemy
params = urllib.parse.quote_plus('DRIVER=/opt/teradata/client/ODBC_64/lib/tdata.so;DBCName=Server;UID=UN;PWD=PW;Database=myDB')

engine = sqlalchemy.create_engine(&quot;mssql+pyodbc:///?odbc_connect=%s&quot; % params)

query = &quot;select top 10 * from TABLE&quot;

df = pd.read_sql_query(query, engine)

I can get the pyodbc connection to work but not the sqlalchemy connection. Any help would be appreciated.
I get this error:
InterfaceError: (pyodbc.InterfaceError) ('IM001', '[IM001] [unixODBC][Driver Manager]Driver does not support this function (0) (SQLGetInfo)')

",1,-1,-1.0,"I can't figure out the right ODBC string I need to pass to the create engine statement.
This works
import pyodbc
import pandas as pd

cnxn=pyodbc.connect('DRIVER=/opt/teradata/client/ODBC_64/lib/tdata.so;DBCName=Server;UID=UN;PWD=PW;Database=myDB')

query = &quot;select top 10 * from TABLE&quot;
df = pd.read_sql(query,cnxn) 

This does not work
import urllib
import sqlalchemy
params = urllib.parse.quote_plus('DRIVER=/opt/teradata/client/ODBC_64/lib/tdata.so;DBCName=Server;UID=UN;PWD=PW;Database=myDB')

engine = sqlalchemy.create_engine(&quot;mssql+pyodbc:///?odbc_connect=%s&quot; % params)

query = &quot;select top 10 * from TABLE&quot;

df = pd.read_sql_query(query, engine)

I can get the pyodbc connection to work but not the sqlalchemy connection. Any help would be appreciated.
I get this error:
InterfaceError: (pyodbc.InterfaceError) ('IM001', '[IM001] [unixODBC][Driver Manager]Driver does not support this function (0) (SQLGetInfo)')

",1
413,56656278,Teradata - Case statement in Where clause?,"This should be simple, but the solution keeps eluding me.
Teradata 16.20.32.10.
Currently in a Teradata Macro.  Macro will eventually be used in an SSRS report, where they can only choose Y or N.

Macro has a parameter: Paramater1

Date is in TableX:
    Datefield
    ID
    Q1 (Y/N)
    Q2 (Y/N)

If Parameter1 = Y
    Then Q1 must = Y
         Q2 must = Y

If Parameter1 = N
    Then everything


My attempt (which errors):

SELECT TOP 10
 DateField
,ID 
,Q1
,Q2
FROM TABLEX
WHERE DateField = DATE  /*Today*/
AND CASE 
     WHEN Parameter1 = 'Y'
       THEN Q1 = 'Y'
           ,Q2 = 'Y'
ELSE Q1 IN ('Y','N')
     Q1 IN ('Y','N')

",-1,-1,-1.0,"This should be simple, but the solution keeps eluding me.
Teradata 16.20.32.10.
Currently in a Teradata Macro.  Macro will eventually be used in an SSRS report, where they can only choose Y or N.

Macro has a parameter: Paramater1

Date is in TableX:
    Datefield
    ID
    Q1 (Y/N)
    Q2 (Y/N)

If Parameter1 = Y
    Then Q1 must = Y
         Q2 must = Y

If Parameter1 = N
    Then everything


My attempt (which errors):

SELECT TOP 10
 DateField
,ID 
,Q1
,Q2
FROM TABLEX
WHERE DateField = DATE  /*Today*/
AND CASE 
     WHEN Parameter1 = 'Y'
       THEN Q1 = 'Y'
           ,Q2 = 'Y'
ELSE Q1 IN ('Y','N')
     Q1 IN ('Y','N')

",3
414,57566094,QVCI feature is disabled' ERROR when using existing Teradata Database to define models for use by flask app,"In attempting to build my flask application around an existing Teradata Database, I have run into an issue defining the db models to be used by my application from this existing database. I am getting sqlalchemy.exc.DatabaseError: (teradata.api.DatabaseError) (9719, '[HY000] [Teradata][ODBC Teradata Driver]Teradata DatabaseQVCI feature is disabled.'). I am using teradata version 16.20 and it appears that the QVCI feature has been disabled in this newest version.

I have been able to successfully make a connection to a Teradata Database using both sqlalchemy and pyodbc through my flask application. So the problem is not in making the initial connection. Where I am struggling is to build my flask application around the existing database that I have already been able to connect to. I have some example code I was able to find based on another question that I was expecting would take my 'users_table' and make it usable as the Class 'Users' to my flask application. However, when trying to have sqlalchemy retrieve the 'users_table', teradata tells me that the QVCI feature has been disabled. Here is the code I was able to find that resulted in the error:

#!/usr/bin/python
# -*- mode: python -*-

from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base

user = 'user_string'
pasw='password_string'
host = 'host_string'

engine = create_engine('teradata://'+ user +':' + pasw + '@'+ host + '/' + '?authentication=LDAP', convert_unicode=True, echo=False)

Base = declarative_base()
Base.metadata.reflect(engine)


from sqlalchemy.orm import relationship, bckref

class Users(Base):
    __table__ = Base.metadata.tables['users_table']


if __name__ == '__main__':
    from sqlalchemy.orm import scoped_session, sessionmaker, Query
    db_session = scoped_session(sessionmaker(bind=engine))
    for item in db_session.query(Users.id, Users.name):
        print(item)


After running, I get the sqlalchemy.exc.DatabaseError: (teradata.api.DatabaseError) (9719, '[HY000] [Teradata][ODBC Teradata Driver]Teradata DatabaseQVCI feature is disabled.'). Thanks for your help in advance.
",-1,-1,-1.0,"In attempting to build my flask application around an existing Teradata Database, I have run into an issue defining the db models to be used by my application from this existing database. I am getting sqlalchemy.exc.DatabaseError: (teradata.api.DatabaseError) (9719, '[HY000] [Teradata][ODBC Teradata Driver]Teradata DatabaseQVCI feature is disabled.'). I am using teradata version 16.20 and it appears that the QVCI feature has been disabled in this newest version.

I have been able to successfully make a connection to a Teradata Database using both sqlalchemy and pyodbc through my flask application. So the problem is not in making the initial connection. Where I am struggling is to build my flask application around the existing database that I have already been able to connect to. I have some example code I was able to find based on another question that I was expecting would take my 'users_table' and make it usable as the Class 'Users' to my flask application. However, when trying to have sqlalchemy retrieve the 'users_table', teradata tells me that the QVCI feature has been disabled. Here is the code I was able to find that resulted in the error:

#!/usr/bin/python
# -*- mode: python -*-

from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base

user = 'user_string'
pasw='password_string'
host = 'host_string'

engine = create_engine('teradata://'+ user +':' + pasw + '@'+ host + '/' + '?authentication=LDAP', convert_unicode=True, echo=False)

Base = declarative_base()
Base.metadata.reflect(engine)


from sqlalchemy.orm import relationship, bckref

class Users(Base):
    __table__ = Base.metadata.tables['users_table']


if __name__ == '__main__':
    from sqlalchemy.orm import scoped_session, sessionmaker, Query
    db_session = scoped_session(sessionmaker(bind=engine))
    for item in db_session.query(Users.id, Users.name):
        print(item)


After running, I get the sqlalchemy.exc.DatabaseError: (teradata.api.DatabaseError) (9719, '[HY000] [Teradata][ODBC Teradata Driver]Teradata DatabaseQVCI feature is disabled.'). Thanks for your help in advance.
",1
415,57597705,How to troubleshoot Django web app connection to Teradata?,"Can anyone please help me to troubleshoot this? A new Unix server is setup as test environment for an existing production django web app/server. On the test server I can pull data using bteq, but the web app doesn't (i.e. the json data doesn't populate. Everything else looks fine). All the configuration/settings are the same as the production server.  

I tried below steps in the production server and it returned with data, but in the test server I got the error.

python manage.py shell
&gt;&gt;&gt; from project_web.shared_functions import teradata_con
&gt;&gt;&gt; import json
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; sql = """"""select distinct rgn_num, rgn_nm from RLDMPROD.SITE_HIER;""""""
&gt;&gt;&gt; pd.read_sql(sql, teradata_con())


Error: python: libltdl/ltdl.c:1197: try_dlopen: Assertion `filename &amp;&amp; *filename' failed.
Aborted (core dumped)
",1,-1,-1.0,"Can anyone please help me to troubleshoot this? A new Unix server is setup as test environment for an existing production django web app/server. On the test server I can pull data using bteq, but the web app doesn't (i.e. the json data doesn't populate. Everything else looks fine). All the configuration/settings are the same as the production server.  

I tried below steps in the production server and it returned with data, but in the test server I got the error.

python manage.py shell
&gt;&gt;&gt; from project_web.shared_functions import teradata_con
&gt;&gt;&gt; import json
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; sql = """"""select distinct rgn_num, rgn_nm from RLDMPROD.SITE_HIER;""""""
&gt;&gt;&gt; pd.read_sql(sql, teradata_con())


Error: python: libltdl/ltdl.c:1197: try_dlopen: Assertion `filename &amp;&amp; *filename' failed.
Aborted (core dumped)
",1
416,57623193,Df to sql to Teradata in python,"I'm trying to load a csv file into a Teradata table with the df.to_sql method.
So far with Teradata python modules i was able to connect, but i can't manage to load my csv file. 

Here is my code :

import teradata
import pandas as pd
global udaExec
global session
global host
global username
global password

def Connexion_Teradata(usernames,passwords):

   host= 'FTGPRDTD'
   udaExec = teradata.UdaExec (appName=""TEST"", version=""1.0"", logConsole=False)
   session=udaExec.connect(method=""odbc"",system=host, username=usernames,password=passwords, driver=""Teradata"")
   print('connection ok')

   df = pd.read_csv(r'C:/Users/c92434/Desktop/Load.csv')
   print('chargement df ok')
   df.to_sql(name = 'DB_FTG_SRS_DATALAB.mdc_load', con = session, if_exists=""replace"", index =""False"" )
   print ('done')


Connexion_Teradata (""******"",""****"")


When I play my script all I got is: 


  DatabaseError: Execution failed on sql 'SELECT name FROM sqlite_master WHERE type='table' AND name=?;': (3707, ""[42000] [Teradata][ODBC Teradata Driver][Teradata Database] Syntax error, expected something like '(' between the 'type' keyword and '='. "")


What can I do?
",-1,-1,-1.0,"I'm trying to load a csv file into a Teradata table with the df.to_sql method.
So far with Teradata python modules i was able to connect, but i can't manage to load my csv file. 

Here is my code :

import teradata
import pandas as pd
global udaExec
global session
global host
global username
global password

def Connexion_Teradata(usernames,passwords):

   host= 'FTGPRDTD'
   udaExec = teradata.UdaExec (appName=""TEST"", version=""1.0"", logConsole=False)
   session=udaExec.connect(method=""odbc"",system=host, username=usernames,password=passwords, driver=""Teradata"")
   print('connection ok')

   df = pd.read_csv(r'C:/Users/c92434/Desktop/Load.csv')
   print('chargement df ok')
   df.to_sql(name = 'DB_FTG_SRS_DATALAB.mdc_load', con = session, if_exists=""replace"", index =""False"" )
   print ('done')


Connexion_Teradata (""******"",""****"")


When I play my script all I got is: 


  DatabaseError: Execution failed on sql 'SELECT name FROM sqlite_master WHERE type='table' AND name=?;': (3707, ""[42000] [Teradata][ODBC Teradata Driver][Teradata Database] Syntax error, expected something like '(' between the 'type' keyword and '='. "")


What can I do?
",1
417,57774138,How can I map subclasses of the sqlalchemy declarative_base() class to their tables in a Teradata Database?,"I have a flask application that relies on an existing Teradata Database to serve up information to and accept input from its users. I am able to successfully make the connection between the application and the Teradata Database, however, I am not able to then define classes that will represent tables already existing in my database.

Currently, I am defining a 'Base' class using sqlalchemy that represents the connection to my database. There is no problem here and I am even able to execute queries using the connection used to build the 'Base' class. However, my problem is in using this 'Base' class to create a subclass 'Users' for my teradata table 'users'. My understanding is that sqlalchemy should allow for me to define a subclass of the superclass 'Base' which will  inherit the metadata from the underlying teradata table that the subclass represents - in this case, my 'users' table. Here is the code I have so far:

import getpass
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.schema import MetaData

user = 'user_id_string'
pasw=getpass.getpass()
host = 'host_string'
db_name = 'db_name'

engine = create_engine(f'{host}?user={user}&amp;password={pasw}&amp;logmech=LDAP') 
connection = engine.connect()
connection.execute(f'DATABASE {db_name}')

md = MetaData(bind=connection, reflect=False, schema='db_name')
md.reflect(only=['users'])

Base = declarative_base(bind=connection, metadata=md)

class Users(Base):
    __table__ = md.tables['db_name.users']


This is the error that I receive when constructing the subclass 'Users':

sqlalchemy.exc.ArgumentError: Mapper mapped class Users-&gt;users could not assemble any primary key columns for mapped table 'users'


Is there some reason that my subclass 'Users' is not automatically being mapped to the table metadata from the existing teradata table 'users' that I have assigned it to in defining the class? The underlying table already has a primary key set so I don't understand why sqlalchemy is not assuming the existing primary key. Thanks for your help in advance.

EDIT: The underlying table DOES NOT have a primary KEY, only a primary INDEX.
",-1,-1,-1.0,"I have a flask application that relies on an existing Teradata Database to serve up information to and accept input from its users. I am able to successfully make the connection between the application and the Teradata Database, however, I am not able to then define classes that will represent tables already existing in my database.

Currently, I am defining a 'Base' class using sqlalchemy that represents the connection to my database. There is no problem here and I am even able to execute queries using the connection used to build the 'Base' class. However, my problem is in using this 'Base' class to create a subclass 'Users' for my teradata table 'users'. My understanding is that sqlalchemy should allow for me to define a subclass of the superclass 'Base' which will  inherit the metadata from the underlying teradata table that the subclass represents - in this case, my 'users' table. Here is the code I have so far:

import getpass
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.schema import MetaData

user = 'user_id_string'
pasw=getpass.getpass()
host = 'host_string'
db_name = 'db_name'

engine = create_engine(f'{host}?user={user}&amp;password={pasw}&amp;logmech=LDAP') 
connection = engine.connect()
connection.execute(f'DATABASE {db_name}')

md = MetaData(bind=connection, reflect=False, schema='db_name')
md.reflect(only=['users'])

Base = declarative_base(bind=connection, metadata=md)

class Users(Base):
    __table__ = md.tables['db_name.users']


This is the error that I receive when constructing the subclass 'Users':

sqlalchemy.exc.ArgumentError: Mapper mapped class Users-&gt;users could not assemble any primary key columns for mapped table 'users'


Is there some reason that my subclass 'Users' is not automatically being mapped to the table metadata from the existing teradata table 'users' that I have assigned it to in defining the class? The underlying table already has a primary key set so I don't understand why sqlalchemy is not assuming the existing primary key. Thanks for your help in advance.

EDIT: The underlying table DOES NOT have a primary KEY, only a primary INDEX.
",1
418,57801659,Unable to connect to Teradata using Anaconda 4.3.1(64-bit) with Python 2.7.13,"I am unable to achieve teradata connectivity using python. I have anaconda set up 4.3.1(64-bit) with python 2.7.13. Further i installed teradata odbc client driver for 16.20 and configured the odbc.ini file with user DSN details.

I have tried the options mentioned in this article - 
  https://github.com/Teradata/PyTd/issues/43
  https://github.com/Teradata/PyTd/issues/89

import teradata
  import os, sys
  os.environ[""ODBCINI""] =""/opt/teradata/client/16.20/odbc_64/odbc.ini""
  os.environ[""O`enter code here`DBCINST""] =""/opt/teradata/client/16.20/odbc_64/odbcinst.ini""
  os.environ[""LD_LIBRARY_PATH""] =""/opt/teradata/client/16.20/odbc_64/lib""

udaExec = teradata.UdaExec (appName=""HelloWorld"", version=""1.0"", odbcLibPath=""/opt/teradata/client/16.20/odbc_64/lib/tdataodbc_sb64.so"")
session = udaExec.connect(method=""odbc"", system=""tdprod"", username=""xxxxx"", password=""yyyyy"")
for row in session.execute(""SELECT GetQueryBand();""):
print(row)


Below is the execution log and error message of this script -

[servername@python]$ python test_teradata.py
Traceback (most recent call last):
File ""test_teradata.py"", line 17, in 
session = udaExec.connect(method=""odbc"", system=""tdprod"", username=""xxxxx"", password=""yyyyy"")
File ""/.../.../anaconda/lib/python2.7/site-packages/teradata/udaexec.py"", line 183, in connect
**args))
File ""/.../.../anaconda/lib/python2.7/site-packages/teradata/tdodbc.py"", line 421, in __init__init(odbcLibPath)
File ""/.../.../anaconda/lib/python2.7/site-packages/teradata/tdodbc.py"", line 367, in init initFunctionPrototypes()
File ""/.../.../anaconda/lib/python2.7/site-packages/teradata/tdodbc.py"", line 298, in initFunctionPrototypes prototype(odbc.SQLDrivers, SQLHANDLE, SQLUSMALLINT, PTR(SQLCHAR),
File ""/.../.../anaconda/lib/python2.7/ctypes/init.py"", line 375, in __getattr__func = self.getitem(name)
File ""/.../.../anaconda/lib/python2.7/ctypes/init.py"", line 380,in __getitem__func = self._FuncPtr((name_or_ordinal, self))
AttributeError: /opt/teradata/client/16.20/odbc_64/lib/tdataodbc_sb64.so: undefined symbol: SQLDrivers

",-1,-1,-1.0,"I am unable to achieve teradata connectivity using python. I have anaconda set up 4.3.1(64-bit) with python 2.7.13. Further i installed teradata odbc client driver for 16.20 and configured the odbc.ini file with user DSN details.

I have tried the options mentioned in this article - 
  https://github.com/Teradata/PyTd/issues/43
  https://github.com/Teradata/PyTd/issues/89

import teradata
  import os, sys
  os.environ[""ODBCINI""] =""/opt/teradata/client/16.20/odbc_64/odbc.ini""
  os.environ[""O`enter code here`DBCINST""] =""/opt/teradata/client/16.20/odbc_64/odbcinst.ini""
  os.environ[""LD_LIBRARY_PATH""] =""/opt/teradata/client/16.20/odbc_64/lib""

udaExec = teradata.UdaExec (appName=""HelloWorld"", version=""1.0"", odbcLibPath=""/opt/teradata/client/16.20/odbc_64/lib/tdataodbc_sb64.so"")
session = udaExec.connect(method=""odbc"", system=""tdprod"", username=""xxxxx"", password=""yyyyy"")
for row in session.execute(""SELECT GetQueryBand();""):
print(row)


Below is the execution log and error message of this script -

[servername@python]$ python test_teradata.py
Traceback (most recent call last):
File ""test_teradata.py"", line 17, in 
session = udaExec.connect(method=""odbc"", system=""tdprod"", username=""xxxxx"", password=""yyyyy"")
File ""/.../.../anaconda/lib/python2.7/site-packages/teradata/udaexec.py"", line 183, in connect
**args))
File ""/.../.../anaconda/lib/python2.7/site-packages/teradata/tdodbc.py"", line 421, in __init__init(odbcLibPath)
File ""/.../.../anaconda/lib/python2.7/site-packages/teradata/tdodbc.py"", line 367, in init initFunctionPrototypes()
File ""/.../.../anaconda/lib/python2.7/site-packages/teradata/tdodbc.py"", line 298, in initFunctionPrototypes prototype(odbc.SQLDrivers, SQLHANDLE, SQLUSMALLINT, PTR(SQLCHAR),
File ""/.../.../anaconda/lib/python2.7/ctypes/init.py"", line 375, in __getattr__func = self.getitem(name)
File ""/.../.../anaconda/lib/python2.7/ctypes/init.py"", line 380,in __getitem__func = self._FuncPtr((name_or_ordinal, self))
AttributeError: /opt/teradata/client/16.20/odbc_64/lib/tdataodbc_sb64.so: undefined symbol: SQLDrivers

",1
419,57990263,Teradata Express Does not respond after restart,"Instead of commenting on thread Teradata Viewpoint on Teradata Express 16.10 is not working, I am creating new thread but it is with respect to linked so added this link to post,

I have restarted Teradata Express with command tpareset -x ""increasing memory size"" and increased the memory, but after restart, queries on system are running very slowly and sometimes SQl Assistant goes into Not Respoding state.

To resolve this temporary, I used tpa stop and then tpa start, after this, system runs smoothly without lag but sometimes later it again goes into slow state.

Not sure what is happening here, can someone guide on how to resolve it permanently ?
",-1,-1,-1.0,"Instead of commenting on thread Teradata Viewpoint on Teradata Express 16.10 is not working, I am creating new thread but it is with respect to linked so added this link to post,

I have restarted Teradata Express with command tpareset -x ""increasing memory size"" and increased the memory, but after restart, queries on system are running very slowly and sometimes SQl Assistant goes into Not Respoding state.

To resolve this temporary, I used tpa stop and then tpa start, after this, system runs smoothly without lag but sometimes later it again goes into slow state.

Not sure what is happening here, can someone guide on how to resolve it permanently ?
",3
420,58034373,Trying to upload a pandas dataframe using teradataml copy_to_sql function,"I'm pretty new to uploading data to teradata.  The method I know works is inserting row by row using insert statements but would like to avoid that.    I am trying to directly upload my panda's dataframe to teradata but have not been successful yet.   I've tried 2 methods and my preference is to get method 1 to work but want to get a working solution first.

I've tried 2 methods.
1.Teradataml module - copy_to_sql
2.Teradata module - using insert statement

method 1: Create table using copy_to_sql function

from teradataml.dataframe.copy_to import copy_to_sql
from teradataml import create_context, remove_context

df # some dataframe

table_name=""db.table""
copy_to_sql(df = df_new, table_name = ""db.table"", primary_index=""index"", if_exists=""replace"")


method 2: Add to already created table using insert statement

import teradata

udaExec = teradata.UdaExec (appName=appname, version=""1.0"", logConsole=False)
connect = udaExec.connect(method=""odbc"",system=host, username=user,
                            password=passwrd)

num_of_chunks=100
table_name=""db.table""
query='INSERT INTO '+table_name+' values(?,?,?,?,?);'
df_chunks=np.array_split(df_new2, num_of_chunks)
for i,_ in enumerate(df_chunks):
    data = [tuple(x) for x in df_chunks[i].to_records(index=False)]
    connect.executemany(query, data,batch=True)

**method 1** get the following error related to access.  Not sure while the SQL statement is adding quotes for the bolded table below:
OperationalError: (teradatasql.OperationalError) [Version 16.20.0.48] [Session 5229096] [Teradata Database] [Error 3524] The user does not have CREATE TABLE access to database U378597.
[SQL: 
CREATE multiset TABLE **""db.table""** (
    ""PBP"" VARCHAR(1024) CHAR SET UNICODE, 
    recon VARCHAR(1024) CHAR SET UNICODE, 
    date2 TIMESTAMP(6), 
    ""CF"" FLOAT, 
    ""index"" VARCHAR(1024) CHAR SET UNICODE
)
primary index( ""index"" )

]
**method 2** get a error about inserting dates.  Assume datetime needs to be converted in someway to work in teradata table but unsure how

DatabaseError: (6760, '[HY000] [Teradata][ODBC Teradata Driver][Teradata Database] Invalid timestamp. ')

",-1,-1,-1.0,"I'm pretty new to uploading data to teradata.  The method I know works is inserting row by row using insert statements but would like to avoid that.    I am trying to directly upload my panda's dataframe to teradata but have not been successful yet.   I've tried 2 methods and my preference is to get method 1 to work but want to get a working solution first.

I've tried 2 methods.
1.Teradataml module - copy_to_sql
2.Teradata module - using insert statement

method 1: Create table using copy_to_sql function

from teradataml.dataframe.copy_to import copy_to_sql
from teradataml import create_context, remove_context

df # some dataframe

table_name=""db.table""
copy_to_sql(df = df_new, table_name = ""db.table"", primary_index=""index"", if_exists=""replace"")


method 2: Add to already created table using insert statement

import teradata

udaExec = teradata.UdaExec (appName=appname, version=""1.0"", logConsole=False)
connect = udaExec.connect(method=""odbc"",system=host, username=user,
                            password=passwrd)

num_of_chunks=100
table_name=""db.table""
query='INSERT INTO '+table_name+' values(?,?,?,?,?);'
df_chunks=np.array_split(df_new2, num_of_chunks)
for i,_ in enumerate(df_chunks):
    data = [tuple(x) for x in df_chunks[i].to_records(index=False)]
    connect.executemany(query, data,batch=True)

**method 1** get the following error related to access.  Not sure while the SQL statement is adding quotes for the bolded table below:
OperationalError: (teradatasql.OperationalError) [Version 16.20.0.48] [Session 5229096] [Teradata Database] [Error 3524] The user does not have CREATE TABLE access to database U378597.
[SQL: 
CREATE multiset TABLE **""db.table""** (
    ""PBP"" VARCHAR(1024) CHAR SET UNICODE, 
    recon VARCHAR(1024) CHAR SET UNICODE, 
    date2 TIMESTAMP(6), 
    ""CF"" FLOAT, 
    ""index"" VARCHAR(1024) CHAR SET UNICODE
)
primary index( ""index"" )

]
**method 2** get a error about inserting dates.  Assume datetime needs to be converted in someway to work in teradata table but unsure how

DatabaseError: (6760, '[HY000] [Teradata][ODBC Teradata Driver][Teradata Database] Invalid timestamp. ')

",1
421,58100000,Move panda df to teradata table: [HY000] [Teradata][ODBC Teradata Driver][Teradata Database] Invalid timestamp,"I have a df that i want to move to a teradata table. I am using a framework that was discussed on this platform. However I am getting a few errors.
The entire logic behind loading the df to teradata is:

1) If table doesnt exist then create table else skip creation.

2) Start loading the df to the table. (Note i will be passing multiple xlsx files to a df and eventually appending it to the teradata table)

I have written a bteq script to create a table which goes like this:

    FROM DBC.TABLES WHERE DATABASENAME = 'abc' AND TABLENAME = 'sample';

.IF ACTIVITYCOUNT &lt;&gt; 0 THEN .GOTO SKIP_CREATION
.IF ACTIVITYCOUNT = 0 THEN .GOTO TABLE_NOT_EXISTS

.LABEL TABLE_NOT_EXISTS 
CREATE TABLE abc.sample ( 
col1 VARCHAR(400) CHARACTER  SET LATIN NOT CASESPECIFIC, 
col2 VARCHAR(400) CHARACTER SET LATIN NOT CASESPECIFIC,
.
.
col23  TIMESTAMP(0) WITH TIME ZONE FORMAT 'YYYY-MM-DD HH:MI:SSZ', 
col24 TIMESTAMP(0) WITH TIME ZONE FORMAT 'YYYY-MM-DD HH:MI:SSZ'
);

.LABEL SKIP_CREATION
.LOGOFF


My python code to move the df to teradata is:

df=some data frame
host,username,password = 'host','username', ""password""
num_of_chunks = 1000
insert_query= ""INSERT INTO abc.sample VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)""
udaExec = teradata.UdaExec (appName=""IMC"", version=""1.0"", logConsole=False)
with udaExec.connect(method=""odbc"",system=host, username = username,
                         password=password, driver=""Teradata"") as session:
    file_exist=session.execute(file=r""Path of the bteq file"" ,fileType=""bteq"",ignoreErrors=[3803])
    schedule_chunks = np.array_split(df, num_of_chunks)

    for i,_ in enumerate(schedule_chunks):

        data = [tuple(x) for x in schedule_chunks[i].to_records(index=False)]

            session.executemany(insert_query, data,batch=True) 


When I run this i get the following error message:


  DatabaseError: [HY000] [Teradata][ODBC Teradata Driver][Teradata
  Database] Invalid timestamp.


Can someone help me with whee am i going wrong? Also need some suggestion if I am writing the bteq script correctly. I want to avoid dropping tables and creating a new one each time. 
",-1,-1,-1.0,"I have a df that i want to move to a teradata table. I am using a framework that was discussed on this platform. However I am getting a few errors.
The entire logic behind loading the df to teradata is:

1) If table doesnt exist then create table else skip creation.

2) Start loading the df to the table. (Note i will be passing multiple xlsx files to a df and eventually appending it to the teradata table)

I have written a bteq script to create a table which goes like this:

    FROM DBC.TABLES WHERE DATABASENAME = 'abc' AND TABLENAME = 'sample';

.IF ACTIVITYCOUNT &lt;&gt; 0 THEN .GOTO SKIP_CREATION
.IF ACTIVITYCOUNT = 0 THEN .GOTO TABLE_NOT_EXISTS

.LABEL TABLE_NOT_EXISTS 
CREATE TABLE abc.sample ( 
col1 VARCHAR(400) CHARACTER  SET LATIN NOT CASESPECIFIC, 
col2 VARCHAR(400) CHARACTER SET LATIN NOT CASESPECIFIC,
.
.
col23  TIMESTAMP(0) WITH TIME ZONE FORMAT 'YYYY-MM-DD HH:MI:SSZ', 
col24 TIMESTAMP(0) WITH TIME ZONE FORMAT 'YYYY-MM-DD HH:MI:SSZ'
);

.LABEL SKIP_CREATION
.LOGOFF


My python code to move the df to teradata is:

df=some data frame
host,username,password = 'host','username', ""password""
num_of_chunks = 1000
insert_query= ""INSERT INTO abc.sample VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)""
udaExec = teradata.UdaExec (appName=""IMC"", version=""1.0"", logConsole=False)
with udaExec.connect(method=""odbc"",system=host, username = username,
                         password=password, driver=""Teradata"") as session:
    file_exist=session.execute(file=r""Path of the bteq file"" ,fileType=""bteq"",ignoreErrors=[3803])
    schedule_chunks = np.array_split(df, num_of_chunks)

    for i,_ in enumerate(schedule_chunks):

        data = [tuple(x) for x in schedule_chunks[i].to_records(index=False)]

            session.executemany(insert_query, data,batch=True) 


When I run this i get the following error message:


  DatabaseError: [HY000] [Teradata][ODBC Teradata Driver][Teradata
  Database] Invalid timestamp.


Can someone help me with whee am i going wrong? Also need some suggestion if I am writing the bteq script correctly. I want to avoid dropping tables and creating a new one each time. 
",1
422,58261239,teradatasql Python module only works when scripting but not when running code,"I have run into a peculiar issue while using the teradatasql package (installed from pypi). I use the following code (let's call it pytera.py) to query a database:

from dotenv import load_dotenv
import pandas as pd
import teradatasql

# Load the database credentials from .env file
_ = load_dotenv()
db_host = os.getenv('db_host')
db_username = os.getenv('db_username')
db_password = os.getenv('db_password')


def run_query(query):
    """"""Run query string on teradata and return DataFrame.""""""
    if query.strip()[-1] != ';':
        query += ';'

    with teradatasql.connect(host=db_host, user=db_username,
                         password=db_password) as connect:
        df = pd.read_sql(query, connect)
    return df



When I import this function in the IPython/Python interpreter or in Jupyter Notebook, I can run queries just fine like so:

import pytera as pt

pt.run_query('select top 5 * from table_name;')



However, if I save the above code in a .py file and try to run it, I get an error message most of the time (not all the time). The error message is below.

E   teradatasql.OperationalError: [Version 16.20.0.49] [Session 0] [Teradata SQL Driver] Hostname lookup failed for None
E    at gosqldriver/teradatasql.(*teradataConnection).makeDriverError TeradataConnection.go:1046
E    at gosqldriver/teradatasql.(*Lookup).getAddresses CopDiscovery.go:65
E    at gosqldriver/teradatasql.discoverCops CopDiscovery.go:137
E    at gosqldriver/teradatasql.newTeradataConnection TeradataConnection.go:133
E    at gosqldriver/teradatasql.(*teradataDriver).Open TeradataDriver.go:32
E    at database/sql.dsnConnector.Connect sql.go:600
E    at database/sql.(*DB).conn sql.go:1103
E    at database/sql.(*DB).Conn sql.go:1619
E    at main.goCreateConnection goside.go:229
E    at main._cgoexpwrap_e6e101e164fa_goCreateConnection _cgo_gotypes.go:214
E    at runtime.call64 asm_amd64.s:574
E    at runtime.cgocallbackg1 cgocall.go:316
E    at runtime.cgocallbackg cgocall.go:194
E    at runtime.cgocallback_gofunc asm_amd64.s:826
E    at runtime.goexit asm_amd64.s:2361
E   Caused by lookup None on &lt;ip address redacted&gt;: server misbehaving


I am using Python 3.7.3 and teradatasql 16.20.0.49 on Ubuntu (WSL) 18.04.

Perhaps not coincidentally, I run into a similar issue when trying a similar workflow on Windows (using the teradata package and the Teradata Python drivers installed). Works when I connect inside the interpreter or in Jupyter, but not in a script. In the Windows case, the error is:

E teradata.api.DatabaseError: (10380, '[08001] [Teradata][ODBC] (10380) Unable to establish connection with data source. Missing settings: {[DBCName]}')


I have a feeling that there's something basic that I'm missing, but I can't find a solution to this anywhere.
",-1,-1,-1.0,"I have run into a peculiar issue while using the teradatasql package (installed from pypi). I use the following code (let's call it pytera.py) to query a database:

from dotenv import load_dotenv
import pandas as pd
import teradatasql

# Load the database credentials from .env file
_ = load_dotenv()
db_host = os.getenv('db_host')
db_username = os.getenv('db_username')
db_password = os.getenv('db_password')


def run_query(query):
    """"""Run query string on teradata and return DataFrame.""""""
    if query.strip()[-1] != ';':
        query += ';'

    with teradatasql.connect(host=db_host, user=db_username,
                         password=db_password) as connect:
        df = pd.read_sql(query, connect)
    return df



When I import this function in the IPython/Python interpreter or in Jupyter Notebook, I can run queries just fine like so:

import pytera as pt

pt.run_query('select top 5 * from table_name;')



However, if I save the above code in a .py file and try to run it, I get an error message most of the time (not all the time). The error message is below.

E   teradatasql.OperationalError: [Version 16.20.0.49] [Session 0] [Teradata SQL Driver] Hostname lookup failed for None
E    at gosqldriver/teradatasql.(*teradataConnection).makeDriverError TeradataConnection.go:1046
E    at gosqldriver/teradatasql.(*Lookup).getAddresses CopDiscovery.go:65
E    at gosqldriver/teradatasql.discoverCops CopDiscovery.go:137
E    at gosqldriver/teradatasql.newTeradataConnection TeradataConnection.go:133
E    at gosqldriver/teradatasql.(*teradataDriver).Open TeradataDriver.go:32
E    at database/sql.dsnConnector.Connect sql.go:600
E    at database/sql.(*DB).conn sql.go:1103
E    at database/sql.(*DB).Conn sql.go:1619
E    at main.goCreateConnection goside.go:229
E    at main._cgoexpwrap_e6e101e164fa_goCreateConnection _cgo_gotypes.go:214
E    at runtime.call64 asm_amd64.s:574
E    at runtime.cgocallbackg1 cgocall.go:316
E    at runtime.cgocallbackg cgocall.go:194
E    at runtime.cgocallback_gofunc asm_amd64.s:826
E    at runtime.goexit asm_amd64.s:2361
E   Caused by lookup None on &lt;ip address redacted&gt;: server misbehaving


I am using Python 3.7.3 and teradatasql 16.20.0.49 on Ubuntu (WSL) 18.04.

Perhaps not coincidentally, I run into a similar issue when trying a similar workflow on Windows (using the teradata package and the Teradata Python drivers installed). Works when I connect inside the interpreter or in Jupyter, but not in a script. In the Windows case, the error is:

E teradata.api.DatabaseError: (10380, '[08001] [Teradata][ODBC] (10380) Unable to establish connection with data source. Missing settings: {[DBCName]}')


I have a feeling that there's something basic that I'm missing, but I can't find a solution to this anywhere.
",1
423,58700872,NoneType' object is not iterable error when using read_sql for teradata,"I use pyodbc to connect to Teradata, and use read_sql to select data.  This used to work when I used Python2.7 and pyodbc3.0.7.  Now that I switched to Python3.5 and pyodbc4.0.17, I get error ""'NoneType' object is not iterable"".
My pandas version is 0.23.1

I did some search here and tried to add ""set nocount on"", but Teradata doesn't seem to have this equivalent.

Here's how I connect to teradata:

cnxn = pyodbc.connect('DSN=***;PWD=***',autocommit=True)


And here's how I try to get data:

DF=pd.read_sql_query(""SELECT * from tbl"",con=cnxn)  


The table is not empty, and I should get thousands of records.  Instead, this is the error I get:

  File ""/Applications/anaconda2/envs/python3.5/lib/python3.5/site-packages/pandas/io/sql.py"", line 314, in read_sql_query
    parse_dates=parse_dates, chunksize=chunksize)
  File ""/Applications/anaconda2/envs/python3.5/lib/python3.5/site-packages/pandas/io/sql.py"", line 1414, in read_query
    columns = [col_desc[0] for col_desc in cursor.description]
TypeError: 'NoneType' object is not iterable


Any pointers on how I can fix this?
",-1,-1,-1.0,"I use pyodbc to connect to Teradata, and use read_sql to select data.  This used to work when I used Python2.7 and pyodbc3.0.7.  Now that I switched to Python3.5 and pyodbc4.0.17, I get error ""'NoneType' object is not iterable"".
My pandas version is 0.23.1

I did some search here and tried to add ""set nocount on"", but Teradata doesn't seem to have this equivalent.

Here's how I connect to teradata:

cnxn = pyodbc.connect('DSN=***;PWD=***',autocommit=True)


And here's how I try to get data:

DF=pd.read_sql_query(""SELECT * from tbl"",con=cnxn)  


The table is not empty, and I should get thousands of records.  Instead, this is the error I get:

  File ""/Applications/anaconda2/envs/python3.5/lib/python3.5/site-packages/pandas/io/sql.py"", line 314, in read_sql_query
    parse_dates=parse_dates, chunksize=chunksize)
  File ""/Applications/anaconda2/envs/python3.5/lib/python3.5/site-packages/pandas/io/sql.py"", line 1414, in read_query
    columns = [col_desc[0] for col_desc in cursor.description]
TypeError: 'NoneType' object is not iterable


Any pointers on how I can fix this?
",1
424,58785305,Not able to connect to Teradata using Python,"import teradata
udaExec = teradata.UdaExec (appName=""testconnec"", version=""1.0"",logConsole=False) 
session = udaExec.connect(method=""odbc"", dsn=""TeraDev"",username=""usename"", password=""password"");



  above code is failing with error: 
  Traceback (most recent call
  last):   File """", line 1, in    File
  ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/udaexec.py"",
  line 183, in connect
      **args))   File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/tdodbc.py"",
  line 421, in init
      init(odbcLibPath)   File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/tdodbc.py"",
  line 366, in init
      initOdbcLibrary(odbcLibPath)   File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/tdodbc.py"",
  line 319, in initOdbcLibrary
      odbc = ctypes.cdll.LoadLibrary(odbcLibPath)   File ""/usr/lib64/python2.7/ctypes/init.py"", line 438, in LoadLibrary
      return self._dlltype(name)   File ""/usr/lib64/python2.7/ctypes/init.py"", line 360, in init
      self._handle = _dlopen(self._name, mode) OSError: libodbc.so: cannot open shared object file: No such file or directory`


export ODBCINI=/opt/teradata/client/16.20/odbc_64/odbc.ini
export ODBCINST=/opt/teradata/client/16.20/odbc_64/odbcinst.ini
export LD_LIBRARY_PATH=/opt/teradata/client/16.20/odbc_64/lib


***After adjusting above env variable****

import teradata



udaExec=teradata.UdaExec(odbcLibPath=""/opt/teradata/client/16.20/odbc_64/lib/libodbc.so"",appName=""testconnec"", version=""1.0"",logConsole=False)

session = udaExec.connect(method=""odbc"", dsn=""TeraDev"",username=""user"", password=""password"", driver=""Teradata Database ODBC Driver 16.20"");


Error:
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/udaexec.py"", line 183, in connect
    **args))
  File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/tdodbc.py"", line 454, in __init__
    checkStatus(rc, hDbc=self.hDbc, method=""SQLDriverConnectW"")
  File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/tdodbc.py"", line 231, in checkStatus
    raise DatabaseError(i[2], u""[{}] {}"".format(i[0], msg), i[0])
teradata.api.DatabaseError: (0, u'[IM002] [DataDirect][ODBC lib] Data source name not found and no default driver specified')

",-1,-1,-1.0,"import teradata
udaExec = teradata.UdaExec (appName=""testconnec"", version=""1.0"",logConsole=False) 
session = udaExec.connect(method=""odbc"", dsn=""TeraDev"",username=""usename"", password=""password"");



  above code is failing with error: 
  Traceback (most recent call
  last):   File """", line 1, in    File
  ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/udaexec.py"",
  line 183, in connect
      **args))   File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/tdodbc.py"",
  line 421, in init
      init(odbcLibPath)   File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/tdodbc.py"",
  line 366, in init
      initOdbcLibrary(odbcLibPath)   File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/tdodbc.py"",
  line 319, in initOdbcLibrary
      odbc = ctypes.cdll.LoadLibrary(odbcLibPath)   File ""/usr/lib64/python2.7/ctypes/init.py"", line 438, in LoadLibrary
      return self._dlltype(name)   File ""/usr/lib64/python2.7/ctypes/init.py"", line 360, in init
      self._handle = _dlopen(self._name, mode) OSError: libodbc.so: cannot open shared object file: No such file or directory`


export ODBCINI=/opt/teradata/client/16.20/odbc_64/odbc.ini
export ODBCINST=/opt/teradata/client/16.20/odbc_64/odbcinst.ini
export LD_LIBRARY_PATH=/opt/teradata/client/16.20/odbc_64/lib


***After adjusting above env variable****

import teradata



udaExec=teradata.UdaExec(odbcLibPath=""/opt/teradata/client/16.20/odbc_64/lib/libodbc.so"",appName=""testconnec"", version=""1.0"",logConsole=False)

session = udaExec.connect(method=""odbc"", dsn=""TeraDev"",username=""user"", password=""password"", driver=""Teradata Database ODBC Driver 16.20"");


Error:
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/udaexec.py"", line 183, in connect
    **args))
  File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/tdodbc.py"", line 454, in __init__
    checkStatus(rc, hDbc=self.hDbc, method=""SQLDriverConnectW"")
  File ""/home/hadoop/.local/lib/python2.7/site-packages/teradata/tdodbc.py"", line 231, in checkStatus
    raise DatabaseError(i[2], u""[{}] {}"".format(i[0], msg), i[0])
teradata.api.DatabaseError: (0, u'[IM002] [DataDirect][ODBC lib] Data source name not found and no default driver specified')

",1
425,58943463,Teradata c# Connect with tdwallet,"I'm trying to connect using the string connection below, but with errors.

using (TdConnection cn = 
new TdConnection(@""Data Source=ip;
    User ID=myuser;
    Password=$tdwallet(mytdwallet);
    Authentication Mechanism=ldap;""))


I'm able to conect like that,works :

using(TdConnection cn = 
new TdConnection(@""Data Source=ip;
    User ID=myuser;
    Password=mypass;
    Authentication Mechanism=LDAP;""))


But the Problem is I'dont wann put my password in the code,somebody knows some solutions to this problem?


  InnerException    {Teradata.Net.Security.TdgssException:
  TdgssAuthenticationTokenExchange delegate threw an exception.  See the
  inner exception for details. ErrorCode: -2146233088 Severity: Warning
  Facility: LoadTdgss ---> Teradata.Client.Provider.TdException:
  [Teradata Database] [8017] The UserId, Password or Account is invalid.
  at Teradata.Client.Provider.WpMessageManager.CheckForError(Request
  request)    at
  Teradata.Client.Provider.WpSecurityManager.GetSsoResponseToken()    at
  Teradata.Net.Security.Mechanisms.ldapSession.AuthenticateAsClientImplementation(TdgssAuthenticationTokenExchange
  tokenAuthenticationExchange, String credential, String targetName)
  --- End of inner exception stack trace ---    at Teradata.Net.Security.Mechanisms.ldapSession.AuthenticateAsClientImplementation(TdgssAuthenticationTokenExchange
  tokenAuthenticationExchange, String credential, String targetName)
  at
  Teradata.Net.Security.Mechanisms.Session.AuthenticateAsClient(TdgssAuthenticationTokenExchange
  tokenAuthenticationExchange, String credential, String targetName)
  at
  Teradata.Client.Provider.WpSecurityManager.Action()}  System.Exception
  {Teradata.Net.Security.TdgssException}

",1,-1,-1.0,"I'm trying to connect using the string connection below, but with errors.

using (TdConnection cn = 
new TdConnection(@""Data Source=ip;
    User ID=myuser;
    Password=$tdwallet(mytdwallet);
    Authentication Mechanism=ldap;""))


I'm able to conect like that,works :

using(TdConnection cn = 
new TdConnection(@""Data Source=ip;
    User ID=myuser;
    Password=mypass;
    Authentication Mechanism=LDAP;""))


But the Problem is I'dont wann put my password in the code,somebody knows some solutions to this problem?


  InnerException    {Teradata.Net.Security.TdgssException:
  TdgssAuthenticationTokenExchange delegate threw an exception.  See the
  inner exception for details. ErrorCode: -2146233088 Severity: Warning
  Facility: LoadTdgss ---> Teradata.Client.Provider.TdException:
  [Teradata Database] [8017] The UserId, Password or Account is invalid.
  at Teradata.Client.Provider.WpMessageManager.CheckForError(Request
  request)    at
  Teradata.Client.Provider.WpSecurityManager.GetSsoResponseToken()    at
  Teradata.Net.Security.Mechanisms.ldapSession.AuthenticateAsClientImplementation(TdgssAuthenticationTokenExchange
  tokenAuthenticationExchange, String credential, String targetName)
  --- End of inner exception stack trace ---    at Teradata.Net.Security.Mechanisms.ldapSession.AuthenticateAsClientImplementation(TdgssAuthenticationTokenExchange
  tokenAuthenticationExchange, String credential, String targetName)
  at
  Teradata.Net.Security.Mechanisms.Session.AuthenticateAsClient(TdgssAuthenticationTokenExchange
  tokenAuthenticationExchange, String credential, String targetName)
  at
  Teradata.Client.Provider.WpSecurityManager.Action()}  System.Exception
  {Teradata.Net.Security.TdgssException}

",1
426,59854961,Unable to connect to teradata with Integrated Security authentication using JDBC connection string,"Unable to connect to teradata with Integrated Security authentication using JDBC connection string.
Unfortunately there isn't much help for teradata, but I have referred to SQL server and used the below connection string. 

String Connurl=""jdbc:teradata://hostname/database=xxxx,integratedSecurity=true"";
Class.forName(""com.teradata.jdbc.TeraDriver"");
Connection conn=DriverManager.getConnection(connurl);


I am getting below error. Not sure if I am missing something, can anyone throw me some light please?


  java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 16.20.00.10] [Error 1536] [SQLState HY000] Invalid connection parameter name integratedSecurity


Basically my intention is to connect to Teradata using windows authentication, instead of providing username and password.
",1,-1,-1.0,"Unable to connect to teradata with Integrated Security authentication using JDBC connection string.
Unfortunately there isn't much help for teradata, but I have referred to SQL server and used the below connection string. 

String Connurl=""jdbc:teradata://hostname/database=xxxx,integratedSecurity=true"";
Class.forName(""com.teradata.jdbc.TeraDriver"");
Connection conn=DriverManager.getConnection(connurl);


I am getting below error. Not sure if I am missing something, can anyone throw me some light please?


  java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 16.20.00.10] [Error 1536] [SQLState HY000] Invalid connection parameter name integratedSecurity


Basically my intention is to connect to Teradata using windows authentication, instead of providing username and password.
",1
427,59870691,I am getting error 3802 while running a SQL query in Teradata SQL,"I am running the following query.

create multiset volatile table table1 as (
select
a.variable1,
a.variable2, 
b.variable3, 
b.variable4, 
c.variable5, 
c.variable6, 
d.variable7,
d.variable8

from data1 as a
left join data2 as b
on a.ID1=b.ID1 and a.date1=b.date1
left join data3 as c
on a.ID1=c.ID1 and a.date1=c.date1
left join data4 as d
on a.ID1=d.ID1 and a.date1=d.date1
where a.variable1&gt;100 and a.varaiable2 in ('A','B'))
with data primary index (id1,id2,id3,date1) on commit preserve rows ;


I am getting the error- 
PreparedStatementCallback; bad SQL grammar []; nested exception is java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.08] [Error 3802] [SQLState 42S02] Database 'd' does not exist.""
]

However, I am able to run the query select * from data4 which means that data4 does exist. 

It will be great if you guys can help me find out what's generating this error. 

Many Thanks in advance.
",-1,-1,-1.0,"I am running the following query.

create multiset volatile table table1 as (
select
a.variable1,
a.variable2, 
b.variable3, 
b.variable4, 
c.variable5, 
c.variable6, 
d.variable7,
d.variable8

from data1 as a
left join data2 as b
on a.ID1=b.ID1 and a.date1=b.date1
left join data3 as c
on a.ID1=c.ID1 and a.date1=c.date1
left join data4 as d
on a.ID1=d.ID1 and a.date1=d.date1
where a.variable1&gt;100 and a.varaiable2 in ('A','B'))
with data primary index (id1,id2,id3,date1) on commit preserve rows ;


I am getting the error- 
PreparedStatementCallback; bad SQL grammar []; nested exception is java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.08] [Error 3802] [SQLState 42S02] Database 'd' does not exist.""
]

However, I am able to run the query select * from data4 which means that data4 does exist. 

It will be great if you guys can help me find out what's generating this error. 

Many Thanks in advance.
",3
428,60052943,java.net.SocketTimeoutException: connect timed out while loading dataframe to teradata using spark 2.2,"I am trying to load dataframe in to teradata using JDBC driver, but getting below error.

jdbc version:- 16.10.00.07
cloudera version :- 5.15

invoking spark shell using below command.
 spark2-shell --jars terajdbc4.jar,tdgssconfig.jar,teradata-connector-1.4.4.jar

we are able to get schema of the table but not able to select or insert operation for all the columns of the table.

also the code that i am using can be found below.

val insertdf = spark.sql(query)
val prop = new java.util.Properties
prop.setProperty(""driver"", ""com.teradata.jdbc.TeraDriver"")
prop.setProperty(""user"", ""xxxxxx"")
prop.setProperty(""password"", ""*******"") 

val url = ""jdbc:teradata://10.xxx.130.69/database=stg_database, TMODE=TERA""
val table = ""test_table""
insertdf.write.mode(""append"").jdbc(url, table, prop)


ERROR:-

  Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2024)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2045)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2064)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)
  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:644)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:603)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:612)
  ... 48 elided
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 16.10.00.07] [Error 1277] [SQLState 08S01] Login timeout for Connection to 10.130.130.69 Tue Feb 04 02:05:21 EST 2020 socket orig=10.130.130.69 cid=44c91563 sess=0 java.net.SocketTimeoutException: connect timed out  at java.net.PlainSocketImpl.socketConnect(Native Method)  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)  at java.net.Socket.connect(Socket.java:589)  at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$ConnectThread.run(TDNetworkIOIF.java:1242)
  at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
  at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
  at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeIoJDBCException(ErrorFactory.java:208)
  at com.teradata.jdbc.jdbc_4.util.ErrorAnalyzer.analyzeIoError(ErrorAnalyzer.java:59)
  at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.createSocketConnection(TDNetworkIOIF.java:163)
  at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.&lt;init&gt;(TDNetworkIOIF.java:142)
  at com.teradata.jdbc.jdbc.GenericTeradataConnection.getIO(GenericTeradataConnection.java:142)
  at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:100)
  at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:211)
  at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:36)
  at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)
  at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:178)
  at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:168)
  at com.teradata.jdbc.TeraDriver.doConnect(TeraDriver.java:236)
  at com.teradata.jdbc.TeraDriver.connect(TeraDriver.java:162)
  at org.apache.spark.sql.execution.datasources.jdbc.DriverWrapper.connect(DriverWrapper.scala:45)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:61)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:52)
  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:286)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
  at org.apache.spark.scheduler.Task.run(Task.scala:108)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.SocketTimeoutException: connect timed out
  at java.net.PlainSocketImpl.socketConnect(Native Method)
  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
  at java.net.Socket.connect(Socket.java:589)

",-1,-1,-1.0,"I am trying to load dataframe in to teradata using JDBC driver, but getting below error.

jdbc version:- 16.10.00.07
cloudera version :- 5.15

invoking spark shell using below command.
 spark2-shell --jars terajdbc4.jar,tdgssconfig.jar,teradata-connector-1.4.4.jar

we are able to get schema of the table but not able to select or insert operation for all the columns of the table.

also the code that i am using can be found below.

val insertdf = spark.sql(query)
val prop = new java.util.Properties
prop.setProperty(""driver"", ""com.teradata.jdbc.TeraDriver"")
prop.setProperty(""user"", ""xxxxxx"")
prop.setProperty(""password"", ""*******"") 

val url = ""jdbc:teradata://10.xxx.130.69/database=stg_database, TMODE=TERA""
val table = ""test_table""
insertdf.write.mode(""append"").jdbc(url, table, prop)


ERROR:-

  Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2024)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2045)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2064)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)
  at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:644)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:603)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:612)
  ... 48 elided
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 16.10.00.07] [Error 1277] [SQLState 08S01] Login timeout for Connection to 10.130.130.69 Tue Feb 04 02:05:21 EST 2020 socket orig=10.130.130.69 cid=44c91563 sess=0 java.net.SocketTimeoutException: connect timed out  at java.net.PlainSocketImpl.socketConnect(Native Method)  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)  at java.net.Socket.connect(Socket.java:589)  at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$ConnectThread.run(TDNetworkIOIF.java:1242)
  at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
  at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
  at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeIoJDBCException(ErrorFactory.java:208)
  at com.teradata.jdbc.jdbc_4.util.ErrorAnalyzer.analyzeIoError(ErrorAnalyzer.java:59)
  at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.createSocketConnection(TDNetworkIOIF.java:163)
  at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.&lt;init&gt;(TDNetworkIOIF.java:142)
  at com.teradata.jdbc.jdbc.GenericTeradataConnection.getIO(GenericTeradataConnection.java:142)
  at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:100)
  at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:211)
  at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:36)
  at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)
  at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:178)
  at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:168)
  at com.teradata.jdbc.TeraDriver.doConnect(TeraDriver.java:236)
  at com.teradata.jdbc.TeraDriver.connect(TeraDriver.java:162)
  at org.apache.spark.sql.execution.datasources.jdbc.DriverWrapper.connect(DriverWrapper.scala:45)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:61)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:52)
  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:286)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
  at org.apache.spark.scheduler.Task.run(Task.scala:108)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.SocketTimeoutException: connect timed out
  at java.net.PlainSocketImpl.socketConnect(Native Method)
  at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
  at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
  at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
  at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
  at java.net.Socket.connect(Socket.java:589)

",0
429,60071633,Multiple rows into One Teradata,"My volatile table in Teradata contains data like the following. 

  ID |  RECORD_TIMESTAMP |  CHANNEL
1    |    20200101       |  A
1    |    20200102       |  B
1    |    20200103       |  C
2    |    20200104       |  D
3    |    20200105       |  E 


My required output is to concatenate the data based on ID and delimit with pipe. The ID may or maynot have multiple instances (multiple instances go upto 50 per ID)

Required Output:

1|20200101|A|1|20200102|B|1|20200103|C
2|20200104|D
3|20200105|E


How can we achieve this? I tried self-join which were creating duplicates, tried XMLAGG, but couldnt replicate the format i needed.
",-1,0,-1.0,"My volatile table in Teradata contains data like the following. 

  ID |  RECORD_TIMESTAMP |  CHANNEL
1    |    20200101       |  A
1    |    20200102       |  B
1    |    20200103       |  C
2    |    20200104       |  D
3    |    20200105       |  E 


My required output is to concatenate the data based on ID and delimit with pipe. The ID may or maynot have multiple instances (multiple instances go upto 50 per ID)

Required Output:

1|20200101|A|1|20200102|B|1|20200103|C
2|20200104|D
3|20200105|E


How can we achieve this? I tried self-join which were creating duplicates, tried XMLAGG, but couldnt replicate the format i needed.
",3
430,60097704,Spark read from Teradata view which has column with Title,"I want to try read from Teradata using Spark.

I have a Teradata view from one table which has column(order_id) with title( ""ORDER ID"") column name.

So while using spark it is giving me following error.

Caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.08] [Error 5628] [SQLState HY000] Column ORDER ID not found in ST.

 val Query  = ""(select order_id from table sample 100) ST "";

 var reader = sparkSession.read
      .format(""jdbc"")
      .option(""url"", ""jdbc:teradata://xx/charset=xx, DBS_PORT=xx, TMODE=TERA, user=xx, password=xx"")
      .option(""dbtable"", Query);

    reader.load().show(100);

",-1,-1,-1.0,"I want to try read from Teradata using Spark.

I have a Teradata view from one table which has column(order_id) with title( ""ORDER ID"") column name.

So while using spark it is giving me following error.

Caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.08] [Error 5628] [SQLState HY000] Column ORDER ID not found in ST.

 val Query  = ""(select order_id from table sample 100) ST "";

 var reader = sparkSession.read
      .format(""jdbc"")
      .option(""url"", ""jdbc:teradata://xx/charset=xx, DBS_PORT=xx, TMODE=TERA, user=xx, password=xx"")
      .option(""dbtable"", Query);

    reader.load().show(100);

",3
431,59670134,Migrating data from Teradata to BigQuery,"I'm running Teradata express on VMWare player. In order to migrate the data to BigQuery I'm trying to follow the official documentation capturing BigQuery transfer job.

https://cloud.google.com/bigquery-transfer/docs/teradata-migration 

I have downloaded the required jars and trying to start the migration process alfer successful initialization. Providing the config file contents generated after successful initialization.

TDExpress1620_Sles11:~/Desktop # cat testpath/bq.config
{
  ""agent-id"": ""84f7cf04-2133-4c5a-ae43-dd22a30281ba"",
  ""transfer-configuration"": {
    ""project-id"": ""106730138445"",
    ""location"": ""us"",
    ""id"": ""5e1c7eda-0000-249c-aab6-94eb2c06245a""
  },
  ""source-type"": ""teradata"",
  ""console-log"": false,
  ""silent"": false,
  ""teradata-config"": {
    ""connection"": {
      ""host"": ""localhost""
    },
    ""local-processing-space"": ""testpath"",
    ""database-credentials-file-path"": """",
    ""max-local-storage"": ""200GB"",
    ""gcs-upload-chunk-size"": ""32MB"",
    ""use-tpt"": false,
    ""max-sessions"": 0,
    ""spool-mode"": ""NoSpool"",
    ""max-parallel-upload"": 1,
    ""max-parallel-extract-threads"": 1,
    ""session-charset"": ""UTF8"",
    ""max-unload-file-size"": ""2GB""
  }


After running the command to start the migration process, I come across below error.

TDExpress1620_Sles11:~/Desktop # java -cp terajdbc4.jar:mirroring-agent.jar com.google.cloud.bigquery.dms.Agent --configuration-file=testpath/bq.config
Reading data from gs://data_transfer_agent/latest/version
WARNING: Failed to get the latest released agent version with error: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
Exception in thread ""main"" com.google.api.gax.rpc.UnavailableException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
    at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:69)
    at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:72)
    at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:60)
    at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97)
    at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:68)
    at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1349)
    at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:398)
    at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1024)
    at com.google.common.util.concurrent.AbstractFuture.addListener(AbstractFuture.java:670)
    at com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:45)
    at com.google.api.core.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:52)
    at com.google.common.util.concurrent.Futures.addCallback(Futures.java:1330)
    at com.google.api.core.ApiFutures.addCallback(ApiFutures.java:63)
    at com.google.api.gax.grpc.GrpcExceptionCallable.futureCall(GrpcExceptionCallable.java:67)
    at com.google.api.gax.rpc.AttemptCallable.call(AttemptCallable.java:86)
    at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
    at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
    at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
    Suppressed: com.google.api.gax.rpc.AsyncTaskException: Asynchronous task failed
        at com.google.api.gax.rpc.ApiExceptions.callAndTranslateApiException(ApiExceptions.java:57)
        at com.google.api.gax.rpc.UnaryCallable.call(UnaryCallable.java:112)
        at com.google.cloud.bigquery.datatransfer.v1.DataTransferServiceClient.getTransferConfig(DataTransferServiceClient.java:741)
        at com.google.cloud.bigquery.datatransfer.v1.DataTransferServiceClient.getTransferConfig(DataTransferServiceClient.java:718)
        at com.google.cloud.bigquery.dms.gcloud.TransferServiceClient.getTransferConfig(TransferServiceClient.java:62)
        at com.google.cloud.bigquery.dms.gcloud.TransferServiceClient.getDestinationBucketName(TransferServiceClient.java:94)
        at com.google.cloud.bigquery.dms.Agent.main(Agent.java:235)
Caused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
    at io.grpc.Status.asRuntimeException(Status.java:533)
    at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:490)
    at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
    at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
    at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
    at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:700)
    at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
    at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
    at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
    at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:399)
    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:500)
    at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:65)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:592)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$700(ClientCallImpl.java:508)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:632)
    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
    ... 7 more
Caused by: io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: null: bigquerydatatransfer.googleapis.com/2404:6800:4009:810:0:0:0:200a:443
    at io.grpc.netty.shaded.io.netty.channel.unix.Errors.throwConnectException(Errors.java:104)
    at io.grpc.netty.shaded.io.netty.channel.unix.Socket.connect(Socket.java:257)
    at io.grpc.netty.shaded.io.netty.channel.epoll.AbstractEpollChannel.doConnect0(AbstractEpollChannel.java:730)
    at io.grpc.netty.shaded.io.netty.channel.epoll.AbstractEpollChannel.doConnect(AbstractEpollChannel.java:715)
    at io.grpc.netty.shaded.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.connect(AbstractEpollChannel.java:557)
    at io.grpc.netty.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1340)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:532)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:517)
    at io.grpc.netty.shaded.io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50)
    at io.grpc.netty.shaded.io.grpc.netty.WriteBufferingAndExceptionHandler.connect(WriteBufferingAndExceptionHandler.java:136)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:532)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.access$1000(AbstractChannelHandlerContext.java:38)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext$9.run(AbstractChannelHandlerContext.java:522)
    at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
    at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404)
    at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:333)
    at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:905)
    at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    ... 1 more
Caused by: java.net.NoRouteToHostException
    ... 19 more


Please let me know if somebody has successfully implemented the same or able to figure out the exact issue being faced by me.
",-1,-1,-1.0,"I'm running Teradata express on VMWare player. In order to migrate the data to BigQuery I'm trying to follow the official documentation capturing BigQuery transfer job.

https://cloud.google.com/bigquery-transfer/docs/teradata-migration 

I have downloaded the required jars and trying to start the migration process alfer successful initialization. Providing the config file contents generated after successful initialization.

TDExpress1620_Sles11:~/Desktop # cat testpath/bq.config
{
  ""agent-id"": ""84f7cf04-2133-4c5a-ae43-dd22a30281ba"",
  ""transfer-configuration"": {
    ""project-id"": ""106730138445"",
    ""location"": ""us"",
    ""id"": ""5e1c7eda-0000-249c-aab6-94eb2c06245a""
  },
  ""source-type"": ""teradata"",
  ""console-log"": false,
  ""silent"": false,
  ""teradata-config"": {
    ""connection"": {
      ""host"": ""localhost""
    },
    ""local-processing-space"": ""testpath"",
    ""database-credentials-file-path"": """",
    ""max-local-storage"": ""200GB"",
    ""gcs-upload-chunk-size"": ""32MB"",
    ""use-tpt"": false,
    ""max-sessions"": 0,
    ""spool-mode"": ""NoSpool"",
    ""max-parallel-upload"": 1,
    ""max-parallel-extract-threads"": 1,
    ""session-charset"": ""UTF8"",
    ""max-unload-file-size"": ""2GB""
  }


After running the command to start the migration process, I come across below error.

TDExpress1620_Sles11:~/Desktop # java -cp terajdbc4.jar:mirroring-agent.jar com.google.cloud.bigquery.dms.Agent --configuration-file=testpath/bq.config
Reading data from gs://data_transfer_agent/latest/version
WARNING: Failed to get the latest released agent version with error: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
Exception in thread ""main"" com.google.api.gax.rpc.UnavailableException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
    at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:69)
    at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:72)
    at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:60)
    at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97)
    at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:68)
    at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1349)
    at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:398)
    at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1024)
    at com.google.common.util.concurrent.AbstractFuture.addListener(AbstractFuture.java:670)
    at com.google.common.util.concurrent.ForwardingListenableFuture.addListener(ForwardingListenableFuture.java:45)
    at com.google.api.core.ApiFutureToListenableFuture.addListener(ApiFutureToListenableFuture.java:52)
    at com.google.common.util.concurrent.Futures.addCallback(Futures.java:1330)
    at com.google.api.core.ApiFutures.addCallback(ApiFutures.java:63)
    at com.google.api.gax.grpc.GrpcExceptionCallable.futureCall(GrpcExceptionCallable.java:67)
    at com.google.api.gax.rpc.AttemptCallable.call(AttemptCallable.java:86)
    at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
    at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
    at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
    Suppressed: com.google.api.gax.rpc.AsyncTaskException: Asynchronous task failed
        at com.google.api.gax.rpc.ApiExceptions.callAndTranslateApiException(ApiExceptions.java:57)
        at com.google.api.gax.rpc.UnaryCallable.call(UnaryCallable.java:112)
        at com.google.cloud.bigquery.datatransfer.v1.DataTransferServiceClient.getTransferConfig(DataTransferServiceClient.java:741)
        at com.google.cloud.bigquery.datatransfer.v1.DataTransferServiceClient.getTransferConfig(DataTransferServiceClient.java:718)
        at com.google.cloud.bigquery.dms.gcloud.TransferServiceClient.getTransferConfig(TransferServiceClient.java:62)
        at com.google.cloud.bigquery.dms.gcloud.TransferServiceClient.getDestinationBucketName(TransferServiceClient.java:94)
        at com.google.cloud.bigquery.dms.Agent.main(Agent.java:235)
Caused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
    at io.grpc.Status.asRuntimeException(Status.java:533)
    at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:490)
    at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
    at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
    at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
    at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:700)
    at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
    at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
    at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
    at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:399)
    at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:500)
    at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:65)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:592)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$700(ClientCallImpl.java:508)
    at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:632)
    at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
    at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
    ... 7 more
Caused by: io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: null: bigquerydatatransfer.googleapis.com/2404:6800:4009:810:0:0:0:200a:443
    at io.grpc.netty.shaded.io.netty.channel.unix.Errors.throwConnectException(Errors.java:104)
    at io.grpc.netty.shaded.io.netty.channel.unix.Socket.connect(Socket.java:257)
    at io.grpc.netty.shaded.io.netty.channel.epoll.AbstractEpollChannel.doConnect0(AbstractEpollChannel.java:730)
    at io.grpc.netty.shaded.io.netty.channel.epoll.AbstractEpollChannel.doConnect(AbstractEpollChannel.java:715)
    at io.grpc.netty.shaded.io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.connect(AbstractEpollChannel.java:557)
    at io.grpc.netty.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1340)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:532)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:517)
    at io.grpc.netty.shaded.io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50)
    at io.grpc.netty.shaded.io.grpc.netty.WriteBufferingAndExceptionHandler.connect(WriteBufferingAndExceptionHandler.java:136)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:532)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.access$1000(AbstractChannelHandlerContext.java:38)
    at io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext$9.run(AbstractChannelHandlerContext.java:522)
    at io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
    at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404)
    at io.grpc.netty.shaded.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:333)
    at io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:905)
    at io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    ... 1 more
Caused by: java.net.NoRouteToHostException
    ... 19 more


Please let me know if somebody has successfully implemented the same or able to figure out the exact issue being faced by me.
",2
432,59530793,storing date in 'CCYYMMDD' format in Teradata,"I would like to store dates in the format CCYYMMDD in Teradata, but I fail to do so. Find below what I tried so far:

query 1:

SEL CAST(CAST(CURRENT_DATE AS DATE FORMAT 'YYYYMMDD') AS VARCHAR(8))
-- Output: 20191230  ==&gt; this works!


query 2:

SEL CAST(CAST(CURRENT_DATE AS DATE FORMAT 'CCYYMMDD') AS VARCHAR(8))
-- output: SELECT Failed.  [3530] Invalid FORMAT string 'CCYYMMDD'.


It seems that the CCYYMMDD is not available in Teradata right away. Is there a workaround?

Tool used: Teradata SQL assistant
",-1,-1,-1.0,"I would like to store dates in the format CCYYMMDD in Teradata, but I fail to do so. Find below what I tried so far:

query 1:

SEL CAST(CAST(CURRENT_DATE AS DATE FORMAT 'YYYYMMDD') AS VARCHAR(8))
-- Output: 20191230  ==&gt; this works!


query 2:

SEL CAST(CAST(CURRENT_DATE AS DATE FORMAT 'CCYYMMDD') AS VARCHAR(8))
-- output: SELECT Failed.  [3530] Invalid FORMAT string 'CCYYMMDD'.


It seems that the CCYYMMDD is not available in Teradata right away. Is there a workaround?

Tool used: Teradata SQL assistant
",3
433,59306591,Performance Issue with Teradata and Insert statement using ODBC driver,"I have some performance issues using the Teradata ODBC driver and the Ado.net when using a Teradata database. Both are available in version 16.20.x. A simple INSERT statement takes between 40ms and 105ms to execute. A comparison was made with a SQLServer that is between 1 and 2ms. All SQL commands are executed exactly the same for both systems. 

Create table query

CREATE TABLE DD_Teradata_ODBC (ID VARCHAR (255) NOT NULL, Feld1 VARCHAR (255), Feld2 VARCHAR (255), 
Feld3 VARCHAR (255), Feld4 VARCHAR (255), Feld5 VARCHAR (255), Feld6 VARCHAR (255), Feld7 VARCHAR (255), 
Feld8 VARCHAR (255), Feld9 VARCHAR (255), Feld10 VARCHAR (255), Feld11 VARCHAR (255), Feld12 VARCHAR 
(255), Feld13 VARCHAR (255) , PRIMARY KEY(ID))


Run this query within a for loop for abput 50.000 times. 

INSERT INTO DD_Teradata_ODBC (ID, Feld1, 
Feld2,Feld3,Feld4,Feld5,Feld6,Feld7,Feld8,Feld9,Feld10,Feld11,Feld12,Feld13) VALUES ('{0}', '{1}', 
'{2}', '{3}', '{4}', '{5}', '{6}', '{7}', '{8}', '{9}', '{10}', '{11}', '{12}', '{13}');


The connection is opened once and the function ExecuteQuery is called within the for loop. 

private static void ExecuteQuery(string query)
{
    var data = new List&lt;string&gt;();
    _connections.ForEach(conn =&gt;
    {
        using (var command = new OdbcCommand(query, conn))
        {
            try
            {
                var sw = Stopwatch.StartNew();
                var result = command.ExecuteNonQuery();
                data.Add($""{conn.Driver}\t {sw.ElapsedMilliseconds}"");
            }
            catch (Exception ex)
            {
                Console.WriteLine(ex.Message);
            }
        }

    });

    var line = string.Join(""\t"", data);
    Console.WriteLine(line);

    File.AppendAllLines(@""c:\temp\teradata_vs_sqlserver.csv"", new[] { line });
}


Id did some performance tracing and faced out, that the Ado.net and the odbc driver behave the same. 








It got really confusing when I made a chart out of the execution times. 


Does anyone have any idea what might go wrong here? What is going on with the timings?
",-1,-1,-1.0,"I have some performance issues using the Teradata ODBC driver and the Ado.net when using a Teradata database. Both are available in version 16.20.x. A simple INSERT statement takes between 40ms and 105ms to execute. A comparison was made with a SQLServer that is between 1 and 2ms. All SQL commands are executed exactly the same for both systems. 

Create table query

CREATE TABLE DD_Teradata_ODBC (ID VARCHAR (255) NOT NULL, Feld1 VARCHAR (255), Feld2 VARCHAR (255), 
Feld3 VARCHAR (255), Feld4 VARCHAR (255), Feld5 VARCHAR (255), Feld6 VARCHAR (255), Feld7 VARCHAR (255), 
Feld8 VARCHAR (255), Feld9 VARCHAR (255), Feld10 VARCHAR (255), Feld11 VARCHAR (255), Feld12 VARCHAR 
(255), Feld13 VARCHAR (255) , PRIMARY KEY(ID))


Run this query within a for loop for abput 50.000 times. 

INSERT INTO DD_Teradata_ODBC (ID, Feld1, 
Feld2,Feld3,Feld4,Feld5,Feld6,Feld7,Feld8,Feld9,Feld10,Feld11,Feld12,Feld13) VALUES ('{0}', '{1}', 
'{2}', '{3}', '{4}', '{5}', '{6}', '{7}', '{8}', '{9}', '{10}', '{11}', '{12}', '{13}');


The connection is opened once and the function ExecuteQuery is called within the for loop. 

private static void ExecuteQuery(string query)
{
    var data = new List&lt;string&gt;();
    _connections.ForEach(conn =&gt;
    {
        using (var command = new OdbcCommand(query, conn))
        {
            try
            {
                var sw = Stopwatch.StartNew();
                var result = command.ExecuteNonQuery();
                data.Add($""{conn.Driver}\t {sw.ElapsedMilliseconds}"");
            }
            catch (Exception ex)
            {
                Console.WriteLine(ex.Message);
            }
        }

    });

    var line = string.Join(""\t"", data);
    Console.WriteLine(line);

    File.AppendAllLines(@""c:\temp\teradata_vs_sqlserver.csv"", new[] { line });
}


Id did some performance tracing and faced out, that the Ado.net and the odbc driver behave the same. 








It got really confusing when I made a chart out of the execution times. 


Does anyone have any idea what might go wrong here? What is going on with the timings?
",3
434,59239610,Is there a way to export a blob column in teradata to a CSV file via a bteq script?,"I have the following shell script that has a bteq call to export a Teradata BLOB field to a text file. I discovered if the file is > 64K it does not get written. I know I can set the INDICDATA DEFERLIMITS=0,0 and then it will write the name of the BLOB file it creates in the filename I supplied. The filename I supply contains a path and that file is being written in the correct location. The BLOB file that is being created I cannot find on the server. I thought it would be created in the same directory but I don't see it. Do I have to set that location also? If so, how?

-------------------------------
Shell Script with BTEQ:
-----------------------------------
#!/bin/sh
modelId=$1
runId=$2
filename=$3
file1=""/export/home/pc8admin/pc8store/infa_shared/bin/ENGV_D042A/${filename}""

echo $modelId
echo $runId
echo $filename 
echo $file1

bteq  &lt;&lt;lbl_btq


.RUN FILE=/export/home/pc8admin/pc8store/infa_shared/OAS/td_mlbtq_engv_etl.scr

.EXPORT INDICDATA FILE = $file1;

.SET LARGEDATAMODE ON;
.EXPORT LDOPREFIX 'doc_'
.EXPORT LDOSUFFIX 'txt' 

DATABASE XXX_XX;
.IF errorlevel &lt;&gt; 0 THEN .QUIT errorcode;

SELECT  IL.FIL_OBJ_UD
FROM    XXX_XX.SIMLTR_CFG_INV_LOAD IL
INNER JOIN XXX_XX.SIMLTR_CFG_INV_LOAD_STAT ST ON IL.FIL_ID = ST.FIL_ID
WHERE ST.MDL_ID = $modelId AND ST.SIMLTR_RUN_ID = $runId AND ST.INV_TYP_ID = 'E'
AND IL.FIL_ID = 5;

.IF errorlevel &lt;&gt; 0 THEN .QUIT errorcode;

.END EXPORT 
.LOGOFF
.EXIT 0

lbl_btq



The filename I supplied contains the text: doc_FIL_OBJ_UD_r1.txt

I can't find the doc_FIL_OBJ_UD_r1.txt file. 
",-1,-1,-1.0,"I have the following shell script that has a bteq call to export a Teradata BLOB field to a text file. I discovered if the file is > 64K it does not get written. I know I can set the INDICDATA DEFERLIMITS=0,0 and then it will write the name of the BLOB file it creates in the filename I supplied. The filename I supply contains a path and that file is being written in the correct location. The BLOB file that is being created I cannot find on the server. I thought it would be created in the same directory but I don't see it. Do I have to set that location also? If so, how?

-------------------------------
Shell Script with BTEQ:
-----------------------------------
#!/bin/sh
modelId=$1
runId=$2
filename=$3
file1=""/export/home/pc8admin/pc8store/infa_shared/bin/ENGV_D042A/${filename}""

echo $modelId
echo $runId
echo $filename 
echo $file1

bteq  &lt;&lt;lbl_btq


.RUN FILE=/export/home/pc8admin/pc8store/infa_shared/OAS/td_mlbtq_engv_etl.scr

.EXPORT INDICDATA FILE = $file1;

.SET LARGEDATAMODE ON;
.EXPORT LDOPREFIX 'doc_'
.EXPORT LDOSUFFIX 'txt' 

DATABASE XXX_XX;
.IF errorlevel &lt;&gt; 0 THEN .QUIT errorcode;

SELECT  IL.FIL_OBJ_UD
FROM    XXX_XX.SIMLTR_CFG_INV_LOAD IL
INNER JOIN XXX_XX.SIMLTR_CFG_INV_LOAD_STAT ST ON IL.FIL_ID = ST.FIL_ID
WHERE ST.MDL_ID = $modelId AND ST.SIMLTR_RUN_ID = $runId AND ST.INV_TYP_ID = 'E'
AND IL.FIL_ID = 5;

.IF errorlevel &lt;&gt; 0 THEN .QUIT errorcode;

.END EXPORT 
.LOGOFF
.EXIT 0

lbl_btq



The filename I supplied contains the text: doc_FIL_OBJ_UD_r1.txt

I can't find the doc_FIL_OBJ_UD_r1.txt file. 
",3
435,59095750,Teradata SQL getting last not null value,"I have a table in Teradata that looks like this: 

Account_Num Install_Due_Dt  Install_Num Install_Pay_Dt
12805196    12/08/2019                1     12/08/2019
12805196    10/09/2019                2              ?
12805196    10/10/2019                3              ?
12805196    11/11/2019                4     13/09/2019
12805196    10/12/2019                5              ?


I need to fill the column Install_Pay_Dt with the first not null value. For example, it should look like this:

Account_Num Install_Due_Dt  Install_Num Install_Pay_Dt
12805196    12/08/2019                1     12/08/2019
12805196    10/09/2019                2     12/08/2019
12805196    10/10/2019                3     12/08/2019
12805196    11/11/2019                4     13/09/2019
12805196    10/12/2019                5     13/09/2019


I'm using Teradata 15 so I can't use lag. I've been searching a lot but I can't find a solution. The ID column is Account_Num and the order column is Install_num.

I've tried to do something like this: 

coalesce(Install_Pay_Dt, MAX(lag_) 
OVER(PARTITION BY 1 ORDER BY Install_Num asc
ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING)) as lag


But it only fills the second row. 
",-1,-1,-1.0,"I have a table in Teradata that looks like this: 

Account_Num Install_Due_Dt  Install_Num Install_Pay_Dt
12805196    12/08/2019                1     12/08/2019
12805196    10/09/2019                2              ?
12805196    10/10/2019                3              ?
12805196    11/11/2019                4     13/09/2019
12805196    10/12/2019                5              ?


I need to fill the column Install_Pay_Dt with the first not null value. For example, it should look like this:

Account_Num Install_Due_Dt  Install_Num Install_Pay_Dt
12805196    12/08/2019                1     12/08/2019
12805196    10/09/2019                2     12/08/2019
12805196    10/10/2019                3     12/08/2019
12805196    11/11/2019                4     13/09/2019
12805196    10/12/2019                5     13/09/2019


I'm using Teradata 15 so I can't use lag. I've been searching a lot but I can't find a solution. The ID column is Account_Num and the order column is Install_num.

I've tried to do something like this: 

coalesce(Install_Pay_Dt, MAX(lag_) 
OVER(PARTITION BY 1 ORDER BY Install_Num asc
ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING)) as lag


But it only fills the second row. 
",3
436,59016954,what is the correct format for inserting dates from text file using Teradata SQL Assistant,"I am trying to insert data from a tab delimited text file to a Teradata table i created.
It seems Teradata SQL Assistant does not recognize the dates in the file as dates

If I try the following code

create set table my_table
(
    update_date date
    , status_code smallint
)

INSERT INTO my_table 
VALUES (?, ?)


I recieve the error: Invalid value for update_date

However, when I try the code

create set table my_table
(
    update_date varchar(32)
    , status_code smallint
)

INSERT INTO my_table 
VALUES (?, ?)


The upload works smoothly.
I tried several formats: 28/08/2019, 2019-08-28 and also '2019-08-28'. All had yielded the same error
",0,-1,-1.0,"I am trying to insert data from a tab delimited text file to a Teradata table i created.
It seems Teradata SQL Assistant does not recognize the dates in the file as dates

If I try the following code

create set table my_table
(
    update_date date
    , status_code smallint
)

INSERT INTO my_table 
VALUES (?, ?)


I recieve the error: Invalid value for update_date

However, when I try the code

create set table my_table
(
    update_date varchar(32)
    , status_code smallint
)

INSERT INTO my_table 
VALUES (?, ?)


The upload works smoothly.
I tried several formats: 28/08/2019, 2019-08-28 and also '2019-08-28'. All had yielded the same error
",3
437,58886646,How to generate this teradata table?,"I would like to create a Teradata table with a table structure that looks like below.

12-29-2008 will be start date. Basically I'd like to assign this as the start date with week number corresponding to this as 209. Everything else afterwards should follow automatically i.e. calculated. 

Column 1/Weekstart/Monday     Week num
12/28/2008                    209
1/5/2009                      210
...
12/27/2021                    887


I tried below but it could only create one row.

Please see image attached for an idea of what I want as the final dataset. 
Table structure image is below

SEL 
  WeekBegin
, (WeekBegin - DATE '2008-12-29') / 7 + 209 as week_num
FROM Sys_Calendar.BusinessCalendar 
WHERE calendar_date = DATE '2008-12-29'
; 


Thanks for your help! 

Edit: After doing below-

SEL WeekBegin
, (WeekBegin - DATE '2008-12-29')/7 + 209 as week_num
FROM Sys_Calendar.BusinessCalendar 
WHERE 
  calendar_date &gt;= DATE '2008-12-29' 
  and calendar_date &lt;= DATE '2021-12-27';


My result looks like this:
ResultsImage1
ResultsImage2

It is not displaying the Monday of the week. Instead I see Sunday at the end of the table. Also the weeknum is 886, instead it should be 887.
",-1,-1,-1.0,"I would like to create a Teradata table with a table structure that looks like below.

12-29-2008 will be start date. Basically I'd like to assign this as the start date with week number corresponding to this as 209. Everything else afterwards should follow automatically i.e. calculated. 

Column 1/Weekstart/Monday     Week num
12/28/2008                    209
1/5/2009                      210
...
12/27/2021                    887


I tried below but it could only create one row.

Please see image attached for an idea of what I want as the final dataset. 
Table structure image is below

SEL 
  WeekBegin
, (WeekBegin - DATE '2008-12-29') / 7 + 209 as week_num
FROM Sys_Calendar.BusinessCalendar 
WHERE calendar_date = DATE '2008-12-29'
; 


Thanks for your help! 

Edit: After doing below-

SEL WeekBegin
, (WeekBegin - DATE '2008-12-29')/7 + 209 as week_num
FROM Sys_Calendar.BusinessCalendar 
WHERE 
  calendar_date &gt;= DATE '2008-12-29' 
  and calendar_date &lt;= DATE '2021-12-27';


My result looks like this:
ResultsImage1
ResultsImage2

It is not displaying the Monday of the week. Instead I see Sunday at the end of the table. Also the weeknum is 886, instead it should be 887.
",3
438,58880200,Data load with Teradata TPT failing,"Here i'm trying load a csv file into teradata tables using TPT utility
,but is filing with an error:

Here is my TPT script: 

DEFINE JOB test_tpt
DESCRIPTION 'Load a Teradata table from a file'
(
DEFINE SCHEMA SCHEMA_EMP_NAME
(
NAME VARCHAR(50),
AGE VARCHAR(50)    
);

DEFINE OPERATOR od_EMP_NAME
TYPE DDL
ATTRIBUTES
(
VARCHAR PrivateLogName = 'tpt_log',
VARCHAR LogonMech      = 'LDAP',
VARCHAR TdpId          = 'TeraDev',
VARCHAR UserName       = 'user',
VARCHAR UserPassword   = 'pwd',
VARCHAR ErrorList      = '3807'
);

DEFINE OPERATOR op_EMP_NAME
TYPE DATACONNECTOR PRODUCER
SCHEMA SCHEMA_EMP_NAME
ATTRIBUTES
(
VARCHAR  DirectoryPath= '/home/hadoop/retail/',
VARCHAR  FileName = 'emp_age.csv',
VARCHAR  Format   = 'Delimited',
VARCHAR  OpenMode = 'Read',
VARCHAR  TextDelimiter =','
);


DEFINE OPERATOR ol_EMP_NAME
TYPE LOAD
SCHEMA *
ATTRIBUTES
(
VARCHAR LogonMech           = 'LDAP',
VARCHAR TdpId          = 'TeraDev',
VARCHAR UserName       = 'user',
VARCHAR UserPassword   = 'pwd',
VARCHAR  LogTable     =  'EMP_NAME_LG',
VARCHAR  ErrorTable1  =  'EMP_NAME_ET',
VARCHAR  ErrorTable2  =  'EMP_NAME_UV',
VARCHAR  TargetTable  =  'EMP_NAME'
);

STEP stSetup_Tables
(
APPLY
('DROP TABLE EMP_NAME_LG;'),
('DROP TABLE EMP_NAME_ET;'),
('DROP TABLE EMP_NAME_UV;'),
('DROP TABLE EMP_NAME;'),
('CREATE TABLE EMP_NAME(NAME VARCHAR(50), AGE VARCHAR(2));')
TO OPERATOR (od_EMP_NAME);
);

STEP stLOAD_FILE_NAME
(
APPLY
('INSERT INTO EMP_NAME
(Name,Age)
VALUES
(:Name,:Age);
')
TO OPERATOR (ol_EMP_NAME)
SELECT * FROM OPERATOR(op_EMP_NAME);
);
);


Call TPT:

tbuild -f test_tpt.sql


Above TPT script is failing with following error:

Teradata Parallel Transporter Version 15.10.01.02 64-Bit
TPT_INFRA: Syntax error at or near line 6 of Job Script File 'test_tpt.sql':
TPT_INFRA: At ""NAME"" missing RPAREN_ in Rule: Explicit Schema Element List
TPT_INFRA: Syntax error at or near line 8 of Job Script File 'test_tpt.sql':
TPT_INFRA: TPT03020: Rule: DEFINE SCHEMA


Compilation failed due to errors. Execution Plan was not generated.
Job script compilation failed                                                                .

Am i missing any detail in here? 
",-1,-1,-1.0,"Here i'm trying load a csv file into teradata tables using TPT utility
,but is filing with an error:

Here is my TPT script: 

DEFINE JOB test_tpt
DESCRIPTION 'Load a Teradata table from a file'
(
DEFINE SCHEMA SCHEMA_EMP_NAME
(
NAME VARCHAR(50),
AGE VARCHAR(50)    
);

DEFINE OPERATOR od_EMP_NAME
TYPE DDL
ATTRIBUTES
(
VARCHAR PrivateLogName = 'tpt_log',
VARCHAR LogonMech      = 'LDAP',
VARCHAR TdpId          = 'TeraDev',
VARCHAR UserName       = 'user',
VARCHAR UserPassword   = 'pwd',
VARCHAR ErrorList      = '3807'
);

DEFINE OPERATOR op_EMP_NAME
TYPE DATACONNECTOR PRODUCER
SCHEMA SCHEMA_EMP_NAME
ATTRIBUTES
(
VARCHAR  DirectoryPath= '/home/hadoop/retail/',
VARCHAR  FileName = 'emp_age.csv',
VARCHAR  Format   = 'Delimited',
VARCHAR  OpenMode = 'Read',
VARCHAR  TextDelimiter =','
);


DEFINE OPERATOR ol_EMP_NAME
TYPE LOAD
SCHEMA *
ATTRIBUTES
(
VARCHAR LogonMech           = 'LDAP',
VARCHAR TdpId          = 'TeraDev',
VARCHAR UserName       = 'user',
VARCHAR UserPassword   = 'pwd',
VARCHAR  LogTable     =  'EMP_NAME_LG',
VARCHAR  ErrorTable1  =  'EMP_NAME_ET',
VARCHAR  ErrorTable2  =  'EMP_NAME_UV',
VARCHAR  TargetTable  =  'EMP_NAME'
);

STEP stSetup_Tables
(
APPLY
('DROP TABLE EMP_NAME_LG;'),
('DROP TABLE EMP_NAME_ET;'),
('DROP TABLE EMP_NAME_UV;'),
('DROP TABLE EMP_NAME;'),
('CREATE TABLE EMP_NAME(NAME VARCHAR(50), AGE VARCHAR(2));')
TO OPERATOR (od_EMP_NAME);
);

STEP stLOAD_FILE_NAME
(
APPLY
('INSERT INTO EMP_NAME
(Name,Age)
VALUES
(:Name,:Age);
')
TO OPERATOR (ol_EMP_NAME)
SELECT * FROM OPERATOR(op_EMP_NAME);
);
);


Call TPT:

tbuild -f test_tpt.sql


Above TPT script is failing with following error:

Teradata Parallel Transporter Version 15.10.01.02 64-Bit
TPT_INFRA: Syntax error at or near line 6 of Job Script File 'test_tpt.sql':
TPT_INFRA: At ""NAME"" missing RPAREN_ in Rule: Explicit Schema Element List
TPT_INFRA: Syntax error at or near line 8 of Job Script File 'test_tpt.sql':
TPT_INFRA: TPT03020: Rule: DEFINE SCHEMA


Compilation failed due to errors. Execution Plan was not generated.
Job script compilation failed                                                                .

Am i missing any detail in here? 
",3
439,58800452,Teradata session abort AMP CPU threshold,"I have an SSIS package that is refreshing SSAS cube partitions. Each partition is a SQL query running against Teradata. I am using ""NET data provider for Teradata"" in the SSIS package. The SSAS cube is using ""Teradata.Client.Provider"". Any particular query executes just fine. However, after 5-10 iterations, I get a SESSION ABORT. After some investigation I found out that all queries are using in the same session:

sel * from DBC.DBQLogTbl where sessionid = 123456


This query returns multiple rows with queries from the loop. Each query by itself does not exceed the CPU AMP threshold, however combined AMP usage exceeds the threshold.
I have configured the connection strings in SSIS and SSAS as following:


SSAS: Connection Timeout=180;Authentication Mechanism=LDAP;Response Buffer Size=64000;User Id=***;Data Source=my.production.server;Persist Security Info=True;Session Character Set=UTF16;Connection Pooling=False;Connection Pooling Timeout=15;Command Timeout=900
SSIS: Data Source=my.production.server;User Id=***;Authentication Mechanism=LDAP;Persist Security Info=True;Session Character Set=UTF16;Connection Pooling=False;


I have disabled parallel execution on the cube. I have disabled the connection pooling as you can see from the connection string. I have set ""Maximum number of connections"" to 1 on the cube.

Any ideas on how I can force each query to have its own session on Teradata?

Update

After some investigation, I am pretty sure there is a bug hidden somewhere in the cube or database driver. The cube just ignores the 


  Connection Pooling=false;


parameter and executes each SQL command in the same session. So I've looked into how the driver handles pooling. According to the documentation, the connection string is used as a key when storing a connection resource in the pool. Therefore if you supply a different connection string the driver handles such case as ""new connection"" and does not use the one already existing in the pool. Now I have a script task after each iteration that ""shuffles"" the parameters of a given connection string and updates the connection on the cube. This is definitely a hack and I do not recommend this as a solution but desperate times call for desperate measures...

string dcs = @""Connection Timeout=180;Authentication Mechanism=LDAP;Response Buffer Size=64000;User Id=***;Data Source=my.production.server;Persist Security Info=True;Session Character Set=UTF16;Connection Pooling=False;Connection Pooling Timeout=15;Command Timeout=900"";
Random r = new Random();
var shuffled = dcs.Split(';').OrderBy(x =&gt; r.Next()).ToArray();
var newCs = string.Join("";"", shuffled);

",-1,-1,-1.0,"I have an SSIS package that is refreshing SSAS cube partitions. Each partition is a SQL query running against Teradata. I am using ""NET data provider for Teradata"" in the SSIS package. The SSAS cube is using ""Teradata.Client.Provider"". Any particular query executes just fine. However, after 5-10 iterations, I get a SESSION ABORT. After some investigation I found out that all queries are using in the same session:

sel * from DBC.DBQLogTbl where sessionid = 123456


This query returns multiple rows with queries from the loop. Each query by itself does not exceed the CPU AMP threshold, however combined AMP usage exceeds the threshold.
I have configured the connection strings in SSIS and SSAS as following:


SSAS: Connection Timeout=180;Authentication Mechanism=LDAP;Response Buffer Size=64000;User Id=***;Data Source=my.production.server;Persist Security Info=True;Session Character Set=UTF16;Connection Pooling=False;Connection Pooling Timeout=15;Command Timeout=900
SSIS: Data Source=my.production.server;User Id=***;Authentication Mechanism=LDAP;Persist Security Info=True;Session Character Set=UTF16;Connection Pooling=False;


I have disabled parallel execution on the cube. I have disabled the connection pooling as you can see from the connection string. I have set ""Maximum number of connections"" to 1 on the cube.

Any ideas on how I can force each query to have its own session on Teradata?

Update

After some investigation, I am pretty sure there is a bug hidden somewhere in the cube or database driver. The cube just ignores the 


  Connection Pooling=false;


parameter and executes each SQL command in the same session. So I've looked into how the driver handles pooling. According to the documentation, the connection string is used as a key when storing a connection resource in the pool. Therefore if you supply a different connection string the driver handles such case as ""new connection"" and does not use the one already existing in the pool. Now I have a script task after each iteration that ""shuffles"" the parameters of a given connection string and updates the connection on the cube. This is definitely a hack and I do not recommend this as a solution but desperate times call for desperate measures...

string dcs = @""Connection Timeout=180;Authentication Mechanism=LDAP;Response Buffer Size=64000;User Id=***;Data Source=my.production.server;Persist Security Info=True;Session Character Set=UTF16;Connection Pooling=False;Connection Pooling Timeout=15;Command Timeout=900"";
Random r = new Random();
var shuffled = dcs.Split(';').OrderBy(x =&gt; r.Next()).ToArray();
var newCs = string.Join("";"", shuffled);

",1
440,58547697,Teradata connection string using Windows Authentication,"I am using Python3.6 and pyodbc to try and connect to Teradata. I need this for a script I want to hand off to an end user to update from data. I would prefer that each user not have to be instructed to setup their own DSN connection with a specific name I hard encode into the script.

I think I have the driver correct now as well as the server and DB name. What I can't figure out is how to get the connection string to not require a username and password. 
Helpful info:
This is for windows OS. When I go to ODBC connections in windows, connection mechanism is listed as LDAP and i can connect through the Teradata program as well.

What does work:
Connection directly through the Teradata program (showing Teradata itself knows who I am)
Using DSN=""DSN_name"" where DSN_Name is the name under ODBC connections found in windows. Showing I can connect via DSN.

-Looking at ODBC Data Source Admin shows Driver matches what I have. ""Server Name or ID"" matches ""servername"" below.

Tried:

connect_string = 'Driver={Teradata Database ODBC Driver 
16.20};DBCName=servername;Database=db_name;MechanismName=LDAP;UseIntegratedSecurity=1;'
con = pyodbc.connect(connect_string)


which gives:

pyodbc.Error: ('HY000', '[HY000] [Teradata][ODBC] (11210) Operation not allowed during the transaction state. (11210) (SQLExecDirectW)')

connect_string = 'Driver={Teradata Database ODBC Driver 
16.20};DBCName=servername;Database=db_name;MechanismName=LDAP;'
con = pyodbc.connect(connect_string)


or

connect_string = 'Driver={Teradata Database ODBC Driver 
16.20};DBCName=servername;Database=db_name;Authentication=LDAP;'
con = pyodbc.connect(connect_string)


which gives:
pyodbc.InterfaceError: ('28000', '[28000] [Teradata][ODBC Teradata Driver] (2) Unable to logon with Authentication Mechanism selected. (2) (SQLDriverConnect)')

I have also tried a few other suggested authentication mechanisms in addition to LDAP, but this is the one listed in Teradata and ODBC connections.

In SQL_Server I use something like trusted_connection=yes to achieve the same effect I desire which doesn't seem to work in the above examples.

error:
pyodbc.OperationalError: ('08001', '[08001] [Teradata][ODBC] (10380) Unable to establish connection 
with data source. Missing settings: {[Password] [Username]} (10380) (SQLDriverConnect)')

",1,-1,-1.0,"I am using Python3.6 and pyodbc to try and connect to Teradata. I need this for a script I want to hand off to an end user to update from data. I would prefer that each user not have to be instructed to setup their own DSN connection with a specific name I hard encode into the script.

I think I have the driver correct now as well as the server and DB name. What I can't figure out is how to get the connection string to not require a username and password. 
Helpful info:
This is for windows OS. When I go to ODBC connections in windows, connection mechanism is listed as LDAP and i can connect through the Teradata program as well.

What does work:
Connection directly through the Teradata program (showing Teradata itself knows who I am)
Using DSN=""DSN_name"" where DSN_Name is the name under ODBC connections found in windows. Showing I can connect via DSN.

-Looking at ODBC Data Source Admin shows Driver matches what I have. ""Server Name or ID"" matches ""servername"" below.

Tried:

connect_string = 'Driver={Teradata Database ODBC Driver 
16.20};DBCName=servername;Database=db_name;MechanismName=LDAP;UseIntegratedSecurity=1;'
con = pyodbc.connect(connect_string)


which gives:

pyodbc.Error: ('HY000', '[HY000] [Teradata][ODBC] (11210) Operation not allowed during the transaction state. (11210) (SQLExecDirectW)')

connect_string = 'Driver={Teradata Database ODBC Driver 
16.20};DBCName=servername;Database=db_name;MechanismName=LDAP;'
con = pyodbc.connect(connect_string)


or

connect_string = 'Driver={Teradata Database ODBC Driver 
16.20};DBCName=servername;Database=db_name;Authentication=LDAP;'
con = pyodbc.connect(connect_string)


which gives:
pyodbc.InterfaceError: ('28000', '[28000] [Teradata][ODBC Teradata Driver] (2) Unable to logon with Authentication Mechanism selected. (2) (SQLDriverConnect)')

I have also tried a few other suggested authentication mechanisms in addition to LDAP, but this is the one listed in Teradata and ODBC connections.

In SQL_Server I use something like trusted_connection=yes to achieve the same effect I desire which doesn't seem to work in the above examples.

error:
pyodbc.OperationalError: ('08001', '[08001] [Teradata][ODBC] (10380) Unable to establish connection 
with data source. Missing settings: {[Password] [Username]} (10380) (SQLDriverConnect)')

",1
441,57556867,Teradata Viewpoint on Teradata Express 16.10 is not working,"I understand we have multiple post regarding viewpoint is not working on TD Express but didn't find solution on it and also those people have issues in starting the viewpoint using vp-control.sh start command but my issue is different.

I have installed TD Express 16.10 and everything is working perfectly except viewpoint.

I used Viewpoint Start icon to start the viewpoint and it started without any issue,



I also checked the status of viewpoint using vp-control.sh status and all services are running except the viewpoint is Unused



Then I tried to open Viewpoint in firefox provided in the image of TD Express by entering the various options like localhost\c , 127.0.0.1\c , 127.0.0.1:80,etc but Viewpoint did not open,



I tried wget localhost then I could discover that it failed to establish connection,



Can someone guide me or point any issues that I am not able to understand here, As per below post, it looks straight forward but it is not,

https://downloads.teradata.com/database/articles/teradata-express-14-0-for-vmware-user-guide
",-1,-1,-1.0,"I understand we have multiple post regarding viewpoint is not working on TD Express but didn't find solution on it and also those people have issues in starting the viewpoint using vp-control.sh start command but my issue is different.

I have installed TD Express 16.10 and everything is working perfectly except viewpoint.

I used Viewpoint Start icon to start the viewpoint and it started without any issue,



I also checked the status of viewpoint using vp-control.sh status and all services are running except the viewpoint is Unused



Then I tried to open Viewpoint in firefox provided in the image of TD Express by entering the various options like localhost\c , 127.0.0.1\c , 127.0.0.1:80,etc but Viewpoint did not open,



I tried wget localhost then I could discover that it failed to establish connection,



Can someone guide me or point any issues that I am not able to understand here, As per below post, it looks straight forward but it is not,

https://downloads.teradata.com/database/articles/teradata-express-14-0-for-vmware-user-guide
",3
442,57472121,Passing multiple values as parameter to teradata view,"We are trying to pass values from report to teradata view as parameter. How do we  pass multiple values to teradata view ? 

AND (v_fact_xyz in (?) or 'ALL' in (?)) 

is the line of code written currently
where ? can be single value('Abd, EFG(ORM)') or multiple values like these

The report is working fine with single parameter passed but throws error while passing multiple values

.net data provider for teradata 110083 error.
A Null has been specified as the value for a parameter
",-1,-1,-1.0,"We are trying to pass values from report to teradata view as parameter. How do we  pass multiple values to teradata view ? 

AND (v_fact_xyz in (?) or 'ALL' in (?)) 

is the line of code written currently
where ? can be single value('Abd, EFG(ORM)') or multiple values like these

The report is working fine with single parameter passed but throws error while passing multiple values

.net data provider for teradata 110083 error.
A Null has been specified as the value for a parameter
",3
443,57401288,how can I update a top 1 records in teradata,"I am trying to update the first record clubcode + campaigncode + region based on their tenureyear. (Tenureyears is causing the duplicate records but I need them)

This works in T-SQL but not Teradata

update MYTable
set qtyupdate =(SELECT quantity
  FROM codes
  WHERE MYTable.clubcode=codes.clubcode 
        AND MYTable.campaigncode=codes.campaigncode
        and MYTable.region=codes.region)
where identity_column in (select top 1 x.identity_column from MYTable X where x.qtyupdate = MMYTable.qtyupdate order by x.tenureyears)


It is only updating 1 records on the table and I also get error 3706 cannot use order by in subqueries.
",-1,-1,-1.0,"I am trying to update the first record clubcode + campaigncode + region based on their tenureyear. (Tenureyears is causing the duplicate records but I need them)

This works in T-SQL but not Teradata

update MYTable
set qtyupdate =(SELECT quantity
  FROM codes
  WHERE MYTable.clubcode=codes.clubcode 
        AND MYTable.campaigncode=codes.campaigncode
        and MYTable.region=codes.region)
where identity_column in (select top 1 x.identity_column from MYTable X where x.qtyupdate = MMYTable.qtyupdate order by x.tenureyears)


It is only updating 1 records on the table and I also get error 3706 cannot use order by in subqueries.
",3
444,57169016,Defining number of decimals in teradata column during Select statement,"In teradata proc sql in SAS Enterprise guide environment, I am trying to create a column by multiplying two fields, but I have been having overflow issue. How do I specify the number of decimals in the resulting field during Select statment? numeric(20,2)

proc sql exec feedback stimer;

connect to teradata as teravw (authdomain=teradataauth TDPID=DWPROD2 MODE=TERADATA SCHEMA=EDW_NOPHI CONNECTION=GLOBAL);

CREATE TABLE WORK.tbl1 AS
SELECT * from connection to teravw
(SELECT

...
    ,case 
        when PCL.CLM_SOR_CD = '512' then cast(round(sum(PCL.PRCG_WHLSL_ALWBL_COST_AMT * CL.PAID_SRVC_UNIT_CNT) ,0.01) as numeric(20,2))
        else SUM(PCL.PRCG_WHLSL_ALWBL_COST_AMT) 
    end AS WAC 
...

);

disconnect from teravw;
quit;
run;


error message:

ERROR: Teradata row not delivered (trget): Numeric overflow occurred during computation.

",-1,-1,-1.0,"In teradata proc sql in SAS Enterprise guide environment, I am trying to create a column by multiplying two fields, but I have been having overflow issue. How do I specify the number of decimals in the resulting field during Select statment? numeric(20,2)

proc sql exec feedback stimer;

connect to teradata as teravw (authdomain=teradataauth TDPID=DWPROD2 MODE=TERADATA SCHEMA=EDW_NOPHI CONNECTION=GLOBAL);

CREATE TABLE WORK.tbl1 AS
SELECT * from connection to teravw
(SELECT

...
    ,case 
        when PCL.CLM_SOR_CD = '512' then cast(round(sum(PCL.PRCG_WHLSL_ALWBL_COST_AMT * CL.PAID_SRVC_UNIT_CNT) ,0.01) as numeric(20,2))
        else SUM(PCL.PRCG_WHLSL_ALWBL_COST_AMT) 
    end AS WAC 
...

);

disconnect from teravw;
quit;
run;


error message:

ERROR: Teradata row not delivered (trget): Numeric overflow occurred during computation.

",3
445,57081765,Connecting to teradata using DBI R package,"I am trying to connect to Teradata database from R terminal using code below, 

 ""&gt; con &lt;- DBI::dbConnect(odbc::odbc(),
  + Driver = ""Teradata"",
  + Host   = ""xxxx"",
  + DBCName = ""xxxx"",
  + UID    = ""xxxx""
  + )""


I have created /etc/odbc.ini and /etc/odbcinst.ini and below are the contents of the same, 

odbc.ini:

 [ODBC]
 InstallDir=/opt/teradata/client/16.20/odbc_64
 DataEncryption='ON'

 [ngmtdd01]
 Driver=/opt/teradata/client/16.20/odbc_64/lib/tdataodbc_sb64.so
 Description=Teradata Dev
 DBCName=xxxx.dev.uk.capitalone.com
 LoginTimeout=120


odbcinst.ini

[ODBC DRIVERS]
Teradata Database ODBC Driver 16.20=Installed

[Teradata Database ODBC Driver 16.20]
Driver=/opt/teradata/client/16.20/odbc_64/lib/tdataodbc_sb64.so
APILevel=CORE
ConnectFunctions=YYY
DriverODBCVer=3.51
SQLLevel=1


I expected to get connected to the teradata cli but I am getting below error. 


  Error: nanodbc/nanodbc.cpp:950: IM002: [DataDirect][ODBC lib] Data source name not found and no default driver specified"" 

",-1,-1,-1.0,"I am trying to connect to Teradata database from R terminal using code below, 

 ""&gt; con &lt;- DBI::dbConnect(odbc::odbc(),
  + Driver = ""Teradata"",
  + Host   = ""xxxx"",
  + DBCName = ""xxxx"",
  + UID    = ""xxxx""
  + )""


I have created /etc/odbc.ini and /etc/odbcinst.ini and below are the contents of the same, 

odbc.ini:

 [ODBC]
 InstallDir=/opt/teradata/client/16.20/odbc_64
 DataEncryption='ON'

 [ngmtdd01]
 Driver=/opt/teradata/client/16.20/odbc_64/lib/tdataodbc_sb64.so
 Description=Teradata Dev
 DBCName=xxxx.dev.uk.capitalone.com
 LoginTimeout=120


odbcinst.ini

[ODBC DRIVERS]
Teradata Database ODBC Driver 16.20=Installed

[Teradata Database ODBC Driver 16.20]
Driver=/opt/teradata/client/16.20/odbc_64/lib/tdataodbc_sb64.so
APILevel=CORE
ConnectFunctions=YYY
DriverODBCVer=3.51
SQLLevel=1


I expected to get connected to the teradata cli but I am getting below error. 


  Error: nanodbc/nanodbc.cpp:950: IM002: [DataDirect][ODBC lib] Data source name not found and no default driver specified"" 

",1
446,60230269,Python Teradataml Connection,"I'm attempting to use the Python teradataml library for Teradata to create a simple database connection using the logmech of LDAP.  However, i'm greeted with an attribute error of 'NoneType' ojbect has no attribute dialect in relation to sqlalchemy_engine.dialect.

My code to create the connection with the hostname changed

import teradataml as tdml
from teradataml import *

hostname = ""address of host""
schema = ""DL_RBA_DSS""

td_context = create_context(host = hostname, logmech='LDAP', temp_database_name = schema)
print(td_context)


When running the code it gives me the following error:

File ""c:/Python/teradatamlconnection.py"", line 7, in &lt;module&gt;
    td_context = create_context(host = hostname, logmech='LDAP', temp_database_name = schema)
  File ""C:\Users\UserID\AppData\Local\Programs\Python\Python36\lib\site-packages\teradataml\context\context.py"", line 287, in create_context
    _load_function_aliases()
  File ""C:\Users\UserID\AppData\Local\Programs\Python\Python36\lib\site-packages\teradataml\context\context.py"", line 532, in _load_function_aliases
    __set_vantage_version()
  File ""C:\Users\UserID\AppData\Local\Programs\Python\Python36\lib\site-packages\teradataml\context\context.py"", line 562, in __set_vantage_version
    if td_sqlalchemy_engine.dialect.has_table(td_sqlalchemy_engine, ""versionInfo"", schema=""pm""):
AttributeError: 'NoneType' object has no attribute 'dialect'


I have tried to update the teradatasqlalchemy and sqlalchemy library, but i still am greeted with the error.  It shows all requirements are satisfied:

PS C:\Python\dsssecurity> pip install teradatasqlalchemy --upgrade

Requirement already up-to-date: teradatasqlalchemy in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (16.20.0.8)
Requirement already satisfied, skipping upgrade: teradatasql in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradatasqlalchemy) (16.20.0.52)
Requirement already satisfied, skipping upgrade: sqlalchemy&gt;=1.2.11 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradatasqlalchemy) (1.3.13)
Requirement already satisfied, skipping upgrade: pycryptodome in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradatasql-&gt;teradatasqlalchemy) (3.9.0)
PS C:\Python\dsssecurity&gt; pip install teradataml --upgrade
Requirement already up-to-date: teradataml in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (16.20.0.4)
Requirement already satisfied, skipping upgrade: teradatasql in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradataml) (16.20.0.52)
Requirement already satisfied, skipping upgrade: pandas&gt;=0.22 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradataml) (1.0.1)
Requirement already satisfied, skipping upgrade: psutil in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradataml) (5.6.3)
Requirement already satisfied, skipping upgrade: teradatasqlalchemy&gt;=16.20.0.8 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradataml) (16.20.0.8)
Requirement already satisfied, skipping upgrade: pycryptodome in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradatasql-&gt;teradataml) (3.9.0)
Requirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from pandas&gt;=0.22-&gt;teradataml) (1.18.1)
Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from pandas&gt;=0.22-&gt;teradataml) (2017.2)
Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.6.1 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from pandas&gt;=0.22-&gt;teradataml) (2.6.1)
Requirement already satisfied, skipping upgrade: sqlalchemy&gt;=1.2.11 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradatasqlalchemy&gt;=16.20.0.8-&gt;teradataml) (1.3.13)
Requirement already satisfied, skipping upgrade: six&gt;=1.5 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas&gt;=0.22-&gt;teradataml) (1.11.0)

",-1,-1,-1.0,"I'm attempting to use the Python teradataml library for Teradata to create a simple database connection using the logmech of LDAP.  However, i'm greeted with an attribute error of 'NoneType' ojbect has no attribute dialect in relation to sqlalchemy_engine.dialect.

My code to create the connection with the hostname changed

import teradataml as tdml
from teradataml import *

hostname = ""address of host""
schema = ""DL_RBA_DSS""

td_context = create_context(host = hostname, logmech='LDAP', temp_database_name = schema)
print(td_context)


When running the code it gives me the following error:

File ""c:/Python/teradatamlconnection.py"", line 7, in &lt;module&gt;
    td_context = create_context(host = hostname, logmech='LDAP', temp_database_name = schema)
  File ""C:\Users\UserID\AppData\Local\Programs\Python\Python36\lib\site-packages\teradataml\context\context.py"", line 287, in create_context
    _load_function_aliases()
  File ""C:\Users\UserID\AppData\Local\Programs\Python\Python36\lib\site-packages\teradataml\context\context.py"", line 532, in _load_function_aliases
    __set_vantage_version()
  File ""C:\Users\UserID\AppData\Local\Programs\Python\Python36\lib\site-packages\teradataml\context\context.py"", line 562, in __set_vantage_version
    if td_sqlalchemy_engine.dialect.has_table(td_sqlalchemy_engine, ""versionInfo"", schema=""pm""):
AttributeError: 'NoneType' object has no attribute 'dialect'


I have tried to update the teradatasqlalchemy and sqlalchemy library, but i still am greeted with the error.  It shows all requirements are satisfied:

PS C:\Python\dsssecurity> pip install teradatasqlalchemy --upgrade

Requirement already up-to-date: teradatasqlalchemy in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (16.20.0.8)
Requirement already satisfied, skipping upgrade: teradatasql in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradatasqlalchemy) (16.20.0.52)
Requirement already satisfied, skipping upgrade: sqlalchemy&gt;=1.2.11 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradatasqlalchemy) (1.3.13)
Requirement already satisfied, skipping upgrade: pycryptodome in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradatasql-&gt;teradatasqlalchemy) (3.9.0)
PS C:\Python\dsssecurity&gt; pip install teradataml --upgrade
Requirement already up-to-date: teradataml in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (16.20.0.4)
Requirement already satisfied, skipping upgrade: teradatasql in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradataml) (16.20.0.52)
Requirement already satisfied, skipping upgrade: pandas&gt;=0.22 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradataml) (1.0.1)
Requirement already satisfied, skipping upgrade: psutil in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradataml) (5.6.3)
Requirement already satisfied, skipping upgrade: teradatasqlalchemy&gt;=16.20.0.8 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradataml) (16.20.0.8)
Requirement already satisfied, skipping upgrade: pycryptodome in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradatasql-&gt;teradataml) (3.9.0)
Requirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from pandas&gt;=0.22-&gt;teradataml) (1.18.1)
Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from pandas&gt;=0.22-&gt;teradataml) (2017.2)
Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.6.1 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from pandas&gt;=0.22-&gt;teradataml) (2.6.1)
Requirement already satisfied, skipping upgrade: sqlalchemy&gt;=1.2.11 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from teradatasqlalchemy&gt;=16.20.0.8-&gt;teradataml) (1.3.13)
Requirement already satisfied, skipping upgrade: six&gt;=1.5 in c:\users\UserID\appdata\local\programs\python\python36\lib\site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas&gt;=0.22-&gt;teradataml) (1.11.0)

",1
447,60231985,Call Teradata Stored Procedure with output parameters from Python using callproc,"I am running this simple python script to call a Teradata procedure but the result is not returning output parameter via Python. The procedure is working fine and returning output data from Teradata SQL editor. 

import teradata

method,host,username,password = 'odbc','xx.xxxx.com','abcde', 'password123'

udaExec = teradata.UdaExec (appName=""test"", version=""1.0"", logConsole=False,logLevel=""TRACE"")
P1=""ABC""
P2=""TESTING1""

with udaExec.connect(method=method,system=host, username=username,password=password, driver=""Teradata Database ODBC Driver 16.10"",transactionMode='Teradata') as session:

results=session.callproc('db.TEST_PYTHON_CALL_RETURN2',(P1,P2,teradata.OutParam(""Output1"")))
print(results) 


Output returned is

C:\Program Files (x86)\Python37-32&gt;python ./TD_CALL_SP_3.py
['ABC', 'TESTING1', '']


Can someone help me with this? I have tried out various options with no success yet. 
",1,-1,-1.0,"I am running this simple python script to call a Teradata procedure but the result is not returning output parameter via Python. The procedure is working fine and returning output data from Teradata SQL editor. 

import teradata

method,host,username,password = 'odbc','xx.xxxx.com','abcde', 'password123'

udaExec = teradata.UdaExec (appName=""test"", version=""1.0"", logConsole=False,logLevel=""TRACE"")
P1=""ABC""
P2=""TESTING1""

with udaExec.connect(method=method,system=host, username=username,password=password, driver=""Teradata Database ODBC Driver 16.10"",transactionMode='Teradata') as session:

results=session.callproc('db.TEST_PYTHON_CALL_RETURN2',(P1,P2,teradata.OutParam(""Output1"")))
print(results) 


Output returned is

C:\Program Files (x86)\Python37-32&gt;python ./TD_CALL_SP_3.py
['ABC', 'TESTING1', '']


Can someone help me with this? I have tried out various options with no success yet. 
",1
448,60279729,Teradata Database TeraJDBC 16.20.00.04 Error 3610 SQLState HY000 Internal error: Please do not resubmit the last request,"Im requesting a report in the form of excel in my application. The application generates a report by running several(31) sql tasks, everytime i submit a request. But for a particular request the execution of 28th task doesnt complete and throws the below error.

[Teradata Database] [TeraJDBC 16.20.00.04] [Error 3610] [SQLState HY000] Internal error: Please do not resubmit the last request.  SubCode, CrashCode:   0,  2693

01/28/2020 10:35:28,586 ERROR com.xxxx.yyyy.zz.Runner:generateBasicReport:174 - java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.04] [Error 3610] [SQLState HY000] Internal error: Please do not resubmit the last request.  SubCode, CrashCode:   0,  2693

com.xxxx.yyyy.zz.exception.ReportGenerationException: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.04] [Error 3610] [SQLState HY000] Internal error: Please do not resubmit the last request.  SubCode, CrashCode:   0,  2693
    at com.xxxx.yyyy.zz.generator.task.CancellableSqlTask.execute(CancellableSqlTask.java:79)
    at com.xxxx.yyyy.zz.generator.task.TaskExecutor.executeNext(TaskExecutor.java:56)
    at com.xxxx.yyyy.zz.generator.ReportGenerator.executeTasks(ReportGenerator.java:119)
    at com.xxxx.yyyy.zz.generator.ReportGenerator.execute(ReportGenerator.java:83)
    at com.xxxx.yyyy.zz.generator.task.CompositeCancellableTask.execute(CompositeCancellableTask.java:32)
    at com.xxxx.yyyy.zz.Runner.executeGeneration(Runner.java:209)
    at com.xxxx.yyyy.zz.Runner.generateBasicReport(Runner.java:171)
    at com.xxxx.yyyy.zz.Runner.generateReports(Runner.java:137)
    at com.xxxx.yyyy.zz.Runner.startReportGenerator(Runner.java:92)
    at com.xxxx.yyyy.zz.Runner.main(Runner.java:69)
Caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.04] [Error 3610] [SQLState HY000] Internal error: Please do not resubmit the last request.  SubCode, CrashCode:   0,  2693
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:309)
    at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:103)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:311)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:200)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:137)
    at com.teradata.jdbc.jdbc_4.statemachine.PreparedStatementController.run(PreparedStatementController.java:46)
    at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:389)
    at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:331)
    at com.teradata.jdbc.jdbc_4.TDPreparedStatement.doPrepExecuteUpdate(TDPreparedStatement.java:225)
    at com.teradata.jdbc.jdbc_4.TDPreparedStatement.executeUpdate(TDPreparedStatement.java:2769)
    at com.xxxx.yyyy.zz.generator.task.CancellableUpdate.perform(CancellableUpdate.java:36)
    at com.xxxx.yyyy.zz.generator.task.CancellableSqlTask.execute(CancellableSqlTask.java:74)
    ... 9 more

",-1,-1,-1.0,"Im requesting a report in the form of excel in my application. The application generates a report by running several(31) sql tasks, everytime i submit a request. But for a particular request the execution of 28th task doesnt complete and throws the below error.

[Teradata Database] [TeraJDBC 16.20.00.04] [Error 3610] [SQLState HY000] Internal error: Please do not resubmit the last request.  SubCode, CrashCode:   0,  2693

01/28/2020 10:35:28,586 ERROR com.xxxx.yyyy.zz.Runner:generateBasicReport:174 - java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.04] [Error 3610] [SQLState HY000] Internal error: Please do not resubmit the last request.  SubCode, CrashCode:   0,  2693

com.xxxx.yyyy.zz.exception.ReportGenerationException: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.04] [Error 3610] [SQLState HY000] Internal error: Please do not resubmit the last request.  SubCode, CrashCode:   0,  2693
    at com.xxxx.yyyy.zz.generator.task.CancellableSqlTask.execute(CancellableSqlTask.java:79)
    at com.xxxx.yyyy.zz.generator.task.TaskExecutor.executeNext(TaskExecutor.java:56)
    at com.xxxx.yyyy.zz.generator.ReportGenerator.executeTasks(ReportGenerator.java:119)
    at com.xxxx.yyyy.zz.generator.ReportGenerator.execute(ReportGenerator.java:83)
    at com.xxxx.yyyy.zz.generator.task.CompositeCancellableTask.execute(CompositeCancellableTask.java:32)
    at com.xxxx.yyyy.zz.Runner.executeGeneration(Runner.java:209)
    at com.xxxx.yyyy.zz.Runner.generateBasicReport(Runner.java:171)
    at com.xxxx.yyyy.zz.Runner.generateReports(Runner.java:137)
    at com.xxxx.yyyy.zz.Runner.startReportGenerator(Runner.java:92)
    at com.xxxx.yyyy.zz.Runner.main(Runner.java:69)
Caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.04] [Error 3610] [SQLState HY000] Internal error: Please do not resubmit the last request.  SubCode, CrashCode:   0,  2693
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:309)
    at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:103)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:311)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:200)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:137)
    at com.teradata.jdbc.jdbc_4.statemachine.PreparedStatementController.run(PreparedStatementController.java:46)
    at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:389)
    at com.teradata.jdbc.jdbc_4.TDStatement.executeStatement(TDStatement.java:331)
    at com.teradata.jdbc.jdbc_4.TDPreparedStatement.doPrepExecuteUpdate(TDPreparedStatement.java:225)
    at com.teradata.jdbc.jdbc_4.TDPreparedStatement.executeUpdate(TDPreparedStatement.java:2769)
    at com.xxxx.yyyy.zz.generator.task.CancellableUpdate.perform(CancellableUpdate.java:36)
    at com.xxxx.yyyy.zz.generator.task.CancellableSqlTask.execute(CancellableSqlTask.java:74)
    ... 9 more

",0
449,60296188,"Teradata SQL returning previous two months, based on CURRENT_DATE","TLDR:
I want to get a table with the previous two months in Teradata, based on CURRENT_DATE. Currently I can only get the previous month:

SELECT 
TO_CHAR(ADD_MONTHS(CURRENT_DATE  - EXTRACT(DAY FROM CURRENT_DATE )+1, -1), 'YYYY-MM') MM;




Expected output is:

+--------+
|   MM   |
+--------+
| 2020-01|
| 2019-12|
+--------+




Long version:
I want something, that could be used in a bigger query like this, valid for every day of the year, without hardcoding the dates. The bigger query with the hardcoded dates looks like this:

AND TO_CHAR(SOME_DATE, 'YYYY-MM') IN ('2020-01', '2019-12')


and it works perfectly. The below one returns results ok, but only for 1 month. 

AND TO_CHAR(SOME_DATE, 'YYYY-MM') IN 
(
TO_CHAR(ADD_MONTHS(CURRENT_DATE  - EXTRACT(DAY FROM CURRENT_DATE )+1, -1), 'YYYY-MM')
)


Tried to add a comma and add the same line with -2, did not work:

AND TO_CHAR(SOME_DATE, 'YYYY-MM') IN 
(
TO_CHAR(ADD_MONTHS(CURRENT_DATE  - EXTRACT(DAY FROM CURRENT_DATE )+1, -1), 'YYYY-MM'),
TO_CHAR(ADD_MONTHS(CURRENT_DATE  - EXTRACT(DAY FROM CURRENT_DATE )+1, -2), 'YYYY-MM')
)


the error is:


  SELECT Failed 3706: Syntax error: expected something between ')' and '.'.

",-1,-1,-1.0,"TLDR:
I want to get a table with the previous two months in Teradata, based on CURRENT_DATE. Currently I can only get the previous month:

SELECT 
TO_CHAR(ADD_MONTHS(CURRENT_DATE  - EXTRACT(DAY FROM CURRENT_DATE )+1, -1), 'YYYY-MM') MM;




Expected output is:

+--------+
|   MM   |
+--------+
| 2020-01|
| 2019-12|
+--------+




Long version:
I want something, that could be used in a bigger query like this, valid for every day of the year, without hardcoding the dates. The bigger query with the hardcoded dates looks like this:

AND TO_CHAR(SOME_DATE, 'YYYY-MM') IN ('2020-01', '2019-12')


and it works perfectly. The below one returns results ok, but only for 1 month. 

AND TO_CHAR(SOME_DATE, 'YYYY-MM') IN 
(
TO_CHAR(ADD_MONTHS(CURRENT_DATE  - EXTRACT(DAY FROM CURRENT_DATE )+1, -1), 'YYYY-MM')
)


Tried to add a comma and add the same line with -2, did not work:

AND TO_CHAR(SOME_DATE, 'YYYY-MM') IN 
(
TO_CHAR(ADD_MONTHS(CURRENT_DATE  - EXTRACT(DAY FROM CURRENT_DATE )+1, -1), 'YYYY-MM'),
TO_CHAR(ADD_MONTHS(CURRENT_DATE  - EXTRACT(DAY FROM CURRENT_DATE )+1, -2), 'YYYY-MM')
)


the error is:


  SELECT Failed 3706: Syntax error: expected something between ')' and '.'.

",3
450,60348996,Connect Python to Teradata Connection Error,"I'm using sqlalchemy trying to connect to Teradata via ODBC as I need to be able to read/write to Teradata.

from sqlalchemy import create_engine
import sqlalchemy_teradata

user = 'user'
pasw='pasw'
host = 'host'

# connect
td_engine = create_engine(""teradata://""+user+"":""+pasw+""@""+host+""/?authentication=ODBC?driver=Teradata"")

#execute sql
sql = ""select * from table""
result = td_engine.execute(sql)


However I get the following error.

(teradata.api.DatabaseError) (0, '[28000] [Teradata][ODBC Teradata Driver] User Specified Mechanism for Logon is Not Available')
(Background on this error at: http://sqlalche.me/e/4xp6)

The link provided is not very informative unless I'm missing something.  The error is from Teradata but I'm not sure what it actually means.  It looks like it saying that I can't use ODBC? Any suggestions or alternatives?
",-1,-1,-1.0,"I'm using sqlalchemy trying to connect to Teradata via ODBC as I need to be able to read/write to Teradata.

from sqlalchemy import create_engine
import sqlalchemy_teradata

user = 'user'
pasw='pasw'
host = 'host'

# connect
td_engine = create_engine(""teradata://""+user+"":""+pasw+""@""+host+""/?authentication=ODBC?driver=Teradata"")

#execute sql
sql = ""select * from table""
result = td_engine.execute(sql)


However I get the following error.

(teradata.api.DatabaseError) (0, '[28000] [Teradata][ODBC Teradata Driver] User Specified Mechanism for Logon is Not Available')
(Background on this error at: http://sqlalche.me/e/4xp6)

The link provided is not very informative unless I'm missing something.  The error is from Teradata but I'm not sure what it actually means.  It looks like it saying that I can't use ODBC? Any suggestions or alternatives?
",1
451,60529603,Unable to create_context using teradataml due to OperationalError,"I'm using teradataml package version 16.20.0.4 and I get a strange error during the creation of the context, using the usual

import teradataml as tdml    
tdml.create_context(username = 'user', password='pass', host='host')


Then I get 

[...]

OperationalError: (teradatasql.OperationalError) [Version 16.20.0.59] [Session 6751033] [Teradata Database] [Error 3706] Syntax error: expected something between ')' and ','.
 at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError TeradataConnection.go:1102
 at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError TeradataConnection.go:1118
 at gosqldriver/teradatasql.(*teradataConnection).processErrorParcel TeradataConnection.go:1181
 at gosqldriver/teradatasql.(*TeradataRows).processResponseBundle TeradataRows.go:1415
 at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:521
 at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:388
 at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
 at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2044
 at database/sql.ctxDriverQuery ctxutil.go:48
 at database/sql.(*DB).queryDC.func1 sql.go:1464
 at database/sql.withLock sql.go:3032
 at database/sql.(*DB).queryDC sql.go:1459
 at database/sql.(*Conn).QueryContext sql.go:1701
 at main.goCreateRows goside.go:653
 at main._cgoexpwrap_1fc37444973b_goCreateRows _cgo_gotypes.go:357
 at runtime.call64 asm_amd64.s:574
 at runtime.cgocallbackg1 cgocall.go:316
 at runtime.cgocallbackg cgocall.go:194
 at runtime.cgocallback_gofunc asm_amd64.s:826
 at runtime.goexit asm_amd64.s:2361
[SQL: SELECT tablename 
FROM dbc.tablesvx 
WHERE DatabaseName (NOT CASESPECIFIC) = CAST(TRANSLATE(? USING UNICODE_TO_LATIN) as VARCHAR(128)) (NOT CASESPECIFIC) AND TableName=? AND TableKind (NOT CASESPECIFIC) IN ('O' (NOT CASESPECIFIC), 'Q' (NOT CASESPECIFIC), 'T' (NOT CASESPECIFIC))]
[parameters: ('pm', 'versionInfo')]
(Background on this error at: http://sqlalche.me/e/e3q8)


It's as if the library sends to the Teradata server a malformed query ... is that plausible? What can I do about it?

Thank you
",-1,-1,-1.0,"I'm using teradataml package version 16.20.0.4 and I get a strange error during the creation of the context, using the usual

import teradataml as tdml    
tdml.create_context(username = 'user', password='pass', host='host')


Then I get 

[...]

OperationalError: (teradatasql.OperationalError) [Version 16.20.0.59] [Session 6751033] [Teradata Database] [Error 3706] Syntax error: expected something between ')' and ','.
 at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError TeradataConnection.go:1102
 at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError TeradataConnection.go:1118
 at gosqldriver/teradatasql.(*teradataConnection).processErrorParcel TeradataConnection.go:1181
 at gosqldriver/teradatasql.(*TeradataRows).processResponseBundle TeradataRows.go:1415
 at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:521
 at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:388
 at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
 at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2044
 at database/sql.ctxDriverQuery ctxutil.go:48
 at database/sql.(*DB).queryDC.func1 sql.go:1464
 at database/sql.withLock sql.go:3032
 at database/sql.(*DB).queryDC sql.go:1459
 at database/sql.(*Conn).QueryContext sql.go:1701
 at main.goCreateRows goside.go:653
 at main._cgoexpwrap_1fc37444973b_goCreateRows _cgo_gotypes.go:357
 at runtime.call64 asm_amd64.s:574
 at runtime.cgocallbackg1 cgocall.go:316
 at runtime.cgocallbackg cgocall.go:194
 at runtime.cgocallback_gofunc asm_amd64.s:826
 at runtime.goexit asm_amd64.s:2361
[SQL: SELECT tablename 
FROM dbc.tablesvx 
WHERE DatabaseName (NOT CASESPECIFIC) = CAST(TRANSLATE(? USING UNICODE_TO_LATIN) as VARCHAR(128)) (NOT CASESPECIFIC) AND TableName=? AND TableKind (NOT CASESPECIFIC) IN ('O' (NOT CASESPECIFIC), 'Q' (NOT CASESPECIFIC), 'T' (NOT CASESPECIFIC))]
[parameters: ('pm', 'versionInfo')]
(Background on this error at: http://sqlalche.me/e/e3q8)


It's as if the library sends to the Teradata server a malformed query ... is that plausible? What can I do about it?

Thank you
",1
452,60556852,Problem with CASE statement syntax in teradata (BTEQ),"I've the following code with regards to inserting and checking values into the teradata database.My point is that data that is read from the flat file whose value after trimming leading 0 is 0 or NULL should not be loaded and if otherwise, the value should be loaded into the target table.......

VALUES
(    
  CASE STUD_ID WHEN TRIM(LEADING '0' FROM :STUD_ID) NOT IN ('0', NULL) THEN TRIM(LEADING '0' FROM :STUD_ID)
  ELSE NEXT 
  END,
  :B,
  :C

)


I've unsure of if the next statement does exist for teradata in the conditioning statement...I've got this error 

   Illegal expression in WHEN clause of CASE expression.
   Statement# 1, Info =0 


I tried with the select statement in the VALUES area,

VALUES
(
   SELECT (CASE STUD_ID WHEN TRIM(LEADING '0' FROM :STUD_ID) != '0' THEN TRIM(LEADING '0' FROM :STUD_ID)
         ELSE 1000
         END )
  FROM :STUD_ID,
  :B,
  :C

)


I got this error statement...

 Syntax error, expected something like ')' between '(' and 
the 'SELECT' keyword.

",-1,-1,-1.0,"I've the following code with regards to inserting and checking values into the teradata database.My point is that data that is read from the flat file whose value after trimming leading 0 is 0 or NULL should not be loaded and if otherwise, the value should be loaded into the target table.......

VALUES
(    
  CASE STUD_ID WHEN TRIM(LEADING '0' FROM :STUD_ID) NOT IN ('0', NULL) THEN TRIM(LEADING '0' FROM :STUD_ID)
  ELSE NEXT 
  END,
  :B,
  :C

)


I've unsure of if the next statement does exist for teradata in the conditioning statement...I've got this error 

   Illegal expression in WHEN clause of CASE expression.
   Statement# 1, Info =0 


I tried with the select statement in the VALUES area,

VALUES
(
   SELECT (CASE STUD_ID WHEN TRIM(LEADING '0' FROM :STUD_ID) != '0' THEN TRIM(LEADING '0' FROM :STUD_ID)
         ELSE 1000
         END )
  FROM :STUD_ID,
  :B,
  :C

)


I got this error statement...

 Syntax error, expected something like ')' between '(' and 
the 'SELECT' keyword.

",3
453,60700793,Same SQL query which works in Teradata doesn't work in Fitnesse,"Same SQL query which works in Teradata SQL Assistant doesn't work in Fitnesse. I get below error when we run the query

""SELECT failed. 3706: Syntax error: expected something between '(' and the 'SUBSTR' keyword

I have faced the same problem in past. I understand the problem is with copy and paste. In past it was small query so I manually typed and it worked. But now my query expands over 300 lines, so I am not able to retype it. I even tried to copy and paste in plain text via notepad and notepad ++, but still unable to resolve it. Can you throw some light please?
",-1,-1,-1.0,"Same SQL query which works in Teradata SQL Assistant doesn't work in Fitnesse. I get below error when we run the query

""SELECT failed. 3706: Syntax error: expected something between '(' and the 'SUBSTR' keyword

I have faced the same problem in past. I understand the problem is with copy and paste. In past it was small query so I manually typed and it worked. But now my query expands over 300 lines, so I am not able to retype it. I even tried to copy and paste in plain text via notepad and notepad ++, but still unable to resolve it. Can you throw some light please?
",3
454,60920676,Teradata TTU Installation on MACOS...TPT not working,"I've installed Teradata TTU on my mac. After installation BTEQ works, but TPT does not work.

$tbuild
-bash: tbuild: command not found
",-1,-1,-1.0,"I've installed Teradata TTU on my mac. After installation BTEQ works, but TPT does not work.

$tbuild
-bash: tbuild: command not found
",1
455,60967811,Connecting to Teradata using teradatasql module in Python,"I am trying to connect to Teradata using teradatasql module in Python. The code is running fine on localhost, but once deployed on the server as part of the server code, it is throwing the error.

the code:

import teradatasql
try:
    host, username, password = 'hostname', 'username', '****'
    session = teradatasql.connect(host=host, user=username, password=password, logmech=""LDAP"")

except Exception as e:
    print(e)


Error I am getting on server:


  [Version 16.20.0.60] [Session 0] [Teradata SQL Driver] Failure receiving Config Response message header↵ at gosqldriver/teradatasql.
      (*teradataConnection).makeDriverError TeradataConnection.go:1101↵ at gosqldriver/teradatasql.
      (*teradataConnection).sendAndReceive TeradataConnection.go:1397↵ at gosqldriver/teradatasql.newTeradataConnection TeradataConnection.go:180↵ at gosqldriver/teradatasql.(*teradataDriver).
      Open TeradataDriver.go:32↵ at database/sql.dsnConnector.Connect sql.go:600↵ at database/sql.(*DB).conn sql.go:1103↵ at database/sql.
      (*DB).Conn sql.go:1619↵ at main.goCreateConnection goside.go:275↵ at main.
      _cgoexpwrap_212fad278f55_goCreateConnection _cgo_gotypes.go:240↵ at runtime.call64 asm_amd64.s:574↵ at runtime.cgocallbackg1 cgocall.go:316↵ at runtime.cgocallbackg cgocall.go:194↵ at runtime.cgocallback_gofunc asm_amd64.s:826↵ at runtime.goexit asm_amd64.s:2361↵Caused by read tcp IP:PORT->IP:PORT: wsarecv: An existing connection was forcibly closed by the remote host

",-1,-1,-1.0,"I am trying to connect to Teradata using teradatasql module in Python. The code is running fine on localhost, but once deployed on the server as part of the server code, it is throwing the error.

the code:

import teradatasql
try:
    host, username, password = 'hostname', 'username', '****'
    session = teradatasql.connect(host=host, user=username, password=password, logmech=""LDAP"")

except Exception as e:
    print(e)


Error I am getting on server:


  [Version 16.20.0.60] [Session 0] [Teradata SQL Driver] Failure receiving Config Response message header↵ at gosqldriver/teradatasql.
      (*teradataConnection).makeDriverError TeradataConnection.go:1101↵ at gosqldriver/teradatasql.
      (*teradataConnection).sendAndReceive TeradataConnection.go:1397↵ at gosqldriver/teradatasql.newTeradataConnection TeradataConnection.go:180↵ at gosqldriver/teradatasql.(*teradataDriver).
      Open TeradataDriver.go:32↵ at database/sql.dsnConnector.Connect sql.go:600↵ at database/sql.(*DB).conn sql.go:1103↵ at database/sql.
      (*DB).Conn sql.go:1619↵ at main.goCreateConnection goside.go:275↵ at main.
      _cgoexpwrap_212fad278f55_goCreateConnection _cgo_gotypes.go:240↵ at runtime.call64 asm_amd64.s:574↵ at runtime.cgocallbackg1 cgocall.go:316↵ at runtime.cgocallbackg cgocall.go:194↵ at runtime.cgocallback_gofunc asm_amd64.s:826↵ at runtime.goexit asm_amd64.s:2361↵Caused by read tcp IP:PORT->IP:PORT: wsarecv: An existing connection was forcibly closed by the remote host

",1
456,60985978,How to truncate a table in teradata using spark jdbc connection,"I want to truncate a teradata table before loading data using spark and scala. 
I am able to load data but failed to truncate .
please guide.
",-1,-1,-1.0,"I want to truncate a teradata table before loading data using spark and scala. 
I am able to load data but failed to truncate .
please guide.
",3
457,61009671,using columns as lower and upper bound in RANDOM function in Teradata,"This is Teradata specific question. In RANDOM function, I want the lower bound to be taken directly from one of the columns. e.g. I want a random value between age of the subscriber and till date. SO I want to put RANDOM(int_tenure, 0). I am receiving below error:

""Syntax error, expected something like an integer or a decimal number or a floating point number or '+' or '-' between '(' and the word 'int_tenure'""
",-1,-1,-1.0,"This is Teradata specific question. In RANDOM function, I want the lower bound to be taken directly from one of the columns. e.g. I want a random value between age of the subscriber and till date. SO I want to put RANDOM(int_tenure, 0). I am receiving below error:

""Syntax error, expected something like an integer or a decimal number or a floating point number or '+' or '-' between '(' and the word 'int_tenure'""
",3
458,61347740,Informatica Stored Procedure Transformation using Teradata,"Tried using a stored procedure with 1 input &amp; 1 output value. I created the expression transformation with o/p port with

:SP....(input_val,PROC_RESULT)

When I put the o/p port in the Stored procedure as 'Return' port along with output port, it throws an error for invalid function reference while validating the output port in expression as mentioned above

I am working with Teradata. Plz suggest.
",-1,-1,-1.0,"Tried using a stored procedure with 1 input &amp; 1 output value. I created the expression transformation with o/p port with

:SP....(input_val,PROC_RESULT)

When I put the o/p port in the Stored procedure as 'Return' port along with output port, it throws an error for invalid function reference while validating the output port in expression as mentioned above

I am working with Teradata. Plz suggest.
",3
459,61348453,macOS - Java Runtime Environment 1.8 or 10 is required and should be installed before installing Teradata Studio Application,"Good day,

I'm trying to install ""Teradata Studio"" on my Mac. This is a software for working with Teradata database.
However, at the end of installation I get an error ""Java Runtime Environment 1.8 or 10 is required and should be installed before installing Teradata Studio Application.""

Can anyone please tell me what can I do about it? I already installed Java 1.8.

output of terminal commands 

""/usr/libexec/java_home -V"" 

Matching Java Virtual Machines (3):
    14, x86_64: ""Java SE 14""    /Library/Java/JavaVirtualMachines/jdk-14.jdk/Contents/Home
    13.0.2, x86_64: ""Java SE 13.0.2""    /Library/Java/JavaVirtualMachines/jdk-13.0.2.jdk/Contents/Home
    1.8.0_201, x86_64:  ""Java SE 8"" /Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Home


java -version

java version ""1.8.0_201""
Java(TM) SE Runtime Environment (build 1.8.0_201-b09)
Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)


echo $JAVA_HOME

/Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Home


.bash_profile content:

export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Home


So, looks like I already have Java 1.8. 
Any ideas why it's not working?
",-1,-1,-1.0,"Good day,

I'm trying to install ""Teradata Studio"" on my Mac. This is a software for working with Teradata database.
However, at the end of installation I get an error ""Java Runtime Environment 1.8 or 10 is required and should be installed before installing Teradata Studio Application.""

Can anyone please tell me what can I do about it? I already installed Java 1.8.

output of terminal commands 

""/usr/libexec/java_home -V"" 

Matching Java Virtual Machines (3):
    14, x86_64: ""Java SE 14""    /Library/Java/JavaVirtualMachines/jdk-14.jdk/Contents/Home
    13.0.2, x86_64: ""Java SE 13.0.2""    /Library/Java/JavaVirtualMachines/jdk-13.0.2.jdk/Contents/Home
    1.8.0_201, x86_64:  ""Java SE 8"" /Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Home


java -version

java version ""1.8.0_201""
Java(TM) SE Runtime Environment (build 1.8.0_201-b09)
Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)


echo $JAVA_HOME

/Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Home


.bash_profile content:

export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Home


So, looks like I already have Java 1.8. 
Any ideas why it's not working?
",1
460,61389298,"Importing Teradata Code to PowerBI, PowerBI can't accept double quotes","Decided to change the column name in the source table, I do not believe that there is a solution to the issue besides configuring our Teradata connector to work with LDAP or changing the name of the source column.  Since we're still investigating the Teradata connector, this was the faster solution.

I'm attempting to import a table created by a Teradata query into PowerBI as a dataflow using the blank query entity.

The original code is something like this:

Select ft.""Actual Name of the Column""

from fake.fake_table ft


PowerBI is getting hung up on the double quotes, but I can't figure out a way around them since they're required to call the column in Teradata.  Short of changing the column name in the source table, is there a way to call this column without double quotes?

PS: Using the Teradata connection inside PowerBI is not an option for our organization due to the lack of LDAP support.
",1,-1,-1.0,"Decided to change the column name in the source table, I do not believe that there is a solution to the issue besides configuring our Teradata connector to work with LDAP or changing the name of the source column.  Since we're still investigating the Teradata connector, this was the faster solution.

I'm attempting to import a table created by a Teradata query into PowerBI as a dataflow using the blank query entity.

The original code is something like this:

Select ft.""Actual Name of the Column""

from fake.fake_table ft


PowerBI is getting hung up on the double quotes, but I can't figure out a way around them since they're required to call the column in Teradata.  Short of changing the column name in the source table, is there a way to call this column without double quotes?

PS: Using the Teradata connection inside PowerBI is not an option for our organization due to the lack of LDAP support.
",3
461,61401385,How to merge records based on consective fields in Teradata,"I have a source like below table:

+---------+--+--------+--+---------+--+--+------+
|   ID    |  | SEQ_NO |  | UNIT_ID |  |  | D_ID |
+---------+--+--------+--+---------+--+--+------+
| 7979092 |  |      1 |  |      99 |  |  |  759 |
| 7979092 |  |      2 |  |      -1 |  |  |  869 |
| 7979092 |  |      3 |  |      -1 |  |  |  927 |
| 7979092 |  |      4 |  |      -1 |  |  |  812 |
| 7979092 |  |      5 |  |      99 |  |  |  900 |
| 7979092 |  |      6 |  |      99 |  |  |  891 |
| 7979092 |  |      7 |  |      -1 |  |  |  785 |
| 7979092 |  |      8 |  |      -1 |  |  |  762 |
| 7979092 |  |      9 |  |      -1 |  |  |  923 |
+---------+--+--------+--+---------+--+--+------+


I have to merge the rows when consecutive unit_id has same value. We should take max(D_id) when we consolidate the rows. Expected output is:

+---------+---------+------+
|   ID    | UNIT_ID | D_ID |
+---------+---------+------+
| 7979092 |      99 |  759 |
| 7979092 |      -1 |  927 |
| 7979092 |      99 |  900 |
| 7979092 |      -1 |  923 |
+---------+---------+------+


I have tried to find the solution using Teradata ordered analytical function, but did not find the solution. I use Teradata 16.

Thank You.
",-1,-1,-1.0,"I have a source like below table:

+---------+--+--------+--+---------+--+--+------+
|   ID    |  | SEQ_NO |  | UNIT_ID |  |  | D_ID |
+---------+--+--------+--+---------+--+--+------+
| 7979092 |  |      1 |  |      99 |  |  |  759 |
| 7979092 |  |      2 |  |      -1 |  |  |  869 |
| 7979092 |  |      3 |  |      -1 |  |  |  927 |
| 7979092 |  |      4 |  |      -1 |  |  |  812 |
| 7979092 |  |      5 |  |      99 |  |  |  900 |
| 7979092 |  |      6 |  |      99 |  |  |  891 |
| 7979092 |  |      7 |  |      -1 |  |  |  785 |
| 7979092 |  |      8 |  |      -1 |  |  |  762 |
| 7979092 |  |      9 |  |      -1 |  |  |  923 |
+---------+--+--------+--+---------+--+--+------+


I have to merge the rows when consecutive unit_id has same value. We should take max(D_id) when we consolidate the rows. Expected output is:

+---------+---------+------+
|   ID    | UNIT_ID | D_ID |
+---------+---------+------+
| 7979092 |      99 |  759 |
| 7979092 |      -1 |  927 |
| 7979092 |      99 |  900 |
| 7979092 |      -1 |  923 |
+---------+---------+------+


I have tried to find the solution using Teradata ordered analytical function, but did not find the solution. I use Teradata 16.

Thank You.
",3
462,61596700,Error While reading Teradata from Spark. It loaded the Table and Shows Schema But fails to give data set result,"scala&gt; val df = spark.sqlContext.load(""jdbc"", Map(""url"" -&gt; ""jdbc:teradata://tdpw/database=ven68959, TMODE=TERA, user=iv068959, password=xxxxxxx"",""dbtable"" -&gt;""abc_view.vmyk_csat_echat"","" driver"" -&gt; ""com.teradata.jdbc.TeraDriver""))
warning: there was one deprecation warning; re-run with -deprecation for details
df: org.apache.spark.sql.DataFrame = [SRVY_ID: string, SRVY_PRGM_ID: string ... 32 more fields]


scala&gt; df.show(1,false)
20/05/04 10:56:08 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 4, plsq00642d1.corp.sprint.com, executor 19): 
java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.00.00.20] [Error 804] 
[SQLState 08S01] Socket communication failure for Packet receive Mon May 04 10:56:08 CDT 2020 
socket orig=tdpw local=0.0.0.0/0.0.0.0:45714 
remote=tdpwcop7/144.226.222.186:1025 
keepalive=unavailable 
nodelay=unavailable 
receive=unavailable 
send=unavailable 
linger=unavailable 
traffic=unavailable 
concurrent=3 contimeout=10000 conwait=1000 connecttime=1 connecttotaltime=1 connectattempts=1 connectfailures=0 reconnectattempts=0 
recoverable=false redrive=false failurecache={} cid=676567f4 sess=5175054 
java.io.IOException: Bad response message header with invalid message length of 16773956 bytes  
at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNet

",-1,-1,-1.0,"scala&gt; val df = spark.sqlContext.load(""jdbc"", Map(""url"" -&gt; ""jdbc:teradata://tdpw/database=ven68959, TMODE=TERA, user=iv068959, password=xxxxxxx"",""dbtable"" -&gt;""abc_view.vmyk_csat_echat"","" driver"" -&gt; ""com.teradata.jdbc.TeraDriver""))
warning: there was one deprecation warning; re-run with -deprecation for details
df: org.apache.spark.sql.DataFrame = [SRVY_ID: string, SRVY_PRGM_ID: string ... 32 more fields]


scala&gt; df.show(1,false)
20/05/04 10:56:08 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 4, plsq00642d1.corp.sprint.com, executor 19): 
java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 15.00.00.20] [Error 804] 
[SQLState 08S01] Socket communication failure for Packet receive Mon May 04 10:56:08 CDT 2020 
socket orig=tdpw local=0.0.0.0/0.0.0.0:45714 
remote=tdpwcop7/144.226.222.186:1025 
keepalive=unavailable 
nodelay=unavailable 
receive=unavailable 
send=unavailable 
linger=unavailable 
traffic=unavailable 
concurrent=3 contimeout=10000 conwait=1000 connecttime=1 connecttotaltime=1 connectattempts=1 connectfailures=0 reconnectattempts=0 
recoverable=false redrive=false failurecache={} cid=676567f4 sess=5175054 
java.io.IOException: Bad response message header with invalid message length of 16773956 bytes  
at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNet

",0
463,61600487,Teradata UNION not working correctly. blank values for a column when UNION used,"Wondering if anyone has come across this issue before. I have a query as follows that never had an issue against a SQL Server database, but now I'm having an issue with Teradata. 

When each of the individual SELECT statements are executed, all columns contain data. When run together with UNION as below, the second SELECT statement yields a blank values for the 'SR.yyy' column. Likewise, if I swap the SELECT statements (i.e. first SELECT statement is now after the UNION and vice versa), then the 'SR.yyy' column contains values however, the 'SG.xxx' column is now blank. Puzzling.  

Note: all columns are of same data type.   

SELECT AA.name, AA.id, SG.xxx, '' as yyy   
FROM DATABASE1 AA  
LEFT JOIN DATABASE_SG SG ON AA.id = SG.id  
WHERE AA.name = 'John.Doe'  
UNION  
SELECT AA.name, AA.id, '' as xxx,  SR.yyy  
FROM DATABASE1 AA  
LEFT JOIN DATABASE_SR SR ON AA.id = SR.id  
WHERE AA.name = 'John.Doe'  

",-1,-1,-1.0,"Wondering if anyone has come across this issue before. I have a query as follows that never had an issue against a SQL Server database, but now I'm having an issue with Teradata. 

When each of the individual SELECT statements are executed, all columns contain data. When run together with UNION as below, the second SELECT statement yields a blank values for the 'SR.yyy' column. Likewise, if I swap the SELECT statements (i.e. first SELECT statement is now after the UNION and vice versa), then the 'SR.yyy' column contains values however, the 'SG.xxx' column is now blank. Puzzling.  

Note: all columns are of same data type.   

SELECT AA.name, AA.id, SG.xxx, '' as yyy   
FROM DATABASE1 AA  
LEFT JOIN DATABASE_SG SG ON AA.id = SG.id  
WHERE AA.name = 'John.Doe'  
UNION  
SELECT AA.name, AA.id, '' as xxx,  SR.yyy  
FROM DATABASE1 AA  
LEFT JOIN DATABASE_SR SR ON AA.id = SR.id  
WHERE AA.name = 'John.Doe'  

",3
464,61623340,Kafka Teradata Source Connector fetching Column Title,"Using Kafka Confluent Teradata source connector to read data from a table into a Kafka topic. This is working as expected as long as the Teradata table doesn't contain Column Title. If Column Title is present, it's referring the Column Title instead of Column name and getting the error. In the error message Emp ID is the Column Title and EMP_ID is the column name.

Error:
Caused by: org.apache.avro.SchemaParseException: Illegal character in: Emp ID
        at org.apache.avro.Schema.validateName(Schema.java:1532)
        at org.apache.avro.Schema.access$400(Schema.java:87)
        at org.apache.avro.Schema$Field.&lt;init&gt;(Schema.java:520)
        at org.apache.avro.Schema$Field.&lt;init&gt;(Schema.java:559)


Tried multiple options and came up with the following workaround of giving query in property files, this is working as expected. But the problem is we have 100's of tables to pull from Teradata, with this workaround we need to create separate property file for each table and have to mention all the columns explicitly. This makes complicated from maintenance perspective.

query=SELECT EMP_ID AS EMP_ID, CRET_TS AS CRET_TS FROM schema.table


Hoping someone could help over this.
",1,-1,-1.0,"Using Kafka Confluent Teradata source connector to read data from a table into a Kafka topic. This is working as expected as long as the Teradata table doesn't contain Column Title. If Column Title is present, it's referring the Column Title instead of Column name and getting the error. In the error message Emp ID is the Column Title and EMP_ID is the column name.

Error:
Caused by: org.apache.avro.SchemaParseException: Illegal character in: Emp ID
        at org.apache.avro.Schema.validateName(Schema.java:1532)
        at org.apache.avro.Schema.access$400(Schema.java:87)
        at org.apache.avro.Schema$Field.&lt;init&gt;(Schema.java:520)
        at org.apache.avro.Schema$Field.&lt;init&gt;(Schema.java:559)


Tried multiple options and came up with the following workaround of giving query in property files, this is working as expected. But the problem is we have 100's of tables to pull from Teradata, with this workaround we need to create separate property file for each table and have to mention all the columns explicitly. This makes complicated from maintenance perspective.

query=SELECT EMP_ID AS EMP_ID, CRET_TS AS CRET_TS FROM schema.table


Hoping someone could help over this.
",3
465,61670331,How to use DATE format in sql - Teradata,"I have the following result set.

purchase_date              invoice_no
2020-05-06 13:14:54.000000 I2516211
2020-05-05 20:30:08.000000 I2515992
2020-05-05 02:00:26.000000 I2515763
2020-05-03 22:00:51.000000 I2515504


I need to get the result set as follows.(showing how many invoices per day count)

2020-05-03  1
2020-05-06  1
2020-05-05  2


not sure how to use the date format function in the sql.(Im using teradata)
",0,0,-1.0,"I have the following result set.

purchase_date              invoice_no
2020-05-06 13:14:54.000000 I2516211
2020-05-05 20:30:08.000000 I2515992
2020-05-05 02:00:26.000000 I2515763
2020-05-03 22:00:51.000000 I2515504


I need to get the result set as follows.(showing how many invoices per day count)

2020-05-03  1
2020-05-06  1
2020-05-05  2


not sure how to use the date format function in the sql.(Im using teradata)
",3
466,61782748,Need help understanding error in teradata,"When running this query for some reason I always get not valid row handles error and I'm not really sure what it means. I've trying to do a python connection to teradata and print out the results of the queried data.

import teradata
import teradatasql

with teradatasql.connect(host='PPTT', user='**', password='***', logmech='LDAP') as connection:
    connection.autocommit = True
    crsr = connection.cursor() 
    sql_command=""""""SELECT SOURCE_ORDER_NUMBER
    FROM DL_DB.BV_BO
    WHERE TRANS_TYPE='AR'
    """"""
    crsr.execute(sql_command) 

result = crsr.fetchall()

---------------------------------------------------------------------------
OperationalError                          Traceback (most recent call last)
&lt;ipython-input-26-be601f4ff898&gt; in &lt;module&gt;
----&gt; 1 result = crsr.fetchall()

//anaconda3/lib/python3.7/site-packages/teradatasql/__init__.py in fetchall(self)
    921         try:
    922             rows = []
--&gt; 923             for row in self:
    924                 rows.append(row)
    925 

//anaconda3/lib/python3.7/site-packages/teradatasql/__init__.py in __next__(self)
    991                     sErr = ctypes.string_at (pcError).decode ('utf-8')
    992                     goside.goFreePointer (self.connection.uLog, pcError)
--&gt; 993                     raise OperationalError (sErr)
    994 
    995                 if pcColumnValues:

OperationalError: 6 is not a valid rows handle

",-1,-1,-1.0,"When running this query for some reason I always get not valid row handles error and I'm not really sure what it means. I've trying to do a python connection to teradata and print out the results of the queried data.

import teradata
import teradatasql

with teradatasql.connect(host='PPTT', user='**', password='***', logmech='LDAP') as connection:
    connection.autocommit = True
    crsr = connection.cursor() 
    sql_command=""""""SELECT SOURCE_ORDER_NUMBER
    FROM DL_DB.BV_BO
    WHERE TRANS_TYPE='AR'
    """"""
    crsr.execute(sql_command) 

result = crsr.fetchall()

---------------------------------------------------------------------------
OperationalError                          Traceback (most recent call last)
&lt;ipython-input-26-be601f4ff898&gt; in &lt;module&gt;
----&gt; 1 result = crsr.fetchall()

//anaconda3/lib/python3.7/site-packages/teradatasql/__init__.py in fetchall(self)
    921         try:
    922             rows = []
--&gt; 923             for row in self:
    924                 rows.append(row)
    925 

//anaconda3/lib/python3.7/site-packages/teradatasql/__init__.py in __next__(self)
    991                     sErr = ctypes.string_at (pcError).decode ('utf-8')
    992                     goside.goFreePointer (self.connection.uLog, pcError)
--&gt; 993                     raise OperationalError (sErr)
    994 
    995                 if pcColumnValues:

OperationalError: 6 is not a valid rows handle

",1
467,61786735,How to take SAS query for SUM(Case) and make them work in Teradata SQL,"I am trying to run a query that was built in SAS and not getting the case statements to work in Teradata SQL 

Original SAS CODE

CREATE TABLE WORK.RSL_Validation AS 
SELECT 
t.eventDt, 
t.repID,
t.validation1,
t.validation2,
t.validation3,

(SUM((
CASE
WHEN t.validation1 is missing
THEN 0 
ELSE 1
END), 
(CASE
WHEN t.validation2 is missing 
THEN 0
ELSE 1 
END), 
(CASE
WHEN t.validation3 is missing 
THEN 0 
ELSE 1
END))) AS Val_Count
FROM RSL t;


The values for validation1, validtion2 and validation3 show up as '?' in the table I made however they would be null I suppose. 

SQL CODE I am trying is: 

CREATE TABLE WORK.RSL_Validation AS 
SELECT 
t.eventDt, 
t.repID,
t.validation1,
t.validation2,
t.validation3,

SUM((
CASE
WHEN t.validation1 is null
THEN 0 
ELSE 1
END), 
(CASE
WHEN t.validation2 is null
THEN 0
ELSE 1 
END), 
(CASE
WHEN t.validation3 is null 
THEN 0 
ELSE 1
END))) AS Val_Count
FROM RSL t;)
WITH DATA ON COMMIT PRESERVE ROWS;


Nothing seems to work
",-1,-1,-1.0,"I am trying to run a query that was built in SAS and not getting the case statements to work in Teradata SQL 

Original SAS CODE

CREATE TABLE WORK.RSL_Validation AS 
SELECT 
t.eventDt, 
t.repID,
t.validation1,
t.validation2,
t.validation3,

(SUM((
CASE
WHEN t.validation1 is missing
THEN 0 
ELSE 1
END), 
(CASE
WHEN t.validation2 is missing 
THEN 0
ELSE 1 
END), 
(CASE
WHEN t.validation3 is missing 
THEN 0 
ELSE 1
END))) AS Val_Count
FROM RSL t;


The values for validation1, validtion2 and validation3 show up as '?' in the table I made however they would be null I suppose. 

SQL CODE I am trying is: 

CREATE TABLE WORK.RSL_Validation AS 
SELECT 
t.eventDt, 
t.repID,
t.validation1,
t.validation2,
t.validation3,

SUM((
CASE
WHEN t.validation1 is null
THEN 0 
ELSE 1
END), 
(CASE
WHEN t.validation2 is null
THEN 0
ELSE 1 
END), 
(CASE
WHEN t.validation3 is null 
THEN 0 
ELSE 1
END))) AS Val_Count
FROM RSL t;)
WITH DATA ON COMMIT PRESERVE ROWS;


Nothing seems to work
",3
468,61931878,Running Teradatasql driver for python code using spark,"I want write code fetch data from teradata using python. The code should work while running using spark on cluster as well as local. While running using spark I don't want to open connections on executors. So the plan is to run code on driver using teradatasql package. Since teradatasql packages so library I thought I don't have install teradata library on cluster. 

I packaged the dependencies i.e. teradatasql as egg file and passed it as --py-files. But while running on code teradatasql is not able to read library from egg file. 

Os error: teradatasql.so cannot open shared object file. Not a directory.

I followed the below steps to package the egg file.
1. pip install teradatasql --target./src # note all my code is in src folder. Doing this step will install teradatasql package in my src folder. it contains teradatasql.so library
2. In setup.py packages=find_packages('src'), package_data={'teradatasql':['teradatasql.so']}
3. python setup.py bdist_eggg
",-1,-1,-1.0,"I want write code fetch data from teradata using python. The code should work while running using spark on cluster as well as local. While running using spark I don't want to open connections on executors. So the plan is to run code on driver using teradatasql package. Since teradatasql packages so library I thought I don't have install teradata library on cluster. 

I packaged the dependencies i.e. teradatasql as egg file and passed it as --py-files. But while running on code teradatasql is not able to read library from egg file. 

Os error: teradatasql.so cannot open shared object file. Not a directory.

I followed the below steps to package the egg file.
1. pip install teradatasql --target./src # note all my code is in src folder. Doing this step will install teradatasql package in my src folder. it contains teradatasql.so library
2. In setup.py packages=find_packages('src'), package_data={'teradatasql':['teradatasql.so']}
3. python setup.py bdist_eggg
",1
469,61940953,Cannot append teradata volatile table using RODBC,"Alright folks,

I am trying to append a volatile table on a teradata server using RODBC. The code I have written is as follows:

create_query = paste(
  'CREATE VOLATILE TABLE',
  volatile_name,
  '(',  dl_uid, ' VARCHAR(', char_width, '))',
  'UNIQUE PRIMARY INDEX(', dl_uid, ')',
  ""ON COMMIT PRESERVE ROWS;""
)

RODBC::sqlQuery(
  channel,
  query = create_query
)

RODBC::sqlSave(
  channel,
  dat = df2,
  tablename = volatile_name,
  rownames = FALSE,
  append = TRUE
)


This is the error message I get:


  Error in RODBC::sqlSave(channel = edv, dat = df2, tablename =
  volatile_name,  :    42S01 -3803 [Teradata][ODBC Teradata
  Driver][Teradata Database] Table 'edvR_4425' already exists.  [RODBC]
  ERROR: Could not SQLExecDirect 'CREATE TABLE ""edvR_4425""  (""id_num""
  varchar(255))'


I find this a bit strange as I chose the argument append = TRUE so that it would know to append a pre-existing table. I have tried this code with a permanent table and it works fine. Any suggestions?
",-1,-1,-1.0,"Alright folks,

I am trying to append a volatile table on a teradata server using RODBC. The code I have written is as follows:

create_query = paste(
  'CREATE VOLATILE TABLE',
  volatile_name,
  '(',  dl_uid, ' VARCHAR(', char_width, '))',
  'UNIQUE PRIMARY INDEX(', dl_uid, ')',
  ""ON COMMIT PRESERVE ROWS;""
)

RODBC::sqlQuery(
  channel,
  query = create_query
)

RODBC::sqlSave(
  channel,
  dat = df2,
  tablename = volatile_name,
  rownames = FALSE,
  append = TRUE
)


This is the error message I get:


  Error in RODBC::sqlSave(channel = edv, dat = df2, tablename =
  volatile_name,  :    42S01 -3803 [Teradata][ODBC Teradata
  Driver][Teradata Database] Table 'edvR_4425' already exists.  [RODBC]
  ERROR: Could not SQLExecDirect 'CREATE TABLE ""edvR_4425""  (""id_num""
  varchar(255))'


I find this a bit strange as I chose the argument append = TRUE so that it would know to append a pre-existing table. I have tried this code with a permanent table and it works fine. Any suggestions?
",3
470,62173063,drop multiple tables in proc sql (via teradata),"I am trying to drop multiple tables, using the following code, but it raises an error.

the code:

    PROC SQL ;
        CONNECT TO teradata AS TERADATA (server=dbc  mode=teradata) ;
        EXECUTE (drop table TABLE_NAME1, TABLE_NAME2, TABLE_NAME3  ) BY teradata ;
        DISCONNECT FROM teradata ;
    QUIT ;


the error: 
syntax error: expecting something between NAME1 and TABLE
",-1,-1,-1.0,"I am trying to drop multiple tables, using the following code, but it raises an error.

the code:

    PROC SQL ;
        CONNECT TO teradata AS TERADATA (server=dbc  mode=teradata) ;
        EXECUTE (drop table TABLE_NAME1, TABLE_NAME2, TABLE_NAME3  ) BY teradata ;
        DISCONNECT FROM teradata ;
    QUIT ;


the error: 
syntax error: expecting something between NAME1 and TABLE
",3
471,62262973,(434) WSA E TimedOut: Lost socket connection to the Teradata server in R,"I am trying to connect to Teradata in R using an ODBC connection. I use the same configuration on my local machine and it works. However, when I connect through a server it throws the below error.
I have set up an ODBC DSA connection using the teradata driver with all the connection credentials.
How do I fix this?

    &gt;   con &lt;- odbcConnect(""Terr_Con"")
Warning messages:
1: In RODBC::odbcDriverConnect(""DSN=Terr_Con"") :
  [RODBC] ERROR: state HY000, code 434, message [Teradata][socket error] (434) WSA E TimedOut: Lost socket connection to the Teradata server
2: In RODBC::odbcDriverConnect(""DSN=Terr_Con"") :
  ODBC connection failed

",-1,-1,-1.0,"I am trying to connect to Teradata in R using an ODBC connection. I use the same configuration on my local machine and it works. However, when I connect through a server it throws the below error.
I have set up an ODBC DSA connection using the teradata driver with all the connection credentials.
How do I fix this?

    &gt;   con &lt;- odbcConnect(""Terr_Con"")
Warning messages:
1: In RODBC::odbcDriverConnect(""DSN=Terr_Con"") :
  [RODBC] ERROR: state HY000, code 434, message [Teradata][socket error] (434) WSA E TimedOut: Lost socket connection to the Teradata server
2: In RODBC::odbcDriverConnect(""DSN=Terr_Con"") :
  ODBC connection failed

",1
472,62578945,Teradata Table creation throwing an error: Syntax error: expected something between the 'TABLE' keyword and the 'PUBLIC' keyword,"I am connected to Teradata via the dbeaver.  I successfully got connected to it after downloading the libraries from the https://downloads.teradata.com/. I am completely new to teradata. I copied one command from the tutorials point and tried to ran it.
CREATE SET TABLE PUBLIC.EMPLOYEE,FALLBACK ( 
   EmployeeNo INTEGER, 
   FirstName VARCHAR(30), 
   LastName VARCHAR(30), 
   DOB DATE FORMAT 'YYYY-MM-DD', 
   JoinedDate DATE FORMAT 'YYYY-MM-DD', 
   DepartmentNo BYTEINT 
) 
UNIQUE PRIMARY INDEX ( EmployeeNo );

But I keep getting the error:
SQL Error [3706] [42000]: [Teradata Database] [TeraJDBC 16.20.00.06] [Error 3706] [SQLState 42000] Syntax error: expected something between the 'TABLE' keyword and the 'PUBLIC' keyword.
I even tried creating the table form the dbeaver UI then still it is giving me an error. If someone could help me I will really appreciate it.
",-1,-1,-1.0,"I am connected to Teradata via the dbeaver.  I successfully got connected to it after downloading the libraries from the https://downloads.teradata.com/. I am completely new to teradata. I copied one command from the tutorials point and tried to ran it.
CREATE SET TABLE PUBLIC.EMPLOYEE,FALLBACK ( 
   EmployeeNo INTEGER, 
   FirstName VARCHAR(30), 
   LastName VARCHAR(30), 
   DOB DATE FORMAT 'YYYY-MM-DD', 
   JoinedDate DATE FORMAT 'YYYY-MM-DD', 
   DepartmentNo BYTEINT 
) 
UNIQUE PRIMARY INDEX ( EmployeeNo );

But I keep getting the error:
SQL Error [3706] [42000]: [Teradata Database] [TeraJDBC 16.20.00.06] [Error 3706] [SQLState 42000] Syntax error: expected something between the 'TABLE' keyword and the 'PUBLIC' keyword.
I even tried creating the table form the dbeaver UI then still it is giving me an error. If someone could help me I will really appreciate it.
",3
473,62599412,Error connecting to Teradata by Python via teradatasql module || ( failed to follow the required security policy),"I am trying to connect to Teradata DB via python using teradatasql module. Below is my code which is simple one.

    import teradatasql
    import pandas as pd
    
    query=&quot;select * from DBC.tablesv sample 10&quot;
    
    with teradatasql.connect(host='&lt;host ip&gt;', user='&lt;username&gt;', password='&lt;password for user&gt;') as connect:
        df = pd.read_sql(query, connect)
    
    print(df.head())


But I am getting error message on execution like below. I am a beginner to database programming. Any help from experts will be deeply appreciated.
----------------------------------------------------------------

    Traceback (most recent call last):
      File &quot;/data/home/uk393e/anaconda3-5.2.0/lib/python3.6/site-packages/pandas/io/sql.py&quot;, line 1400, in execute
        cur.execute(*args)
      File &quot;/data/home/uk393e/anaconda3-5.2.0/lib/python3.6/site-packages/teradatasql/__init__.py&quot;, line 649, in execute
        self.executemany (sOperation, None, ignoreErrors)
      File &quot;/data/home/uk393e/anaconda3-5.2.0/lib/python3.6/site-packages/teradatasql/__init__.py&quot;, line 896, in executemany
        raise OperationalError (sErr)
    teradatasql.OperationalError: [Version 17.0.0.2] [Session 91835188] [Teradata Database] [Error 8056] A message was received that failed to follow the required security policy
     at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError TeradataConnection.go:1138
     at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError TeradataConnection.go:1154
     at gosqldriver/teradatasql.(*teradataConnection).processErrorParcel TeradataConnection.go:1217
     at gosqldriver/teradatasql.(*TeradataRows).processResponseBundle TeradataRows.go:1716
     at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:552
     at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:418
     at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
     at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2083
     at database/sql.ctxDriverQuery ctxutil.go:48

|
|
|
  File &quot;/data/home/uk393e/anaconda3-5.2.0/lib/python3.6/site-packages/teradatasql/__init__.py&quot;, line 896, in executemany
    raise OperationalError (sErr)
pandas.io.sql.DatabaseError: Execution failed on sql: select * from DBC.tablesv
[Version 17.0.0.2] [Session 91835188] [Teradata Database] [Error 8056] A message was received that failed to follow the required security policy
 at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError TeradataConnection.go:1138
 at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError TeradataConnection.go:1154
 at gosqldriver/teradatasql.(*teradataConnection).processErrorParcel TeradataConnection.go:1217
 at gosqldriver/teradatasql.(*TeradataRows).processResponseBundle TeradataRows.go:1716
 at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:552
 at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:418
 at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
 at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2083
 at database/sql.ctxDriverQuery ctxutil.go:48
 at database/sql.(*DB).queryDC.func1 sql.go:1464
 at database/sql.withLock sql.go:3032
 at database/sql.(*DB).queryDC sql.go:1459
 at database/sql.(*Conn).QueryContext sql.go:1701
 at main.goCreateRows goside.go:652
 at main._cgoexpwrap_212fad278f55_goCreateRows _cgo_gotypes.go:357
 at runtime.call64 asm_amd64.s:574
 at runtime.cgocallbackg1 cgocall.go:316
 at runtime.cgocallbackg cgocall.go:194
 at runtime.cgocallback_gofunc asm_amd64.s:826
 at runtime.goexit asm_amd64.s:2361
unable to rollback
------------------------------------------------------------

",-1,-1,-1.0,"I am trying to connect to Teradata DB via python using teradatasql module. Below is my code which is simple one.

    import teradatasql
    import pandas as pd
    
    query=&quot;select * from DBC.tablesv sample 10&quot;
    
    with teradatasql.connect(host='&lt;host ip&gt;', user='&lt;username&gt;', password='&lt;password for user&gt;') as connect:
        df = pd.read_sql(query, connect)
    
    print(df.head())


But I am getting error message on execution like below. I am a beginner to database programming. Any help from experts will be deeply appreciated.
----------------------------------------------------------------

    Traceback (most recent call last):
      File &quot;/data/home/uk393e/anaconda3-5.2.0/lib/python3.6/site-packages/pandas/io/sql.py&quot;, line 1400, in execute
        cur.execute(*args)
      File &quot;/data/home/uk393e/anaconda3-5.2.0/lib/python3.6/site-packages/teradatasql/__init__.py&quot;, line 649, in execute
        self.executemany (sOperation, None, ignoreErrors)
      File &quot;/data/home/uk393e/anaconda3-5.2.0/lib/python3.6/site-packages/teradatasql/__init__.py&quot;, line 896, in executemany
        raise OperationalError (sErr)
    teradatasql.OperationalError: [Version 17.0.0.2] [Session 91835188] [Teradata Database] [Error 8056] A message was received that failed to follow the required security policy
     at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError TeradataConnection.go:1138
     at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError TeradataConnection.go:1154
     at gosqldriver/teradatasql.(*teradataConnection).processErrorParcel TeradataConnection.go:1217
     at gosqldriver/teradatasql.(*TeradataRows).processResponseBundle TeradataRows.go:1716
     at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:552
     at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:418
     at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
     at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2083
     at database/sql.ctxDriverQuery ctxutil.go:48

|
|
|
  File &quot;/data/home/uk393e/anaconda3-5.2.0/lib/python3.6/site-packages/teradatasql/__init__.py&quot;, line 896, in executemany
    raise OperationalError (sErr)
pandas.io.sql.DatabaseError: Execution failed on sql: select * from DBC.tablesv
[Version 17.0.0.2] [Session 91835188] [Teradata Database] [Error 8056] A message was received that failed to follow the required security policy
 at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError TeradataConnection.go:1138
 at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError TeradataConnection.go:1154
 at gosqldriver/teradatasql.(*teradataConnection).processErrorParcel TeradataConnection.go:1217
 at gosqldriver/teradatasql.(*TeradataRows).processResponseBundle TeradataRows.go:1716
 at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:552
 at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:418
 at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
 at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2083
 at database/sql.ctxDriverQuery ctxutil.go:48
 at database/sql.(*DB).queryDC.func1 sql.go:1464
 at database/sql.withLock sql.go:3032
 at database/sql.(*DB).queryDC sql.go:1459
 at database/sql.(*Conn).QueryContext sql.go:1701
 at main.goCreateRows goside.go:652
 at main._cgoexpwrap_212fad278f55_goCreateRows _cgo_gotypes.go:357
 at runtime.call64 asm_amd64.s:574
 at runtime.cgocallbackg1 cgocall.go:316
 at runtime.cgocallbackg cgocall.go:194
 at runtime.cgocallback_gofunc asm_amd64.s:826
 at runtime.goexit asm_amd64.s:2361
unable to rollback
------------------------------------------------------------

",1
474,62812220,Error 3807 in teradata when trying to run a query,"TERADATA DATABASE
I am getting more rows returning when I do a left join. My base population in my volatile table is 559,157 rows. And what I am trying to do is only have the 559,157 rows from the base population return. But after running this query:
SELECT distinct a.*, b.column1, b.column2, b.column3, b.column4, b.column5, b.column6, c.column3, SUM(c.column3) AS Total_Column3_Profit, AVG(c.column3) AS Column3_Profit_Average
,CASE 
WHEN b.column5 &lt; a.column6 1
      ELSE 0
      END as column_open_flag
,CASE
WHEN a.column4 &lt; 580 THEN 1
     WHEN a.column4 between 580 and 619 THEN 2
     WHEN a.column4 between 620 and 639 THEN 3
     WHEN a.column4 between 640 and 659 THEN 4
     WHEN a.column4 between 660 and 679 THEN 5
     WHEN a.column4 between 680 and 699 THEN 6
     WHEN a.column4 between 700 and 739 THEN 7
     WHEN a.column4 &gt;= 740 THEN 8
     ELSE 0
     END as column4_band
FROM volatile_table1 a
LEFT JOIN database_table1 b
ON a.column1 = b.column0
LEFT JOIN volatile_table2 c
ON a.column1 = c.column2
GROUP BY a.column1
,a.column2
,a.column3
,a.column4
,a.column5
,a.column6
,b.column1
,b.column2
,b.column3
,b.column4
,b.column5
,b.column6
,c.column3

I now end up with over 2,000,000 rows returning. So I wanted to try a UNION clause. Here's my updated query:
SELECT a.column1, a.column2, a.column3, a.column4, a.column5, a.column6
,CASE 
WHEN b.column5 &lt; a.column6 1
      ELSE 0
      END as column_open_flag,
CASE
WHEN a.column4 &lt; 580 THEN 1
     WHEN a.column4 between 580 and 619 THEN 2
     WHEN a.column4 between 620 and 639 THEN 3
     WHEN a.column4 between 640 and 659 THEN 4
     WHEN a.column4 between 660 and 679 THEN 5
     WHEN a.column4 between 680 and 699 THEN 6
     WHEN a.column4 between 700 and 739 THEN 7
     WHEN a.column4 &gt;= 740 THEN 8
     ELSE 0
     END as column4_band
FROM volatile_table1 a
UNION
SELECT b.column1, b.column2, b.column3, b.column4, b.column5, b.column6
FROM database_table1 b
UNION
SELECT c.column3, SUM(c.column3) AS Total_Column3_Profit, AVG(c.column3) AS Column3_Profit_Average
FROM volatile_table2 c
GROUP BY a.column1
,a.column2
,a.column3
,a.column4
,a.column5
,a.column6
,b.column1
,b.column2
,b.column3
,b.column4
,b.column5
,b.column6
,c.column3

Now I keep getting the 3807 error: Object 'a' does not exist. I ran a
HELP volatile table

query and it says both volatile tables do exist. Could someone please point me in the direction of my error? (I also tried removing the first case statement that refers to b.column5&lt;a.column 6. Same error).
EDIT:
I updated to this from comments and now I get error 3653.'All select-lists do not contain the same number of expressions:
SELECT distinct a.column1, a.column2, a.column3, a.column4, a.column5, a.column6, b.column1, b.column2, b.column3, b.column4, b.column5, b.column6, c.column3, SUM(column3) AS Total_Profit, AVG(column3) AS Profit_Averag
,CASE 
WHEN b.column5 &lt; a.column6 1
      ELSE 0
      END as column_open_flag
,CASE
WHEN a.column4 &lt; 580 THEN 1
     WHEN a.column4 between 580 and 619 THEN 2
     WHEN a.column4 between 620 and 639 THEN 3
     WHEN a.column4 between 640 and 659 THEN 4
     WHEN a.column4 between 660 and 679 THEN 5
     WHEN a.column4 between 680 and 699 THEN 6
     WHEN a.column4 between 700 and 739 THEN 7
     WHEN a.column4 &gt;= 740 THEN 8
     ELSE 0
     END as column4_band
FROM 
(
SELECT a.column1, a.column2, a.column3, a.column4, a.column5, a.column6
FROM volatile_table1 a
UNION ALL
SELECT b.column1, b.column2, b.column3, b.column4, b.column5, b.column6
FROM database_table1 b
UNION ALL
SELECT c.column3, SUM(c.column3) AS Total_Column3_Profit, AVG(c.column3) AS Column3_Profit_Average
FROM volatile_table2 c
GROUP BY 1
) d
GROUP BY a.column1
,a.column2
,a.column3
,a.column4
,a.column5
,a.column6
,b.column1
,b.column2
,b.column3
,b.column4
,b.column5
,b.column6
,c.column3

",-1,-1,-1.0,"TERADATA DATABASE
I am getting more rows returning when I do a left join. My base population in my volatile table is 559,157 rows. And what I am trying to do is only have the 559,157 rows from the base population return. But after running this query:
SELECT distinct a.*, b.column1, b.column2, b.column3, b.column4, b.column5, b.column6, c.column3, SUM(c.column3) AS Total_Column3_Profit, AVG(c.column3) AS Column3_Profit_Average
,CASE 
WHEN b.column5 &lt; a.column6 1
      ELSE 0
      END as column_open_flag
,CASE
WHEN a.column4 &lt; 580 THEN 1
     WHEN a.column4 between 580 and 619 THEN 2
     WHEN a.column4 between 620 and 639 THEN 3
     WHEN a.column4 between 640 and 659 THEN 4
     WHEN a.column4 between 660 and 679 THEN 5
     WHEN a.column4 between 680 and 699 THEN 6
     WHEN a.column4 between 700 and 739 THEN 7
     WHEN a.column4 &gt;= 740 THEN 8
     ELSE 0
     END as column4_band
FROM volatile_table1 a
LEFT JOIN database_table1 b
ON a.column1 = b.column0
LEFT JOIN volatile_table2 c
ON a.column1 = c.column2
GROUP BY a.column1
,a.column2
,a.column3
,a.column4
,a.column5
,a.column6
,b.column1
,b.column2
,b.column3
,b.column4
,b.column5
,b.column6
,c.column3

I now end up with over 2,000,000 rows returning. So I wanted to try a UNION clause. Here's my updated query:
SELECT a.column1, a.column2, a.column3, a.column4, a.column5, a.column6
,CASE 
WHEN b.column5 &lt; a.column6 1
      ELSE 0
      END as column_open_flag,
CASE
WHEN a.column4 &lt; 580 THEN 1
     WHEN a.column4 between 580 and 619 THEN 2
     WHEN a.column4 between 620 and 639 THEN 3
     WHEN a.column4 between 640 and 659 THEN 4
     WHEN a.column4 between 660 and 679 THEN 5
     WHEN a.column4 between 680 and 699 THEN 6
     WHEN a.column4 between 700 and 739 THEN 7
     WHEN a.column4 &gt;= 740 THEN 8
     ELSE 0
     END as column4_band
FROM volatile_table1 a
UNION
SELECT b.column1, b.column2, b.column3, b.column4, b.column5, b.column6
FROM database_table1 b
UNION
SELECT c.column3, SUM(c.column3) AS Total_Column3_Profit, AVG(c.column3) AS Column3_Profit_Average
FROM volatile_table2 c
GROUP BY a.column1
,a.column2
,a.column3
,a.column4
,a.column5
,a.column6
,b.column1
,b.column2
,b.column3
,b.column4
,b.column5
,b.column6
,c.column3

Now I keep getting the 3807 error: Object 'a' does not exist. I ran a
HELP volatile table

query and it says both volatile tables do exist. Could someone please point me in the direction of my error? (I also tried removing the first case statement that refers to b.column5&lt;a.column 6. Same error).
EDIT:
I updated to this from comments and now I get error 3653.'All select-lists do not contain the same number of expressions:
SELECT distinct a.column1, a.column2, a.column3, a.column4, a.column5, a.column6, b.column1, b.column2, b.column3, b.column4, b.column5, b.column6, c.column3, SUM(column3) AS Total_Profit, AVG(column3) AS Profit_Averag
,CASE 
WHEN b.column5 &lt; a.column6 1
      ELSE 0
      END as column_open_flag
,CASE
WHEN a.column4 &lt; 580 THEN 1
     WHEN a.column4 between 580 and 619 THEN 2
     WHEN a.column4 between 620 and 639 THEN 3
     WHEN a.column4 between 640 and 659 THEN 4
     WHEN a.column4 between 660 and 679 THEN 5
     WHEN a.column4 between 680 and 699 THEN 6
     WHEN a.column4 between 700 and 739 THEN 7
     WHEN a.column4 &gt;= 740 THEN 8
     ELSE 0
     END as column4_band
FROM 
(
SELECT a.column1, a.column2, a.column3, a.column4, a.column5, a.column6
FROM volatile_table1 a
UNION ALL
SELECT b.column1, b.column2, b.column3, b.column4, b.column5, b.column6
FROM database_table1 b
UNION ALL
SELECT c.column3, SUM(c.column3) AS Total_Column3_Profit, AVG(c.column3) AS Column3_Profit_Average
FROM volatile_table2 c
GROUP BY 1
) d
GROUP BY a.column1
,a.column2
,a.column3
,a.column4
,a.column5
,a.column6
,b.column1
,b.column2
,b.column3
,b.column4
,b.column5
,b.column6
,c.column3

",3
475,62828879,Returning reactivated users teradata sql,"I am trying to form a query for returning, reactivated and WAU as defined below:

Returning WAU - active last week
WAU - not active last week, but active within last 30 days
Reactivated WAU – not seen in 30+ days

I have table for past 60 days containing cust_id, login date but cant lag function to work (Teradata ODBC connection). I keep getting this error:

[3706] Syntax error: Data Type &quot;logindate&quot; does not match a Defined
Type name. My format is: select .... lag(logindate, 1) over (partition
by cust_id order by 1 asc) as lag_ind from ( ....

Please help for the 3 cases above.
",-1,0,-1.0,"I am trying to form a query for returning, reactivated and WAU as defined below:

Returning WAU - active last week
WAU - not active last week, but active within last 30 days
Reactivated WAU – not seen in 30+ days

I have table for past 60 days containing cust_id, login date but cant lag function to work (Teradata ODBC connection). I keep getting this error:

[3706] Syntax error: Data Type &quot;logindate&quot; does not match a Defined
Type name. My format is: select .... lag(logindate, 1) over (partition
by cust_id order by 1 asc) as lag_ind from ( ....

Please help for the 3 cases above.
",3
476,62915424,how to pass list variable in teradata sql in sas?,"i have sas table containing id column and I am querying teradata sql with condition for sql table to have same id as the sas table. I use code below:
libname ss&quot;dir&quot;;
proc sql;
connect to teradata(server);
crreate table ss.new as select * from connection to teradata(
select some from new_db where id in (select id from ss.table));

The code is not recognizing ss library. How can i pass the column as sql parameter ?
",-1,-1,-1.0,"i have sas table containing id column and I am querying teradata sql with condition for sql table to have same id as the sas table. I use code below:
libname ss&quot;dir&quot;;
proc sql;
connect to teradata(server);
crreate table ss.new as select * from connection to teradata(
select some from new_db where id in (select id from ss.table));

The code is not recognizing ss library. How can i pass the column as sql parameter ?
",3
477,63003422,DLL initialization error while trying to connect to Teradata with Python,"I’ve been using Python’s teradatasql library to query Teradata, and just started getting a DLL initialization error with code that had previously been working.
This is the precise error:
OSError: [WinError 1114] A dynamic link library (DLL) initialization routine failed
It throws when I call teradatasql.connect(). Here is the full stack trace:
File &quot;test.py&quot;, line 23, in &lt;module&gt;
    password = db_password
  File &quot;C:\Program Files (x86)\Python\Python37\lib\site-packages\teradatasql\__init__.py&quot;, line 101, in __init__
    goside = ctypes.cdll.LoadLibrary(sLibPathName)
  File &quot;C:\Program Files (x86)\Python\Python37\lib\ctypes\__init__.py&quot;, line 434, in LoadLibrary
    return self._dlltype(name)
  File &quot;C:\Program Files (x86)\Python\Python37\lib\ctypes\__init__.py&quot;, line 356, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 1114] A dynamic link library (DLL) initialization routine failed

I’ve tried manually loading the dll which teradatasql’s _init_ file is looking at, with
import ctypes
goside = ctypes.cdll.LoadLibrary(r'C:\Program Files (x86)\Python\Python37\Lib\site-packages\teradatasql\teradatasql.dll')

And it throws the same error, although ctypes.cdll.LoadLibrary() works for other .dll files on my machine.
Have tried reinstalling teradatasql and reverting to a previous version. Neither was helpful.
Bizarrely, the connection actually does work sometimes. The code which produced the stack trace above works when it’s in an .ipynb file rather than a .py. But I get the same error in .ipynb files with slightly different code.
Here’s my code:
import pandas as pd
import teradatasql 
from getpass import getpass 


db_password = getpass('Teradata password: ')


with teradatasql.connect(
    host = xxxx,
    user = xxxx,
    password = db_password
) as connect:
    query = '''
        …
    '''
    results = pd.read_sql(query, connect)

How do I deal with this dll issue and get the connection working reliably again?
",-1,-1,-1.0,"I’ve been using Python’s teradatasql library to query Teradata, and just started getting a DLL initialization error with code that had previously been working.
This is the precise error:
OSError: [WinError 1114] A dynamic link library (DLL) initialization routine failed
It throws when I call teradatasql.connect(). Here is the full stack trace:
File &quot;test.py&quot;, line 23, in &lt;module&gt;
    password = db_password
  File &quot;C:\Program Files (x86)\Python\Python37\lib\site-packages\teradatasql\__init__.py&quot;, line 101, in __init__
    goside = ctypes.cdll.LoadLibrary(sLibPathName)
  File &quot;C:\Program Files (x86)\Python\Python37\lib\ctypes\__init__.py&quot;, line 434, in LoadLibrary
    return self._dlltype(name)
  File &quot;C:\Program Files (x86)\Python\Python37\lib\ctypes\__init__.py&quot;, line 356, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 1114] A dynamic link library (DLL) initialization routine failed

I’ve tried manually loading the dll which teradatasql’s _init_ file is looking at, with
import ctypes
goside = ctypes.cdll.LoadLibrary(r'C:\Program Files (x86)\Python\Python37\Lib\site-packages\teradatasql\teradatasql.dll')

And it throws the same error, although ctypes.cdll.LoadLibrary() works for other .dll files on my machine.
Have tried reinstalling teradatasql and reverting to a previous version. Neither was helpful.
Bizarrely, the connection actually does work sometimes. The code which produced the stack trace above works when it’s in an .ipynb file rather than a .py. But I get the same error in .ipynb files with slightly different code.
Here’s my code:
import pandas as pd
import teradatasql 
from getpass import getpass 


db_password = getpass('Teradata password: ')


with teradatasql.connect(
    host = xxxx,
    user = xxxx,
    password = db_password
) as connect:
    query = '''
        …
    '''
    results = pd.read_sql(query, connect)

How do I deal with this dll issue and get the connection working reliably again?
",1
478,63307336,Connecting Tableau to Teradata with Custom SQL Query,"EDIT: Even if I try removing that 'AS' everywhere, I continue getting errors about things that seem like normal SQL commands. If I remove the 'AS', the next error I get is:

[IBM][CLI Driver][DB2] SQL0199N The use of the reserved word &quot;CAST&quot;
following &quot;&quot; is not valid. Expected tokens may include: &quot;, )&quot;.
SQLSTATE=42601

Question:
I am trying to connect to Teradata with a Custom SQL Query, and while it is running perfectly in Teradata SQL Assistant, I keep getting errors when trying to run it in Tableau.
One of the errors looks like this, but even if I try to change something, a similar error will come up:

[IBM][CLI Driver][DB2] SQL0199N The use of the reserved word &quot;AS&quot;
following &quot;&quot; is not valid. Expected tokens may include: &quot;JOIN CROSS
INNER LEFT RIGHT FULL (&quot;. SQLSTATE=42601

Here is the query (sorry, it's long):
WITH RQSTS AS(

select CAST( M.RQST_ID AS VARCHAR(20)) AS MASTER_RQST_ID,

M.RQST_TYP_CD

FROM PRTHD.HOST_RQST AS M --MASTER

WHERE M.MSTR_RQST_ID IS NULL

  AND M.crt_user_id='SXB8NBS'

  AND M.OCYC_SOQ_RSN_CD = 170

  AND date(M.CRT_TS) &gt;= CURRENT_DATE - 7 DAY

),

 

RQST_TYPE AS(

select R.MASTER_RQST_ID AS RQST_ID, 

NRT.TYP_DESC AS HOST_TYPE

FROM RQSTS AS R

LEFT JOIN PRTHD.N_HOST_RQST_TYP AS NRT     

ON R.RQST_TYP_CD = NRT.RQST_TYP_CD

GROUP BY R.MASTER_RQST_ID, NRT.TYP_DESC

),

 

PO_DETAILS AS(

select A.* 

FROM (

  SELECT distinct PRTHD.BYO_PO.DSVC_TYP_CD,

  RT.RQST_ID,

  RT.HOST_TYPE,

  PRTHD.BYO_PO.PO_CMT_TXT,

  PRTHD.BYO_PO.BYO_NBR,

  CASE WHEN PRTHD.BYO_PO.DSVC_TYP_CD = 2 THEN RIGHT('00'||PRTHD.BYO_PO.BYO_NBR,2) 

     ELSE RIGHT('00'|| PRTHD.BYO_PO_LOC_SKU.LOC_NBR,2) END

  || PRTHD.BYO_PO.acct_po_typ_cd || RIGHT('00000'||PRTHD.BYO_PO.PO_CTRL_NBR,5) AS PO,

  PRTHD.BYO_PO.ORD_REF_NBR,

  PRTHD.BYO_PO_LOC_SKU.MKT_DC_NBR,

  PRTHD.BYO_PO_LOC_SKU.LOC_NBR,

  PRTHD.BYO_PO_LOC_SKU.SKU_NBR,

  CAST(PRTHD.BYO_PO_LOC_SKU.ORD_QTY AS INTEGER) AS ORD_QTY,

  PRTHD.BYO_PO.CRT_DT,

  PRTHD.BYO_PO.PO_CTRL_NBR,

  M.MVNDR_NM,

  PRTHD.BYO_PO.MVNDR_NBR,

  PRTHD.BYO_PO.OCYC_SOQ_RSN_CD AS RSN_CD

FROM ((PRTHD.BYO_PO AS BP INNER JOIN PRTHD.BYO_PO_LOC_SKU

  ON (PRTHD.BYO_PO.ORD_SEQ_NBR = PRTHD.BYO_PO_LOC_SKU.ORD_SEQ_NBR)

  AND (PRTHD.BYO_PO.PO_CTRL_NBR = PRTHD.BYO_PO_LOC_SKU.PO_CTRL_NBR)

  AND (PRTHD.BYO_PO.ACCT_PO_TYP_CD = PRTHD.BYO_PO_LOC_SKU.ACCT_PO_TYP_CD)

  AND (PRTHD.BYO_PO.MKT_DC_NBR = PRTHD.BYO_PO_LOC_SKU.MKT_DC_NBR)

  AND (PRTHD.BYO_PO.MKT_DC_IND = PRTHD.BYO_PO_LOC_SKU.MKT_DC_IND)

  AND (PRTHD.BYO_PO.BYO_NBR = PRTHD.BYO_PO_LOC_SKU.BYO_NBR))

INNER JOIN PRTHD.SKU 

  ON PRTHD.BYO_PO_LOC_SKU.SKU_NBR = PRTHD.SKU.SKU_NBR)

INNER JOIN PRTHD.BYO_PO_SKU 

  ON (PRTHD.SKU.SKU_NBR = PRTHD.BYO_PO_SKU.SKU_NBR)

  AND (PRTHD.BYO_PO_LOC_SKU.BYO_NBR = PRTHD.BYO_PO_SKU.BYO_NBR) 

  AND (PRTHD.BYO_PO_LOC_SKU.MKT_DC_IND = PRTHD.BYO_PO_SKU.MKT_DC_IND)

  AND (PRTHD.BYO_PO_LOC_SKU.MKT_DC_NBR = PRTHD.BYO_PO_SKU.MKT_DC_NBR)

  AND (PRTHD.BYO_PO_LOC_SKU.ACCT_PO_TYP_CD = PRTHD.BYO_PO_SKU.ACCT_PO_TYP_CD)

  AND (PRTHD.BYO_PO_LOC_SKU.PO_CTRL_NBR = PRTHD.BYO_PO_SKU.PO_CTRL_NBR)

  AND (PRTHD.BYO_PO_LOC_SKU.ORD_SEQ_NBR = PRTHD.BYO_PO_SKU.ORD_SEQ_NBR)

  AND (PRTHD.BYO_PO_LOC_SKU.PO_LINE_NBR = PRTHD.BYO_PO_SKU.PO_LINE_NBR)

LEFT JOIN PRTHD.MVNDR AS M

  ON M.MVNDR_NBR = PRTHD.BYO_PO.MVNDR_NBR

INNER JOIN RQST_TYPE AS RT

  ON TRIM(SUBSTR(BP.PO_CMT_TXT,6,11)) = RT.RQST_ID

) AS A)

 

Select P.RQST_ID,

P.DSVC_TYP_CD,

CASE WHEN P.DSVC_TYP_CD =1 THEN 'DTS'

   WHEN P.DSVC_TYP_CD =2 THEN 'RDC AGG'

   WHEN P.DSVC_TYP_CD =3 THEN 'RDCX'

   END AS DSVC_TYP_DESC ,

CASE WHEN P.DSVC_TYP_CD =2 AND S.STR_NBR IS NOT NULL THEN 'Y'

   ELSE 'N' END AS RDC_AGG_PEG_FLG,

CASE WHEN P.DSVC_TYP_CD =2 AND S.STR_NBR IS NOT NULL THEN P.ORD_REF_NBR

   ELSE P.PO END AS PO_NBR,

P.BYO_NBR,

P.PO_CTRL_NBR,

P.CRT_DT,

P.HOST_TYPE,

P.MVNDR_NBR,

P.MVNDR_NM,

CASE WHEN P.DSVC_TYP_CD =2 AND S.STR_NBR IS NOT NULL THEN AR.MKT_DC_NBR 

   ELSE P.MKT_DC_NBR END AS MKT_DC_NBR,

P.LOC_NBR,

COALESCE(SKU.MER_DEPT_NBR || '-' || D.SHRT_DEPT_NM,'') AS DEPT,

COALESCE(SKU.MER_CLASS_NBR || '-' || MC.SHRT_CLASS_DESC,'') AS CLASS,

COALESCE(SKU.MER_SUB_CLASS_NBR || '-' || MSC.SHRT_SUBCLASS_DESC,'') AS SUB_CLASS,

P.SKU_NBR,

P.ORD_QTY,

COALESCE(SD.AVG_DC_LCOST_AMT*COALESCE(P.ORD_QTY,0),

MSM.CURR_COST_AMT*COALESCE(P.ORD_QTY,0)) AS TOTAL_COST,

SD.AVG_DC_LCOST_AMT AS SD_COST,

MSM.CURR_COST_AMT AS STR_CURR_COST,

MSD.CURR_COST_AMT AS DC_CURR_COST,

P.PO_CMT_TXT,

P.RSN_CD

FROM PO_DETAILS AS P

--SKU HIERARCHY INFO

LEFT JOIN PRTHD.SKU  AS SKU                

  ON SKU.SKU_NBR = P.SKU_NBR

LEFT JOIN PRTHD.DEPT AS D                    

  ON SKU.MER_DEPT_NBR = D.DEPT_NBR

LEFT JOIN PRTHD.MER_CLASS AS MC           

  ON MC.MER_DEPT_NBR= SKU.MER_DEPT_NBR 

  AND MC.MER_CLASS_NBR = SKU.MER_CLASS_NBR

LEFT JOIN PRTHD.MER_SUB_CLASS AS MSC    

  ON MSC.MER_DEPT_NBR= SKU.MER_DEPT_NBR 

  AND MSC.MER_CLASS_NBR = SKU.MER_CLASS_NBR 

  AND MSC.MER_SUB_CLASS_NBR = SKU.MER_SUB_CLASS_NBR

LEFT JOIN PRTHD.MVNDR_SKU_DC AS MSD     

  ON MSD.MVNDR_NBR = P.MVNDR_NBR 

  AND MSD.SKU_NBR = P.SKU_NBR 

  AND MSD.DC_NBR = P.LOC_NBR

LEFT JOIN PRTHD.STR AS S                  

  ON S.STR_NBR = P.LOC_NBR

LEFT JOIN PRTHD.SKU_DC AS SD               

  ON SD.DC_NBR=P.MKT_DC_NBR 

  AND SD.SKU_NBR=P.SKU_NBR

LEFT JOIN PRTHD.MVNDR_SKU_MKT AS MSM    

  ON MSM.MVNDR_NBR = P.MVNDR_NBR 

  AND MSM.SKU_NBR = P.SKU_NBR 

  AND MSM.MKT_NBR = S.MKT_NBR

LEFT JOIN PRTHD.AGG_RQST AS AR 

  ON AR.PO_NBR = P.ORD_REF_NBR 

  AND P.MVNDR_NBR = AR.MVNDR_NBR 

  AND SKU.MER_DEPT_NBR = AR.MER_DEPT_NBR

  AND AR.ORD_DT &gt;= P.CRT_DT

Can anyone help me figure out what might be going wrong here? Thank you!
",-1,-1,-1.0,"EDIT: Even if I try removing that 'AS' everywhere, I continue getting errors about things that seem like normal SQL commands. If I remove the 'AS', the next error I get is:

[IBM][CLI Driver][DB2] SQL0199N The use of the reserved word &quot;CAST&quot;
following &quot;&quot; is not valid. Expected tokens may include: &quot;, )&quot;.
SQLSTATE=42601

Question:
I am trying to connect to Teradata with a Custom SQL Query, and while it is running perfectly in Teradata SQL Assistant, I keep getting errors when trying to run it in Tableau.
One of the errors looks like this, but even if I try to change something, a similar error will come up:

[IBM][CLI Driver][DB2] SQL0199N The use of the reserved word &quot;AS&quot;
following &quot;&quot; is not valid. Expected tokens may include: &quot;JOIN CROSS
INNER LEFT RIGHT FULL (&quot;. SQLSTATE=42601

Here is the query (sorry, it's long):
WITH RQSTS AS(

select CAST( M.RQST_ID AS VARCHAR(20)) AS MASTER_RQST_ID,

M.RQST_TYP_CD

FROM PRTHD.HOST_RQST AS M --MASTER

WHERE M.MSTR_RQST_ID IS NULL

  AND M.crt_user_id='SXB8NBS'

  AND M.OCYC_SOQ_RSN_CD = 170

  AND date(M.CRT_TS) &gt;= CURRENT_DATE - 7 DAY

),

 

RQST_TYPE AS(

select R.MASTER_RQST_ID AS RQST_ID, 

NRT.TYP_DESC AS HOST_TYPE

FROM RQSTS AS R

LEFT JOIN PRTHD.N_HOST_RQST_TYP AS NRT     

ON R.RQST_TYP_CD = NRT.RQST_TYP_CD

GROUP BY R.MASTER_RQST_ID, NRT.TYP_DESC

),

 

PO_DETAILS AS(

select A.* 

FROM (

  SELECT distinct PRTHD.BYO_PO.DSVC_TYP_CD,

  RT.RQST_ID,

  RT.HOST_TYPE,

  PRTHD.BYO_PO.PO_CMT_TXT,

  PRTHD.BYO_PO.BYO_NBR,

  CASE WHEN PRTHD.BYO_PO.DSVC_TYP_CD = 2 THEN RIGHT('00'||PRTHD.BYO_PO.BYO_NBR,2) 

     ELSE RIGHT('00'|| PRTHD.BYO_PO_LOC_SKU.LOC_NBR,2) END

  || PRTHD.BYO_PO.acct_po_typ_cd || RIGHT('00000'||PRTHD.BYO_PO.PO_CTRL_NBR,5) AS PO,

  PRTHD.BYO_PO.ORD_REF_NBR,

  PRTHD.BYO_PO_LOC_SKU.MKT_DC_NBR,

  PRTHD.BYO_PO_LOC_SKU.LOC_NBR,

  PRTHD.BYO_PO_LOC_SKU.SKU_NBR,

  CAST(PRTHD.BYO_PO_LOC_SKU.ORD_QTY AS INTEGER) AS ORD_QTY,

  PRTHD.BYO_PO.CRT_DT,

  PRTHD.BYO_PO.PO_CTRL_NBR,

  M.MVNDR_NM,

  PRTHD.BYO_PO.MVNDR_NBR,

  PRTHD.BYO_PO.OCYC_SOQ_RSN_CD AS RSN_CD

FROM ((PRTHD.BYO_PO AS BP INNER JOIN PRTHD.BYO_PO_LOC_SKU

  ON (PRTHD.BYO_PO.ORD_SEQ_NBR = PRTHD.BYO_PO_LOC_SKU.ORD_SEQ_NBR)

  AND (PRTHD.BYO_PO.PO_CTRL_NBR = PRTHD.BYO_PO_LOC_SKU.PO_CTRL_NBR)

  AND (PRTHD.BYO_PO.ACCT_PO_TYP_CD = PRTHD.BYO_PO_LOC_SKU.ACCT_PO_TYP_CD)

  AND (PRTHD.BYO_PO.MKT_DC_NBR = PRTHD.BYO_PO_LOC_SKU.MKT_DC_NBR)

  AND (PRTHD.BYO_PO.MKT_DC_IND = PRTHD.BYO_PO_LOC_SKU.MKT_DC_IND)

  AND (PRTHD.BYO_PO.BYO_NBR = PRTHD.BYO_PO_LOC_SKU.BYO_NBR))

INNER JOIN PRTHD.SKU 

  ON PRTHD.BYO_PO_LOC_SKU.SKU_NBR = PRTHD.SKU.SKU_NBR)

INNER JOIN PRTHD.BYO_PO_SKU 

  ON (PRTHD.SKU.SKU_NBR = PRTHD.BYO_PO_SKU.SKU_NBR)

  AND (PRTHD.BYO_PO_LOC_SKU.BYO_NBR = PRTHD.BYO_PO_SKU.BYO_NBR) 

  AND (PRTHD.BYO_PO_LOC_SKU.MKT_DC_IND = PRTHD.BYO_PO_SKU.MKT_DC_IND)

  AND (PRTHD.BYO_PO_LOC_SKU.MKT_DC_NBR = PRTHD.BYO_PO_SKU.MKT_DC_NBR)

  AND (PRTHD.BYO_PO_LOC_SKU.ACCT_PO_TYP_CD = PRTHD.BYO_PO_SKU.ACCT_PO_TYP_CD)

  AND (PRTHD.BYO_PO_LOC_SKU.PO_CTRL_NBR = PRTHD.BYO_PO_SKU.PO_CTRL_NBR)

  AND (PRTHD.BYO_PO_LOC_SKU.ORD_SEQ_NBR = PRTHD.BYO_PO_SKU.ORD_SEQ_NBR)

  AND (PRTHD.BYO_PO_LOC_SKU.PO_LINE_NBR = PRTHD.BYO_PO_SKU.PO_LINE_NBR)

LEFT JOIN PRTHD.MVNDR AS M

  ON M.MVNDR_NBR = PRTHD.BYO_PO.MVNDR_NBR

INNER JOIN RQST_TYPE AS RT

  ON TRIM(SUBSTR(BP.PO_CMT_TXT,6,11)) = RT.RQST_ID

) AS A)

 

Select P.RQST_ID,

P.DSVC_TYP_CD,

CASE WHEN P.DSVC_TYP_CD =1 THEN 'DTS'

   WHEN P.DSVC_TYP_CD =2 THEN 'RDC AGG'

   WHEN P.DSVC_TYP_CD =3 THEN 'RDCX'

   END AS DSVC_TYP_DESC ,

CASE WHEN P.DSVC_TYP_CD =2 AND S.STR_NBR IS NOT NULL THEN 'Y'

   ELSE 'N' END AS RDC_AGG_PEG_FLG,

CASE WHEN P.DSVC_TYP_CD =2 AND S.STR_NBR IS NOT NULL THEN P.ORD_REF_NBR

   ELSE P.PO END AS PO_NBR,

P.BYO_NBR,

P.PO_CTRL_NBR,

P.CRT_DT,

P.HOST_TYPE,

P.MVNDR_NBR,

P.MVNDR_NM,

CASE WHEN P.DSVC_TYP_CD =2 AND S.STR_NBR IS NOT NULL THEN AR.MKT_DC_NBR 

   ELSE P.MKT_DC_NBR END AS MKT_DC_NBR,

P.LOC_NBR,

COALESCE(SKU.MER_DEPT_NBR || '-' || D.SHRT_DEPT_NM,'') AS DEPT,

COALESCE(SKU.MER_CLASS_NBR || '-' || MC.SHRT_CLASS_DESC,'') AS CLASS,

COALESCE(SKU.MER_SUB_CLASS_NBR || '-' || MSC.SHRT_SUBCLASS_DESC,'') AS SUB_CLASS,

P.SKU_NBR,

P.ORD_QTY,

COALESCE(SD.AVG_DC_LCOST_AMT*COALESCE(P.ORD_QTY,0),

MSM.CURR_COST_AMT*COALESCE(P.ORD_QTY,0)) AS TOTAL_COST,

SD.AVG_DC_LCOST_AMT AS SD_COST,

MSM.CURR_COST_AMT AS STR_CURR_COST,

MSD.CURR_COST_AMT AS DC_CURR_COST,

P.PO_CMT_TXT,

P.RSN_CD

FROM PO_DETAILS AS P

--SKU HIERARCHY INFO

LEFT JOIN PRTHD.SKU  AS SKU                

  ON SKU.SKU_NBR = P.SKU_NBR

LEFT JOIN PRTHD.DEPT AS D                    

  ON SKU.MER_DEPT_NBR = D.DEPT_NBR

LEFT JOIN PRTHD.MER_CLASS AS MC           

  ON MC.MER_DEPT_NBR= SKU.MER_DEPT_NBR 

  AND MC.MER_CLASS_NBR = SKU.MER_CLASS_NBR

LEFT JOIN PRTHD.MER_SUB_CLASS AS MSC    

  ON MSC.MER_DEPT_NBR= SKU.MER_DEPT_NBR 

  AND MSC.MER_CLASS_NBR = SKU.MER_CLASS_NBR 

  AND MSC.MER_SUB_CLASS_NBR = SKU.MER_SUB_CLASS_NBR

LEFT JOIN PRTHD.MVNDR_SKU_DC AS MSD     

  ON MSD.MVNDR_NBR = P.MVNDR_NBR 

  AND MSD.SKU_NBR = P.SKU_NBR 

  AND MSD.DC_NBR = P.LOC_NBR

LEFT JOIN PRTHD.STR AS S                  

  ON S.STR_NBR = P.LOC_NBR

LEFT JOIN PRTHD.SKU_DC AS SD               

  ON SD.DC_NBR=P.MKT_DC_NBR 

  AND SD.SKU_NBR=P.SKU_NBR

LEFT JOIN PRTHD.MVNDR_SKU_MKT AS MSM    

  ON MSM.MVNDR_NBR = P.MVNDR_NBR 

  AND MSM.SKU_NBR = P.SKU_NBR 

  AND MSM.MKT_NBR = S.MKT_NBR

LEFT JOIN PRTHD.AGG_RQST AS AR 

  ON AR.PO_NBR = P.ORD_REF_NBR 

  AND P.MVNDR_NBR = AR.MVNDR_NBR 

  AND SKU.MER_DEPT_NBR = AR.MER_DEPT_NBR

  AND AR.ORD_DT &gt;= P.CRT_DT

Can anyone help me figure out what might be going wrong here? Thank you!
",2
479,63527174,Teradata Case Then Set Time Interval,"I have a Teradata query where I subtract two TS fields to get the delta in minutes.
I then want to test another field using CASE, and return either the delta TS above or a defined interval.
However, I am getting a 7452: Invalid Interval error with the following code:
(TS2- TS1) day to minute as TS_DELTA,
        case when TIME_CATEGORY in('0-3 HOURS','3-8 HOURS')
        then TS_DELTA
        when TIME_CATEGORY = 'NEGATIVE'
        then interval '0' minute
        when GROUND_TIME = '&gt; 8 HOURS'
        then interval '5' minute
        end as SUM_TIME

I have another CASE that defines TIME_CATEGORY, that works fine. I've narrowed down the error to the
then interval '5' minute

which I can't figure out what's going on. Any ideas?
",-1,-1,-1.0,"I have a Teradata query where I subtract two TS fields to get the delta in minutes.
I then want to test another field using CASE, and return either the delta TS above or a defined interval.
However, I am getting a 7452: Invalid Interval error with the following code:
(TS2- TS1) day to minute as TS_DELTA,
        case when TIME_CATEGORY in('0-3 HOURS','3-8 HOURS')
        then TS_DELTA
        when TIME_CATEGORY = 'NEGATIVE'
        then interval '0' minute
        when GROUND_TIME = '&gt; 8 HOURS'
        then interval '5' minute
        end as SUM_TIME

I have another CASE that defines TIME_CATEGORY, that works fine. I've narrowed down the error to the
then interval '5' minute

which I can't figure out what's going on. Any ideas?
",3
480,63662050,Slow performance reading teradata table in python,"I'm trying to read a table from Teradata big it takes a lot of time.  My table has 5 millions of rows and 60 columns and it took 30 minutes to load in memory. I'm using teradatasql package, but the same table took 5 minutes to load in R with RJDBC package. 
Python code (This take 30 minutes)
import teradatasql
import pandas as pd

conn = teradatasql.connect(host=host, user=user_name, password=password, database=database)
df = pd.read_sql(&quot;SELECT * FROM big_table&quot;, conn)

R code (This take only 3 minutes)
library(RJDBC)

# teradata conecction
con_tera &lt;- dbConnect(drv_tera, &quot;jdbc:teradata://{ip_host}/DATABASE=DBI_MIN,DBS_PORT=1025&quot;,Sys.getenv(&quot;TERA_DB_USER&quot;), Sys.getenv(&quot;TERA_DB_PASS&quot;))

# create query
final_query &lt;- 'select * from big_table'

# get data
dataset_caribu &lt;- dbGetQuery(con_tera,final_query)

I tried to increase arraysize of cursor in python but it doesn't improve execution time much.
",1,-1,-1.0,"I'm trying to read a table from Teradata big it takes a lot of time.  My table has 5 millions of rows and 60 columns and it took 30 minutes to load in memory. I'm using teradatasql package, but the same table took 5 minutes to load in R with RJDBC package. 
Python code (This take 30 minutes)
import teradatasql
import pandas as pd

conn = teradatasql.connect(host=host, user=user_name, password=password, database=database)
df = pd.read_sql(&quot;SELECT * FROM big_table&quot;, conn)

R code (This take only 3 minutes)
library(RJDBC)

# teradata conecction
con_tera &lt;- dbConnect(drv_tera, &quot;jdbc:teradata://{ip_host}/DATABASE=DBI_MIN,DBS_PORT=1025&quot;,Sys.getenv(&quot;TERA_DB_USER&quot;), Sys.getenv(&quot;TERA_DB_PASS&quot;))

# create query
final_query &lt;- 'select * from big_table'

# get data
dataset_caribu &lt;- dbGetQuery(con_tera,final_query)

I tried to increase arraysize of cursor in python but it doesn't improve execution time much.
",3
481,63702851,Creating table inside teradata procedure gives SELECT error,"I am creating a volatile table in TeraData Studio, inside a procedure. I get an error:
Executed as Single statement.  Failed [5315 : HY000] SP_EMPLOYEE:An owner referenced by user 
does not have SELECT access to DB_EMP.all_emp.code. 
Elapsed time = 00:00:00.109 

I know it has something to do with GRANT Option, but I am not able to apply it properly. Here is a small snippet of code:
replace PROCEDURE DBX_HOME.SP_Employee(IN variable INTEGER)
begin

   CREATE VOLATILE TABLE TEST_TABLE AS(
   SELECT distinct ID
   FROM DB_EMP.all_emp as prod left join DB_EMP_DWH.ID_no as infra
   ON prod.code=infra.code_infra
   WHERE AND ID MOD 3  = :variable )WITH DATA  ON COMMIT PRESERVE ROWS;

end;
-- Run the code below.
CALL DBX_HOME.SP_Employee(0)

I followed this link, but I am unable to apply GRANT OPTION properly. Can anyone suggest where to grant permissions, to avoid this error?
",-1,-1,-1.0,"I am creating a volatile table in TeraData Studio, inside a procedure. I get an error:
Executed as Single statement.  Failed [5315 : HY000] SP_EMPLOYEE:An owner referenced by user 
does not have SELECT access to DB_EMP.all_emp.code. 
Elapsed time = 00:00:00.109 

I know it has something to do with GRANT Option, but I am not able to apply it properly. Here is a small snippet of code:
replace PROCEDURE DBX_HOME.SP_Employee(IN variable INTEGER)
begin

   CREATE VOLATILE TABLE TEST_TABLE AS(
   SELECT distinct ID
   FROM DB_EMP.all_emp as prod left join DB_EMP_DWH.ID_no as infra
   ON prod.code=infra.code_infra
   WHERE AND ID MOD 3  = :variable )WITH DATA  ON COMMIT PRESERVE ROWS;

end;
-- Run the code below.
CALL DBX_HOME.SP_Employee(0)

I followed this link, but I am unable to apply GRANT OPTION properly. Can anyone suggest where to grant permissions, to avoid this error?
",3
482,64001256,ssis handling of oracle and teradata passwords,"This has gotten so bad I have to ask if there is a newer feature in ssis (like maybe vs 2017) that handles oracle and teradata passwords.   We work in a secure organization.  They don't want us leaving passwords in the packages for security reasons (I understand that ssis encrypts passwords it knows are passwords in the .dtsx file) so we have to clear them when development is done.  We also use what might be called a password vault for oracle and teradata passwords for our run time jobs which makes the transition from client development to batch job difficult.    Can anyone tell me why it would ever be useful for ssis to attempt a login to Oracle or Teradata when ssis knows the password it has is null or blanks?  A more paranoid part of me might think that Microsoft, with Windows Auth, is trying to drive their users off of Oracle or Teradata.  So is there some ssis job level parameter I can set that instructs ssis not to attempt an oracle or teradata access for metadata unless it has a password?   Some of our jobs access oracle or teradata in 5 or 10 different places, running in there to set Work Offline or Delay Validation for each one of those accesses is difficult.  And I don't want to be told to set Oracle and Teradata to use Windows Auth (if that's even possible).   I don't control that and I need something that I, as an etl developer, control.   I can't believe SSIS competes with products like Datastage with the situation concerning Oracle and Teradata passwords being so bad.
",1,-1,-1.0,"This has gotten so bad I have to ask if there is a newer feature in ssis (like maybe vs 2017) that handles oracle and teradata passwords.   We work in a secure organization.  They don't want us leaving passwords in the packages for security reasons (I understand that ssis encrypts passwords it knows are passwords in the .dtsx file) so we have to clear them when development is done.  We also use what might be called a password vault for oracle and teradata passwords for our run time jobs which makes the transition from client development to batch job difficult.    Can anyone tell me why it would ever be useful for ssis to attempt a login to Oracle or Teradata when ssis knows the password it has is null or blanks?  A more paranoid part of me might think that Microsoft, with Windows Auth, is trying to drive their users off of Oracle or Teradata.  So is there some ssis job level parameter I can set that instructs ssis not to attempt an oracle or teradata access for metadata unless it has a password?   Some of our jobs access oracle or teradata in 5 or 10 different places, running in there to set Work Offline or Delay Validation for each one of those accesses is difficult.  And I don't want to be told to set Oracle and Teradata to use Windows Auth (if that's even possible).   I don't control that and I need something that I, as an etl developer, control.   I can't believe SSIS competes with products like Datastage with the situation concerning Oracle and Teradata passwords being so bad.
",3
483,64128731,Teradata: Error 3504 and generating subset of column in Select,"Hi I was trying to work on a teradata sql problem where I need to exclude all saledate in Aug 2005 and calculate the daily revenue for each store/month/year combination for any stores that have no fewer than 20 sale days.
My idea is to generate a subset of the saledate column in the subquery and work with it. And here is my code.
SELECT Sub.store, Sub.Year_, Sub.Month_, Sub.TotalSaleDate, Sub.Daily_rev, sub.Total_rev
FROM (SELECT (CASE WHEN (NOT (EXTRACT(MONTH from saledate)=8 
             AND EXTRACT(YEAR from saledate)=2005)) THEN saledate END) AS 
             NewSaleDate, COUNT(NewSaleDate) AS TotalSaleDate,
             SUM(amt) AS Total_rev,
             Total_rev/TotalSaleDate AS Daily_rev,
             EXTRACT(MONTH from NewSaleDate) AS Month_,
             EXTRACT(YEAR from NewSaleDate) AS Year_, store
       FROM trnsact
       WHERE stype = 'P' AND saledate = NewSaleDate
       GROUP BY store, Year_, Month_, NewSaleDate) AS Sub
WHERE Sub.TotalSaleDate &gt;= 20
ORDER BY sub.TotalSaledate ASC; 

And this is my output
My result
Here is code from someone that worked
SELECT 
  sub.store, sub.year_num, sub.month_num, sub.num_dates, sub.daily_revenue
FROM (
  SELECT 
  store, 
  EXTRACT (month FROM saledate) AS month_num, 
  EXTRACT (year FROM saledate) AS year_num,
  COUNT (DISTINCT saledate) AS num_dates,
  SUM(amt) AS total_revenue,
  total_revenue/num_dates AS daily_revenue,
  (CASE 
  WHEN (year_num=2005 AND month_num=8) THEN 'cannot' ELSE 'can' 
  END) As can_use_anot
  FROM trnsact
  WHERE stype='p' AND can_use_anot='can'
  GROUP BY store, month_num, year_num
  ) AS sub
HAVING sub.num_dates &gt;=20
GROUP BY sub.store, sub.year_num, sub.month_num, sub.num_dates, sub.daily_revenue
ORDER BY sub.num_dates ASC;

And his result
Correct result
Apparently his Daily revenue is much higher than mine. I wonder whether it is due to I am not counting Distinct saledate in the subquery. However, I tried to add use COUNT(DISTINCT saledate) and I get no output at all, 0 row. I understand how his code works but I`m frustrated where is wrong in my code. ESPECIALLY WHY ADDING DISTINCT GAVE ME 0 ROW, DEEPLY APPRECIATE ANYONE WHO CAN EXPLAIN...
",-1,-1,-1.0,"Hi I was trying to work on a teradata sql problem where I need to exclude all saledate in Aug 2005 and calculate the daily revenue for each store/month/year combination for any stores that have no fewer than 20 sale days.
My idea is to generate a subset of the saledate column in the subquery and work with it. And here is my code.
SELECT Sub.store, Sub.Year_, Sub.Month_, Sub.TotalSaleDate, Sub.Daily_rev, sub.Total_rev
FROM (SELECT (CASE WHEN (NOT (EXTRACT(MONTH from saledate)=8 
             AND EXTRACT(YEAR from saledate)=2005)) THEN saledate END) AS 
             NewSaleDate, COUNT(NewSaleDate) AS TotalSaleDate,
             SUM(amt) AS Total_rev,
             Total_rev/TotalSaleDate AS Daily_rev,
             EXTRACT(MONTH from NewSaleDate) AS Month_,
             EXTRACT(YEAR from NewSaleDate) AS Year_, store
       FROM trnsact
       WHERE stype = 'P' AND saledate = NewSaleDate
       GROUP BY store, Year_, Month_, NewSaleDate) AS Sub
WHERE Sub.TotalSaleDate &gt;= 20
ORDER BY sub.TotalSaledate ASC; 

And this is my output
My result
Here is code from someone that worked
SELECT 
  sub.store, sub.year_num, sub.month_num, sub.num_dates, sub.daily_revenue
FROM (
  SELECT 
  store, 
  EXTRACT (month FROM saledate) AS month_num, 
  EXTRACT (year FROM saledate) AS year_num,
  COUNT (DISTINCT saledate) AS num_dates,
  SUM(amt) AS total_revenue,
  total_revenue/num_dates AS daily_revenue,
  (CASE 
  WHEN (year_num=2005 AND month_num=8) THEN 'cannot' ELSE 'can' 
  END) As can_use_anot
  FROM trnsact
  WHERE stype='p' AND can_use_anot='can'
  GROUP BY store, month_num, year_num
  ) AS sub
HAVING sub.num_dates &gt;=20
GROUP BY sub.store, sub.year_num, sub.month_num, sub.num_dates, sub.daily_revenue
ORDER BY sub.num_dates ASC;

And his result
Correct result
Apparently his Daily revenue is much higher than mine. I wonder whether it is due to I am not counting Distinct saledate in the subquery. However, I tried to add use COUNT(DISTINCT saledate) and I get no output at all, 0 row. I understand how his code works but I`m frustrated where is wrong in my code. ESPECIALLY WHY ADDING DISTINCT GAVE ME 0 ROW, DEEPLY APPRECIATE ANYONE WHO CAN EXPLAIN...
",3
484,64136620,Delete data from teradata using pyspark,"I  am trying to delete the record from teradata and then write into the table for avoiding duplicates
So i have tried in many ways which is not working

I have tried deleting while reading the data which is giving syntax error like '(' expected between table and delete
spark.read.format('jdbc').options('driver','com.TeradataDriver').options('user','user').options('pwd','pwd').options('dbtable','delete from table').load()

Also tried like below, which is also giving syntax error like something expected between '('and delete
options('dbtable','(delete from table) as td')
2)I have tried deleting while writing the data which is not working
df.write.format('jdbc').options('driver','com.TeradataDriver').options('user','user').options('pwd','pwd').options('dbtable','table').('preactions','delete from table').save()
",-1,-1,-1.0,"I  am trying to delete the record from teradata and then write into the table for avoiding duplicates
So i have tried in many ways which is not working

I have tried deleting while reading the data which is giving syntax error like '(' expected between table and delete
spark.read.format('jdbc').options('driver','com.TeradataDriver').options('user','user').options('pwd','pwd').options('dbtable','delete from table').load()

Also tried like below, which is also giving syntax error like something expected between '('and delete
options('dbtable','(delete from table) as td')
2)I have tried deleting while writing the data which is not working
df.write.format('jdbc').options('driver','com.TeradataDriver').options('user','user').options('pwd','pwd').options('dbtable','table').('preactions','delete from table').save()
",3
485,64191213,Teradata Insert Error: ValueError: The truth value of a DataFrame is ambiguous,"I am trying to insert a pandas dataframe into Teradata and running into this error. The connection to Teradata is working as I was able to insert the same data one record at a time. Please help.
import pandas as pd
import teradata as td

Car_Sales = pd.DataFrame([
    {&quot;Sale_Dt&quot;:&quot;2019-10-01&quot;, &quot;Brand&quot;:&quot;Mercedes&quot;,&quot;Sale_Type&quot;:&quot;New&quot;,&quot;Dealer_Type&quot;:&quot;Urban&quot;,&quot;SalesVol&quot;:&quot;5&quot;},
    {&quot;Sale_Dt&quot;:&quot;2019-10-02&quot;, &quot;Brand&quot;:&quot;BMW&quot;,&quot;Sale_Type&quot;:&quot;Used&quot;,&quot;Dealer_Type&quot;:&quot;Sub-Urban&quot;,&quot;SalesVol&quot;:&quot;3&quot;},
    ])

udaExec = td.UdaExec()
with udaExec.connect(method=&quot;odbc&quot;,DSN = &quot;dsn1&quot;,driver = 'Teradata Database ODBC Driver 16.20') as session:
#session.execute(&quot;INSERT INTO db.tbl_Cars (Sale_Dt,Brand,Sale_Type,Dealer_Type,SalesVol) VALUES (?,?,?,?,?)&quot;,(&quot;2019-10-01&quot;,&quot;Mercedes&quot;,&quot;New&quot;,&quot;Urban&quot;,5))
query2 = &quot;INSERT INTO db.tbl_Cars&quot;    
session.execute(query2,Car_Sales,batch=True)

Error:
ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
",1,-1,-1.0,"I am trying to insert a pandas dataframe into Teradata and running into this error. The connection to Teradata is working as I was able to insert the same data one record at a time. Please help.
import pandas as pd
import teradata as td

Car_Sales = pd.DataFrame([
    {&quot;Sale_Dt&quot;:&quot;2019-10-01&quot;, &quot;Brand&quot;:&quot;Mercedes&quot;,&quot;Sale_Type&quot;:&quot;New&quot;,&quot;Dealer_Type&quot;:&quot;Urban&quot;,&quot;SalesVol&quot;:&quot;5&quot;},
    {&quot;Sale_Dt&quot;:&quot;2019-10-02&quot;, &quot;Brand&quot;:&quot;BMW&quot;,&quot;Sale_Type&quot;:&quot;Used&quot;,&quot;Dealer_Type&quot;:&quot;Sub-Urban&quot;,&quot;SalesVol&quot;:&quot;3&quot;},
    ])

udaExec = td.UdaExec()
with udaExec.connect(method=&quot;odbc&quot;,DSN = &quot;dsn1&quot;,driver = 'Teradata Database ODBC Driver 16.20') as session:
#session.execute(&quot;INSERT INTO db.tbl_Cars (Sale_Dt,Brand,Sale_Type,Dealer_Type,SalesVol) VALUES (?,?,?,?,?)&quot;,(&quot;2019-10-01&quot;,&quot;Mercedes&quot;,&quot;New&quot;,&quot;Urban&quot;,5))
query2 = &quot;INSERT INTO db.tbl_Cars&quot;    
session.execute(query2,Car_Sales,batch=True)

Error:
ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
",1
486,63988885,Install teradatasql for R 4.0.2,"I try to install teradatasql as explained here:
https://github.com/Teradata/r-driver#Installation
However, it immediately throws this error:
install.packages('teradatasql',repos=c('https://teradata-download.s3.amazonaws.com','https://cloud.r-project.org'))
Installing package into ‘/home/ckiefer/RProjects/cki_new_db_interface/packrat/lib/x86_64-pc-linux-gnu/4.0.2’
(as ‘lib’ is unspecified)
Warning: unable to access index for repository https://teradata-download.s3.amazonaws.com/src/contrib:
Line starting '&lt;?xml version=&quot;1.0&quot;  ...' is malformed!
Warning message:
package ‘teradatasql’ is not available (for R version 4.0.2)

So, it cannot be installed for this version of R although it says that the package requires 64-bit R 3.4.3 or later?
Any help is appreciated.
BR, Christoph
",-1,-1,-1.0,"I try to install teradatasql as explained here:
https://github.com/Teradata/r-driver#Installation
However, it immediately throws this error:
install.packages('teradatasql',repos=c('https://teradata-download.s3.amazonaws.com','https://cloud.r-project.org'))
Installing package into ‘/home/ckiefer/RProjects/cki_new_db_interface/packrat/lib/x86_64-pc-linux-gnu/4.0.2’
(as ‘lib’ is unspecified)
Warning: unable to access index for repository https://teradata-download.s3.amazonaws.com/src/contrib:
Line starting '&lt;?xml version=&quot;1.0&quot;  ...' is malformed!
Warning message:
package ‘teradatasql’ is not available (for R version 4.0.2)

So, it cannot be installed for this version of R although it says that the package requires 64-bit R 3.4.3 or later?
Any help is appreciated.
BR, Christoph
",1
487,63473569,Caused by [Version 17.0.0.2] [Session 8085885] [Teradata SQL Driver] Failure receiving Start Response message header,"I am doing an insert query using the teradatasql module with python and getting this error.
I am trying to run multiple scripts simultaneously and I guess I cannot do multiple insert queries at the same time?
Traceback (most recent call last):

  File &quot;C:\Users\LAMKAJO\Desktop\Work J\Depot Capacity\Script\pac.py&quot;, line 446, in &lt;module&gt;
    new_main.run()

  File &quot;C:\Users\LAMKAJO\Desktop\Work J\Depot Capacity\Script\pac.py&quot;, line 440, in run
    tc.map(self.city_calc, self.mp_city_lst)

  File &quot;C:\Users\LAMKAJO\AppData\Local\Continuum\anaconda3\lib\multiprocessing\pool.py&quot;, line 268, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()

  File &quot;C:\Users\LAMKAJO\AppData\Local\Continuum\anaconda3\lib\multiprocessing\pool.py&quot;, line 657, in get
    raise self._value

  File &quot;C:\Users\LAMKAJO\AppData\Local\Continuum\anaconda3\lib\multiprocessing\pool.py&quot;, line 121, in worker
    result = (True, func(*args, **kwds))

  File &quot;C:\Users\LAMKAJO\AppData\Local\Continuum\anaconda3\lib\multiprocessing\pool.py&quot;, line 44, in mapstar
    return list(map(*args))

  File &quot;C:\Users\LAMKAJO\Desktop\Work J\Depot Capacity\Script\pac.py&quot;, line 242, in city_calc
    self.city_cluster_wk_hrs(df_lrcity,loc_sep[0],loc_sep[1])

  File &quot;C:\Users\LAMKAJO\Desktop\Work J\Depot Capacity\Script\pac.py&quot;, line 231, in city_cluster_wk_hrs
    cur.executemany(self.insert_sql_query,dtframe.values.tolist())

  File &quot;C:\Users\LAMKAJO\AppData\Local\Continuum\anaconda3\lib\site-packages\teradatasql\__init__.py&quot;, line 896, in executemany
    raise OperationalError (sErr)

OperationalError: [Version 17.0.0.2] [Session 8085885] [Teradata SQL Driver] [Error 528] A failure occurred while executing rows 1 through 12 of a batch request.
 at gosqldriver/teradatasql.(*teradataConnection).makeDriverErrorCode TeradataConnection.go:1120
 at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:396
 at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
 at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2083
 at database/sql.ctxDriverQuery ctxutil.go:48
 at database/sql.(*DB).queryDC.func1 sql.go:1464
 at database/sql.withLock sql.go:3032
 at database/sql.(*DB).queryDC sql.go:1459
 at database/sql.(*Conn).QueryContext sql.go:1701
 at main.goCreateRows goside.go:654
 at main._cgoexpwrap_212fad278f55_goCreateRows _cgo_gotypes.go:357
 at runtime.call64 asm_amd64.s:574
 at runtime.cgocallbackg1 cgocall.go:316
 at runtime.cgocallbackg cgocall.go:194
 at runtime.cgocallback_gofunc asm_amd64.s:826
 at runtime.goexit asm_amd64.s:2361
Caused by [Version 17.0.0.2] [Session 8085885] [Teradata SQL Driver] Failure receiving Start Response message header
 at gosqldriver/teradatasql.(*teradataConnection).makeDriverError TeradataConnection.go:1101
 at gosqldriver/teradatasql.(*teradataConnection).sendAndReceive TeradataConnection.go:1397
 at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:526
 at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:388
 at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
 at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2083
 at database/sql.ctxDriverQuery ctxutil.go:48
 at database/sql.(*DB).queryDC.func1 sql.go:1464
 at database/sql.withLock sql.go:3032
 at database/sql.(*DB).queryDC sql.go:1459
 at database/sql.(*Conn).QueryContext sql.go:1701
 at main.goCreateRows goside.go:654
 at main._cgoexpwrap_212fad278f55_goCreateRows _cgo_gotypes.go:357
 at runtime.call64 asm_amd64.s:574
 at runtime.cgocallbackg1 cgocall.go:316
 at runtime.cgocallbackg cgocall.go:194
 at runtime.cgocallback_gofunc asm_amd64.s:826
 at runtime.goexit asm_amd64.s:2361
Caused by read tcp 10.199.17.157:54103-&gt;10.229.115.22:1025: wsarecv: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.

",-1,-1,-1.0,"I am doing an insert query using the teradatasql module with python and getting this error.
I am trying to run multiple scripts simultaneously and I guess I cannot do multiple insert queries at the same time?
Traceback (most recent call last):

  File &quot;C:\Users\LAMKAJO\Desktop\Work J\Depot Capacity\Script\pac.py&quot;, line 446, in &lt;module&gt;
    new_main.run()

  File &quot;C:\Users\LAMKAJO\Desktop\Work J\Depot Capacity\Script\pac.py&quot;, line 440, in run
    tc.map(self.city_calc, self.mp_city_lst)

  File &quot;C:\Users\LAMKAJO\AppData\Local\Continuum\anaconda3\lib\multiprocessing\pool.py&quot;, line 268, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()

  File &quot;C:\Users\LAMKAJO\AppData\Local\Continuum\anaconda3\lib\multiprocessing\pool.py&quot;, line 657, in get
    raise self._value

  File &quot;C:\Users\LAMKAJO\AppData\Local\Continuum\anaconda3\lib\multiprocessing\pool.py&quot;, line 121, in worker
    result = (True, func(*args, **kwds))

  File &quot;C:\Users\LAMKAJO\AppData\Local\Continuum\anaconda3\lib\multiprocessing\pool.py&quot;, line 44, in mapstar
    return list(map(*args))

  File &quot;C:\Users\LAMKAJO\Desktop\Work J\Depot Capacity\Script\pac.py&quot;, line 242, in city_calc
    self.city_cluster_wk_hrs(df_lrcity,loc_sep[0],loc_sep[1])

  File &quot;C:\Users\LAMKAJO\Desktop\Work J\Depot Capacity\Script\pac.py&quot;, line 231, in city_cluster_wk_hrs
    cur.executemany(self.insert_sql_query,dtframe.values.tolist())

  File &quot;C:\Users\LAMKAJO\AppData\Local\Continuum\anaconda3\lib\site-packages\teradatasql\__init__.py&quot;, line 896, in executemany
    raise OperationalError (sErr)

OperationalError: [Version 17.0.0.2] [Session 8085885] [Teradata SQL Driver] [Error 528] A failure occurred while executing rows 1 through 12 of a batch request.
 at gosqldriver/teradatasql.(*teradataConnection).makeDriverErrorCode TeradataConnection.go:1120
 at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:396
 at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
 at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2083
 at database/sql.ctxDriverQuery ctxutil.go:48
 at database/sql.(*DB).queryDC.func1 sql.go:1464
 at database/sql.withLock sql.go:3032
 at database/sql.(*DB).queryDC sql.go:1459
 at database/sql.(*Conn).QueryContext sql.go:1701
 at main.goCreateRows goside.go:654
 at main._cgoexpwrap_212fad278f55_goCreateRows _cgo_gotypes.go:357
 at runtime.call64 asm_amd64.s:574
 at runtime.cgocallbackg1 cgocall.go:316
 at runtime.cgocallbackg cgocall.go:194
 at runtime.cgocallback_gofunc asm_amd64.s:826
 at runtime.goexit asm_amd64.s:2361
Caused by [Version 17.0.0.2] [Session 8085885] [Teradata SQL Driver] Failure receiving Start Response message header
 at gosqldriver/teradatasql.(*teradataConnection).makeDriverError TeradataConnection.go:1101
 at gosqldriver/teradatasql.(*teradataConnection).sendAndReceive TeradataConnection.go:1397
 at gosqldriver/teradatasql.(*TeradataRows).executeSQLRequest TeradataRows.go:526
 at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:388
 at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
 at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2083
 at database/sql.ctxDriverQuery ctxutil.go:48
 at database/sql.(*DB).queryDC.func1 sql.go:1464
 at database/sql.withLock sql.go:3032
 at database/sql.(*DB).queryDC sql.go:1459
 at database/sql.(*Conn).QueryContext sql.go:1701
 at main.goCreateRows goside.go:654
 at main._cgoexpwrap_212fad278f55_goCreateRows _cgo_gotypes.go:357
 at runtime.call64 asm_amd64.s:574
 at runtime.cgocallbackg1 cgocall.go:316
 at runtime.cgocallbackg cgocall.go:194
 at runtime.cgocallback_gofunc asm_amd64.s:826
 at runtime.goexit asm_amd64.s:2361
Caused by read tcp 10.199.17.157:54103-&gt;10.229.115.22:1025: wsarecv: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.

",1
488,63408086,Change Data Capture in Teradata,"I want to capture INSERT, UPDATE, DELETE activities that are applied to a Teradata table; Something similar to how it works in SQL Server.
I went through the Teradata docs and it seems it does not have built-in support for this feature.
Can someone help me here?
",1,-1,-1.0,"I want to capture INSERT, UPDATE, DELETE activities that are applied to a Teradata table; Something similar to how it works in SQL Server.
I went through the Teradata docs and it seems it does not have built-in support for this feature.
Can someone help me here?
",3
489,63110514,How to achieve 2 decimal places even on zero values in Teradata SQL Assistant?,"Below is the code in MS SQL Server
FORMAT(CAST(a.[Sample_Column] as numeric(10,2)), 'C', 'en-US') 

I want to achieve the same in Teradata, for which I tried the below:
TRIM(TO_CHAR (cast(tmp.&quot;Sample_Column&quot; as decimal(10,2)), '$99,999,999.00' ))

Issue is when the Sample column has values as '0', its populating '.00' in Teradata. I want it to be populated as '0.00' instead.
",0,-1,-1.0,"Below is the code in MS SQL Server
FORMAT(CAST(a.[Sample_Column] as numeric(10,2)), 'C', 'en-US') 

I want to achieve the same in Teradata, for which I tried the below:
TRIM(TO_CHAR (cast(tmp.&quot;Sample_Column&quot; as decimal(10,2)), '$99,999,999.00' ))

Issue is when the Sample column has values as '0', its populating '.00' in Teradata. I want it to be populated as '0.00' instead.
",3
490,62613996,Teradata mutliload CSV source file load issue,"I have a comma separated CSV file that needs to be loaded into Teradata Table. Usually fastload is very useful in such scenarios when there is a single file which needs to be loaded into a single table.
The issue here is that the data fields in CSV file are enclosed in quotes (Sample csv file data is shared below for reference). Fastload does not handle/support/load data when source file is in this format.
TPUMP handles this type of data file but it creates macros in the background to issue statements. Unluckily the client for whom I am working does not provide 'Create Macro' privilege and thus TPUMP script is not an option.
Can this file be loaded with the help of Multiload. If yes then would someone be kind enough to guide me on how to load this quoted data in teradata using mload. A small script example would be highly appreciated.
Thanks
&quot;Received&quot;,&quot;From&quot;,&quot;Destination&quot;,&quot;Message Text&quot;,&quot;Status&quot;,&quot;Folder&quot;,&quot;Folder Owner&quot;,
&quot;2020-06-25 13:31:47&quot;,&quot;125674122450&quot;,&quot;61119&quot;,&quot;EEEEEE&quot;,&quot;Not Processed&quot;,&quot;n/a&quot;,&quot;n/a&quot;
&quot;2020-06-25 13:14:15&quot;,&quot;125683011742&quot;,&quot;82332&quot;,&quot;CCC 000000 59303760&quot;,&quot;Processed&quot;,&quot;COMMERCIAL CCC&quot;,&quot;BSSSUser&quot;
&quot;2020-06-25 13:09:48&quot;,&quot;125693033666&quot;,&quot;61112&quot;,&quot;YesLLLL &quot;,&quot;Processed&quot;,&quot;Cooooo AAAAA &quot;,&quot;9890374&quot;


@dnoeth I have written the below TPTLOAD Script but it is giving error


/**************************************************************/
/* IMI_T7_SMS_RESPONSES                                       */
/* Load CSV Data Into TERADATA                                */
/**************************************************************/
DEFINE JOB LOAD_TD_FROM_CSV
DESCRIPTION 'Load Teradata table from CSV File'
(
  DEFINE SCHEMA IMI_T7_SMS_RESPONSES
  DESCRIPTION 'IMI_T7_SMS_RESPONSES'
  (
    Response_Dttm             VARCHAR(100)
    ,From_Number              VARCHAR(20)
    ,To_Shortcode             VARCHAR(5)
    ,Response_Text          VARCHAR(100)
    ,Status                     VARCHAR(100)
    ,Folder                     VARCHAR(100)
    ,Folder_Owner           VARCHAR(100)
  );


  DEFINE OPERATOR DDL_OPERATOR
  TYPE DDL
  ATTRIBUTES
  (
   VARCHAR TdpId           = 'system',
   VARCHAR UserName        = 'username',
   VARCHAR UserPassword    = 'password'
   );


  DEFINE OPERATOR LOAD_CSV
  DESCRIPTION 'Operator to Load CSV Data'
  TYPE LOAD
  SCHEMA IMI_T7_SMS_RESPONSES
  ATTRIBUTES
  (
    VARCHAR PrivateLogName,
    VARCHAR TraceLevel      = 'None',
    INTEGER TenacityHours   = 1,
    INTEGER TenacitySleep   = 1,
    INTEGER MaxSessions     = 4,
    INTEGER MinSessions     = 1,
    VARCHAR TargetTable     = 'trm_lead_history.IMI_T7_SMS_RESPONSES_TEST',
    VARCHAR ErrorTable1     = 'trm_lead_history.IMI_T7_SMS_RESPONSES_E1',
    VARCHAR ErrorTable2     = 'trm_lead_history.IMI_T7_SMS_RESPONSES_E2',
    VARCHAR LogTable        = 'trm_lead_history.IMI_T7_SMS_RESPONSES_LOG',
    VARCHAR TdpId           = 'system',
    VARCHAR UserName        = 'User',
    VARCHAR UserPassword    = 'password'
  );

   DEFINE OPERATOR READ_CSV
   DESCRIPTION 'Operator to Read CSV File'
   TYPE DATACONNECTOR PRODUCER
   SCHEMA IMI_T7_SMS_RESPONSES
   ATTRIBUTES
  (
   VARCHAR  Filename             =   'C:\Users\nofel\Desktop\fastload\nick\IMI_T7_SMS_RESPONSES.csv'
   ,VARCHAR Format               =   'Delimited'
     ,VARCHAR TextDelimiter        =   ','
   ,VARCHAR CloseQuoteMark       =   '&quot;'
   ,VARCHAR OpenQuoteMark        =   '&quot;'
     ,VARCHAR AcceptExcessColumns  =   'N'
     ,VARCHAR PrivateLogName       =   'LOAD_FROM_CSV'
   ,VARCHAR QuotedData            =  'Y'
   ,Varchar EscapeQuoteDelimiter = '&quot;'
  );


Step Setup_Tables
(
   APPLY
     ('DROP   TABLE TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_E1;'  ),
     ('DROP   TABLE TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_E2;'  ),
     ('DROP   TABLE TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_LOG;'),
     ('DROP   TABLE TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_TEST;'),
     ('CREATE SET TABLE TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_TEST,
          NO FALLBACK ,
          NO BEFORE JOURNAL,
          NO AFTER JOURNAL,
          CHECKSUM = DEFAULT,
          DEFAULT MERGEBLOCKRATIO
          (
              Response_Dttm           VARCHAR(100)
              ,From_Number            VARCHAR(20)
              ,To_Shortcode                VARCHAR(5)
              ,Response_Text            VARCHAR(100)
              ,Status                   VARCHAR(100)
              ,Folder                   VARCHAR(100)
              ,Folder_Owner             VARCHAR(100)
           );')   
           TO OPERATOR (DDL_OPERATOR);
);

Step Load_Table
(
   APPLY ('INSERT INTO TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_TEST
                                  (
                                  :Response_Dttm
                                  ,:From_Number
                                  ,:To_Shortcode
                                  ,:Response_Text
                                  ,:Status
                                  ,:Folder
                                  ,:Folder_Owner );')
   TO OPERATOR (LOAD_CSV)

   SELECT * FROM (OPERATOR READ_CSV);
);

);

it is giving error
&quot;syntax error at or near line 111 of job script file 'IMI_T7_SMS_RESPONSS.tpt:
TPT_INFRA:At &quot;Select&quot; missing semicol_ in Rule:Step
Compilation failed due to errors
For reference from code above line 111 is at the end of code &quot;SELECT * FROM (OPERATOR READ_CSV);&quot;. I have tried everything but nothing seems to work. Can you kindly guide.
",-1,-1,-1.0,"I have a comma separated CSV file that needs to be loaded into Teradata Table. Usually fastload is very useful in such scenarios when there is a single file which needs to be loaded into a single table.
The issue here is that the data fields in CSV file are enclosed in quotes (Sample csv file data is shared below for reference). Fastload does not handle/support/load data when source file is in this format.
TPUMP handles this type of data file but it creates macros in the background to issue statements. Unluckily the client for whom I am working does not provide 'Create Macro' privilege and thus TPUMP script is not an option.
Can this file be loaded with the help of Multiload. If yes then would someone be kind enough to guide me on how to load this quoted data in teradata using mload. A small script example would be highly appreciated.
Thanks
&quot;Received&quot;,&quot;From&quot;,&quot;Destination&quot;,&quot;Message Text&quot;,&quot;Status&quot;,&quot;Folder&quot;,&quot;Folder Owner&quot;,
&quot;2020-06-25 13:31:47&quot;,&quot;125674122450&quot;,&quot;61119&quot;,&quot;EEEEEE&quot;,&quot;Not Processed&quot;,&quot;n/a&quot;,&quot;n/a&quot;
&quot;2020-06-25 13:14:15&quot;,&quot;125683011742&quot;,&quot;82332&quot;,&quot;CCC 000000 59303760&quot;,&quot;Processed&quot;,&quot;COMMERCIAL CCC&quot;,&quot;BSSSUser&quot;
&quot;2020-06-25 13:09:48&quot;,&quot;125693033666&quot;,&quot;61112&quot;,&quot;YesLLLL &quot;,&quot;Processed&quot;,&quot;Cooooo AAAAA &quot;,&quot;9890374&quot;


@dnoeth I have written the below TPTLOAD Script but it is giving error


/**************************************************************/
/* IMI_T7_SMS_RESPONSES                                       */
/* Load CSV Data Into TERADATA                                */
/**************************************************************/
DEFINE JOB LOAD_TD_FROM_CSV
DESCRIPTION 'Load Teradata table from CSV File'
(
  DEFINE SCHEMA IMI_T7_SMS_RESPONSES
  DESCRIPTION 'IMI_T7_SMS_RESPONSES'
  (
    Response_Dttm             VARCHAR(100)
    ,From_Number              VARCHAR(20)
    ,To_Shortcode             VARCHAR(5)
    ,Response_Text          VARCHAR(100)
    ,Status                     VARCHAR(100)
    ,Folder                     VARCHAR(100)
    ,Folder_Owner           VARCHAR(100)
  );


  DEFINE OPERATOR DDL_OPERATOR
  TYPE DDL
  ATTRIBUTES
  (
   VARCHAR TdpId           = 'system',
   VARCHAR UserName        = 'username',
   VARCHAR UserPassword    = 'password'
   );


  DEFINE OPERATOR LOAD_CSV
  DESCRIPTION 'Operator to Load CSV Data'
  TYPE LOAD
  SCHEMA IMI_T7_SMS_RESPONSES
  ATTRIBUTES
  (
    VARCHAR PrivateLogName,
    VARCHAR TraceLevel      = 'None',
    INTEGER TenacityHours   = 1,
    INTEGER TenacitySleep   = 1,
    INTEGER MaxSessions     = 4,
    INTEGER MinSessions     = 1,
    VARCHAR TargetTable     = 'trm_lead_history.IMI_T7_SMS_RESPONSES_TEST',
    VARCHAR ErrorTable1     = 'trm_lead_history.IMI_T7_SMS_RESPONSES_E1',
    VARCHAR ErrorTable2     = 'trm_lead_history.IMI_T7_SMS_RESPONSES_E2',
    VARCHAR LogTable        = 'trm_lead_history.IMI_T7_SMS_RESPONSES_LOG',
    VARCHAR TdpId           = 'system',
    VARCHAR UserName        = 'User',
    VARCHAR UserPassword    = 'password'
  );

   DEFINE OPERATOR READ_CSV
   DESCRIPTION 'Operator to Read CSV File'
   TYPE DATACONNECTOR PRODUCER
   SCHEMA IMI_T7_SMS_RESPONSES
   ATTRIBUTES
  (
   VARCHAR  Filename             =   'C:\Users\nofel\Desktop\fastload\nick\IMI_T7_SMS_RESPONSES.csv'
   ,VARCHAR Format               =   'Delimited'
     ,VARCHAR TextDelimiter        =   ','
   ,VARCHAR CloseQuoteMark       =   '&quot;'
   ,VARCHAR OpenQuoteMark        =   '&quot;'
     ,VARCHAR AcceptExcessColumns  =   'N'
     ,VARCHAR PrivateLogName       =   'LOAD_FROM_CSV'
   ,VARCHAR QuotedData            =  'Y'
   ,Varchar EscapeQuoteDelimiter = '&quot;'
  );


Step Setup_Tables
(
   APPLY
     ('DROP   TABLE TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_E1;'  ),
     ('DROP   TABLE TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_E2;'  ),
     ('DROP   TABLE TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_LOG;'),
     ('DROP   TABLE TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_TEST;'),
     ('CREATE SET TABLE TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_TEST,
          NO FALLBACK ,
          NO BEFORE JOURNAL,
          NO AFTER JOURNAL,
          CHECKSUM = DEFAULT,
          DEFAULT MERGEBLOCKRATIO
          (
              Response_Dttm           VARCHAR(100)
              ,From_Number            VARCHAR(20)
              ,To_Shortcode                VARCHAR(5)
              ,Response_Text            VARCHAR(100)
              ,Status                   VARCHAR(100)
              ,Folder                   VARCHAR(100)
              ,Folder_Owner             VARCHAR(100)
           );')   
           TO OPERATOR (DDL_OPERATOR);
);

Step Load_Table
(
   APPLY ('INSERT INTO TRM_LEAD_HISTORY.IMI_T7_SMS_RESPONSES_TEST
                                  (
                                  :Response_Dttm
                                  ,:From_Number
                                  ,:To_Shortcode
                                  ,:Response_Text
                                  ,:Status
                                  ,:Folder
                                  ,:Folder_Owner );')
   TO OPERATOR (LOAD_CSV)

   SELECT * FROM (OPERATOR READ_CSV);
);

);

it is giving error
&quot;syntax error at or near line 111 of job script file 'IMI_T7_SMS_RESPONSS.tpt:
TPT_INFRA:At &quot;Select&quot; missing semicol_ in Rule:Step
Compilation failed due to errors
For reference from code above line 111 is at the end of code &quot;SELECT * FROM (OPERATOR READ_CSV);&quot;. I have tried everything but nothing seems to work. Can you kindly guide.
",3
491,62584514,Teradata SQL Assistance to discern if Previous Row is a specific location and within a set time frame to give me a 1-count (Flag),"I am using Teradata. The query below runs, but somehow, I cannot get it to calculate at each hospital admission looking back to previous row if the admission is to an ED and if so is it within 1 day. It works if I limit my data to a single MEMBER_NO and there is one Hospital admission, but when there are several over a few years, it will not give me &quot;1&quot; in &quot;ED Visit within 1 Day of this Hospital Admit&quot; column. Also, I am not able to get it Null at each start of MEMBER_NO for the &quot;Previous LOCATION&quot; and Null at the end for the &quot;Next LOCATION.&quot;
SELECT js.*
  ,CASE
     WHEN LOCATION = 'Hospital'
     THEN Count(CASE WHEN LOCATION = 'Hospital' AND &quot;Days Since ED Discharge&quot;  &lt;= 1 THEN 1 END)
          Over (PARTITION BY TRIM(MEMBER_NO) 
                ORDER BY DISCHARGE_DATE, ADMIT_DATE
                RESET WHEN LOCATION = 'HOSPITAL')
   END AS &quot;ED Visit within 1 Day of this Hospital Admit&quot;

  ,CASE
     WHEN LOCATION = 'Hospital'
     THEN Count(CASE WHEN LOCATION = 'Nursing Facility' AND &quot;Days Since Last Hospital Discharge&quot;  &lt;= 7 THEN 1 END)
          Over (PARTITION BY TRIM(MEMBER_NO)
                ORDER BY DISCHARGE_DATE, ADMIT_DATE
                RESET WHEN LOCATION = 'HOSPITAL')
   END AS &quot;Nursing Facility Admit within 7 Days after this Hospital Discharge&quot;
   
  
FROM
( 
  SELECT CLIENT, TRIM(MEMBER_NO) &quot;MEMBER ID&quot;, AGE, ADMIT_NO, LOCATION, ADMIT_DATE, DISCHARGE_DATE
, COALESCE(MIN(LOCATION) over (order by ADMIT_NO rows between 1 preceding and 1 preceding), LOCATION) &quot;Previous LOCATION&quot;
, COALESCE(MIN(LOCATION) over (order by ADMIT_NO rows between 1 following and 1 following), LOCATION) &quot;Next LOCATION&quot;


,CASE
  WHEN LOCATION = 'Hospital'
        THEN ADMIT_DATE - 
             MIN(CASE WHEN LOCATION = 'ED' THEN DISCHARGE_DATE END)
             Over (PARTITION BY TRIM(MEMBER_NO) 
                   ORDER BY DISCHARGE_DATE, ADMIT_DATE
                   ROWS Unbounded Preceding)
      END AS &quot;Days Since ED Discharge&quot;  
     
,CASE
  WHEN LOCATION &lt;&gt; 'Hospital'
        THEN ADMIT_DATE -
             Max(CASE WHEN LOCATION = 'Hospital' THEN DISCHARGE_DATE END)
             Over (PARTITION BY TRIM(MEMBER_NO) 
                   ORDER BY DISCHARGE_DATE, ADMIT_DATE
                   ROWS Unbounded Preceding)
      END AS &quot;Days Since Last Hospital Discharge&quot; 
      
  
   FROM CLAIMS_DB.FACILITY_CLAIMS 
   WHERE ADMIT_DATE BETWEEN DATE '2017-01-01' AND DATE '2020-12-31'
 ) AS js;

Here is the display of the result of the query now. The cells shaded in red are incorrect. J6 should be &quot;0&quot; and L6 should be &quot;1&quot;. As stated earlier, if I change the date range to only have one Hospital location reported then J6 works. Also, F2 and G8 should be Null. 
",-1,-1,-1.0,"I am using Teradata. The query below runs, but somehow, I cannot get it to calculate at each hospital admission looking back to previous row if the admission is to an ED and if so is it within 1 day. It works if I limit my data to a single MEMBER_NO and there is one Hospital admission, but when there are several over a few years, it will not give me &quot;1&quot; in &quot;ED Visit within 1 Day of this Hospital Admit&quot; column. Also, I am not able to get it Null at each start of MEMBER_NO for the &quot;Previous LOCATION&quot; and Null at the end for the &quot;Next LOCATION.&quot;
SELECT js.*
  ,CASE
     WHEN LOCATION = 'Hospital'
     THEN Count(CASE WHEN LOCATION = 'Hospital' AND &quot;Days Since ED Discharge&quot;  &lt;= 1 THEN 1 END)
          Over (PARTITION BY TRIM(MEMBER_NO) 
                ORDER BY DISCHARGE_DATE, ADMIT_DATE
                RESET WHEN LOCATION = 'HOSPITAL')
   END AS &quot;ED Visit within 1 Day of this Hospital Admit&quot;

  ,CASE
     WHEN LOCATION = 'Hospital'
     THEN Count(CASE WHEN LOCATION = 'Nursing Facility' AND &quot;Days Since Last Hospital Discharge&quot;  &lt;= 7 THEN 1 END)
          Over (PARTITION BY TRIM(MEMBER_NO)
                ORDER BY DISCHARGE_DATE, ADMIT_DATE
                RESET WHEN LOCATION = 'HOSPITAL')
   END AS &quot;Nursing Facility Admit within 7 Days after this Hospital Discharge&quot;
   
  
FROM
( 
  SELECT CLIENT, TRIM(MEMBER_NO) &quot;MEMBER ID&quot;, AGE, ADMIT_NO, LOCATION, ADMIT_DATE, DISCHARGE_DATE
, COALESCE(MIN(LOCATION) over (order by ADMIT_NO rows between 1 preceding and 1 preceding), LOCATION) &quot;Previous LOCATION&quot;
, COALESCE(MIN(LOCATION) over (order by ADMIT_NO rows between 1 following and 1 following), LOCATION) &quot;Next LOCATION&quot;


,CASE
  WHEN LOCATION = 'Hospital'
        THEN ADMIT_DATE - 
             MIN(CASE WHEN LOCATION = 'ED' THEN DISCHARGE_DATE END)
             Over (PARTITION BY TRIM(MEMBER_NO) 
                   ORDER BY DISCHARGE_DATE, ADMIT_DATE
                   ROWS Unbounded Preceding)
      END AS &quot;Days Since ED Discharge&quot;  
     
,CASE
  WHEN LOCATION &lt;&gt; 'Hospital'
        THEN ADMIT_DATE -
             Max(CASE WHEN LOCATION = 'Hospital' THEN DISCHARGE_DATE END)
             Over (PARTITION BY TRIM(MEMBER_NO) 
                   ORDER BY DISCHARGE_DATE, ADMIT_DATE
                   ROWS Unbounded Preceding)
      END AS &quot;Days Since Last Hospital Discharge&quot; 
      
  
   FROM CLAIMS_DB.FACILITY_CLAIMS 
   WHERE ADMIT_DATE BETWEEN DATE '2017-01-01' AND DATE '2020-12-31'
 ) AS js;

Here is the display of the result of the query now. The cells shaded in red are incorrect. J6 should be &quot;0&quot; and L6 should be &quot;1&quot;. As stated earlier, if I change the date range to only have one Hospital location reported then J6 works. Also, F2 and G8 should be Null. 
",3
492,62293548,Connection to a Teradata DB using LDAP and SQL,"**import pandas as pd 
import pyodbc
conn = pyodbc.connect('Driver={Teradata};'
                  'Server=XXXXXXX;'
                  'DBCNAME=DB_name;'
                  'AUTHENTICATION=LDAP;'
                  ""UID=XXXXXXX;""
                  ""PWD='XXXXXXX;""
                  'Trusted_Connection=yes;')
 # Read the sql file
SQL_Query = pd.read_sql(''


Select *

FROM DB_name.table_name

'''
, conn)**




I'm getting the error below: I'm thinking my Parameters in pyodbc.connect are not correct.  Can anybody help?

OperationalError: ('08001', ""[08001] [WSock32 DLL] 10065 WSA E HostUnreach: The Teradata server can't currently be reached over this network (10065) (SQLDriverConnect)"")
",1,-1,-1.0,"**import pandas as pd 
import pyodbc
conn = pyodbc.connect('Driver={Teradata};'
                  'Server=XXXXXXX;'
                  'DBCNAME=DB_name;'
                  'AUTHENTICATION=LDAP;'
                  ""UID=XXXXXXX;""
                  ""PWD='XXXXXXX;""
                  'Trusted_Connection=yes;')
 # Read the sql file
SQL_Query = pd.read_sql(''


Select *

FROM DB_name.table_name

'''
, conn)**




I'm getting the error below: I'm thinking my Parameters in pyodbc.connect are not correct.  Can anybody help?

OperationalError: ('08001', ""[08001] [WSock32 DLL] 10065 WSA E HostUnreach: The Teradata server can't currently be reached over this network (10065) (SQLDriverConnect)"")
",1
493,62096361,Teradatabase SQL Assistant foreign key,"I have created a table called 'customer' on TERADATA-SQL-ASSISTANT. 

CREATE TABLE customer(
customerid   CHAR(8)                   NOT NULL,
firstname    VARCHAR(20)              NOT NULL,
lastname     VARCHAR(20)              NOT NULL,
email        VARCHAR(20)             NOT NULL,
creditno       INT                    NOT NULL,
phoneno        INT                    NOT NULL,
dateabo     DATE FORMAT 'YY/MM/DD'    NOT NULL,
PRIMARY KEY(customerid),
FOREIGN KEY (customerid) REFERENCES WITH CHECK OPTION membership (membershipid));

INSERT INTO customer VALUES('AAAA','Brooks','Swarng','Brooks@gmail.com','706994253476882810','608851734',' 200202','Basic');


This is my code so far it executes without errors but the problem is that the FOREIGN KEY membershipid from membership table does not show on my column of the table CUSTOMER. So when I try to insert my data into the table customer it says  // ''[Teradata Database] [3535] A character string failed conversion to a numeric value.''
And my membershipid  still does not appear on the customer table as a column.

--Basic is part of membership table from column membershipid.
Your help will be greatly appreciated. 
",-1,-1,-1.0,"I have created a table called 'customer' on TERADATA-SQL-ASSISTANT. 

CREATE TABLE customer(
customerid   CHAR(8)                   NOT NULL,
firstname    VARCHAR(20)              NOT NULL,
lastname     VARCHAR(20)              NOT NULL,
email        VARCHAR(20)             NOT NULL,
creditno       INT                    NOT NULL,
phoneno        INT                    NOT NULL,
dateabo     DATE FORMAT 'YY/MM/DD'    NOT NULL,
PRIMARY KEY(customerid),
FOREIGN KEY (customerid) REFERENCES WITH CHECK OPTION membership (membershipid));

INSERT INTO customer VALUES('AAAA','Brooks','Swarng','Brooks@gmail.com','706994253476882810','608851734',' 200202','Basic');


This is my code so far it executes without errors but the problem is that the FOREIGN KEY membershipid from membership table does not show on my column of the table CUSTOMER. So when I try to insert my data into the table customer it says  // ''[Teradata Database] [3535] A character string failed conversion to a numeric value.''
And my membershipid  still does not appear on the customer table as a column.

--Basic is part of membership table from column membershipid.
Your help will be greatly appreciated. 
",3
494,62081343,loading teradata table using pandas taking so much time,"Pandas gets ridiculously slow when loading more than 10 million records from a Teradata server using teradatasql and mainly the function pandas.read_sql(query,teradata_con). it takes 40-45 minutes to load 1-1.5 million records from teradata table.
sql_query = &quot;select * from DB.TableName where columnname= 'values'&quot;


df = pd.read_sql(sql_query, con_t)


I used chunksize option alse , but it doesnt reduce the execution time , only it loads data in chunks with same time .
I tried to explore on IOPro package also, but didn't get much info on that. Is there any way to reduce the execution time ? cause, when I excute the same sql query directly in managment tool, it takes 1/3 rd time compare to pandas.
",-1,-1,-1.0,"Pandas gets ridiculously slow when loading more than 10 million records from a Teradata server using teradatasql and mainly the function pandas.read_sql(query,teradata_con). it takes 40-45 minutes to load 1-1.5 million records from teradata table.
sql_query = &quot;select * from DB.TableName where columnname= 'values'&quot;


df = pd.read_sql(sql_query, con_t)


I used chunksize option alse , but it doesnt reduce the execution time , only it loads data in chunks with same time .
I tried to explore on IOPro package also, but didn't get much info on that. Is there any way to reduce the execution time ? cause, when I excute the same sql query directly in managment tool, it takes 1/3 rd time compare to pandas.
",3
495,61876599,Teradata DB connection issue,"I've installed Teradata image(https://aws.amazon.com/marketplace/pp/B06Y4Z54R5) on AWS account. I'm able to log in and connect to DB. To save costs, I've stopped the instance and restarted it and try to connect to the DB again. I'm getting the following error.

SMP001-01:~ # bteq

 Teradata BTEQ 16.20.00.07 (32-bit) for LINUX. PID: 12571
 Copyright 1984-2018 Teradata. All rights reserved.
 Enter your logon or BTEQ command:
.logon DBC

.logon DBC
Password: 

 *** Warning: RDBMS CRASHED OR SESSIONS RESET.  RECOVERY IN PROGRESS.
^C^C^C *** Warning: Exiting because of three BREAKs!

 *** Exiting BTEQ.


I've tried restarting the DB using the following commands but it is not working.

/etc/init.d/tpa start

SMP001-01:~ # pdestate -a             
PDE state is RUN/READY.
DBS state is 0/-1: DBS is not running
SMP001-01:~ # psh pdestate -a

&lt;---------------------  localhost  --------------------------------&gt;
PDE state is RUN/READY.
DBS state is 0/-1: DBS is not running

",-1,-1,-1.0,"I've installed Teradata image(https://aws.amazon.com/marketplace/pp/B06Y4Z54R5) on AWS account. I'm able to log in and connect to DB. To save costs, I've stopped the instance and restarted it and try to connect to the DB again. I'm getting the following error.

SMP001-01:~ # bteq

 Teradata BTEQ 16.20.00.07 (32-bit) for LINUX. PID: 12571
 Copyright 1984-2018 Teradata. All rights reserved.
 Enter your logon or BTEQ command:
.logon DBC

.logon DBC
Password: 

 *** Warning: RDBMS CRASHED OR SESSIONS RESET.  RECOVERY IN PROGRESS.
^C^C^C *** Warning: Exiting because of three BREAKs!

 *** Exiting BTEQ.


I've tried restarting the DB using the following commands but it is not working.

/etc/init.d/tpa start

SMP001-01:~ # pdestate -a             
PDE state is RUN/READY.
DBS state is 0/-1: DBS is not running
SMP001-01:~ # psh pdestate -a

&lt;---------------------  localhost  --------------------------------&gt;
PDE state is RUN/READY.
DBS state is 0/-1: DBS is not running

",1
496,64691436,How to apply multiple whereclause in sqlalchmey in dask while fetching large dataset from teradata,"I am trying to fetch larger dataset from teradata using dask and sqlalchmey. I am able to apply single whereclause and able to fetch data.below is the working code
td_engine = create_engine(connString)
metadata = MetaData()
t = Table(
    &quot;table&quot;,
    metadata,
    Column(&quot;c1&quot;),
    schema=&quot;schema&quot;,
  )
sql = select([t]).where(
        t.c.c1 == 'abc',
    )
)
start = perf_counter()
df = dd.read_sql_table(sql, connString, index_col=&quot;c1&quot;,schema=&quot;schema&quot;)
end = perf_counter()
print(&quot;Time taken to execute the code {}&quot;.format(end - start))
print(df.head())

but when I am trying to apply and in whereclause I am getting error
sql = select([t]).where(
and_(
        t.c.c1 == 'abc',
        t.c.c2 == 'xyz'
    )
)

",-1,1,-1.0,"I am trying to fetch larger dataset from teradata using dask and sqlalchmey. I am able to apply single whereclause and able to fetch data.below is the working code
td_engine = create_engine(connString)
metadata = MetaData()
t = Table(
    &quot;table&quot;,
    metadata,
    Column(&quot;c1&quot;),
    schema=&quot;schema&quot;,
  )
sql = select([t]).where(
        t.c.c1 == 'abc',
    )
)
start = perf_counter()
df = dd.read_sql_table(sql, connString, index_col=&quot;c1&quot;,schema=&quot;schema&quot;)
end = perf_counter()
print(&quot;Time taken to execute the code {}&quot;.format(end - start))
print(df.head())

but when I am trying to apply and in whereclause I am getting error
sql = select([t]).where(
and_(
        t.c.c1 == 'abc',
        t.c.c2 == 'xyz'
    )
)

",3
497,65017434,Docker unable to resolve Teradata database source url,"I'm using docker to deploy a service which request Teradata for some information. The teradata only works on VPN which I'm connected now.
I'm using this syntax to connect to database :
 Class.forName(&quot;com.teradata.jdbc.TeraDriver&quot;);
 String serverURL = &quot;jdbc:teradata://TDPG/LOGMECH=LDAP&quot;;
 String username = ldap_username;
 String password = ldap_password;
 Connection con = DriverManager.getConnection(serverURL, username, password);

It's working when I connect from local environment, but when I use docker to deploy this service I get this error :
Caused by: java.net.UnknownHostException: TDPG: Name does not resolve

TDPG is the alias for database source and I don't have the IP for it. I don't know what is causing the problem because I'm able to access the url from local machine but couldn't from docker on the same machine.
I don't think it's a VPN problem because when I switch off VPN and then try to access from local machine I get different error (nodename nor server name specified) whereas the error inside docker remains same irrespective of I'm connected to VPN or not.
Someone please help.
",-1,-1,-1.0,"I'm using docker to deploy a service which request Teradata for some information. The teradata only works on VPN which I'm connected now.
I'm using this syntax to connect to database :
 Class.forName(&quot;com.teradata.jdbc.TeraDriver&quot;);
 String serverURL = &quot;jdbc:teradata://TDPG/LOGMECH=LDAP&quot;;
 String username = ldap_username;
 String password = ldap_password;
 Connection con = DriverManager.getConnection(serverURL, username, password);

It's working when I connect from local environment, but when I use docker to deploy this service I get this error :
Caused by: java.net.UnknownHostException: TDPG: Name does not resolve

TDPG is the alias for database source and I don't have the IP for it. I don't know what is causing the problem because I'm able to access the url from local machine but couldn't from docker on the same machine.
I don't think it's a VPN problem because when I switch off VPN and then try to access from local machine I get different error (nodename nor server name specified) whereas the error inside docker remains same irrespective of I'm connected to VPN or not.
Someone please help.
",1
498,64784238,Create stored procedure in Teradata from .net code,"I try to create a stored procedure from .NET code using Teradata.Client.Provider.
I used a query like this:
&quot;REPLACE PROCEDURE test_db.testproc() BEGIN END;&quot;

All code:
...
    var connectionTd = new TdConnection(&quot;[ConnectionString]&quot;);
    var cmdTd = connectionTd.CreateCommand();
    cmdTd.CommandText = &quot;REPLACE PROCEDURE test_db.testproc() BEGIN END;&quot;;
    connectionTd.Open();
    cmdTd.ExecuteNonQuery();
...

but I get an error:

[Teradata Database] [3706] Syntax error: Invalid  SQL Statement.

In IDE like dBeaver all works fine.
P.S.
Sample from this page causes the same error.
",1,-1,-1.0,"I try to create a stored procedure from .NET code using Teradata.Client.Provider.
I used a query like this:
&quot;REPLACE PROCEDURE test_db.testproc() BEGIN END;&quot;

All code:
...
    var connectionTd = new TdConnection(&quot;[ConnectionString]&quot;);
    var cmdTd = connectionTd.CreateCommand();
    cmdTd.CommandText = &quot;REPLACE PROCEDURE test_db.testproc() BEGIN END;&quot;;
    connectionTd.Open();
    cmdTd.ExecuteNonQuery();
...

but I get an error:

[Teradata Database] [3706] Syntax error: Invalid  SQL Statement.

In IDE like dBeaver all works fine.
P.S.
Sample from this page causes the same error.
",3
499,64746999,Compare multiple numbers row by row of same table in teradata,"Here is my table data from which I want to assign values to the record.
     Member_ID |    Claim_ID |    Codes     |  Pull

       123     |     Y   |  12,23,35,78 |   Y

       123     |     N   |     12,35    |   Y

       123     |     N   |     23,34    |   N

       123     |     N   |     33,34    |   N

I am using the teradata to assign 'Y' or 'N' to Pull depending on the codes and claims.
     SEL A.MEMBER_ID,A.CLAIM_ID,A.CODES,
     'Y' AS PULL
     FROM (SEL * FROM DBC.PULL_COMP WHERE CLAIM_ID='Y') A
     INNER JOIN ((SEL * FROM DBC.PULL_COMP WHERE CLAIM_ID='N') B
     ON A.MEMBER_ID=B.MEMBER_ID
     UNION
     SEL B.MEMBER_ID,B.CLAIM_ID,B.CODES,
     CASE WHEN OREPLACE(A.CODES,B.CODES,B.CODES)=A.CODES THEN 'Y'
     ELSE 'N' END AS PULL
     FROM (SEL * FROM DBC.PULL_COMP WHERE CLAIM_ID='Y') A
     INNER JOIN ((SEL * FROM DBC.PULL_COMP WHERE CLAIM_ID='N') B
     ON A.MEMBER_ID=B.MEMBER_ID

If the Claim_id is 'Y' the Pull will remain 'Y'. I want to compare the records whose claim_id is 'Y' with those whose claim_id id 'N'. The second record contains no new numbers when comparing with 1st record so Pull='Y'. The 3rd record contains one new number(34) hence Pull='N'. The 4th record contains all new numbers compared to 1st record hence 'N'. Even if there is one new number then Pull='N'. If all the numbers(Codes) of Claim_id='N' matches with the Codes of Claim_id='Y' then only Pull='Y'. I am populating the Pull column looking at member_id, claim_id and codes.
I am getting not the desired result with above query.
",-1,-1,-1.0,"Here is my table data from which I want to assign values to the record.
     Member_ID |    Claim_ID |    Codes     |  Pull

       123     |     Y   |  12,23,35,78 |   Y

       123     |     N   |     12,35    |   Y

       123     |     N   |     23,34    |   N

       123     |     N   |     33,34    |   N

I am using the teradata to assign 'Y' or 'N' to Pull depending on the codes and claims.
     SEL A.MEMBER_ID,A.CLAIM_ID,A.CODES,
     'Y' AS PULL
     FROM (SEL * FROM DBC.PULL_COMP WHERE CLAIM_ID='Y') A
     INNER JOIN ((SEL * FROM DBC.PULL_COMP WHERE CLAIM_ID='N') B
     ON A.MEMBER_ID=B.MEMBER_ID
     UNION
     SEL B.MEMBER_ID,B.CLAIM_ID,B.CODES,
     CASE WHEN OREPLACE(A.CODES,B.CODES,B.CODES)=A.CODES THEN 'Y'
     ELSE 'N' END AS PULL
     FROM (SEL * FROM DBC.PULL_COMP WHERE CLAIM_ID='Y') A
     INNER JOIN ((SEL * FROM DBC.PULL_COMP WHERE CLAIM_ID='N') B
     ON A.MEMBER_ID=B.MEMBER_ID

If the Claim_id is 'Y' the Pull will remain 'Y'. I want to compare the records whose claim_id is 'Y' with those whose claim_id id 'N'. The second record contains no new numbers when comparing with 1st record so Pull='Y'. The 3rd record contains one new number(34) hence Pull='N'. The 4th record contains all new numbers compared to 1st record hence 'N'. Even if there is one new number then Pull='N'. If all the numbers(Codes) of Claim_id='N' matches with the Codes of Claim_id='Y' then only Pull='Y'. I am populating the Pull column looking at member_id, claim_id and codes.
I am getting not the desired result with above query.
",3
500,65501926,Unable to insert into teradata using Spring Batch,"I am trying to insert data into Teradata using tasklet and I'm specifying the database in the jdbc url in application.yml file. I am receiving the below error
java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.04] [Error 3802] [SQLState 42S02] Database 'information_schema' does not exist.
information_schema is not the database mentioned in the url. Need help in resolving this error.
P.S: this is my first time using spring batch, I am an IIB Developer.
",-1,-1,-1.0,"I am trying to insert data into Teradata using tasklet and I'm specifying the database in the jdbc url in application.yml file. I am receiving the below error
java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.04] [Error 3802] [SQLState 42S02] Database 'information_schema' does not exist.
information_schema is not the database mentioned in the url. Need help in resolving this error.
P.S: this is my first time using spring batch, I am an IIB Developer.
",0
501,65504307,How to Insert new Record into Table if the Record is not Present in the Table in Teradata,"I want to insert a new record if the record is not present in the table
For that I am using below query in Teradata
 INSERT INTO sample(id, name) VALUES('12','rao')
 WHERE NOT EXISTS (SELECT id FROM sample WHERE id = '12'); 

When I execute the above query I am getting below error.
 WHERE NOT EXISTS
 
Failure 3706 Syntax error: expected something between ')' and the 'WHERE' keyword.

Can anyone help with the above issue. It will be very helpful.
",-1,-1,-1.0,"I want to insert a new record if the record is not present in the table
For that I am using below query in Teradata
 INSERT INTO sample(id, name) VALUES('12','rao')
 WHERE NOT EXISTS (SELECT id FROM sample WHERE id = '12'); 

When I execute the above query I am getting below error.
 WHERE NOT EXISTS
 
Failure 3706 Syntax error: expected something between ')' and the 'WHERE' keyword.

Can anyone help with the above issue. It will be very helpful.
",3
502,65576037,Sqoop Export from hive table to teradata table,"I am trying to load data from hive to teradata.
For that purpose, I have created an external table in hive, inserted values into it, and ran a sqoop export command.
create external table Sample_load
(
name varchar(32),
addr varchar(50),
);

insert into table Sample_load select F_name, Address from data where age &gt; 18;

sqoop export -Dhadoop.security.credential.provider.path=pwd_file_for_connection-
Dmapreduce.map.java.opts=&quot; -Duser.timezone=GMT&quot; --connect jdbc_url/database=Tera_db,charset=utf8 -m 
10 --username uname --password-alias pwd --table Teradata_table --hcatalog-database db_name -- 
hcatalog-table sample_load ;

but Im getting the following error :
Error: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:135)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        ... 10 more
Caused by: com.teradata.connector.common.exception.ConnectorException: java.lang.ClassNotFoundException: org.apache.hive.hcatalog.mapreduce.HCatSplit
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:276)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:281)
        ... 15 more

21/01/05 03:21:01 INFO mapreduce.Job: Task Id : attempt_1608639506866_65714_m_000000_1, Status : FAILED
Error: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:135)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        ... 10 more
Caused by: com.teradata.connector.common.exception.ConnectorException: java.lang.ClassNotFoundException: org.apache.hive.hcatalog.mapreduce.HCatSplit
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:276)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:281)
        ... 15 more

21/01/05 03:21:08 INFO mapreduce.Job: Task Id : attempt_1608639506866_65714_m_000000_2, Status : FAILED
Error: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:135)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        ... 10 more
Caused by: com.teradata.connector.common.exception.ConnectorException: java.lang.ClassNotFoundException: org.apache.hive.hcatalog.mapreduce.HCatSplit
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:276)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:281)
        ... 15 more

21/01/05 03:21:16 INFO mapreduce.Job:  map 100% reduce 0%
21/01/05 03:21:16 INFO mapreduce.Job: Job job_1608639506866_65714 failed with state FAILED due to: Task failed task_1608639506866_65714_m_000000
Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0

21/01/05 03:21:16 INFO mapreduce.Job: Counters: 9
        Job Counters
                Failed map tasks=4
                Launched map tasks=4
                Other local map tasks=3
                Data-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=76036
                Total time spent by all reduces in occupied slots (ms)=0
                Total time spent by all map tasks (ms)=19009
                Total vcore-milliseconds taken by all map tasks=19009
                Total megabyte-milliseconds taken by all map tasks=77860864
21/01/05 03:21:16 INFO processor.TeradataOutputProcessor: output postprocessor com.teradata.connector.teradata.processor.TeradataBatchInsertProcessor starts at:  1609834876349
21/01/05 03:21:16 INFO processor.TeradataBatchInsertProcessor: drop stage table &quot;Sample_A01_032038570&quot;
21/01/05 03:21:16 INFO processor.TeradataOutputProcessor: output postprocessor com.teradata.connector.teradata.processor.TeradataBatchInsertProcessor ends at:  1609834876349
21/01/05 03:21:16 INFO processor.TeradataOutputProcessor: the total elapsed time of output postprocessor com.teradata.connector.teradata.processor.TeradataBatchInsertProcessor is: 0s
21/01/05 03:21:16 INFO teradata.TeradataSqoopExportHelper: Teradata export job completed with exit code 1
21/01/05 03:21:16 ERROR tool.ExportTool: Error during export:
Import Job failed
        at org.apache.sqoop.teradata.TeradataConnManager.exportTable(TeradataConnManager.java:550)
        at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:94)
        at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:113)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:151)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:187)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:241)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:250)
        at org.apache.sqoop.Sqoop.main(Sqoop.java:259)
21/01/05 03:21:16 INFO metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 1

When checked the logs, I couldnt find the map process running, i.e it has failed even before the map process could start
Please help me out to find out the issue.
",-1,-1,-1.0,"I am trying to load data from hive to teradata.
For that purpose, I have created an external table in hive, inserted values into it, and ran a sqoop export command.
create external table Sample_load
(
name varchar(32),
addr varchar(50),
);

insert into table Sample_load select F_name, Address from data where age &gt; 18;

sqoop export -Dhadoop.security.credential.provider.path=pwd_file_for_connection-
Dmapreduce.map.java.opts=&quot; -Duser.timezone=GMT&quot; --connect jdbc_url/database=Tera_db,charset=utf8 -m 
10 --username uname --password-alias pwd --table Teradata_table --hcatalog-database db_name -- 
hcatalog-table sample_load ;

but Im getting the following error :
Error: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:135)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        ... 10 more
Caused by: com.teradata.connector.common.exception.ConnectorException: java.lang.ClassNotFoundException: org.apache.hive.hcatalog.mapreduce.HCatSplit
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:276)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:281)
        ... 15 more

21/01/05 03:21:01 INFO mapreduce.Job: Task Id : attempt_1608639506866_65714_m_000000_1, Status : FAILED
Error: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:135)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        ... 10 more
Caused by: com.teradata.connector.common.exception.ConnectorException: java.lang.ClassNotFoundException: org.apache.hive.hcatalog.mapreduce.HCatSplit
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:276)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:281)
        ... 15 more

21/01/05 03:21:08 INFO mapreduce.Job: Task Id : attempt_1608639506866_65714_m_000000_2, Status : FAILED
Error: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:135)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        ... 10 more
Caused by: com.teradata.connector.common.exception.ConnectorException: java.lang.ClassNotFoundException: org.apache.hive.hcatalog.mapreduce.HCatSplit
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:276)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
        at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:378)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)

        at com.teradata.connector.hcat.ConnectorCombineFileHCatSplit.&lt;init&gt;(ConnectorCombineFileHCatInputFormat.java:281)
        ... 15 more

21/01/05 03:21:16 INFO mapreduce.Job:  map 100% reduce 0%
21/01/05 03:21:16 INFO mapreduce.Job: Job job_1608639506866_65714 failed with state FAILED due to: Task failed task_1608639506866_65714_m_000000
Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0

21/01/05 03:21:16 INFO mapreduce.Job: Counters: 9
        Job Counters
                Failed map tasks=4
                Launched map tasks=4
                Other local map tasks=3
                Data-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=76036
                Total time spent by all reduces in occupied slots (ms)=0
                Total time spent by all map tasks (ms)=19009
                Total vcore-milliseconds taken by all map tasks=19009
                Total megabyte-milliseconds taken by all map tasks=77860864
21/01/05 03:21:16 INFO processor.TeradataOutputProcessor: output postprocessor com.teradata.connector.teradata.processor.TeradataBatchInsertProcessor starts at:  1609834876349
21/01/05 03:21:16 INFO processor.TeradataBatchInsertProcessor: drop stage table &quot;Sample_A01_032038570&quot;
21/01/05 03:21:16 INFO processor.TeradataOutputProcessor: output postprocessor com.teradata.connector.teradata.processor.TeradataBatchInsertProcessor ends at:  1609834876349
21/01/05 03:21:16 INFO processor.TeradataOutputProcessor: the total elapsed time of output postprocessor com.teradata.connector.teradata.processor.TeradataBatchInsertProcessor is: 0s
21/01/05 03:21:16 INFO teradata.TeradataSqoopExportHelper: Teradata export job completed with exit code 1
21/01/05 03:21:16 ERROR tool.ExportTool: Error during export:
Import Job failed
        at org.apache.sqoop.teradata.TeradataConnManager.exportTable(TeradataConnManager.java:550)
        at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:94)
        at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:113)
        at org.apache.sqoop.Sqoop.run(Sqoop.java:151)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:187)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:241)
        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:250)
        at org.apache.sqoop.Sqoop.main(Sqoop.java:259)
21/01/05 03:21:16 INFO metastore.HiveMetaStoreClient: Closed a connection to metastore, current connections: 1

When checked the logs, I couldnt find the map process running, i.e it has failed even before the map process could start
Please help me out to find out the issue.
",0
503,65615560,Exporting data from Teradata - Append issue,"I'm manually exporting data from teradata into .txt
As this needs to be done daily, I choose to append the new data into the previous file.
The issue here is there's spaces and lines in between the days causing errors when importing files into PBI
example
Anyone knows how to get rid of these so the data doesn't have any blank rows between the days?
Thanks
",-1,-1,-1.0,"I'm manually exporting data from teradata into .txt
As this needs to be done daily, I choose to append the new data into the previous file.
The issue here is there's spaces and lines in between the days causing errors when importing files into PBI
example
Anyone knows how to get rid of these so the data doesn't have any blank rows between the days?
Thanks
",3
504,65903493,Teradata SQL - Date format/transformation,"I have this table:
Original_Date  Date_        Date_1      YearMonth  YearMonth_1
03-07-2020     2020-07-03   2020-06-03  03-0       2020
13-03-2020     2020-03-13   2020-02-13  13-0       2020
08-01-2020     2020-01-08   2019-12-08  08-0       2019
13-11-2020     2020-11-13   2020-10-13  13-1       2020
23-03-2020     2020-03-23   2020-02-23  23-0       2020
30-07-2020     2020-07-30   2020-06-30  30-0       2020
13-07-2020     2020-07-13   2020-06-13  13-0       2020
24-01-2020     2020-01-24   2019-12-24  24-0       2019
05-10-2020     2020-10-05   2020-09-05  05-1       2020
11-07-2020     2020-07-11   2020-06-11  11-0       2020 

Where if I use TYPE() function on them they return:
Original Date: VARCHAR
Date_: DATE
Date_1: DATE
YearMonth_1: VARCHAR
YearMonth: VARCHAR

These columns are built based on Original_Date column, where:
SELECT
Original_Date
CAST(Original_Date AS DATE FORMAT 'DD-MM-YYYY') AS Date_, -- This is in a subquery, put for simplicity I put it here
ADD_MONTHS(CAST(Original_Date AS DATE FORMAT 'DD-MM-YYYY'),-1) AS Date_1, -- This is in a subquery, put for simplicity I put it here
LEFT(CAST(Date_ AS varchar(10)),4) AS YearMonth,
LEFT(CAST(Date_1 AS varchar(10)),4) AS YearMonth_1,
FROM TABLE

The problem is:

I can't understand why YearMonth takes the last (right) four characters instead of the first (left) four characters.
I was looking for a YearMonth with type DATE and format like YYYY-MM. But as I didn't find any solution with Teradata (if you can provide one, it will be better than the workaround I'm doing now), I'm going for YYYYMM as integer, but as you can see when I'm applying LEFT() function it seem to work like a RIGHT() function, even when both (Date_ and Date_1) have the same data types. why? How can I solve this?

EDIT
SELECT     
LEFT(CAST(Date_ AS varchar(10)),4) AS YearMonth_v2,
LEFT(CAST(Date_1 AS varchar(10)),4) AS YearMonth_1_v2,
FROM TABLE

Results in:
YearMonth_v2  YearMonth_1_v2
03-07-2020    2020-06-03
13-03-2020    2020-02-13
08-01-2020    2019-12-08
13-11-2020    2020-10-13
23-03-2020    2020-02-23
30-07-2020    2020-06-30
13-07-2020    2020-06-13
24-01-2020    2019-12-24
05-10-2020    2020-09-05
11-07-2020    2020-06-11

Why transforming the data type of these columns behave differently even when both of them (Date and Date_1) have the same data type and date format?
",1,-1,-1.0,"I have this table:
Original_Date  Date_        Date_1      YearMonth  YearMonth_1
03-07-2020     2020-07-03   2020-06-03  03-0       2020
13-03-2020     2020-03-13   2020-02-13  13-0       2020
08-01-2020     2020-01-08   2019-12-08  08-0       2019
13-11-2020     2020-11-13   2020-10-13  13-1       2020
23-03-2020     2020-03-23   2020-02-23  23-0       2020
30-07-2020     2020-07-30   2020-06-30  30-0       2020
13-07-2020     2020-07-13   2020-06-13  13-0       2020
24-01-2020     2020-01-24   2019-12-24  24-0       2019
05-10-2020     2020-10-05   2020-09-05  05-1       2020
11-07-2020     2020-07-11   2020-06-11  11-0       2020 

Where if I use TYPE() function on them they return:
Original Date: VARCHAR
Date_: DATE
Date_1: DATE
YearMonth_1: VARCHAR
YearMonth: VARCHAR

These columns are built based on Original_Date column, where:
SELECT
Original_Date
CAST(Original_Date AS DATE FORMAT 'DD-MM-YYYY') AS Date_, -- This is in a subquery, put for simplicity I put it here
ADD_MONTHS(CAST(Original_Date AS DATE FORMAT 'DD-MM-YYYY'),-1) AS Date_1, -- This is in a subquery, put for simplicity I put it here
LEFT(CAST(Date_ AS varchar(10)),4) AS YearMonth,
LEFT(CAST(Date_1 AS varchar(10)),4) AS YearMonth_1,
FROM TABLE

The problem is:

I can't understand why YearMonth takes the last (right) four characters instead of the first (left) four characters.
I was looking for a YearMonth with type DATE and format like YYYY-MM. But as I didn't find any solution with Teradata (if you can provide one, it will be better than the workaround I'm doing now), I'm going for YYYYMM as integer, but as you can see when I'm applying LEFT() function it seem to work like a RIGHT() function, even when both (Date_ and Date_1) have the same data types. why? How can I solve this?

EDIT
SELECT     
LEFT(CAST(Date_ AS varchar(10)),4) AS YearMonth_v2,
LEFT(CAST(Date_1 AS varchar(10)),4) AS YearMonth_1_v2,
FROM TABLE

Results in:
YearMonth_v2  YearMonth_1_v2
03-07-2020    2020-06-03
13-03-2020    2020-02-13
08-01-2020    2019-12-08
13-11-2020    2020-10-13
23-03-2020    2020-02-23
30-07-2020    2020-06-30
13-07-2020    2020-06-13
24-01-2020    2019-12-24
05-10-2020    2020-09-05
11-07-2020    2020-06-11

Why transforming the data type of these columns behave differently even when both of them (Date and Date_1) have the same data type and date format?
",3
505,65918909,Spark having issue reading teradata table which have display names for the table columns,"I am trying to read from Teradata table:
val df = spark.read.format(&quot;jdbc&quot;)
      .option(&quot;driver&quot;, &quot;com.teradata.jdbc.TeraDriver&quot;)
      .option(&quot;url&quot;, &quot;jdbc:teradata://abc.xyz.com/database=db_name&quot;)
      .option(&quot;dbtable&quot;, &quot;table_name&quot;)
      .option(&quot;user&quot;, &quot;user_name&quot;)
      .option(&quot;password&quot;, &quot;password&quot;)
      .load()
  

I am getting error:
java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.13] [Error 5628] [SQLState HY000] Column Data Rate not found in db_name.table_name.

In Teradata schema, column is defined like:
data_rate DECIMAL(18,0) FORMAT '-(18)9' TITLE 'Data Rate',

Please let me know how to solve this issue. Can anything be done on Spark side to solve this..?
Thanks
",-1,-1,-1.0,"I am trying to read from Teradata table:
val df = spark.read.format(&quot;jdbc&quot;)
      .option(&quot;driver&quot;, &quot;com.teradata.jdbc.TeraDriver&quot;)
      .option(&quot;url&quot;, &quot;jdbc:teradata://abc.xyz.com/database=db_name&quot;)
      .option(&quot;dbtable&quot;, &quot;table_name&quot;)
      .option(&quot;user&quot;, &quot;user_name&quot;)
      .option(&quot;password&quot;, &quot;password&quot;)
      .load()
  

I am getting error:
java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.13] [Error 5628] [SQLState HY000] Column Data Rate not found in db_name.table_name.

In Teradata schema, column is defined like:
data_rate DECIMAL(18,0) FORMAT '-(18)9' TITLE 'Data Rate',

Please let me know how to solve this issue. Can anything be done on Spark side to solve this..?
Thanks
",3
506,65926098,how copy teradata table to Azure sql with Azure function?,"I'm trying to copy data from one teradata table to a table in azure SQL using azure function, is this possible? can you give me a little example?
I can read the data with the teradata.client.provider, but I can't insert the data in the destination table in azure sql.
thanks u
",-1,-1,-1.0,"I'm trying to copy data from one teradata table to a table in azure SQL using azure function, is this possible? can you give me a little example?
I can read the data with the teradata.client.provider, but I can't insert the data in the destination table in azure sql.
thanks u
",3
507,65944454,Spark JDBC Write to Teradata: multiple spark tasks failing with Transaction ABORTed due to deadlock error resulting in Stage failure,"I am using spark JDBC write to load data from hive to teradata view. I am using 200 vcores and partitioned the data into 10000 partitions.
Spark tasks are failing with the below error resulting in stage failure. Sometimes the application finishes successfully but with some duplicate records
caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.10] [Error 2631] [SQLState 40001] Transaction ABORTed due to deadlock.
Below is the code I have used:
val df = spark.sql(&quot;select * from hive table&quot;).distinct.repartition(10000).write.mode(overwrite)
.option(&quot;truncate&quot;, Truncate).jdbc(url,dbTable, dproperties)
Teradata view is created with &quot;AS LOCKING ROW FOR ACCESS&quot;. The table also has a unique PI.
I am unable to figure out why some spark tasks are failing with dead lock error and is there a way I can stop my entire spark application from failing because of the task failures.
",-1,-1,-1.0,"I am using spark JDBC write to load data from hive to teradata view. I am using 200 vcores and partitioned the data into 10000 partitions.
Spark tasks are failing with the below error resulting in stage failure. Sometimes the application finishes successfully but with some duplicate records
caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.10] [Error 2631] [SQLState 40001] Transaction ABORTed due to deadlock.
Below is the code I have used:
val df = spark.sql(&quot;select * from hive table&quot;).distinct.repartition(10000).write.mode(overwrite)
.option(&quot;truncate&quot;, Truncate).jdbc(url,dbTable, dproperties)
Teradata view is created with &quot;AS LOCKING ROW FOR ACCESS&quot;. The table also has a unique PI.
I am unable to figure out why some spark tasks are failing with dead lock error and is there a way I can stop my entire spark application from failing because of the task failures.
",0
508,66031407,Query contains parameters but import file contains different values [importing csv to Teradata SQL],"I am using Teradata SQL to import a CSV file. I clicked import to activate the import operation, then typed the following
insert into databasename.tablename values(?,?,?,...)

I made sure to specify the database name as well as what I want the table to be named, and I put 13 commas--the number of columns in my CSV file.
It gives me the following error:
Query contains 13 parameters but Import file contains 1 data values

I have no idea what the issue is.
",-1,-1,-1.0,"I am using Teradata SQL to import a CSV file. I clicked import to activate the import operation, then typed the following
insert into databasename.tablename values(?,?,?,...)

I made sure to specify the database name as well as what I want the table to be named, and I put 13 commas--the number of columns in my CSV file.
It gives me the following error:
Query contains 13 parameters but Import file contains 1 data values

I have no idea what the issue is.
",3
509,66036599,Convert teradata data with timezone and load to timestamp with time zone colum,"I have a date data with time zone as below:
eventTime: 2021-02-02T23:10:00Z
and my teradata column data type is TIMESTAMP(6) with timezone.
I am not able to convert and load this data into the teradata table, so could you please anyone help me to resolve this.
",1,-1,-1.0,"I have a date data with time zone as below:
eventTime: 2021-02-02T23:10:00Z
and my teradata column data type is TIMESTAMP(6) with timezone.
I am not able to convert and load this data into the teradata table, so could you please anyone help me to resolve this.
",3
510,66426718,Teradata JDBC connection error in springboot,"I am creating springboot rest api which connect Teradata database but when i start the application i get below error , is there any configuration missing? here is the version of dependency i am using
&lt;dependency&gt;
            &lt;groupId&gt;com.teradata&lt;/groupId&gt;
            &lt;artifactId&gt;tdgssconfig&lt;/artifactId&gt;
            &lt;version&gt;14.00.00.13&lt;/version&gt;

        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.teradata&lt;/groupId&gt;
            &lt;artifactId&gt;terajdbc4&lt;/artifactId&gt;
            &lt;version&gt;14.00.00.13&lt;/version&gt;

        &lt;/dependency&gt;

application.properties changes
spring.datasource.driver-class-name=com.teradata.jdbc.TeraDriver
spring.datasource.url=jdbc:teradata://HOST/database=mydb
spring.datasource.username=MYID
spring.datasource.password=MYPW

ERROR
2021-03-01 12:18:33.430  WARN 18104 --- [         task-1] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata : [Teradata JDBC Driver] [TeraJDBC 14.00.00.13] [Error 165] [SQLState HY000] isValid: function not supported in this version
",-1,-1,-1.0,"I am creating springboot rest api which connect Teradata database but when i start the application i get below error , is there any configuration missing? here is the version of dependency i am using
&lt;dependency&gt;
            &lt;groupId&gt;com.teradata&lt;/groupId&gt;
            &lt;artifactId&gt;tdgssconfig&lt;/artifactId&gt;
            &lt;version&gt;14.00.00.13&lt;/version&gt;

        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.teradata&lt;/groupId&gt;
            &lt;artifactId&gt;terajdbc4&lt;/artifactId&gt;
            &lt;version&gt;14.00.00.13&lt;/version&gt;

        &lt;/dependency&gt;

application.properties changes
spring.datasource.driver-class-name=com.teradata.jdbc.TeraDriver
spring.datasource.url=jdbc:teradata://HOST/database=mydb
spring.datasource.username=MYID
spring.datasource.password=MYPW

ERROR
2021-03-01 12:18:33.430  WARN 18104 --- [         task-1] o.h.e.j.e.i.JdbcEnvironmentInitiator     : HHH000342: Could not obtain connection to query metadata : [Teradata JDBC Driver] [TeraJDBC 14.00.00.13] [Error 165] [SQLState HY000] isValid: function not supported in this version
",0
511,66650844,Greatest function in Teradata is throwing error with more than two arguments,"When I am executing the below code select greatest(13,6,20,30) in Teradata , it is throwing the below error:
ERROR [HY000] [Teradata][ODBC Teradata Driver][Teradata Database] Function 'greatest' called with an invalid number or type of parameters
SELECT Command Failed.

Although it is working with two arguments. As per documentation of version 16.* this function can accept till 10 arguments. I am working with Teradata version 16.2. Is there anything wrong with the SQL statement?
",-1,-1,-1.0,"When I am executing the below code select greatest(13,6,20,30) in Teradata , it is throwing the below error:
ERROR [HY000] [Teradata][ODBC Teradata Driver][Teradata Database] Function 'greatest' called with an invalid number or type of parameters
SELECT Command Failed.

Although it is working with two arguments. As per documentation of version 16.* this function can accept till 10 arguments. I am working with Teradata version 16.2. Is there anything wrong with the SQL statement?
",3
512,66961301,Teradata Rows to Columns (Need to Merge Doubled Row Count),"I have a query I'm running as follows -
SELECT 
item1,
item2,
item3,
CASE WHEN item4 = 'type A' THEN COUNT(DISTINCT field) END AS dcount_field_A,
CASE WHEN item4 = 'type B' THEN COUNT(DISTINCT field) END AS dcount_field_B,
FROM table
GROUP BY
item1
item2
item3
item4

and my output is:




Item1
Item2
Item 3
dcount_field_B
dcount_field_A




Item1Value1
Item2Value1
Item3Value1
NULL
1234


Item1Value1
Item2Value1
Item3Value1
1234
NULL




but what I need is for these rows to be merged, like:




Item1
Item2
Item 3
dcount_field_B
dcount_field_A




Item1Value1
Item2Value1
Item3Value1
1234
1234




Other attempted adjustments:

removing item4 from the group by gives me an error &quot;Selected
non-aggregate values must be part of the associated group.&quot;
PIVOT is not an option in my version of teradata
I have also tried a subquery of

    (SELECT
    COUNT(DISTINCT(CASE WHEN item4 = 'type A' THEN field END)) AS dcount_field_A,
    COUNT(DISTINCT(CASE WHEN item4 = 'type B' THEN field END)) AS dcount_field_B
    FROM table
    GROUP BY item4),


and I get the error &quot;Too many expressions in the select list of a subquery.&quot;

",-1,-1,-1.0,"I have a query I'm running as follows -
SELECT 
item1,
item2,
item3,
CASE WHEN item4 = 'type A' THEN COUNT(DISTINCT field) END AS dcount_field_A,
CASE WHEN item4 = 'type B' THEN COUNT(DISTINCT field) END AS dcount_field_B,
FROM table
GROUP BY
item1
item2
item3
item4

and my output is:




Item1
Item2
Item 3
dcount_field_B
dcount_field_A




Item1Value1
Item2Value1
Item3Value1
NULL
1234


Item1Value1
Item2Value1
Item3Value1
1234
NULL




but what I need is for these rows to be merged, like:




Item1
Item2
Item 3
dcount_field_B
dcount_field_A




Item1Value1
Item2Value1
Item3Value1
1234
1234




Other attempted adjustments:

removing item4 from the group by gives me an error &quot;Selected
non-aggregate values must be part of the associated group.&quot;
PIVOT is not an option in my version of teradata
I have also tried a subquery of

    (SELECT
    COUNT(DISTINCT(CASE WHEN item4 = 'type A' THEN field END)) AS dcount_field_A,
    COUNT(DISTINCT(CASE WHEN item4 = 'type B' THEN field END)) AS dcount_field_B
    FROM table
    GROUP BY item4),


and I get the error &quot;Too many expressions in the select list of a subquery.&quot;

",3
513,67077143,Update value in Teradata with different datatype Error 3754,"I'm trying to update a column value on Teradata , the source column are a varchar and the target a decimal (COD_ARTMDL to NUM_ARTAPP)
I tried to do a SET NUM_ARTAPP = TO_NUMBER(COD_ARTMDL)
But have an error :
UPDATE Failed. 3754:  Precision error in FLOAT type constant or during implicit conversions.
Can you help me with this ?
Thanks


",-1,-1,-1.0,"I'm trying to update a column value on Teradata , the source column are a varchar and the target a decimal (COD_ARTMDL to NUM_ARTAPP)
I tried to do a SET NUM_ARTAPP = TO_NUMBER(COD_ARTMDL)
But have an error :
UPDATE Failed. 3754:  Precision error in FLOAT type constant or during implicit conversions.
Can you help me with this ?
Thanks


",3
514,67131817,Create Row level data from a range in Teradata,"I have a table like below




EMP_NUM
START_RNG
END_RNG




123
H1
H3


456
H4
H6




and I need it to look like the one below.




EMP_NUM
ID




123
H1


123
H2


123
H3


456
H4


456
H5


456
H6




I had asked a similar question in reference to dates in my earlier post, however I don't think EXPAND ON will work since it works on time periods. Is there a way to solve for this in Teradata. I am new to recursion as well. Tried the below logic but getting an error in Teradata. Not sure if alternate syntax exists.
WITH temp AS
(
   SELECT 1 AS ID 
   UNION ALL 
   SELECT t.ID + 1 FROM temp t
   WHERE t.ID &lt; 100000
) -- return table with id from 1 to 100000
SELECT t.ID,  y.EMP_NUM
FROM Table1 y
INNER JOIN temp t ON t.ID BETWEEN y.START_RNG AND y.END_RNG
OPTION (MAXRECURSION 0);

",-1,-1,-1.0,"I have a table like below




EMP_NUM
START_RNG
END_RNG




123
H1
H3


456
H4
H6




and I need it to look like the one below.




EMP_NUM
ID




123
H1


123
H2


123
H3


456
H4


456
H5


456
H6




I had asked a similar question in reference to dates in my earlier post, however I don't think EXPAND ON will work since it works on time periods. Is there a way to solve for this in Teradata. I am new to recursion as well. Tried the below logic but getting an error in Teradata. Not sure if alternate syntax exists.
WITH temp AS
(
   SELECT 1 AS ID 
   UNION ALL 
   SELECT t.ID + 1 FROM temp t
   WHERE t.ID &lt; 100000
) -- return table with id from 1 to 100000
SELECT t.ID,  y.EMP_NUM
FROM Table1 y
INNER JOIN temp t ON t.ID BETWEEN y.START_RNG AND y.END_RNG
OPTION (MAXRECURSION 0);

",3
515,67317720,Propagate missing dates in teradata - select query,"I have a table that looks like this:




my_date
item_id.
sales




2020-03-01
GMZS72429
2


2020-03-07
GMZS72429
2


2020-03-09
GMZS72429
1


2020-03-04
GMZS72425
1




And I want it to look like this




my_date
item_id
sales




2020-03-01
GMZS72429
2


2020-03-02
GMZS72429
0


...
...
...


2020-03-05
GMZS72429
0


2020-03-06
GMZS72429
0


2020-03-07
GMZS72429
2


2020-03-08
GMZS72429
0


2020-03-09
GMZS72429
1


2020-03-01
GMZS72425
0


2020-03-02
GMZS72425
0


2020-03-03
GMZS72425
0


2020-03-04
GMZS72425
1


...
...
...


2020-03-09
GMZS72425
0




Since I was struggling with the documentation from Teradata, I have tried generating the pair item_id - my_date using another table, followed by a left join:
with a1 as(
select distinct my_date, item_id from some_table_with_the_item_ids_and_all_dates
) 
select a1.my_date, a1.item_id, coalesce(sales, 0) as sales
from a1 left join my_table on a1.item_id=my_table.item_id and a1.my_date=my_table.my_date;

This worked but it is terribly slow, and ugly. I was wondering if there is a better built-in (or alternative) method to do this. Thanks
",1,-1,-1.0,"I have a table that looks like this:




my_date
item_id.
sales




2020-03-01
GMZS72429
2


2020-03-07
GMZS72429
2


2020-03-09
GMZS72429
1


2020-03-04
GMZS72425
1




And I want it to look like this




my_date
item_id
sales




2020-03-01
GMZS72429
2


2020-03-02
GMZS72429
0


...
...
...


2020-03-05
GMZS72429
0


2020-03-06
GMZS72429
0


2020-03-07
GMZS72429
2


2020-03-08
GMZS72429
0


2020-03-09
GMZS72429
1


2020-03-01
GMZS72425
0


2020-03-02
GMZS72425
0


2020-03-03
GMZS72425
0


2020-03-04
GMZS72425
1


...
...
...


2020-03-09
GMZS72425
0




Since I was struggling with the documentation from Teradata, I have tried generating the pair item_id - my_date using another table, followed by a left join:
with a1 as(
select distinct my_date, item_id from some_table_with_the_item_ids_and_all_dates
) 
select a1.my_date, a1.item_id, coalesce(sales, 0) as sales
from a1 left join my_table on a1.item_id=my_table.item_id and a1.my_date=my_table.my_date;

This worked but it is terribly slow, and ugly. I was wondering if there is a better built-in (or alternative) method to do this. Thanks
",3
516,67339200,Is there a faster way to put the result of a SQL query from Teradata into a pandas Data Frame?,"I have a SQL query from Teradata with 3 million lines and 50 columns.
I am trying to use it in python as it follows:
data = pd.DataFrame(tera.execute_response('''select * from table'''))

This process is running till my kernel dies and I lose everything.
",0,-1,-1.0,"I have a SQL query from Teradata with 3 million lines and 50 columns.
I am trying to use it in python as it follows:
data = pd.DataFrame(tera.execute_response('''select * from table'''))

This process is running till my kernel dies and I lose everything.
",3
517,67402797,Get Distinct Values Based on a Second Column in Teradata,"My table looks like this:

and I am trying to get distinct values of repunit, based on a second column in teradata like this:

by running this query:
SELECT 
    DISTINCT foo.&quot;repunit&quot;, 
    COALESCE(foo.&quot;country&quot;,'NO-COUNTRY') 
    FROM db.table AS foo

It does not work as expected, I am getting quite some NULLs as results and none NO-COUNTRY.
",-1,-1,-1.0,"My table looks like this:

and I am trying to get distinct values of repunit, based on a second column in teradata like this:

by running this query:
SELECT 
    DISTINCT foo.&quot;repunit&quot;, 
    COALESCE(foo.&quot;country&quot;,'NO-COUNTRY') 
    FROM db.table AS foo

It does not work as expected, I am getting quite some NULLs as results and none NO-COUNTRY.
",3
518,67515841,Loading XML data from SQL Server into Teradata,"I am trying to load XML column (along with multiple other columns) from MS SQL to Teradata. The column datatype is XML in both the server. I am using TPT to extract and load. I am using TPT ODBC Operator to extract the data from the SQL Server and TPT Inserter Operator to insert the data into Teradata table. But during the insert, I am getting &quot;Parsing error&quot; even though the XML data is in correct format. I verified it using different validators.
I went through the Teradata Documentation (https://docs.teradata.com/r/NMjDGHKjr0Tkeos1ItQDhA/GXKdU6AYKUeNIieLN7JJKA) and I found the following statement regarding the ODBC Operator.

The ODBC operator does not support extracting LOB data from the third-party server and loading it to Teradata table. The operator can extract the LOB data, but does not prepare the data in Teradata format, which has 8-byte length preceding the data.

Now, my question is there any way to overcome this issue or there any other alternatives to load the XML from SQL Server to Teradata. I am not able to attach any code or data as they are confidential.
",-1,-1,-1.0,"I am trying to load XML column (along with multiple other columns) from MS SQL to Teradata. The column datatype is XML in both the server. I am using TPT to extract and load. I am using TPT ODBC Operator to extract the data from the SQL Server and TPT Inserter Operator to insert the data into Teradata table. But during the insert, I am getting &quot;Parsing error&quot; even though the XML data is in correct format. I verified it using different validators.
I went through the Teradata Documentation (https://docs.teradata.com/r/NMjDGHKjr0Tkeos1ItQDhA/GXKdU6AYKUeNIieLN7JJKA) and I found the following statement regarding the ODBC Operator.

The ODBC operator does not support extracting LOB data from the third-party server and loading it to Teradata table. The operator can extract the LOB data, but does not prepare the data in Teradata format, which has 8-byte length preceding the data.

Now, my question is there any way to overcome this issue or there any other alternatives to load the XML from SQL Server to Teradata. I am not able to attach any code or data as they are confidential.
",3
519,67521167,Teradata left join dropping nulls,"In Teradata 16.20.53.29, when I do a left join and aggregation as follows:
select
  b.department,
  sum(a.sales) as sales

from table1 a
  left join table2 b
    on b.product = a.product

where a.date_purchase &gt;= '2018-01-01'
  and a.date_purchase &lt;= '2020-01-01'
  and a.brand = 'ACME'
  and a.quantity &gt; 0

group by 1
order by 1

I expect the missing rows that are in table1 and not in table2 to be included but they are not.
Output:

However, if I restructure the query like this:
select
   b.department,
   sum(a.sales) as sales

from table1 a
    left join table2 b
        on b.product = a.product
        and a.date_purchase &gt;= '2018-01-01'
        and a.date_purchase &lt;= '2020-01-01'
        and a.brand = 'ACME'
        and a.quantity &gt; 0

group by 1
order by 1


Then the missing rows are included.
Output:

This is the opposite of my expectation, I would expect the first query to include the nulls and the second query to effectively become an inner join and thus exclude the nulls. Why is this happening?
",-1,-1,-1.0,"In Teradata 16.20.53.29, when I do a left join and aggregation as follows:
select
  b.department,
  sum(a.sales) as sales

from table1 a
  left join table2 b
    on b.product = a.product

where a.date_purchase &gt;= '2018-01-01'
  and a.date_purchase &lt;= '2020-01-01'
  and a.brand = 'ACME'
  and a.quantity &gt; 0

group by 1
order by 1

I expect the missing rows that are in table1 and not in table2 to be included but they are not.
Output:

However, if I restructure the query like this:
select
   b.department,
   sum(a.sales) as sales

from table1 a
    left join table2 b
        on b.product = a.product
        and a.date_purchase &gt;= '2018-01-01'
        and a.date_purchase &lt;= '2020-01-01'
        and a.brand = 'ACME'
        and a.quantity &gt; 0

group by 1
order by 1


Then the missing rows are included.
Output:

This is the opposite of my expectation, I would expect the first query to include the nulls and the second query to effectively become an inner join and thus exclude the nulls. Why is this happening?
",3
520,67606424,Do a SHA256 of a query field in Teradata,"As part of a project I need to retrieve personal user information from a Teradata table.
To preserve user privacy we need to have access only on the sha256 hash of the values.
Would it be possible in Teradata to hash the value directly in the query?
I tried:
SELECT sha256(login_email) as sha_login_email
FROM tablex;

but the function sha256(binary) doesn't work with Strings.
I get an error: The string contains an untranslatable character.
whichever string is processed by the query.
",-1,-1,-1.0,"As part of a project I need to retrieve personal user information from a Teradata table.
To preserve user privacy we need to have access only on the sha256 hash of the values.
Would it be possible in Teradata to hash the value directly in the query?
I tried:
SELECT sha256(login_email) as sha_login_email
FROM tablex;

but the function sha256(binary) doesn't work with Strings.
I get an error: The string contains an untranslatable character.
whichever string is processed by the query.
",3
521,67890574,Adapting Cursor in Teradata SQL,"I have a simple Cursor in SQL Server that I would like to adapt to use in Teradata.
The goal of the cursor is to collect the names of a series of tables and rename them using cursor logic.
I already change most of the cursor code to use in Teradata, but I'm still having some trouble to finish this.
So far I have:
    DECLARE varTableOldName VARCHAR(500);
    DECLARE varTableNewName VARCHAR(500);
    DECLARE vardbName VARCHAR(100);
    DECLARE varIDCod VARCHAR(5);
    DECLARE varRename VARCHAR(100);
    DECLARE varCt INT DEFAULT 0; 

DECLARE renameTables CURSOR FOR
    
    SELECT
        DBname
        ,TBname
    FROM (
        SELECT
            DatabaseName AS DBname
            ,TableName AS TBname
            ,LastAccessTimeStamp AS LADate
            ,(CURRENT_DATE - CAST(LastAccessTimeStamp AS DATE)) AS NAccessDate
        FROM
            DBC.TablesV
        WHERE
            1=1
            AND TableKind = 'T'
            AND DatabaseName IN ('PD_BACKUP')
        GROUP BY
            DatabaseName
            ,TableName
            ,LastAccessTimeStamp
            ,LastAlterTimeStamp
    ) tbHig
    WHERE NAccessDate IS NULL OR NAccessDate &gt;= 180
    ORDER BY DBname, TBname

FOR READ ONLY;

OPEN renameTables;

    FETCH NEXT FROM renameTables
    INTO vardbName, varTableOldName;
    
    WHILE (SQLCODE = 0)
    
        DO
        
        SET varTableNewName = vardbName || '_V' || CAST(EXTRACT(YEAR FROM CURRENT_TIMESTAMP) AS VARCHAR(20)) || '_' || CAST(varCt AS VARCHAR(20));
    
    
        SET varTableOldName = vardbName || '.' || varTableOldName;
    
        SET varRename = 'RENAME TABLE ' || varTableOldName || ' TO ' || varTableNewName;
        
        EXECUTE IMMEDIATE varRename;
    
        SET varCt = varCt + 1;
    
        FETCH NEXT FROM renameTables INTO vardbName, varTableOldName;

    END WHILE;

CLOSE renameTables;

I think the problem is with some syntax details or something.
Can anyone guide me with this please?
",-1,-1,-1.0,"I have a simple Cursor in SQL Server that I would like to adapt to use in Teradata.
The goal of the cursor is to collect the names of a series of tables and rename them using cursor logic.
I already change most of the cursor code to use in Teradata, but I'm still having some trouble to finish this.
So far I have:
    DECLARE varTableOldName VARCHAR(500);
    DECLARE varTableNewName VARCHAR(500);
    DECLARE vardbName VARCHAR(100);
    DECLARE varIDCod VARCHAR(5);
    DECLARE varRename VARCHAR(100);
    DECLARE varCt INT DEFAULT 0; 

DECLARE renameTables CURSOR FOR
    
    SELECT
        DBname
        ,TBname
    FROM (
        SELECT
            DatabaseName AS DBname
            ,TableName AS TBname
            ,LastAccessTimeStamp AS LADate
            ,(CURRENT_DATE - CAST(LastAccessTimeStamp AS DATE)) AS NAccessDate
        FROM
            DBC.TablesV
        WHERE
            1=1
            AND TableKind = 'T'
            AND DatabaseName IN ('PD_BACKUP')
        GROUP BY
            DatabaseName
            ,TableName
            ,LastAccessTimeStamp
            ,LastAlterTimeStamp
    ) tbHig
    WHERE NAccessDate IS NULL OR NAccessDate &gt;= 180
    ORDER BY DBname, TBname

FOR READ ONLY;

OPEN renameTables;

    FETCH NEXT FROM renameTables
    INTO vardbName, varTableOldName;
    
    WHILE (SQLCODE = 0)
    
        DO
        
        SET varTableNewName = vardbName || '_V' || CAST(EXTRACT(YEAR FROM CURRENT_TIMESTAMP) AS VARCHAR(20)) || '_' || CAST(varCt AS VARCHAR(20));
    
    
        SET varTableOldName = vardbName || '.' || varTableOldName;
    
        SET varRename = 'RENAME TABLE ' || varTableOldName || ' TO ' || varTableNewName;
        
        EXECUTE IMMEDIATE varRename;
    
        SET varCt = varCt + 1;
    
        FETCH NEXT FROM renameTables INTO vardbName, varTableOldName;

    END WHILE;

CLOSE renameTables;

I think the problem is with some syntax details or something.
Can anyone guide me with this please?
",3
522,67965954,How to find the total number of columns in a table in teradata?,"I am trying to get the number of columns in a row for a given table in teradata. I am finding it difficult to find the total number with the usual SQL command. This is what I have tried but did not give me any result:
SELECT COUNT(*)
  FROM INFORMATION_SCHEMA.COLUMNS
 WHERE table_catalog = 'WMS' 
   AND table_name = 'RM_SELLER_ITEM_MST'

",-1,-1,-1.0,"I am trying to get the number of columns in a row for a given table in teradata. I am finding it difficult to find the total number with the usual SQL command. This is what I have tried but did not give me any result:
SELECT COUNT(*)
  FROM INFORMATION_SCHEMA.COLUMNS
 WHERE table_catalog = 'WMS' 
   AND table_name = 'RM_SELLER_ITEM_MST'

",3
523,68005132,Error uploading data into Teradata using Teradatasql module in python,"I am trying to upload data into a table in Teradata using TeradataSQL module in python, but I am getting the following error:
A failure occurred while executing rows 1 through 2 of a batch request...Caused by [Version 17.0.0.8] [Session 21731976] [Teradata Database] [Error 3939] 
There is a mismatch between the number of parameters specified and the number of parameters required.

Here is the code I am using:
import teradatasql

listbatch = x.values.tolist()

with teradatasql.connect ('{&quot;host&quot;:&quot;whomooz&quot;,&quot;user&quot;:&quot;guest&quot;,&quot;password&quot;:&quot;please&quot;}') as con:
    with con.cursor () as cur:
        cur.executemany (&quot;insert into mytableinTeradata  ('?,?,?')&quot;, listbatch)

This is what listbatch looks like
[[0, 'Rep Entered', '8cfc12c9-000d3a8fc913'],
 [0, 'Rep Entered', '699831ad-0fa60249f33e']]

looking a data types, they seem to match:
For listbatch:
TEST_IND               int64
SOURCE_LABEL           object
OPPORTUNITYID          object
dtype: object

For mytableinTeradata:
TEST_IND                        I 
SOURCE_LABEL                    CV
OPPORTUNITYID                   CV

I have been able to use this same code before in a different project with no problems. I am not sure what is different here. Any thoughts on what that error means and how to resolve it?
",-1,-1,-1.0,"I am trying to upload data into a table in Teradata using TeradataSQL module in python, but I am getting the following error:
A failure occurred while executing rows 1 through 2 of a batch request...Caused by [Version 17.0.0.8] [Session 21731976] [Teradata Database] [Error 3939] 
There is a mismatch between the number of parameters specified and the number of parameters required.

Here is the code I am using:
import teradatasql

listbatch = x.values.tolist()

with teradatasql.connect ('{&quot;host&quot;:&quot;whomooz&quot;,&quot;user&quot;:&quot;guest&quot;,&quot;password&quot;:&quot;please&quot;}') as con:
    with con.cursor () as cur:
        cur.executemany (&quot;insert into mytableinTeradata  ('?,?,?')&quot;, listbatch)

This is what listbatch looks like
[[0, 'Rep Entered', '8cfc12c9-000d3a8fc913'],
 [0, 'Rep Entered', '699831ad-0fa60249f33e']]

looking a data types, they seem to match:
For listbatch:
TEST_IND               int64
SOURCE_LABEL           object
OPPORTUNITYID          object
dtype: object

For mytableinTeradata:
TEST_IND                        I 
SOURCE_LABEL                    CV
OPPORTUNITYID                   CV

I have been able to use this same code before in a different project with no problems. I am not sure what is different here. Any thoughts on what that error means and how to resolve it?
",1
524,68056927,Teradataml error mlengine_alias_definitions_v1.0' is not defined for the current Vantage version 'vantage1.0',"i'm trying to create an exe file which take advantage of teradataml python. I'm trying to create a table in teradata and import the data form pandas dataframe.
here is my code.
import pandas as pd
from sqlalchemy import create_engine
from teradataml.context.context import *
from sqlalchemy import *
from teradataml.dataframe.copy_to import copy_to_sql
from sqlalchemy.dialects import registry
from teradatasqlalchemy import dialect
registry.register('teradata', 'teradatasqlalchemy', 'dialect')

user = 'dbc'
pasw=user
host = '192.168.1.7'

td_engine = create_engine('teradata://'+ user +':' + pasw + '@'+ host )

create_context(tdsqlengine =td_engine)

df = pd.read_csv(r&quot;C:/krishna/data/FL_insurance_sample1.csv&quot;, delimiter=',')
copy_to_sql(df = df, table_name = &quot;Insurece_sample&quot;, primary_index=&quot;InsurenceID&quot;, if_exists=&quot;replace&quot;)
remove_context()

initially i was getting below error however i fixed that one.
sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:teradata

pyinstaller command which i tried:
pyinstaller --add-binary &quot;C:\Users\krishna\AppData\Local\Programs\Python\Python38\Lib\site-packages\teradatasql\teradatasql.dll;teradatasql&quot;-F pyinstalletest.py

the error which i'm getting now:
Traceback (most recent call last):
  File &quot;pyinstalletest.py&quot;, line 18, in &lt;module&gt;
  File &quot;teradataml\context\context.py&quot;, line 459, in create_context
  File &quot;teradataml\context\context.py&quot;, line 751, in _load_function_aliases
  File &quot;teradataml\common\utils.py&quot;, line 1591, in _check_alias_config_file_exists
teradataml.common.exceptions.TeradataMlException: [Teradata][teradataml](TDML_2069) Alias config file 'C:\Users\krishna\AppData\Local\Temp\_MEI63962\teradataml\config\mlengine_alias_definitions_v1.0' is not defined for the current Vantage version 'vantage1.0'. Please add the config file.
[1660] Failed to execute script pyinstalletest

please help me to resolve the error.
",-1,-1,-1.0,"i'm trying to create an exe file which take advantage of teradataml python. I'm trying to create a table in teradata and import the data form pandas dataframe.
here is my code.
import pandas as pd
from sqlalchemy import create_engine
from teradataml.context.context import *
from sqlalchemy import *
from teradataml.dataframe.copy_to import copy_to_sql
from sqlalchemy.dialects import registry
from teradatasqlalchemy import dialect
registry.register('teradata', 'teradatasqlalchemy', 'dialect')

user = 'dbc'
pasw=user
host = '192.168.1.7'

td_engine = create_engine('teradata://'+ user +':' + pasw + '@'+ host )

create_context(tdsqlengine =td_engine)

df = pd.read_csv(r&quot;C:/krishna/data/FL_insurance_sample1.csv&quot;, delimiter=',')
copy_to_sql(df = df, table_name = &quot;Insurece_sample&quot;, primary_index=&quot;InsurenceID&quot;, if_exists=&quot;replace&quot;)
remove_context()

initially i was getting below error however i fixed that one.
sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:teradata

pyinstaller command which i tried:
pyinstaller --add-binary &quot;C:\Users\krishna\AppData\Local\Programs\Python\Python38\Lib\site-packages\teradatasql\teradatasql.dll;teradatasql&quot;-F pyinstalletest.py

the error which i'm getting now:
Traceback (most recent call last):
  File &quot;pyinstalletest.py&quot;, line 18, in &lt;module&gt;
  File &quot;teradataml\context\context.py&quot;, line 459, in create_context
  File &quot;teradataml\context\context.py&quot;, line 751, in _load_function_aliases
  File &quot;teradataml\common\utils.py&quot;, line 1591, in _check_alias_config_file_exists
teradataml.common.exceptions.TeradataMlException: [Teradata][teradataml](TDML_2069) Alias config file 'C:\Users\krishna\AppData\Local\Temp\_MEI63962\teradataml\config\mlengine_alias_definitions_v1.0' is not defined for the current Vantage version 'vantage1.0'. Please add the config file.
[1660] Failed to execute script pyinstalletest

please help me to resolve the error.
",1
525,68088379,sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:teradatasql [10076] Failed to execute script pyinstalletest,"i'm trying to create a exe which import some data into teradata.As a test script i'm trying to connect Teradata using below code. i can able to successfully connect in my local.
from sqlalchemy import create_engine

user = 'dbc'
pasw=user
host = '192.168.1.7'
# connect
td_engine = create_engine('teradatasql://'+ user +':' + pasw + '@'+ host + '')
sql=&quot;select * from dbc.usersV&quot;
x=td_engine.execute(sql)

i have generate the exe using below command.
pyinstaller --add-binary &quot;C:\Users\krishna\AppData\Local\Programs\Python\Python38\Lib\site-packages\teradatasql\teradatasql.dll;teradatasql&quot; -F pyinstalletest.py 

once the exe is generated i'm trying to execute the exe and i'm getting below error.
Traceback (most recent call last):
  File &quot;pyinstalletest.py&quot;, line 8, in &lt;module&gt;
  File &quot;sqlalchemy\engine\__init__.py&quot;, line 479, in create_engine
  File &quot;sqlalchemy\engine\strategies.py&quot;, line 61, in create
  File &quot;sqlalchemy\engine\url.py&quot;, line 172, in _get_entrypoint
  File &quot;sqlalchemy\util\langhelpers.py&quot;, line 267, in load
sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:teradatasql
[10076] Failed to execute script pyinstalletest

i have gone through various links but no luck. please help me to resolve the error.
",-1,-1,-1.0,"i'm trying to create a exe which import some data into teradata.As a test script i'm trying to connect Teradata using below code. i can able to successfully connect in my local.
from sqlalchemy import create_engine

user = 'dbc'
pasw=user
host = '192.168.1.7'
# connect
td_engine = create_engine('teradatasql://'+ user +':' + pasw + '@'+ host + '')
sql=&quot;select * from dbc.usersV&quot;
x=td_engine.execute(sql)

i have generate the exe using below command.
pyinstaller --add-binary &quot;C:\Users\krishna\AppData\Local\Programs\Python\Python38\Lib\site-packages\teradatasql\teradatasql.dll;teradatasql&quot; -F pyinstalletest.py 

once the exe is generated i'm trying to execute the exe and i'm getting below error.
Traceback (most recent call last):
  File &quot;pyinstalletest.py&quot;, line 8, in &lt;module&gt;
  File &quot;sqlalchemy\engine\__init__.py&quot;, line 479, in create_engine
  File &quot;sqlalchemy\engine\strategies.py&quot;, line 61, in create
  File &quot;sqlalchemy\engine\url.py&quot;, line 172, in _get_entrypoint
  File &quot;sqlalchemy\util\langhelpers.py&quot;, line 267, in load
sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:teradatasql
[10076] Failed to execute script pyinstalletest

i have gone through various links but no luck. please help me to resolve the error.
",1
526,68098754,How to count rows with rounded valus in SQL Teradata?,"I work in SQL Teradata.
I would liek to count how many rows (clients) have rounded value in column &quot;amount&quot; (means for example 140.00 not 157.76 and so on). I use code like below:
    select 
    client_id, 
    count(amount mod 1 = 0)
    from table
    group by client_id

Nevertheless, I have an error like: SELECT Failed. 3706: Syntax error: expected something between an integer and '='.
What can I do ?
",-1,-1,-1.0,"I work in SQL Teradata.
I would liek to count how many rows (clients) have rounded value in column &quot;amount&quot; (means for example 140.00 not 157.76 and so on). I use code like below:
    select 
    client_id, 
    count(amount mod 1 = 0)
    from table
    group by client_id

Nevertheless, I have an error like: SELECT Failed. 3706: Syntax error: expected something between an integer and '='.
What can I do ?
",3
527,68133716,Custom stored procedure in Teradata SQL,"In the past I adapted a cursor I have in a stored procedure on SQL Server to Teradata but I'm having some troubles trying to use. My procedure code is:
CREATE PROCEDURE sp_QrRnTables()

BEGIN

DECLARE varTableOldName VARCHAR(500);
DECLARE varTableNewName VARCHAR(500);
DECLARE vardbName VARCHAR(100);
DECLARE varIDCod VARCHAR(5);
DECLARE varRename VARCHAR(100);
DECLARE varCt INT DEFAULT 0;
    
DECLARE renameTables CURSOR 
FOR
    
    SELECT
        nomeDB
        ,nomeTabela
    FROM (
        SELECT
            DatabaseName AS nomeDB
            ,TableName AS nomeTabela
            ,LastAccessTimeStamp AS dtUltimoAcesso
            ,(CURRENT_DATE - CAST(LastAccessTimeStamp AS DATE)) AS diasSemAcesso
        FROM
            DBC.TablesV
        WHERE
            1=1
            AND TableKind = 'T'
            AND DatabaseName IN ('P_BACKUP')
        GROUP BY
            DatabaseName
            ,TableName
            ,LastAccessTimeStamp
            ,LastAlterTimeStamp
    ) tbHig
    WHERE diasSemAcesso IS NULL OR diasSemAcesso &gt;= 180
    ORDER BY nomeDB, nomeTabela

FOR READ ONLY;

OPEN renameTables;

    FETCH NEXT FROM renameTables INTO vardbName, varTableOldName;

    WHILE (SQLCODE = 0)
    
        DO
        
        SET varTableNewName = vardbName || '.' || vardbName || '_V' || CAST(EXTRACT(YEAR FROM CURRENT_TIMESTAMP) AS VARCHAR(20)) || '_' || CAST(varCt AS VARCHAR(20));
    
        SET varTableOldName = vardbName || '.' || varTableOldName;
    
        SET varRename = 'RENAME TABLE ' || varTableOldName || ' TO ' || varTableNewName;
        
        EXECUTE IMMEDIATE varRename;
    
        SET varCt = varCt + 1;
    
        FETCH NEXT FROM renameTables INTO vardbName, varTableOldName;
        
    END WHILE;

CLOSE renameTables;

END;

The goal is is to collect the names of a series of tables and rename them using the cursor logic. Something like this: If I have three tables on my database &quot;myTableA&quot;, &quot;myTableB&quot;, &quot;myTableC&quot;, by the cursor logic I hope these are renamed to &quot;P_BACKUP_V2021_0&quot;, &quot;P_BACKUP_V2021_1&quot;, &quot;P_BACKUP_V2021_2&quot;.
The problem happens when I try to call the procedure. Only the first line of execution works and then an error is apparently returned when the cursor tries to continue its execution:
Executed as Single statement. Failed [3722 : HY000] SP_QRRNTABLES:Only a COMMIT WORK or null statement is legal after a DDL Statement.
Elapsed time = 00:00:05.668
 
STATEMENT 1: Unknown failed.

Maybe the problem is with some syntax details, but I can't see what might be wrong.
Can anyone guide me with this please?
",-1,-1,-1.0,"In the past I adapted a cursor I have in a stored procedure on SQL Server to Teradata but I'm having some troubles trying to use. My procedure code is:
CREATE PROCEDURE sp_QrRnTables()

BEGIN

DECLARE varTableOldName VARCHAR(500);
DECLARE varTableNewName VARCHAR(500);
DECLARE vardbName VARCHAR(100);
DECLARE varIDCod VARCHAR(5);
DECLARE varRename VARCHAR(100);
DECLARE varCt INT DEFAULT 0;
    
DECLARE renameTables CURSOR 
FOR
    
    SELECT
        nomeDB
        ,nomeTabela
    FROM (
        SELECT
            DatabaseName AS nomeDB
            ,TableName AS nomeTabela
            ,LastAccessTimeStamp AS dtUltimoAcesso
            ,(CURRENT_DATE - CAST(LastAccessTimeStamp AS DATE)) AS diasSemAcesso
        FROM
            DBC.TablesV
        WHERE
            1=1
            AND TableKind = 'T'
            AND DatabaseName IN ('P_BACKUP')
        GROUP BY
            DatabaseName
            ,TableName
            ,LastAccessTimeStamp
            ,LastAlterTimeStamp
    ) tbHig
    WHERE diasSemAcesso IS NULL OR diasSemAcesso &gt;= 180
    ORDER BY nomeDB, nomeTabela

FOR READ ONLY;

OPEN renameTables;

    FETCH NEXT FROM renameTables INTO vardbName, varTableOldName;

    WHILE (SQLCODE = 0)
    
        DO
        
        SET varTableNewName = vardbName || '.' || vardbName || '_V' || CAST(EXTRACT(YEAR FROM CURRENT_TIMESTAMP) AS VARCHAR(20)) || '_' || CAST(varCt AS VARCHAR(20));
    
        SET varTableOldName = vardbName || '.' || varTableOldName;
    
        SET varRename = 'RENAME TABLE ' || varTableOldName || ' TO ' || varTableNewName;
        
        EXECUTE IMMEDIATE varRename;
    
        SET varCt = varCt + 1;
    
        FETCH NEXT FROM renameTables INTO vardbName, varTableOldName;
        
    END WHILE;

CLOSE renameTables;

END;

The goal is is to collect the names of a series of tables and rename them using the cursor logic. Something like this: If I have three tables on my database &quot;myTableA&quot;, &quot;myTableB&quot;, &quot;myTableC&quot;, by the cursor logic I hope these are renamed to &quot;P_BACKUP_V2021_0&quot;, &quot;P_BACKUP_V2021_1&quot;, &quot;P_BACKUP_V2021_2&quot;.
The problem happens when I try to call the procedure. Only the first line of execution works and then an error is apparently returned when the cursor tries to continue its execution:
Executed as Single statement. Failed [3722 : HY000] SP_QRRNTABLES:Only a COMMIT WORK or null statement is legal after a DDL Statement.
Elapsed time = 00:00:05.668
 
STATEMENT 1: Unknown failed.

Maybe the problem is with some syntax details, but I can't see what might be wrong.
Can anyone guide me with this please?
",3
528,68134607,Error returned: 'OLE DB or ODBC error: [DataSource.Error] Teradata: [Teradata Database] [3119] Continue request submitted but no response to return,"Failed to save modifications to the server. Error returned: 'OLE DB or ODBC error: [DataSource.Error] Teradata: [Teradata Database] [3119] Continue request submitted but no response to return..
'.

When trying to connect a View to my power bi file, I get the above error when around 25M records are imported. I do not have any issue on smaller tables.

",-1,-1,-1.0,"Failed to save modifications to the server. Error returned: 'OLE DB or ODBC error: [DataSource.Error] Teradata: [Teradata Database] [3119] Continue request submitted but no response to return..
'.

When trying to connect a View to my power bi file, I get the above error when around 25M records are imported. I do not have any issue on smaller tables.

",1
529,68314841,Teradata - An illegally formed character string was encountered during translation,"I am fetching tweets via Twitter API in pandas dataframe and writing the data to teradata database. However, unlike other tweets one cell has specific tweet which contains data in bold. When I try to insert it in database, it pops up the following error:
OperationalError: [Version 17.0.0.4] [Session 3046127] [Teradata SQL Driver] [Error 528] A failure occurred while executing rows 1 through 292 of a batch request.
 at gosqldriver/teradatasql.(*teradataConnection).makeDriverErrorCode TeradataConnection.go:1120
 at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:396
 at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
 at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2083
 at database/sql.ctxDriverQuery ctxutil.go:48
 at database/sql.(*DB).queryDC.func1 sql.go:1579
 at database/sql.withLock sql.go:3204
 at database/sql.(*DB).queryDC sql.go:1574
 at database/sql.(*Conn).QueryContext sql.go:1823
 at main.goCreateRows goside.go:654
 at main._cgoexpwrap_cfa80c8a3acb_goCreateRows _cgo_gotypes.go:363
 at runtime.cgocallbackg1 cgocall.go:332
 at runtime.cgocallbackg cgocall.go:207
 at runtime.cgocallback_gofunc asm_amd64.s:793
 at runtime.goexit asm_amd64.s:1373
Caused by [Version 17.0.0.4] [Session 3046127] [Teradata Database] [Error 6705] An illegally formed character string was encountered during translation.
 at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError TeradataConnection.go:1138
 at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError TeradataConnection.go:1154

The tweets datatype in database is &quot;varchar(1000) CHARACTER SET UNICODE NOT CASESPECIFIC&quot;
Here is the sample data:

The tweet containing bold text is causing the problem in insertion. How do I mitigate this?
",-1,-1,-1.0,"I am fetching tweets via Twitter API in pandas dataframe and writing the data to teradata database. However, unlike other tweets one cell has specific tweet which contains data in bold. When I try to insert it in database, it pops up the following error:
OperationalError: [Version 17.0.0.4] [Session 3046127] [Teradata SQL Driver] [Error 528] A failure occurred while executing rows 1 through 292 of a batch request.
 at gosqldriver/teradatasql.(*teradataConnection).makeDriverErrorCode TeradataConnection.go:1120
 at gosqldriver/teradatasql.newTeradataRows TeradataRows.go:396
 at gosqldriver/teradatasql.(*teradataStatement).QueryContext TeradataStatement.go:122
 at gosqldriver/teradatasql.(*teradataConnection).QueryContext TeradataConnection.go:2083
 at database/sql.ctxDriverQuery ctxutil.go:48
 at database/sql.(*DB).queryDC.func1 sql.go:1579
 at database/sql.withLock sql.go:3204
 at database/sql.(*DB).queryDC sql.go:1574
 at database/sql.(*Conn).QueryContext sql.go:1823
 at main.goCreateRows goside.go:654
 at main._cgoexpwrap_cfa80c8a3acb_goCreateRows _cgo_gotypes.go:363
 at runtime.cgocallbackg1 cgocall.go:332
 at runtime.cgocallbackg cgocall.go:207
 at runtime.cgocallback_gofunc asm_amd64.s:793
 at runtime.goexit asm_amd64.s:1373
Caused by [Version 17.0.0.4] [Session 3046127] [Teradata Database] [Error 6705] An illegally formed character string was encountered during translation.
 at gosqldriver/teradatasql.(*teradataConnection).formatDatabaseError TeradataConnection.go:1138
 at gosqldriver/teradatasql.(*teradataConnection).makeChainedDatabaseError TeradataConnection.go:1154

The tweets datatype in database is &quot;varchar(1000) CHARACTER SET UNICODE NOT CASESPECIFIC&quot;
Here is the sample data:

The tweet containing bold text is causing the problem in insertion. How do I mitigate this?
",1
530,68387360,PySpark JDBC Teradata Connection,"I submit spark jobs with spark-submit and specify the latest verison of Teradata JDBC Driver (17.10.00.14) to load via the --jars flag. The logs confirm that the JAR file is added successfully.
However, when making a connection to Teradata with
(
  spark.read.format(&quot;jdbc&quot;).
    option(&quot;url&quot;, url).
    option(&quot;user&quot;, user).
    option(&quot;password&quot;, password).
    option(&quot;dbtable&quot;, table)
)

get the error: Invalid connection parameter name dbtable.
This java.sql.SQLException is thrown by the com.teradata.jdbc.jdbc_4 library.
Logically, if I remove dbtable from the options
(
  spark.read.format(&quot;jdbc&quot;).
    option(&quot;url&quot;, url).
    option(&quot;user&quot;, user).
    option(&quot;password&quot;, password).
    option(&quot;dbtable&quot;, table)
)

I get another error: Option 'dbtable' or 'query' is required.
This IllegalArgumentException is thrown by pyspark.
My understanding is that there is an incompatibility between Spark 3.1.1 and Teradata JDBC Driver 17.10.00.14.
",-1,-1,-1.0,"I submit spark jobs with spark-submit and specify the latest verison of Teradata JDBC Driver (17.10.00.14) to load via the --jars flag. The logs confirm that the JAR file is added successfully.
However, when making a connection to Teradata with
(
  spark.read.format(&quot;jdbc&quot;).
    option(&quot;url&quot;, url).
    option(&quot;user&quot;, user).
    option(&quot;password&quot;, password).
    option(&quot;dbtable&quot;, table)
)

get the error: Invalid connection parameter name dbtable.
This java.sql.SQLException is thrown by the com.teradata.jdbc.jdbc_4 library.
Logically, if I remove dbtable from the options
(
  spark.read.format(&quot;jdbc&quot;).
    option(&quot;url&quot;, url).
    option(&quot;user&quot;, user).
    option(&quot;password&quot;, password).
    option(&quot;dbtable&quot;, table)
)

I get another error: Option 'dbtable' or 'query' is required.
This IllegalArgumentException is thrown by pyspark.
My understanding is that there is an incompatibility between Spark 3.1.1 and Teradata JDBC Driver 17.10.00.14.
",0
531,68592720,"Teradata Client 3.1, EF Core 3.1, Connection / Session Lives On","I'm having trouble finding specific documentation around manually ending the session via .exit or .quit or if its even possible with FromSqlRaw.
Problem Statement:
On connect and execution of a stored procedure a Teradata session is created. After destruction of the db context the session appears to live on for some unknown amount of time. Restarting the service clears it immediately.
Attempts to manually end the session via .exit fail.
Examples:
With dependency injection:
services.AddDbContext&lt;TeradataContext&gt;(options =&gt; 
    options.UseTeradata(Configuration.GetConnectionString(&quot;EDW&quot;), opts =&gt; {
        opts.CommandTimeout(120);
    })
 );

It's my understanding connection pooling would not happen as it is lifecycle and not added as a pool. But since garbage collection could take some time I decided to try moving it as a test:
 using (var dbContext = new TeradataContext())
 {         
     int result = dbContext.Database.ExecuteSqlRaw(&quot;CALL .... ({0});&quot;, bar);
     dbContext.Database.CloseConnection();
 }

The session on Teradata still persists and the next call fails as the stored procedure fails to create the already existing volatile table.
I tried adding:
dbContext.Database.ExecuteSqlRaw(&quot;.exit&quot;);

I get back:

[Teradata Database] [3706] Syntax error: expected something between ';' and '.'.

Does anyone know the correct way to call .exit here? Or any other way to force the close and end of session using Teradata Client 3.1,   EF Core 3.1
dbContext.Database.CloseConnection();

But the session still persists so I don't believe their provider is performing a quit or exit.
I am not every experienced with TeraData...
I was really hoping to use EF but I guess I could fall back to other options if we can't figure this out.
Thanks for any insights.
",-1,-1,-1.0,"I'm having trouble finding specific documentation around manually ending the session via .exit or .quit or if its even possible with FromSqlRaw.
Problem Statement:
On connect and execution of a stored procedure a Teradata session is created. After destruction of the db context the session appears to live on for some unknown amount of time. Restarting the service clears it immediately.
Attempts to manually end the session via .exit fail.
Examples:
With dependency injection:
services.AddDbContext&lt;TeradataContext&gt;(options =&gt; 
    options.UseTeradata(Configuration.GetConnectionString(&quot;EDW&quot;), opts =&gt; {
        opts.CommandTimeout(120);
    })
 );

It's my understanding connection pooling would not happen as it is lifecycle and not added as a pool. But since garbage collection could take some time I decided to try moving it as a test:
 using (var dbContext = new TeradataContext())
 {         
     int result = dbContext.Database.ExecuteSqlRaw(&quot;CALL .... ({0});&quot;, bar);
     dbContext.Database.CloseConnection();
 }

The session on Teradata still persists and the next call fails as the stored procedure fails to create the already existing volatile table.
I tried adding:
dbContext.Database.ExecuteSqlRaw(&quot;.exit&quot;);

I get back:

[Teradata Database] [3706] Syntax error: expected something between ';' and '.'.

Does anyone know the correct way to call .exit here? Or any other way to force the close and end of session using Teradata Client 3.1,   EF Core 3.1
dbContext.Database.CloseConnection();

But the session still persists so I don't believe their provider is performing a quit or exit.
I am not every experienced with TeraData...
I was really hoping to use EF but I guess I could fall back to other options if we can't figure this out.
Thanks for any insights.
",3
532,68628242,Increasing performance in connection to Teradata,"I need to connect to Teradata using Python. The problem I am facing is the long time to connect to database.
I am trying with teradata, teradatasql and pyodbc packages in Python. The code I am using is something like:
import teradata
import teradatasql
import pyodbc
import time

udaExec = teradata.UdaExec (appName=&quot;HelloWorld&quot;, version=&quot;1.0&quot;, logConsole=False)

try:
  host, username, password = 'teradata', 'xxxxx', 'xxxxx'
  tic = time.perf_counter()
  session = teradatasql.connect(host=host, user=username, password=password, logmech=&quot;LDAP&quot;)
  toc = time.perf_counter()
  print(f&quot;TERADATASQL:  {toc - tic:0.4f} seconds&quot;)

  tic = time.perf_counter()
  udaExec.connect(method=&quot;odbc&quot;, dsn=&quot;TD&quot;)
  toc = time.perf_counter()
  print(f&quot;TERADATA:  {toc - tic:0.4f} seconds&quot;)

  tic = time.perf_counter()
  connection = pyodbc.connect('DSN=TD')
  toc = time.perf_counter()
  print(f&quot;PYODBC:  {toc - tic:0.4f} seconds&quot;)

except Exception as e:
  print(e)

The usual result is something around the following values:

TERADATASQL:  6.2150 seconds


TERADATA:  2.8512 seconds


PYODBC:  2.6051 seconds

The problem is that most part of time I´ll make very simple queries that costs much less than 1 second.
I developed (by myself) a scheme using a pool of 5 pre opened connections. But I think this is very rudimentar, since it is a django serving a REST API service with multiple users. Are there any way to increase the performance to open connections? Any other solutions?
",1,-1,-1.0,"I need to connect to Teradata using Python. The problem I am facing is the long time to connect to database.
I am trying with teradata, teradatasql and pyodbc packages in Python. The code I am using is something like:
import teradata
import teradatasql
import pyodbc
import time

udaExec = teradata.UdaExec (appName=&quot;HelloWorld&quot;, version=&quot;1.0&quot;, logConsole=False)

try:
  host, username, password = 'teradata', 'xxxxx', 'xxxxx'
  tic = time.perf_counter()
  session = teradatasql.connect(host=host, user=username, password=password, logmech=&quot;LDAP&quot;)
  toc = time.perf_counter()
  print(f&quot;TERADATASQL:  {toc - tic:0.4f} seconds&quot;)

  tic = time.perf_counter()
  udaExec.connect(method=&quot;odbc&quot;, dsn=&quot;TD&quot;)
  toc = time.perf_counter()
  print(f&quot;TERADATA:  {toc - tic:0.4f} seconds&quot;)

  tic = time.perf_counter()
  connection = pyodbc.connect('DSN=TD')
  toc = time.perf_counter()
  print(f&quot;PYODBC:  {toc - tic:0.4f} seconds&quot;)

except Exception as e:
  print(e)

The usual result is something around the following values:

TERADATASQL:  6.2150 seconds


TERADATA:  2.8512 seconds


PYODBC:  2.6051 seconds

The problem is that most part of time I´ll make very simple queries that costs much less than 1 second.
I developed (by myself) a scheme using a pool of 5 pre opened connections. But I think this is very rudimentar, since it is a django serving a REST API service with multiple users. Are there any way to increase the performance to open connections? Any other solutions?
",1
533,68628470,How can we use dynamic SQL to pull data from Teradata using Python to execute the query?,"I have a query that looks like this.
query_exp = &quot;&quot;&quot;SELECT *
               FROM MASTER_TBLS.EXP 
               WHERE PROJECT_TYPE_ID IN ('109926',
                                        '103471',
                                        '120135')&quot;&quot;&quot;

In reality, I have a little over 18k numbers in PROJECT_TYPE_CD, so it's a bit unwieldly to use. In SQL Server I could create a dynamic SQL statement and pass that to the DB engine to execute, bu t in the Teradata world, I don't know how to do this. Also, this is running in a Jupyter Notebook, so somehow, I need to get Python to do the call to Teradata.
In Python, the first thing I do is get all unique IDs, like this.
unique_ID = df_exp.PROJECT_TYPE_ID.unique()
unique_ID = pd.DataFrame(unique_ID)

I think I'm ok with the Python stuff, but I really don't know how to create the dynamic SQL in Teradata.
",-1,-1,-1.0,"I have a query that looks like this.
query_exp = &quot;&quot;&quot;SELECT *
               FROM MASTER_TBLS.EXP 
               WHERE PROJECT_TYPE_ID IN ('109926',
                                        '103471',
                                        '120135')&quot;&quot;&quot;

In reality, I have a little over 18k numbers in PROJECT_TYPE_CD, so it's a bit unwieldly to use. In SQL Server I could create a dynamic SQL statement and pass that to the DB engine to execute, bu t in the Teradata world, I don't know how to do this. Also, this is running in a Jupyter Notebook, so somehow, I need to get Python to do the call to Teradata.
In Python, the first thing I do is get all unique IDs, like this.
unique_ID = df_exp.PROJECT_TYPE_ID.unique()
unique_ID = pd.DataFrame(unique_ID)

I think I'm ok with the Python stuff, but I really don't know how to create the dynamic SQL in Teradata.
",3
534,68723050,Teradata performances issues with left join big table,"I have a sql performance issue when joining two tables on Teradata sql assistant, one of them ( table B) contain more than 3 billions rows so the join take more than 2 hours.
table A contain this columns
name|id_number|id_product|creation_date|cp_date|amount|rang

Table B Contain this columns
name|id_number|id_product_cp|creation_date|cp_date|amount|year_|month_

So i'm trying to get the amounts of each name/id_number/id_product
--&gt; if the amount in table A = 0 then we get the amount of table B if it's not null
else we take the a amount.
My query is
select
    a.name,
    a.id_number,
    a.id_product,
    a.creation_date,
    case
        when
            sum(a.amount) = 0 and sum(net.amount) is not null then
                sum(net.amount)
            else
                sum(a.amount)
        end
    as amount
from 
    A a    
        left join (
            select
                a.name,
                a.id_number,
                a.cp_date(date) as cp_date,
                a.year_,
                a.month_,
                cp.id_product,
                sum(a.amount) as amount
            from
                B a 
                    join C cp
                    on cp.id_product_cp = a.id_product_cp
            group by 1,2,3,4,5,6
        ) net
        on
            a.name= net.name
            and a.id_number= net.id_number
            and a.id_product = net.id_product 
            and a.cp_date= net.cp_date
            and (
                        extract(year from a.cp_date) &lt; net.year_
                    or (
                                extract(year from a.cp_date) = net.year_
                            and net.month_ &gt;= extract(month from a.cp_date)
                        )
                ) 
    where a.rang &lt;&gt; 1
    group by 1,2,3,4

the below picture  is the results from the table dbc.QryLogStepsV for the query

I think that the subquery in the left join is the cause of the performance issue.
There is any way to perform this query please !
Thank you
",-1,-1,-1.0,"I have a sql performance issue when joining two tables on Teradata sql assistant, one of them ( table B) contain more than 3 billions rows so the join take more than 2 hours.
table A contain this columns
name|id_number|id_product|creation_date|cp_date|amount|rang

Table B Contain this columns
name|id_number|id_product_cp|creation_date|cp_date|amount|year_|month_

So i'm trying to get the amounts of each name/id_number/id_product
--&gt; if the amount in table A = 0 then we get the amount of table B if it's not null
else we take the a amount.
My query is
select
    a.name,
    a.id_number,
    a.id_product,
    a.creation_date,
    case
        when
            sum(a.amount) = 0 and sum(net.amount) is not null then
                sum(net.amount)
            else
                sum(a.amount)
        end
    as amount
from 
    A a    
        left join (
            select
                a.name,
                a.id_number,
                a.cp_date(date) as cp_date,
                a.year_,
                a.month_,
                cp.id_product,
                sum(a.amount) as amount
            from
                B a 
                    join C cp
                    on cp.id_product_cp = a.id_product_cp
            group by 1,2,3,4,5,6
        ) net
        on
            a.name= net.name
            and a.id_number= net.id_number
            and a.id_product = net.id_product 
            and a.cp_date= net.cp_date
            and (
                        extract(year from a.cp_date) &lt; net.year_
                    or (
                                extract(year from a.cp_date) = net.year_
                            and net.month_ &gt;= extract(month from a.cp_date)
                        )
                ) 
    where a.rang &lt;&gt; 1
    group by 1,2,3,4

the below picture  is the results from the table dbc.QryLogStepsV for the query

I think that the subquery in the left join is the cause of the performance issue.
There is any way to perform this query please !
Thank you
",3
535,68732987,Testing the max() functions in SQL (Teradata) without having to query a database,"This is probably a simple one, but I'm not able to find it anywhere. I just want to check what the MAX() function in a Teradata SQL query would return, using a list of strings. I have a Teradata database I'm working with, but I don't have a table to query (I need to know the output of MAX() before I can populate the table data). I'm using Aqua Data Studio.
I tried
select max('test1','untest2','_test3')

and I also tried
select max(field_name) where field_name in ('test1','untest2','_test3')

but both throw an error.
What am I missing?
",-1,-1,-1.0,"This is probably a simple one, but I'm not able to find it anywhere. I just want to check what the MAX() function in a Teradata SQL query would return, using a list of strings. I have a Teradata database I'm working with, but I don't have a table to query (I need to know the output of MAX() before I can populate the table data). I'm using Aqua Data Studio.
I tried
select max('test1','untest2','_test3')

and I also tried
select max(field_name) where field_name in ('test1','untest2','_test3')

but both throw an error.
What am I missing?
",3
536,68821535,Optimizing pandas.read_sql for teradata,"How can one read a teradata sql into a tempfile? The goal is to improve performance when ingesting data from an sql query into a pandas df.
On https://towardsdatascience.com/optimizing-pandas-read-sql-for-postgres-f31cd7f707ab Tristan Crockett shows how this is done for postgres.
def read_sql_tmpfile(query, db_engine):
    with tempfile.TemporaryFile() as tmpfile:
        copy_sql = &quot;COPY ({query}) TO STDOUT WITH CSV {head}&quot;.format(
           query=query, head=&quot;HEADER&quot;
        )
        conn = db_engine.raw_connection()
        cur = conn.cursor()
        cur.copy_expert(copy_sql, tmpfile)
        tmpfile.seek(0)
        df = pandas.read_csv(tmpfile)
        return df

I couldn't figure out how to rewrite this code to make it work with a teradata server.
",-1,-1,-1.0,"How can one read a teradata sql into a tempfile? The goal is to improve performance when ingesting data from an sql query into a pandas df.
On https://towardsdatascience.com/optimizing-pandas-read-sql-for-postgres-f31cd7f707ab Tristan Crockett shows how this is done for postgres.
def read_sql_tmpfile(query, db_engine):
    with tempfile.TemporaryFile() as tmpfile:
        copy_sql = &quot;COPY ({query}) TO STDOUT WITH CSV {head}&quot;.format(
           query=query, head=&quot;HEADER&quot;
        )
        conn = db_engine.raw_connection()
        cur = conn.cursor()
        cur.copy_expert(copy_sql, tmpfile)
        tmpfile.seek(0)
        df = pandas.read_csv(tmpfile)
        return df

I couldn't figure out how to rewrite this code to make it work with a teradata server.
",1
537,68850550,Teradata ALTER TABLE to CURRENT taking too long,"I am running a procedure in Teradata which has the statement &quot;ALTER TABLE TABLE_NAME to CURRENT&quot;
It is taking too long to run, I have also collected stats on the given table.
Table size is 130 GB.
Can someone help what can be the issue?
",1,-1,-1.0,"I am running a procedure in Teradata which has the statement &quot;ALTER TABLE TABLE_NAME to CURRENT&quot;
It is taking too long to run, I have also collected stats on the given table.
Table size is 130 GB.
Can someone help what can be the issue?
",3
538,68890346,Export column value from teradata as separate .txt files,"im trying to write a code to export data present in column &quot;task detail&quot;, and its export file name should be the same rows &quot; task name &quot; value
eg:
Task Detail      task name
task detail1     task name1
task detail2     task name2
the task details 1 ans 2 should be exported into separate txt files with task name 1 and 2 as the file names respectively
I tried exporting the task name values into a separate txt file first called input_filename.txt , then loop through it line by line, assign the line value to a variable and if the variable and task name value matches, export  the task details with variable name
code:
.LOGON 


while read -r line ; do

File_name=$(echo $line);


.Export data file=${PATH1}/${File_name}.txt, close


SELECT  Tasks.Task_SQL
FROM {DBAAAS}.Tasks AS Tasks
WHERE Tasks.Task_Name=${File_name};
.EXPORT RESET

done &lt;${PATHH}/input_filename.txt

.LOGOFF;
.QUIT;

Im facing 2 errors,

expect something between start of request and while enter code hereKeyword
2)Failure 3704 '{' ('7B'X) is not a valid Teradata SQL token.

Can someone please help me with what im doing wrong?
",-1,-1,-1.0,"im trying to write a code to export data present in column &quot;task detail&quot;, and its export file name should be the same rows &quot; task name &quot; value
eg:
Task Detail      task name
task detail1     task name1
task detail2     task name2
the task details 1 ans 2 should be exported into separate txt files with task name 1 and 2 as the file names respectively
I tried exporting the task name values into a separate txt file first called input_filename.txt , then loop through it line by line, assign the line value to a variable and if the variable and task name value matches, export  the task details with variable name
code:
.LOGON 


while read -r line ; do

File_name=$(echo $line);


.Export data file=${PATH1}/${File_name}.txt, close


SELECT  Tasks.Task_SQL
FROM {DBAAAS}.Tasks AS Tasks
WHERE Tasks.Task_Name=${File_name};
.EXPORT RESET

done &lt;${PATHH}/input_filename.txt

.LOGOFF;
.QUIT;

Im facing 2 errors,

expect something between start of request and while enter code hereKeyword
2)Failure 3704 '{' ('7B'X) is not a valid Teradata SQL token.

Can someone please help me with what im doing wrong?
",3
539,69199570,Load Teradata table from Python Pandas Dataframe,"I am getting below error while trying to load Teradata table from Python Pandas,  Any idea ?
teradatasql and pandas - writing dataframe into TD table - Error 3707 - Syntax error, expected something like '(' between the 'type' keyword and '='
import teradatasql
import pandas as pd
 
 conTD = teradatasql.connect(host=Host, user=User, password=Passwd, logmech=&quot;LDAP&quot;, encryptdata=&quot;true&quot;)
 df.to_sql(tableName, conTD, schema=schemaName, if_exists='fail', index=False)

",-1,-1,-1.0,"I am getting below error while trying to load Teradata table from Python Pandas,  Any idea ?
teradatasql and pandas - writing dataframe into TD table - Error 3707 - Syntax error, expected something like '(' between the 'type' keyword and '='
import teradatasql
import pandas as pd
 
 conTD = teradatasql.connect(host=Host, user=User, password=Passwd, logmech=&quot;LDAP&quot;, encryptdata=&quot;true&quot;)
 df.to_sql(tableName, conTD, schema=schemaName, if_exists='fail', index=False)

",1
540,69369724,want to insert statement in a loop in teradata,"I'm inserting data into table from another table using below query in Teradata and I want to run this statement until table reaches 20GB. So I want to run below statement in a loop to achieve that. However I written one but it's giving query invalid error when I'm trying to execute. Could you please help me as I'm new to Teradata. Thanks.
insert into schema1.xyx select * from schema2.abc;

",-1,-1,-1.0,"I'm inserting data into table from another table using below query in Teradata and I want to run this statement until table reaches 20GB. So I want to run below statement in a loop to achieve that. However I written one but it's giving query invalid error when I'm trying to execute. Could you please help me as I'm new to Teradata. Thanks.
insert into schema1.xyx select * from schema2.abc;

",3
541,69542686,select rows from teradata database using python based on another column in df,"I am trying to limit my query to a table in teradata database using python
#1.params=dfm['cust'].astype(str).to_list()
params=tuple(list(dfm['cust'].astype(str)))
import teradata
import pandas as pd


#Make a connection
udaExec = teradata.UdaExec (appName=&quot;test&quot;, version=&quot;1.0&quot;, logConsole=False)


with udaExec.connect(method=&quot;ODBC&quot;,system=host, username=uid,
                             password=pwd, driver=&quot;Teradata Database ODBC Driver 16.20&quot;,authentication=&quot;LDAP&quot;) as connect:

    
    query = &quot;SELECT Cust, Flag FROM DBName.Tablename where Cust in %(params)s&quot;

    #Reading query to df
    df = pd.read_sql(query,connect,params=params)


I tried to supply params as a list and it did not work. I tried to supply params as a tuple and it still did not work. I got the following error.
Execution failed on sql 'SELECT Cust,Flag FROM DBName.TableName where Cust in %(params)s': (3704, &quot;[42000] [Teradata][ODBC Teradata Driver][Teradata Database](-3704)'%' ('25'X) is not a valid Teradata SQL token.&quot;)

What is wrong with the code?
",-1,-1,-1.0,"I am trying to limit my query to a table in teradata database using python
#1.params=dfm['cust'].astype(str).to_list()
params=tuple(list(dfm['cust'].astype(str)))
import teradata
import pandas as pd


#Make a connection
udaExec = teradata.UdaExec (appName=&quot;test&quot;, version=&quot;1.0&quot;, logConsole=False)


with udaExec.connect(method=&quot;ODBC&quot;,system=host, username=uid,
                             password=pwd, driver=&quot;Teradata Database ODBC Driver 16.20&quot;,authentication=&quot;LDAP&quot;) as connect:

    
    query = &quot;SELECT Cust, Flag FROM DBName.Tablename where Cust in %(params)s&quot;

    #Reading query to df
    df = pd.read_sql(query,connect,params=params)


I tried to supply params as a list and it did not work. I tried to supply params as a tuple and it still did not work. I got the following error.
Execution failed on sql 'SELECT Cust,Flag FROM DBName.TableName where Cust in %(params)s': (3704, &quot;[42000] [Teradata][ODBC Teradata Driver][Teradata Database](-3704)'%' ('25'X) is not a valid Teradata SQL token.&quot;)

What is wrong with the code?
",1
542,69305780,Teradata RegExp_Split_To_Table Error on TD,"Not the nest at coding
with t as
 (
   select MHKAPPEALSINTERNALID as id, 
      Cast(Note as VARCHAR(100)) as Note, 
      UPDATEDAT,
      Cast(MHK_TYPE  as VARCHAR(100)) as MHK_TYPE
   from vcoreMEDHOK_MHK_Notes
   Where cast(UPDATEDAT as Date) &gt;= '2021/08/15'
 ) --- pull table

select MHKAPPEALSINTERNALID, MHK_Type, UpdateAt, Note, 
   regexp_substr(Token, '.*&lt;b&gt;*\K.*'),
   dt.*-- trim everything up to '&lt;b&gt;' 
FROM TABLE
 ( RegExp_Split_To_Table(t.id, t.Note,'&lt;b&gt;|&lt;\/b&gt;|&lt;BR&gt;|&lt;/BR&gt;*', 'i')     -- split whenever '&lt;\/b&gt;' occurs
   RETURNS ( MHKAPPEALSINTERNALID BIGINT,
             TokenNum INT,
             Token VARCHAR(100) CHARACTER SET Unicode,
             MHK_TYPE VARCHAR(100) CHARACTER SET Unicode, 
             UPDATEDAT Date)
           ) AS dt

Teradata exception: [Teradata Database] [9881]
Function 'RegExp_Split_To_Table' called with an invalid number or type of parameters
Trying to figure out why i am getting this message as im going between RegExp and RegSplit
Field has mostly alot of data in it most split with ~ etc. but try to pull out sections of it is always a challenge especially when end user have free reign to do whatever.
",-1,-1,-1.0,"Not the nest at coding
with t as
 (
   select MHKAPPEALSINTERNALID as id, 
      Cast(Note as VARCHAR(100)) as Note, 
      UPDATEDAT,
      Cast(MHK_TYPE  as VARCHAR(100)) as MHK_TYPE
   from vcoreMEDHOK_MHK_Notes
   Where cast(UPDATEDAT as Date) &gt;= '2021/08/15'
 ) --- pull table

select MHKAPPEALSINTERNALID, MHK_Type, UpdateAt, Note, 
   regexp_substr(Token, '.*&lt;b&gt;*\K.*'),
   dt.*-- trim everything up to '&lt;b&gt;' 
FROM TABLE
 ( RegExp_Split_To_Table(t.id, t.Note,'&lt;b&gt;|&lt;\/b&gt;|&lt;BR&gt;|&lt;/BR&gt;*', 'i')     -- split whenever '&lt;\/b&gt;' occurs
   RETURNS ( MHKAPPEALSINTERNALID BIGINT,
             TokenNum INT,
             Token VARCHAR(100) CHARACTER SET Unicode,
             MHK_TYPE VARCHAR(100) CHARACTER SET Unicode, 
             UPDATEDAT Date)
           ) AS dt

Teradata exception: [Teradata Database] [9881]
Function 'RegExp_Split_To_Table' called with an invalid number or type of parameters
Trying to figure out why i am getting this message as im going between RegExp and RegSplit
Field has mostly alot of data in it most split with ~ etc. but try to pull out sections of it is always a challenge especially when end user have free reign to do whatever.
",3
543,69268836,Teradata Parallel Transporter DDL Operator - missing { EXTENDED_LITERAL_ CHAR_STRING_LITERAL_ } in Rule: Character String Literal ERROR,"What I want to do is check in my database if my table exists, if yes drop it. Here is my .tpt :
DEFINE JOB DELETE_ET_TABLES
DESCRIPTION 'Delete ET tables'
(
        DEFINE OPERATOR DDL_OPERATOR
          DESCRIPTION 'Teradata Parallel Transporter DDL Operator'
          TYPE DDL
          ATTRIBUTES
          (
                varchar TdpId = @TERADATA_TDP,
                varchar UserName = @User,
                varchar UserPassword = @Pwd
          );

        APPLY  
                'SELECT (CASE WHEN TableName = ''Test_Del'' 
                              THEN (''DROP TABLE @Table;'')
                              ELSE NULL
                         END)
                 FROM dbc.TablesV WHERE databasename = @Db;'  TO OPERATOR(DDL_OPERATOR);

And this is the error message I am getting :
Running &quot;tbuild&quot; command: tbuild -f /$HOME/loaders/test_deleteETTables.tpt -u TERADATA_TDP=$TDP, TERADATA_DATABASE=$DB -L /$LOG/
Teradata Parallel Transporter Version 16.20.00.09 64-Bit
TPT_INFRA: Syntax error at or near line 18 of Job Script File '/$HOME/loaders/test_deleteETTables.tpt':
TPT_INFRA: At &quot;(&quot; missing { EXTENDED_LITERAL_ CHAR_STRING_LITERAL_ } in Rule: Character String Literal
Compilation failed due to errors. Execution Plan was not generated.

Do you have any idea ? I have tried multiple things, such as :
SELECT 1 FROM dbc.TablesV WHERE databasename = @Db AND TABLENAME ='TEST_DEL';
CASE WHEN ACTIVITYCOUNT = 1
    THEN (DROP TABLE @Table)
    ELSE ( QUIT )
END;

All my variables have been declared. I feel that it is a problem with using single quotes inside que statement but I am not sure and I don't know how to resolve it. Thank you very much for your time.
",-1,-1,-1.0,"What I want to do is check in my database if my table exists, if yes drop it. Here is my .tpt :
DEFINE JOB DELETE_ET_TABLES
DESCRIPTION 'Delete ET tables'
(
        DEFINE OPERATOR DDL_OPERATOR
          DESCRIPTION 'Teradata Parallel Transporter DDL Operator'
          TYPE DDL
          ATTRIBUTES
          (
                varchar TdpId = @TERADATA_TDP,
                varchar UserName = @User,
                varchar UserPassword = @Pwd
          );

        APPLY  
                'SELECT (CASE WHEN TableName = ''Test_Del'' 
                              THEN (''DROP TABLE @Table;'')
                              ELSE NULL
                         END)
                 FROM dbc.TablesV WHERE databasename = @Db;'  TO OPERATOR(DDL_OPERATOR);

And this is the error message I am getting :
Running &quot;tbuild&quot; command: tbuild -f /$HOME/loaders/test_deleteETTables.tpt -u TERADATA_TDP=$TDP, TERADATA_DATABASE=$DB -L /$LOG/
Teradata Parallel Transporter Version 16.20.00.09 64-Bit
TPT_INFRA: Syntax error at or near line 18 of Job Script File '/$HOME/loaders/test_deleteETTables.tpt':
TPT_INFRA: At &quot;(&quot; missing { EXTENDED_LITERAL_ CHAR_STRING_LITERAL_ } in Rule: Character String Literal
Compilation failed due to errors. Execution Plan was not generated.

Do you have any idea ? I have tried multiple things, such as :
SELECT 1 FROM dbc.TablesV WHERE databasename = @Db AND TABLENAME ='TEST_DEL';
CASE WHEN ACTIVITYCOUNT = 1
    THEN (DROP TABLE @Table)
    ELSE ( QUIT )
END;

All my variables have been declared. I feel that it is a problem with using single quotes inside que statement but I am not sure and I don't know how to resolve it. Thank you very much for your time.
",3
544,69233340,VBA call a SQL query from Teradata encounter 3535 error message while this SQL statement runs well in Teradata,"The VB for application version is 7.1; Teradata version 16.20.53.27.
The SQL statement is very long, the length of it is 59972.I run it into Teradata directly, it works well.
I use the following to execute this SQL in VBA:
set connection = CreateOjbect(&quot;ADODB.connection&quot;)
connection.open &quot;DSN=xxx UID=XXX PWD=XXX&quot;
Set ObjRecordSet=CreateObject(&quot;ADO.RecordSet&quot;)
ObjRecordSet.open StrQuery connection
ObjRecordSet.close
connection.close

the error message is
Run-time error '-2147217833 (80040e57)
[Teradata] [ODBC Teradata Driver]Teradata Database
A character string failed conversion to a numeric value
I check the long SQL statement, could not find the convert function for numeric value, only converting string into varchar(80). Thanks!
",-1,1,-1.0,"The VB for application version is 7.1; Teradata version 16.20.53.27.
The SQL statement is very long, the length of it is 59972.I run it into Teradata directly, it works well.
I use the following to execute this SQL in VBA:
set connection = CreateOjbect(&quot;ADODB.connection&quot;)
connection.open &quot;DSN=xxx UID=XXX PWD=XXX&quot;
Set ObjRecordSet=CreateObject(&quot;ADO.RecordSet&quot;)
ObjRecordSet.open StrQuery connection
ObjRecordSet.close
connection.close

the error message is
Run-time error '-2147217833 (80040e57)
[Teradata] [ODBC Teradata Driver]Teradata Database
A character string failed conversion to a numeric value
I check the long SQL statement, could not find the convert function for numeric value, only converting string into varchar(80). Thanks!
",3
545,69812072,Issue when reading Teradata table via Apache Spark,"I'm reading a Teradata table using Spark. Here is my code:
spark.read.format(&quot;jdbc&quot;)
      .option(&quot;url&quot;, &quot;jdbc:teradata://127.0.0.1/database=test, TMODE=TERA&quot;)
      .option(&quot;username&quot;, &quot;test&quot;)
      .option(&quot;password&quot;, &quot;test&quot;)
      .option(&quot;dbtable&quot;, &quot;(SELECT TOP 5 * FROM test.users) AS tmp&quot;)
      .option(&quot;driver&quot;, &quot;com.teradata.jdbc.TeraDriver&quot;)      
      .load().show()

And I get this error:
Caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.08] [Error 5628] [SQLState HY000] Column Login Date not found in tmp.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:309)
    at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:103)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:311)

However, what I can see from Teradata table is that there are all needed columns:
spark.read.format(&quot;jdbc&quot;)
      .option(&quot;url&quot;, &quot;jdbc:teradata://127.0.0.1/database=test, TMODE=TERA&quot;)
      .option(&quot;username&quot;, &quot;test&quot;)
      .option(&quot;password&quot;, &quot;test&quot;)
      .option(&quot;dbtable&quot;, &quot;(SELECT * FROM dbc.columnsV WHERE databasename = 'test' AND tablename = 'users') tmp&quot;)
      .option(&quot;driver&quot;, &quot;com.teradata.jdbc.TeraDriver&quot;)
      .load().show()

Output:
+-------------------+-------------------------+----------------------+------------+---------------------------------+---------------+----------+-------------+------------+------------+--------+-------------+------------------+...
|DatabaseName       |TableName                |ColumnName            |ColumnFormat|ColumnTitle                      |SPParameterType|ColumnType|ColumnUDTName|ColumnLength|DefaultValue|Nullable|CommentString|DecimalTotalDigits|...
+-------------------+-------------------------+----------------------+------------+---------------------------------+---------------+----------+-------------+------------+------------+--------+-------------+------------------+...
|test               |users                    |active_flag           |-(18)9      |Is User Active                   |               |D         |null         |8           |null        |Y       |null         |18                |...
|test               |users                    |user_id               |-(18)9      |User Identifier                  |               |D         |null         |8           |null        |Y       |null         |18                |....
|test               |users                    |login_date            |yyyy-mm-dd  |Login Date                       |               |DA        |null         |4           |null        |Y       |null         |null              |....
+-------------------+-------------------------+----------------------+------------+---------------------------------+---------------+----------+-------------+------------+------------+--------+-------------+------------------+....

Any ideas why Spark can't read the table and why it can't get columns?
",-1,-1,-1.0,"I'm reading a Teradata table using Spark. Here is my code:
spark.read.format(&quot;jdbc&quot;)
      .option(&quot;url&quot;, &quot;jdbc:teradata://127.0.0.1/database=test, TMODE=TERA&quot;)
      .option(&quot;username&quot;, &quot;test&quot;)
      .option(&quot;password&quot;, &quot;test&quot;)
      .option(&quot;dbtable&quot;, &quot;(SELECT TOP 5 * FROM test.users) AS tmp&quot;)
      .option(&quot;driver&quot;, &quot;com.teradata.jdbc.TeraDriver&quot;)      
      .load().show()

And I get this error:
Caused by: java.sql.SQLException: [Teradata Database] [TeraJDBC 16.20.00.08] [Error 5628] [SQLState HY000] Column Login Date not found in tmp.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:309)
    at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:103)
    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:311)

However, what I can see from Teradata table is that there are all needed columns:
spark.read.format(&quot;jdbc&quot;)
      .option(&quot;url&quot;, &quot;jdbc:teradata://127.0.0.1/database=test, TMODE=TERA&quot;)
      .option(&quot;username&quot;, &quot;test&quot;)
      .option(&quot;password&quot;, &quot;test&quot;)
      .option(&quot;dbtable&quot;, &quot;(SELECT * FROM dbc.columnsV WHERE databasename = 'test' AND tablename = 'users') tmp&quot;)
      .option(&quot;driver&quot;, &quot;com.teradata.jdbc.TeraDriver&quot;)
      .load().show()

Output:
+-------------------+-------------------------+----------------------+------------+---------------------------------+---------------+----------+-------------+------------+------------+--------+-------------+------------------+...
|DatabaseName       |TableName                |ColumnName            |ColumnFormat|ColumnTitle                      |SPParameterType|ColumnType|ColumnUDTName|ColumnLength|DefaultValue|Nullable|CommentString|DecimalTotalDigits|...
+-------------------+-------------------------+----------------------+------------+---------------------------------+---------------+----------+-------------+------------+------------+--------+-------------+------------------+...
|test               |users                    |active_flag           |-(18)9      |Is User Active                   |               |D         |null         |8           |null        |Y       |null         |18                |...
|test               |users                    |user_id               |-(18)9      |User Identifier                  |               |D         |null         |8           |null        |Y       |null         |18                |....
|test               |users                    |login_date            |yyyy-mm-dd  |Login Date                       |               |DA        |null         |4           |null        |Y       |null         |null              |....
+-------------------+-------------------------+----------------------+------------+---------------------------------+---------------+----------+-------------+------------+------------+--------+-------------+------------------+....

Any ideas why Spark can't read the table and why it can't get columns?
",0
546,69849882,what's the meaning of format Z(19)9 in teradata?,"i see a sql including &quot;format Z(19)9&quot; in terdata environment, and i checked in teradata doc,
the following is for &quot;Z&quot;:
Zero‑suppressed decimal digit.

Translates to blank if the digit is zero and preceding digits are also zero.

A FORMAT phrase containing Z characters (including Z(I) and Z(F)), a combination of commas, 
dots, G, or D, and no other formatting characters means “blank when zero.”

For example, ZZZZZ, ZZ.Z, GZ(I)DZ(F), GZZZZZZDZZ and Z,ZZZ.ZZ print only blanks if the number 
is zero.

A Z cannot follow a 9.

Repeated Z characters must appear to the left of any combination of the radix and any 9 
formatting characters.

The characters to the right of the radix cannot be a combination of 9 and Z characters; they 
must be all 9s or all Zs. If they are all Zs, then the characters to the left of the radix 
must also be all Zs.

If a group of repeated Z characters appears in a FORMAT phrase with a group of repeated sign 
characters, the group of Z characters must immediately follow the group of sign characters. 
For example, --ZZZ.

and the following is for 9:
Decimal digit (no zero suppress).

but i am still confused, what's the meaning of format Z(19)9 ? for example, the data is 0000.234, then the result will be .234000000 ?
",-1,-1,-1.0,"i see a sql including &quot;format Z(19)9&quot; in terdata environment, and i checked in teradata doc,
the following is for &quot;Z&quot;:
Zero‑suppressed decimal digit.

Translates to blank if the digit is zero and preceding digits are also zero.

A FORMAT phrase containing Z characters (including Z(I) and Z(F)), a combination of commas, 
dots, G, or D, and no other formatting characters means “blank when zero.”

For example, ZZZZZ, ZZ.Z, GZ(I)DZ(F), GZZZZZZDZZ and Z,ZZZ.ZZ print only blanks if the number 
is zero.

A Z cannot follow a 9.

Repeated Z characters must appear to the left of any combination of the radix and any 9 
formatting characters.

The characters to the right of the radix cannot be a combination of 9 and Z characters; they 
must be all 9s or all Zs. If they are all Zs, then the characters to the left of the radix 
must also be all Zs.

If a group of repeated Z characters appears in a FORMAT phrase with a group of repeated sign 
characters, the group of Z characters must immediately follow the group of sign characters. 
For example, --ZZZ.

and the following is for 9:
Decimal digit (no zero suppress).

but i am still confused, what's the meaning of format Z(19)9 ? for example, the data is 0000.234, then the result will be .234000000 ?
",3
547,70012975,Update another table using output of cte in teradata,"Technology: Teradata 16.20
I'm trying to update a table, that takes data from another table which uses CTE data.
I'm getting below error:
Error:
select failed 3707 syntax error, expected something like a 'SELECT' keyword or '(' or a 'TRANSACTIONTIME' keyword or a 'VALIDTIME' keyword between ')' and the 'UPDATE' keyword

Question:
Is it possible to update a table using another table that is getting joined with cte?
I know it can be done with a volatile table. I have seen CTE with insert statement and with select statement, but never seen cte with an update statement. Writing the same sql over and over for multiple self join only increases the lines of code. If this update can be done using CTE, that would be much easier and understandable
Code:
WITH NAME_CTE AS (
    SELECT FIRST.ID,FIRST.NAM,LAST.NAME
    FROM TABLE_FIRST FIRST INNER JOIN TABLE_LAST LAST
    ON FIRST.ID = LAST.ID
)
UPDATE SUBJECT_TEACHER_TABLE
FROM (
    SELECT CROSS_TBL.SUB_ID,
    COALESCE(CTE1.NAM,CTE1.NAME,CTE2.NAM,CTE2.NAME) AS FINAL_NAME
    FROM CROSS_REFERENCE_TABLE CROSS_TBL 
    LEFT JOIN NAME_CTE CTE1 ON CROSS_TBL.CR_ID1 = CTE.ID
    LEFT JOIN NAME_CTE CTE2 ON CROSS_TBL.CR_ID2 = CTE.ID
) PV
SET 
FINAL_NAME = PV.FINAL_NAME
WHERE SUBJECT_TEACHER_TABLE.SUB_ID = PV.SUB_ID;

Modified Query: As per @dnoeth suggesstion
UPDATE SUBJECT_TEACHER_TABLE
FROM (
    WITH NAME_CTE AS (
    SELECT FIRST.ID,FIRST.NAM,LAST.NAME
    FROM TABLE_FIRST FIRST INNER JOIN TABLE_LAST LAST
    ON FIRST.ID = LAST.ID)
    SELECT CROSS_TBL.SUB_ID,
    COALESCE(CTE1.NAM,CTE1.NAME,CTE2.NAM,CTE2.NAME) AS FINAL_NAME
    FROM CROSS_REFERENCE_TABLE CROSS_TBL 
    LEFT JOIN NAME_CTE CTE1 ON CROSS_TBL.CR_ID1 = CTE.ID
    LEFT JOIN NAME_CTE CTE2 ON CROSS_TBL.CR_ID2 = CTE.ID
) PV
SET 
FINAL_NAME = PV.FINAL_NAME
WHERE SUBJECT_TEACHER_TABLE.SUB_ID = PV.SUB_ID;

Error:
SQL Error [3807] [42S02]: [Teradata Database] [TeraJDBC 15.10.00.22] [Error 3807] [SQLState 42S02] Object 'NAME_CTE' does not exist.

",-1,-1,-1.0,"Technology: Teradata 16.20
I'm trying to update a table, that takes data from another table which uses CTE data.
I'm getting below error:
Error:
select failed 3707 syntax error, expected something like a 'SELECT' keyword or '(' or a 'TRANSACTIONTIME' keyword or a 'VALIDTIME' keyword between ')' and the 'UPDATE' keyword

Question:
Is it possible to update a table using another table that is getting joined with cte?
I know it can be done with a volatile table. I have seen CTE with insert statement and with select statement, but never seen cte with an update statement. Writing the same sql over and over for multiple self join only increases the lines of code. If this update can be done using CTE, that would be much easier and understandable
Code:
WITH NAME_CTE AS (
    SELECT FIRST.ID,FIRST.NAM,LAST.NAME
    FROM TABLE_FIRST FIRST INNER JOIN TABLE_LAST LAST
    ON FIRST.ID = LAST.ID
)
UPDATE SUBJECT_TEACHER_TABLE
FROM (
    SELECT CROSS_TBL.SUB_ID,
    COALESCE(CTE1.NAM,CTE1.NAME,CTE2.NAM,CTE2.NAME) AS FINAL_NAME
    FROM CROSS_REFERENCE_TABLE CROSS_TBL 
    LEFT JOIN NAME_CTE CTE1 ON CROSS_TBL.CR_ID1 = CTE.ID
    LEFT JOIN NAME_CTE CTE2 ON CROSS_TBL.CR_ID2 = CTE.ID
) PV
SET 
FINAL_NAME = PV.FINAL_NAME
WHERE SUBJECT_TEACHER_TABLE.SUB_ID = PV.SUB_ID;

Modified Query: As per @dnoeth suggesstion
UPDATE SUBJECT_TEACHER_TABLE
FROM (
    WITH NAME_CTE AS (
    SELECT FIRST.ID,FIRST.NAM,LAST.NAME
    FROM TABLE_FIRST FIRST INNER JOIN TABLE_LAST LAST
    ON FIRST.ID = LAST.ID)
    SELECT CROSS_TBL.SUB_ID,
    COALESCE(CTE1.NAM,CTE1.NAME,CTE2.NAM,CTE2.NAME) AS FINAL_NAME
    FROM CROSS_REFERENCE_TABLE CROSS_TBL 
    LEFT JOIN NAME_CTE CTE1 ON CROSS_TBL.CR_ID1 = CTE.ID
    LEFT JOIN NAME_CTE CTE2 ON CROSS_TBL.CR_ID2 = CTE.ID
) PV
SET 
FINAL_NAME = PV.FINAL_NAME
WHERE SUBJECT_TEACHER_TABLE.SUB_ID = PV.SUB_ID;

Error:
SQL Error [3807] [42S02]: [Teradata Database] [TeraJDBC 15.10.00.22] [Error 3807] [SQLState 42S02] Object 'NAME_CTE' does not exist.

",3
548,70165147,Teradata not accepting pandas date variable,"I have date variable in pandas that I would like to pass through a Teradata sql query -
import numpy as np
import pyodbc
import time
import os
import teradata as td
from teradata import tdodbc

#create parameters for date
date_from_1 = pd.to_datetime('2021-09-29').strftime('%Y-%m-%d')
date_to_1 = pd.to_datetime('2021-10-30').strftime('%Y-%m-%d')

#sql query
tran = '''SELECT TRAN.ACCOUNT_NUMBER,
                TRAN.DATE
from TRANSACTION TABLE TRAN
where TRAN.DATE &gt;= ?
   AND TRAN.DATE &lt; ?
   '''
pd.read_sql_query(tran, tdp_conn, params = [date_from_1, date_to_1])


': ('22008', '[22008] [Teradata][ODBC Teradata Driver][Teradata Database] Invalid date supplied
THANK YOU!
",0,-1,-1.0,"I have date variable in pandas that I would like to pass through a Teradata sql query -
import numpy as np
import pyodbc
import time
import os
import teradata as td
from teradata import tdodbc

#create parameters for date
date_from_1 = pd.to_datetime('2021-09-29').strftime('%Y-%m-%d')
date_to_1 = pd.to_datetime('2021-10-30').strftime('%Y-%m-%d')

#sql query
tran = '''SELECT TRAN.ACCOUNT_NUMBER,
                TRAN.DATE
from TRANSACTION TABLE TRAN
where TRAN.DATE &gt;= ?
   AND TRAN.DATE &lt; ?
   '''
pd.read_sql_query(tran, tdp_conn, params = [date_from_1, date_to_1])


': ('22008', '[22008] [Teradata][ODBC Teradata Driver][Teradata Database] Invalid date supplied
THANK YOU!
",1
549,70283297,Extract middle part of string Teradata,"I am stuck trying to figure out why my REGEX expression in Teradata works sometimes but not always: i am trying to extract part of the string in the middle that has a pattern: it starts with ABC and is followed by 3 or 4 digits.
select RegExp_Substr('X886782E-ABC2036-VACCINE COVID', '[ABC]+[0-9]+')

My logic works most of the time, but not always: it does not always recogize part of the string that starts with ABC and is followed by 3-4 digits.
",-1,-1,-1.0,"I am stuck trying to figure out why my REGEX expression in Teradata works sometimes but not always: i am trying to extract part of the string in the middle that has a pattern: it starts with ABC and is followed by 3 or 4 digits.
select RegExp_Substr('X886782E-ABC2036-VACCINE COVID', '[ABC]+[0-9]+')

My logic works most of the time, but not always: it does not always recogize part of the string that starts with ABC and is followed by 3-4 digits.
",3
550,70382960,"Python module teradatasql throwing ""Syntax error: expected something between the beginning of the request and the word""","I am trying to use teradatasql and the sample code given for FastExportTable is throwing error for me.
Code:
#!~/miniconda3/bin/python
# This sample program demonstrates how to FastExport rows from a table.

import teradatasql

with teradatasql.connect (host='TD_DB_IP', user='db_user_name', password='db_password', database='teradata_db_name') as con:
    with con.cursor () as cur:
        with con.cursor () as cur2:
            sTableName = &quot;FastExportTable&quot;
            try:
                sRequest = &quot;DROP TABLE &quot; + sTableName
                print (sRequest)
                cur.execute (sRequest)
            except Exception as ex:
                print (&quot;Ignoring&quot;, str (ex).split (&quot;\n&quot;) [0])

            sRequest = &quot;CREATE TABLE &quot; + sTableName + &quot; (c1 INTEGER NOT NULL, c2 VARCHAR(10))&quot;
            print (sRequest)
            cur.execute (sRequest)

            try:
                sInsert = &quot;INSERT INTO &quot; + sTableName + &quot; VALUES (?, ?)&quot;
                print (sInsert)
                cur.execute (sInsert, [
                    [1, None],
                    [2, &quot;abc&quot;],
                    [3, &quot;def&quot;],
                    [4, &quot;mno&quot;],
                    [5, None],
                    [6, &quot;pqr&quot;],
                    [7, &quot;uvw&quot;],
                    [8, &quot;xyz&quot;],
                    [9, None],
                ])

                sSelect = &quot;{fn teradata_try_fastexport}SELECT * FROM &quot; + sTableName
                print (sSelect)
                cur.execute (sSelect)
                [ print (row) for row in sorted (cur.fetchall ()) ]

                sRequest = &quot;{fn teradata_nativesql}{fn teradata_get_warnings}&quot; + sSelect
                print (sRequest)
                cur2.execute (sRequest)
                [ print (row) for row in cur2.fetchall () ]

                sRequest = &quot;{fn teradata_nativesql}{fn teradata_get_errors}&quot; + sSelect
                print (sRequest)
                cur2.execute (sRequest)
                [ print (row) for row in cur2.fetchall () ]

                sRequest = &quot;{fn teradata_nativesql}{fn teradata_logon_sequence_number}&quot; + sSelect
                print (sRequest)
                cur2.execute (sRequest)
                [ print (row) for row in cur2.fetchall () ]

            finally:
                sRequest = &quot;DROP TABLE &quot; + sTableName
                print (sRequest)
                cur.execute (sRequest)

Error:
Traceback (most recent call last):
  File &quot;./sample_tdsql_fastexporttable.py&quot;, line 38, in &lt;module&gt;
    cur.execute (sSelect)
  File &quot;~/miniconda3/lib/python3.6/site-packages/teradatasql/__init__.py&quot;, line 649, in execute
    self.executemany (sOperation, None, ignoreErrors)
  File &quot;~/miniconda3/lib/python3.6/site-packages/teradatasql/__init__.py&quot;, line 896, in executemany
    raise OperationalError (sErr)
teradatasql.OperationalError: [Version 16.20.0.62] [Session 590874] [Teradata Database] [Error 3706] Syntax error: expected something between the beginning of the request and the word 'teradata_try_fastexportSELECT'.

Only change I did in the sample program provided on github for this module is add database='teradata_db_name'. If I remove database='teradata_db_name' and run same code I get error on line 19: teradatasql.OperationalError: [Version 16.20.0.62] [Session 590869] [Teradata Database] [Error 2644] No more room in database SVC_DEV_XDW_VANTAGE..
We are using Python 3.6.0 :: Continuum Analytics, Inc.
Has anyone got same error? What am I missing here? Any information to resolve this is greatly appreciated.
",-1,-1,-1.0,"I am trying to use teradatasql and the sample code given for FastExportTable is throwing error for me.
Code:
#!~/miniconda3/bin/python
# This sample program demonstrates how to FastExport rows from a table.

import teradatasql

with teradatasql.connect (host='TD_DB_IP', user='db_user_name', password='db_password', database='teradata_db_name') as con:
    with con.cursor () as cur:
        with con.cursor () as cur2:
            sTableName = &quot;FastExportTable&quot;
            try:
                sRequest = &quot;DROP TABLE &quot; + sTableName
                print (sRequest)
                cur.execute (sRequest)
            except Exception as ex:
                print (&quot;Ignoring&quot;, str (ex).split (&quot;\n&quot;) [0])

            sRequest = &quot;CREATE TABLE &quot; + sTableName + &quot; (c1 INTEGER NOT NULL, c2 VARCHAR(10))&quot;
            print (sRequest)
            cur.execute (sRequest)

            try:
                sInsert = &quot;INSERT INTO &quot; + sTableName + &quot; VALUES (?, ?)&quot;
                print (sInsert)
                cur.execute (sInsert, [
                    [1, None],
                    [2, &quot;abc&quot;],
                    [3, &quot;def&quot;],
                    [4, &quot;mno&quot;],
                    [5, None],
                    [6, &quot;pqr&quot;],
                    [7, &quot;uvw&quot;],
                    [8, &quot;xyz&quot;],
                    [9, None],
                ])

                sSelect = &quot;{fn teradata_try_fastexport}SELECT * FROM &quot; + sTableName
                print (sSelect)
                cur.execute (sSelect)
                [ print (row) for row in sorted (cur.fetchall ()) ]

                sRequest = &quot;{fn teradata_nativesql}{fn teradata_get_warnings}&quot; + sSelect
                print (sRequest)
                cur2.execute (sRequest)
                [ print (row) for row in cur2.fetchall () ]

                sRequest = &quot;{fn teradata_nativesql}{fn teradata_get_errors}&quot; + sSelect
                print (sRequest)
                cur2.execute (sRequest)
                [ print (row) for row in cur2.fetchall () ]

                sRequest = &quot;{fn teradata_nativesql}{fn teradata_logon_sequence_number}&quot; + sSelect
                print (sRequest)
                cur2.execute (sRequest)
                [ print (row) for row in cur2.fetchall () ]

            finally:
                sRequest = &quot;DROP TABLE &quot; + sTableName
                print (sRequest)
                cur.execute (sRequest)

Error:
Traceback (most recent call last):
  File &quot;./sample_tdsql_fastexporttable.py&quot;, line 38, in &lt;module&gt;
    cur.execute (sSelect)
  File &quot;~/miniconda3/lib/python3.6/site-packages/teradatasql/__init__.py&quot;, line 649, in execute
    self.executemany (sOperation, None, ignoreErrors)
  File &quot;~/miniconda3/lib/python3.6/site-packages/teradatasql/__init__.py&quot;, line 896, in executemany
    raise OperationalError (sErr)
teradatasql.OperationalError: [Version 16.20.0.62] [Session 590874] [Teradata Database] [Error 3706] Syntax error: expected something between the beginning of the request and the word 'teradata_try_fastexportSELECT'.

Only change I did in the sample program provided on github for this module is add database='teradata_db_name'. If I remove database='teradata_db_name' and run same code I get error on line 19: teradatasql.OperationalError: [Version 16.20.0.62] [Session 590869] [Teradata Database] [Error 2644] No more room in database SVC_DEV_XDW_VANTAGE..
We are using Python 3.6.0 :: Continuum Analytics, Inc.
Has anyone got same error? What am I missing here? Any information to resolve this is greatly appreciated.
",1
551,70581944,Teradata Editdistance function,"I am trying to use the teradata editdistance function, and it works nicely in the case I have static strings, e.g. select editdistance('Hello','Hella');
If I then introduce a column into the first string, that also works nicely e.g.
select editdistance(First_Name1, 'Hello');
But I do not seem to be able to understand how to do it over multiple strings for each input e.g.
select editdistance(First_Name1, First_Name2);
If I was using python a for loop would do the trick but that is not an option for this data unfortunately.




First_Name1
First_Name2




Adam
Adom


Chris
Chriss


Jane
Ben



",-1,-1,-1.0,"I am trying to use the teradata editdistance function, and it works nicely in the case I have static strings, e.g. select editdistance('Hello','Hella');
If I then introduce a column into the first string, that also works nicely e.g.
select editdistance(First_Name1, 'Hello');
But I do not seem to be able to understand how to do it over multiple strings for each input e.g.
select editdistance(First_Name1, First_Name2);
If I was using python a for loop would do the trick but that is not an option for this data unfortunately.




First_Name1
First_Name2




Adam
Adom


Chris
Chriss


Jane
Ben



",3
552,70583144,Teradata query returns bad characters in string column but exporting to CSV from assistant console works,"I am using DBI package in R to connect to teradata this way:
library(teradatasql)

query &lt;- &quot;
    SELECT sku, description
    FROM sku_table
    WHERE sku = '12345'
    &quot;

dbconn &lt;- DBI::dbConnect(
        teradatasql::TeradataDriver(),
        host = teradataHostName, database = teradataDBName,
        user = teradataUserName, password = teradataPassword
    )

dbFetch(dbSendQuery(dbconn, query), -1)

It returns a result as follows:
  SKU                               DESCRIPTION
12345      18V MAXâ×¢ Collated Drywall Screwgun

Notice the bad characters â×¢ above. This is supposed to be superscript TM for trademarked.
When I use SQL assistant to run the query, and export the query results manually to a CSV file, it works fine as in the DESCRIPTION column has correct encoding.
Any idea what is going on and how I can fix this problem? Obviously, I don't want a manual step of exporting to CSV and re-reading results back into R data frame, and into memory.
",-1,-1,-1.0,"I am using DBI package in R to connect to teradata this way:
library(teradatasql)

query &lt;- &quot;
    SELECT sku, description
    FROM sku_table
    WHERE sku = '12345'
    &quot;

dbconn &lt;- DBI::dbConnect(
        teradatasql::TeradataDriver(),
        host = teradataHostName, database = teradataDBName,
        user = teradataUserName, password = teradataPassword
    )

dbFetch(dbSendQuery(dbconn, query), -1)

It returns a result as follows:
  SKU                               DESCRIPTION
12345      18V MAXâ×¢ Collated Drywall Screwgun

Notice the bad characters â×¢ above. This is supposed to be superscript TM for trademarked.
When I use SQL assistant to run the query, and export the query results manually to a CSV file, it works fine as in the DESCRIPTION column has correct encoding.
Any idea what is going on and how I can fix this problem? Obviously, I don't want a manual step of exporting to CSV and re-reading results back into R data frame, and into memory.
",3
553,70653802,Can't connect DB with Teradata express studio,"I have installed Teradata in my VM, and can connect the dbc database with the in-built Terraform studio express .
Now, I want to connect the dbc database with the Terraform studio express outside my VM i.e. in Windows. But when I try to test the connection using the same login details, it shows me &quot;Ping Failed&quot;.
The detailed error message is as follows:
java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 17.10.00.14] [Error 1277] [SQLState 08S01] Login timeout for Connection to 192.168.XXX.XX Mon Jan 10 19:39:51 IST 2022 socket orig=192.168.XXX.XX cid=50e21bd3 sess=0 java.net.SocketTimeoutException: connect timed out   at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method)   at java.net.DualStackPlainSocketImpl.socketConnect(Unknown Source)   at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source)   at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source)   at java.net.AbstractPlainSocketImpl.connect(Unknown Source)   at java.net.PlainSocketImpl.connect(Unknown Source)   at java.net.SocksSocketImpl.connect(Unknown Source)   at java.net.Socket.connect(Unknown Source)   at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$ConnectThread.run(TDNetworkIOIF.java:1543)  
at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeIoJDBCException(ErrorFactory.java:208)
at com.teradata.jdbc.jdbc_4.util.ErrorAnalyzer.analyzeIoError(ErrorAnalyzer.java:59)
at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.createSocketConnection(TDNetworkIOIF.java:188)
at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.createIO(TDNetworkIOIF.java:174)
at com.teradata.jdbc.jdbc.GenericTeradataConnection.&lt;init&gt;(GenericTeradataConnection.java:290)
at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:186)
at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:36)
at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)
at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:181)
at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:171)
at com.teradata.jdbc.TeraDriver.doConnect(TeraDriver.java:230)
at com.teradata.jdbc.TeraDriver.connect(TeraDriver.java:163)
at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.makeConnection(TeradataJDBCConnection.java:286)
at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.createConnection(TeradataJDBCConnection.java:124)
at org.eclipse.datatools.connectivity.DriverConnectionBase.internalCreateConnection(DriverConnectionBase.java:105)
at org.eclipse.datatools.connectivity.DriverConnectionBase.open(DriverConnectionBase.java:54)
at org.eclipse.datatools.connectivity.drivers.jdbc.JDBCConnection.open(JDBCConnection.java:96)
at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataPingFactory.createConnection(TeradataPingFactory.java:36)
at org.eclipse.datatools.connectivity.internal.ConnectionFactoryProvider.createConnection(ConnectionFactoryProvider.java:83)
at org.eclipse.datatools.connectivity.internal.ConnectionProfile.createConnection(ConnectionProfile.java:359)
at org.eclipse.datatools.connectivity.ui.PingJob.createTestConnection(PingJob.java:76)
at org.eclipse.datatools.connectivity.ui.PingJob.run(PingJob.java:59)
at org.eclipse.core.internal.jobs.Worker.run(Worker.java:63)
Caused by: java.net.SocketTimeoutException: connect timed out
    at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method)
    at java.net.DualStackPlainSocketImpl.socketConnect(Unknown Source)
    at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source)
    at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source)
    at java.net.AbstractPlainSocketImpl.connect(Unknown Source)
    at java.net.PlainSocketImpl.connect(Unknown Source)
    at java.net.SocksSocketImpl.connect(Unknown Source)
    at java.net.Socket.connect(Unknown Source)
    at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$ConnectThread.run(TDNetworkIOIF.java:1543)

Please provide any solutions to fix this.
Note: I have also installed the Teradata tools and utilities suite (ODBC driver, BTEQ, etc)
",-1,-1,-1.0,"I have installed Teradata in my VM, and can connect the dbc database with the in-built Terraform studio express .
Now, I want to connect the dbc database with the Terraform studio express outside my VM i.e. in Windows. But when I try to test the connection using the same login details, it shows me &quot;Ping Failed&quot;.
The detailed error message is as follows:
java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 17.10.00.14] [Error 1277] [SQLState 08S01] Login timeout for Connection to 192.168.XXX.XX Mon Jan 10 19:39:51 IST 2022 socket orig=192.168.XXX.XX cid=50e21bd3 sess=0 java.net.SocketTimeoutException: connect timed out   at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method)   at java.net.DualStackPlainSocketImpl.socketConnect(Unknown Source)   at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source)   at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source)   at java.net.AbstractPlainSocketImpl.connect(Unknown Source)   at java.net.PlainSocketImpl.connect(Unknown Source)   at java.net.SocksSocketImpl.connect(Unknown Source)   at java.net.Socket.connect(Unknown Source)   at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$ConnectThread.run(TDNetworkIOIF.java:1543)  
at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeIoJDBCException(ErrorFactory.java:208)
at com.teradata.jdbc.jdbc_4.util.ErrorAnalyzer.analyzeIoError(ErrorAnalyzer.java:59)
at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.createSocketConnection(TDNetworkIOIF.java:188)
at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.createIO(TDNetworkIOIF.java:174)
at com.teradata.jdbc.jdbc.GenericTeradataConnection.&lt;init&gt;(GenericTeradataConnection.java:290)
at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:186)
at com.teradata.jdbc.jdk6.JDK6_SQL_Connection.&lt;init&gt;(JDK6_SQL_Connection.java:36)
at com.teradata.jdbc.jdk6.JDK6ConnectionFactory.constructSQLConnection(JDK6ConnectionFactory.java:25)
at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:181)
at com.teradata.jdbc.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:171)
at com.teradata.jdbc.TeraDriver.doConnect(TeraDriver.java:230)
at com.teradata.jdbc.TeraDriver.connect(TeraDriver.java:163)
at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.makeConnection(TeradataJDBCConnection.java:286)
at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataJDBCConnection.createConnection(TeradataJDBCConnection.java:124)
at org.eclipse.datatools.connectivity.DriverConnectionBase.internalCreateConnection(DriverConnectionBase.java:105)
at org.eclipse.datatools.connectivity.DriverConnectionBase.open(DriverConnectionBase.java:54)
at org.eclipse.datatools.connectivity.drivers.jdbc.JDBCConnection.open(JDBCConnection.java:96)
at com.teradata.datatools.dtp.connectivity.db.teradata.TeradataPingFactory.createConnection(TeradataPingFactory.java:36)
at org.eclipse.datatools.connectivity.internal.ConnectionFactoryProvider.createConnection(ConnectionFactoryProvider.java:83)
at org.eclipse.datatools.connectivity.internal.ConnectionProfile.createConnection(ConnectionProfile.java:359)
at org.eclipse.datatools.connectivity.ui.PingJob.createTestConnection(PingJob.java:76)
at org.eclipse.datatools.connectivity.ui.PingJob.run(PingJob.java:59)
at org.eclipse.core.internal.jobs.Worker.run(Worker.java:63)
Caused by: java.net.SocketTimeoutException: connect timed out
    at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method)
    at java.net.DualStackPlainSocketImpl.socketConnect(Unknown Source)
    at java.net.AbstractPlainSocketImpl.doConnect(Unknown Source)
    at java.net.AbstractPlainSocketImpl.connectToAddress(Unknown Source)
    at java.net.AbstractPlainSocketImpl.connect(Unknown Source)
    at java.net.PlainSocketImpl.connect(Unknown Source)
    at java.net.SocksSocketImpl.connect(Unknown Source)
    at java.net.Socket.connect(Unknown Source)
    at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF$ConnectThread.run(TDNetworkIOIF.java:1543)

Please provide any solutions to fix this.
Note: I have also installed the Teradata tools and utilities suite (ODBC driver, BTEQ, etc)
",0
554,70302373,How to delete space and value from col1 and create create new column based on it in Teradata SQL?,"I have table in Teradata SQL like below:
col1
-------
123 ABC Berlin
44567 ABC Rome
11 ABC New York

And based on &quot;col1&quot; I need to delete one space and &quot;ABC&quot; from column &quot;col1&quot; and stay only number, space and name of city, so as a result I need something like below:
col1                col2
--------------------------------
123 ABC Berlin   | 123 Berlin
44567 ABC Rome   | 44567 Rome
11 ABC New York  | 11 New York

How can I do that in Teradata SQL ?
I have code like that, but it delete both number and &quot;ABC&quot; what is not good in terms of my requirements: REGEXP_REPLACE(col, '[^ ]+ ', '')
",-1,1,-1.0,"I have table in Teradata SQL like below:
col1
-------
123 ABC Berlin
44567 ABC Rome
11 ABC New York

And based on &quot;col1&quot; I need to delete one space and &quot;ABC&quot; from column &quot;col1&quot; and stay only number, space and name of city, so as a result I need something like below:
col1                col2
--------------------------------
123 ABC Berlin   | 123 Berlin
44567 ABC Rome   | 44567 Rome
11 ABC New York  | 11 New York

How can I do that in Teradata SQL ?
I have code like that, but it delete both number and &quot;ABC&quot; what is not good in terms of my requirements: REGEXP_REPLACE(col, '[^ ]+ ', '')
",3
555,70270622,How to insert excel rows into Teradata table,"I have an excel sheet where in I wish to insert selected excel rows into Teradata table using vba. So far I have managed to retrieve data from Teradata using following code.
Private Sub CommandButton1_Click()
    Dim conn As ADODB.Connection
    Dim rec1 As ADODB.Recordset
    Dim thisSql As String

    Set conn = New ADODB.Connection

    conn.Open &quot;PROVIDER=***;AUTHENTICATION=***;Driver=*****;DBCName=******;Uid=*****;Pwd=******;&quot;

    thisSql = &quot;sel * from Database.table1&quot;

    Set rec1 = New ADODB.Recordset
    rec1.Open thisSql, conn

    With Sheet1.QueryTables.Add(Connection:=rec1, Destination:=Sheet1.Range(&quot;A1&quot;))
        .Name = &quot;data&quot;
        .FieldNames = True
        .Refresh BackgroundQuery:=False
    End With
End Sub

This brings all the data from the teradata table (5 columns) in to excel sheet starting at A1:E10.
First Problem: Every time I click the button, the table is inserted from position A1, which means any previous data is pushed to F1:J10. I want to replace/overwrite the data at A-E if button is clicked again and not add/duplicate it by pushing previous data to F-J.
Second Problem: Allow user to type in more rows in same sheet or sheet 2 in same workbook and have a button that takes that data and inserts into teradata table.
I am unable to find a fitting example for my requirements. I checked How to insert data into Teradata Table from Excel (using VBA macro) but it did not help my problem.
",-1,-1,-1.0,"I have an excel sheet where in I wish to insert selected excel rows into Teradata table using vba. So far I have managed to retrieve data from Teradata using following code.
Private Sub CommandButton1_Click()
    Dim conn As ADODB.Connection
    Dim rec1 As ADODB.Recordset
    Dim thisSql As String

    Set conn = New ADODB.Connection

    conn.Open &quot;PROVIDER=***;AUTHENTICATION=***;Driver=*****;DBCName=******;Uid=*****;Pwd=******;&quot;

    thisSql = &quot;sel * from Database.table1&quot;

    Set rec1 = New ADODB.Recordset
    rec1.Open thisSql, conn

    With Sheet1.QueryTables.Add(Connection:=rec1, Destination:=Sheet1.Range(&quot;A1&quot;))
        .Name = &quot;data&quot;
        .FieldNames = True
        .Refresh BackgroundQuery:=False
    End With
End Sub

This brings all the data from the teradata table (5 columns) in to excel sheet starting at A1:E10.
First Problem: Every time I click the button, the table is inserted from position A1, which means any previous data is pushed to F1:J10. I want to replace/overwrite the data at A-E if button is clicked again and not add/duplicate it by pushing previous data to F-J.
Second Problem: Allow user to type in more rows in same sheet or sheet 2 in same workbook and have a button that takes that data and inserts into teradata table.
I am unable to find a fitting example for my requirements. I checked How to insert data into Teradata Table from Excel (using VBA macro) but it did not help my problem.
",3
556,70702071,Teradata fastload fails when pyspark dataframe has more than one partition,"I am trying to write a spark dataframe into teradata using FASTLOAD. The write operation works if I force the dataframe to have only one partition by using df_final = df_final.repartition(1). But, fails if there are more than one partition. Since the data size is huge if repartitioned(1) is applied on the dataframe it will be overhead on the master node. I even tried to match partitions with # of sessions it didn't work.


    df_final.write.option(""truncate"",truncate)\
    .mode(mode).option(""batchsize"",100000)\
    .jdbc(url=""jdbc:teradata://host/DBS_PORT=port,LOGMECH=TD2,TMODE=ANSI,CHARSET=UTF16,ENCRYPTDATA=ON,TYPE=FASTLOAD,SESSIONS=2,ERROR_TABLE_DATABASE=errortble"",
    table=""tempdb.temptable"",
    properties=connectionProperties)


Teradata Version:16.20.53.04 
JDBC Version: 17.00.00.03
Stack Trace:


2022-01-13 15:58:04.701899: Loading data into tempdb.temptable with write mode as overwrite and truncate as true
An error occurred while calling o1002.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 31, X.X.X.X, executor 0): java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""TempDB"".""temptable"". Details of the failure can be found in the exception chain that is accessible with getNextException.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:149)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:133)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2389)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:691)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:858)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:856)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1001)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1001)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2379)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
    at org.apache.spark.scheduler.Task.run(Task.scala:117)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:655)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:658)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1147] [SQLState HY000] The next failure(s) in the exception chain occurred while beginning FastLoad of database table ""TempDB"".""temptable""
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.beginFastLoad(FastLoadManagerPreparedStatement.java:966)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2210)
    ... 15 more

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2339)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2360)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2379)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2404)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1001)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:395)
    at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:999)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:856)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:58)
    at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:91)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:200)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$3(SparkPlan.scala:252)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:158)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:157)
    at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:999)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:249)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:199)
    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:999)
    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:437)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:421)
    at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:827)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:295)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""TempDB"".""temptable"". Details of the failure can be found in the exception chain that is accessible with getNextException.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:149)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:133)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2389)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:691)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:858)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:856)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1001)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1001)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2379)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
    at org.apache.spark.scheduler.Task.run(Task.scala:117)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:655)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:658)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1147] [SQLState HY000] The next failure(s) in the exception chain occurred while beginning FastLoad of database table ""TempDB"".""temptable""
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.beginFastLoad(FastLoadManagerPreparedStatement.java:966)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2210)
    ... 15 more

",-1,-1,-1.0,"I am trying to write a spark dataframe into teradata using FASTLOAD. The write operation works if I force the dataframe to have only one partition by using df_final = df_final.repartition(1). But, fails if there are more than one partition. Since the data size is huge if repartitioned(1) is applied on the dataframe it will be overhead on the master node. I even tried to match partitions with # of sessions it didn't work.


    df_final.write.option(""truncate"",truncate)\
    .mode(mode).option(""batchsize"",100000)\
    .jdbc(url=""jdbc:teradata://host/DBS_PORT=port,LOGMECH=TD2,TMODE=ANSI,CHARSET=UTF16,ENCRYPTDATA=ON,TYPE=FASTLOAD,SESSIONS=2,ERROR_TABLE_DATABASE=errortble"",
    table=""tempdb.temptable"",
    properties=connectionProperties)


Teradata Version:16.20.53.04 
JDBC Version: 17.00.00.03
Stack Trace:


2022-01-13 15:58:04.701899: Loading data into tempdb.temptable with write mode as overwrite and truncate as true
An error occurred while calling o1002.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 31, X.X.X.X, executor 0): java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""TempDB"".""temptable"". Details of the failure can be found in the exception chain that is accessible with getNextException.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:149)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:133)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2389)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:691)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:858)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:856)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1001)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1001)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2379)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
    at org.apache.spark.scheduler.Task.run(Task.scala:117)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:655)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:658)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1147] [SQLState HY000] The next failure(s) in the exception chain occurred while beginning FastLoad of database table ""TempDB"".""temptable""
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.beginFastLoad(FastLoadManagerPreparedStatement.java:966)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2210)
    ... 15 more

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2339)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2360)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2379)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2404)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1001)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:395)
    at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:999)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:856)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:58)
    at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:91)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:200)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$3(SparkPlan.scala:252)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:248)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:158)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:157)
    at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:999)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:249)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:845)
    at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:199)
    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:999)
    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:437)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:421)
    at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:827)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)
    at py4j.Gateway.invoke(Gateway.java:295)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:251)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table ""TempDB"".""temptable"". Details of the failure can be found in the exception chain that is accessible with getNextException.
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:149)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeBatchUpdateException(ErrorFactory.java:133)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2389)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:691)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:858)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:856)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1001)
    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1001)
    at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2379)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)
    at org.apache.spark.scheduler.Task.run(Task.scala:117)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:655)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:658)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: java.sql.SQLException: [Teradata JDBC Driver] [TeraJDBC 17.00.00.03] [Error 1147] [SQLState HY000] The next failure(s) in the exception chain occurred while beginning FastLoad of database table ""TempDB"".""temptable""
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:95)
    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDriverJDBCException(ErrorFactory.java:70)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.beginFastLoad(FastLoadManagerPreparedStatement.java:966)
    at com.teradata.jdbc.jdbc.fastload.FastLoadManagerPreparedStatement.executeBatch(FastLoadManagerPreparedStatement.java:2210)
    ... 15 more

",0
557,70750566,Best approach to extract 1000 Gbs of Data from Teradata,"What should be the best approach to download data from Teradata using python ? I have more than 3000 GBs of data to be migrated from teradata to another code service. Any approach or advise would be great.
So far I have explored -

Using Pandas it is slow and inefficient. I have also used {fn teradata_try_fastexport} but it seems to be no difference.
Using tdload I am not able to partition the file and no support for downloading file with byte,clob,blob data type.
Using dask, not able to connect with teradata and get the data using dask dataframe. I have read the sql using pandas and then use dask to write the data. However, that seems to be of little help in comparison to huge amount of data.
Pyspark - I am exploring but it seems difficult with too much file configuration and dependency issue.

Any advise or suggested approach that I should follow for the efficient and effective data extraction.
",1,-1,-1.0,"What should be the best approach to download data from Teradata using python ? I have more than 3000 GBs of data to be migrated from teradata to another code service. Any approach or advise would be great.
So far I have explored -

Using Pandas it is slow and inefficient. I have also used {fn teradata_try_fastexport} but it seems to be no difference.
Using tdload I am not able to partition the file and no support for downloading file with byte,clob,blob data type.
Using dask, not able to connect with teradata and get the data using dask dataframe. I have read the sql using pandas and then use dask to write the data. However, that seems to be of little help in comparison to huge amount of data.
Pyspark - I am exploring but it seems difficult with too much file configuration and dependency issue.

Any advise or suggested approach that I should follow for the efficient and effective data extraction.
",3
558,70753146,problem with spaces added while running queries with the teradatasql package,"While comparing two methods to run Teradata queries under Python, I noticed that the one using teradatasql adds spaces to columns of type varchar.
Method 1 (no spaces added)
import teradata
import pandas 

... #defining connection credentials
udaExec = teradata.UdaExec(appConfigFile = ConfigPath, 
     logFile = LogPath, logLevel = LogLev, logConsole=False)
session = udaExec.connect(method = 'odbc', system = hostname, 
     username= usn, password=pwd,driver = drv)

output = pandas.read_sql(query, session)

Method 2 (spaces added)
import teradatasql
import pandas

... #defining connection credentials
with teradatasql.connect (host = hostname, user= usn, password= pwd) as conn:
   output = pandas.read_sql(query, conn)

Could you kindly help me find a way to fix this issue ?
",1,-1,-1.0,"While comparing two methods to run Teradata queries under Python, I noticed that the one using teradatasql adds spaces to columns of type varchar.
Method 1 (no spaces added)
import teradata
import pandas 

... #defining connection credentials
udaExec = teradata.UdaExec(appConfigFile = ConfigPath, 
     logFile = LogPath, logLevel = LogLev, logConsole=False)
session = udaExec.connect(method = 'odbc', system = hostname, 
     username= usn, password=pwd,driver = drv)

output = pandas.read_sql(query, session)

Method 2 (spaces added)
import teradatasql
import pandas

... #defining connection credentials
with teradatasql.connect (host = hostname, user= usn, password= pwd) as conn:
   output = pandas.read_sql(query, conn)

Could you kindly help me find a way to fix this issue ?
",1
559,70823865,How to connect Pyspark with Teradata?,"I need to connect Pyspark with Teradata, and the code I'm using for this is:
from pyspark.sql import SparkSession

appName = &quot;PySpark Teradata Example&quot;
master = &quot;local&quot;

# Create Spark session
spark = SparkSession.builder \
    .appName(appName) \
    .master(master) \
    .getOrCreate()

driver = 'com.teradata.jdbc.TeraDriver'

# Define the function to load data from Teradata


def load_data(driver, jdbc_url, sql, user, password):
    return spark.read \
        .format('jdbc') \
        .option('driver', driver) \
        .option('url', jdbc_url) \
        .option('dbtable', '({sql}) as src'.format(sql=sql)) \
        .option('user', user) \
        .option('password', password) \
        .load()

sql = &quot;select * from AdventureWorksDW.DimAccount&quot;
url = &quot;192.168.xx.xxx&quot;
user = &quot;dbc&quot;
password = &quot;dbc&quot;

df_td = load_data(driver,url,sql,user,password)
df_td.show(10)

I'm running the above script using the following Spark-submit command in cmd
spark-submit --jars terajdbc4.jar spark.py

And I get the error as:

File
&quot;..\spark.py&quot;,
line 32, in 
df_td = load_data(driver,url,sql,user,password)
File &quot;..\spark.py&quot;,
line 25, in load_data
.load()
File &quot;C:\SPARK\spark-3.2.0-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\readwriter.py&quot;, line 164, in load
File
&quot;C:\SPARK\spark-3.2.0-bin-hadoop2.7\python\lib\py4j-0.10.9.2-src.zip\py4j\java_gateway.py&quot;, line 1309, in call
File
&quot;C:\SPARK\spark-3.2.0-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py&quot;,
line 117, in deco pyspark.sql.utils.IllegalArgumentException:
requirement failed: The driver could not open a JDBC connection. Check
the URL: 192.168.xx.xxx

What could be the issue?
",-1,-1,-1.0,"I need to connect Pyspark with Teradata, and the code I'm using for this is:
from pyspark.sql import SparkSession

appName = &quot;PySpark Teradata Example&quot;
master = &quot;local&quot;

# Create Spark session
spark = SparkSession.builder \
    .appName(appName) \
    .master(master) \
    .getOrCreate()

driver = 'com.teradata.jdbc.TeraDriver'

# Define the function to load data from Teradata


def load_data(driver, jdbc_url, sql, user, password):
    return spark.read \
        .format('jdbc') \
        .option('driver', driver) \
        .option('url', jdbc_url) \
        .option('dbtable', '({sql}) as src'.format(sql=sql)) \
        .option('user', user) \
        .option('password', password) \
        .load()

sql = &quot;select * from AdventureWorksDW.DimAccount&quot;
url = &quot;192.168.xx.xxx&quot;
user = &quot;dbc&quot;
password = &quot;dbc&quot;

df_td = load_data(driver,url,sql,user,password)
df_td.show(10)

I'm running the above script using the following Spark-submit command in cmd
spark-submit --jars terajdbc4.jar spark.py

And I get the error as:

File
&quot;..\spark.py&quot;,
line 32, in 
df_td = load_data(driver,url,sql,user,password)
File &quot;..\spark.py&quot;,
line 25, in load_data
.load()
File &quot;C:\SPARK\spark-3.2.0-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\readwriter.py&quot;, line 164, in load
File
&quot;C:\SPARK\spark-3.2.0-bin-hadoop2.7\python\lib\py4j-0.10.9.2-src.zip\py4j\java_gateway.py&quot;, line 1309, in call
File
&quot;C:\SPARK\spark-3.2.0-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py&quot;,
line 117, in deco pyspark.sql.utils.IllegalArgumentException:
requirement failed: The driver could not open a JDBC connection. Check
the URL: 192.168.xx.xxx

What could be the issue?
",0
560,70855762,R teradata DBI:dbConnect() error: TimedOut: No response received when attempting to connect to the Teradata server,"I am going to ask and answer this question because I spent more time than I'd like to admit searching for a response and couldn't find one. I installed Teradata ODBC Driver 16.20. In the ODBC Data Source Administrator, I added a Data Source. I named it teradata, put in the name of the Teradata Server to connect to and my username and password for authentication. When I tried running  the following code in RStudio:
con &lt;- DBI::dbConnect(odbc::odbc(),
                      &quot;teradata&quot;)

I would get an error:
Error: nanodbc/nanodbc.cpp:1021: HY000: [Teradata][WSock32 DLL] (434) WSA E TimedOut: No response received when attempting to connect to the Teradata server

",-1,-1,-1.0,"I am going to ask and answer this question because I spent more time than I'd like to admit searching for a response and couldn't find one. I installed Teradata ODBC Driver 16.20. In the ODBC Data Source Administrator, I added a Data Source. I named it teradata, put in the name of the Teradata Server to connect to and my username and password for authentication. When I tried running  the following code in RStudio:
con &lt;- DBI::dbConnect(odbc::odbc(),
                      &quot;teradata&quot;)

I would get an error:
Error: nanodbc/nanodbc.cpp:1021: HY000: [Teradata][WSock32 DLL] (434) WSA E TimedOut: No response received when attempting to connect to the Teradata server

",1
561,70864450,Why I am not able to deploy my Jar file via eclipse plugin for Teradata,"I installed the eclipse plugin for Teradata and try to follow the following tutorial.
When I try to deploy my jar file I see the error User does not have EXECUTE PROCEDURE access to SQLJ.REPLACE_Jar.
I also grant access to user over the database by using the following queries.
GRANT CREATE PROCEDURE ON Project TO user1;
GRANT EXECUTE PROCEDURE ON Project TO user1;
GRANT CREATE EXTERNAL PROCEDURE ON Project TO user1;
     
GRANT ALL PRIVILEGES ON Project TO user1; 

These queries run successfully but still, I am not able to deploy my jar file. Any help will be appreciated.
Thanks a lot.

Picture of Doc

",-1,-1,-1.0,"I installed the eclipse plugin for Teradata and try to follow the following tutorial.
When I try to deploy my jar file I see the error User does not have EXECUTE PROCEDURE access to SQLJ.REPLACE_Jar.
I also grant access to user over the database by using the following queries.
GRANT CREATE PROCEDURE ON Project TO user1;
GRANT EXECUTE PROCEDURE ON Project TO user1;
GRANT CREATE EXTERNAL PROCEDURE ON Project TO user1;
     
GRANT ALL PRIVILEGES ON Project TO user1; 

These queries run successfully but still, I am not able to deploy my jar file. Any help will be appreciated.
Thanks a lot.

Picture of Doc

",3
562,70867218,(-2621) Bad character in format or data of column X when loading data from python to teradata,"I am trying to insert new data values into an existing table in Teradata from python. I was able to establish the connection but then it gave me this error: [Teradata database] (-2621) Bad character in format or data of df.columnX')
Below is the value I am trying to load in:
Column A    Column B     Column X
IH78        0.00          39
SK901       0.00          NaN

Code that I used (ref: Connecting Python with Teradata using Teradata module):
import teradata
import pandas as pd
import numpy as np

udaExec = teradata.UdaExec (appName=&quot;test&quot;, version=&quot;1.0&quot;, logConsole=False)

with udaExec.connect(method=&quot;odbc&quot;,system=&quot;DBName&quot;, username=&quot;UserName&quot;,
                      password=&quot;Password&quot;, driver=&quot;DriverName&quot;) as connect:

    #We can divide our huge_df to small chuncks. E.g. 100 churchs
    chunks_df = np.array_split(huge_df, 100)

    #Import chuncks to Teradata
    for i,_ in enumerate(chunks_df):

        data = [tuple(x) for x in chuncks_df[i].to_records(index=False)]
        connect.executemany(&quot;INSERT INTO DATABASE.TABLEWITH5COL values(?,?,?)&quot;,data,batch=True)


type in python for Column X: Int32
type in teradata for Column X: Integer

I can load the first line in with no issue at all but it is the second one that returns me an error message. Thank you all and any help would be appreciated!
",-1,-1,-1.0,"I am trying to insert new data values into an existing table in Teradata from python. I was able to establish the connection but then it gave me this error: [Teradata database] (-2621) Bad character in format or data of df.columnX')
Below is the value I am trying to load in:
Column A    Column B     Column X
IH78        0.00          39
SK901       0.00          NaN

Code that I used (ref: Connecting Python with Teradata using Teradata module):
import teradata
import pandas as pd
import numpy as np

udaExec = teradata.UdaExec (appName=&quot;test&quot;, version=&quot;1.0&quot;, logConsole=False)

with udaExec.connect(method=&quot;odbc&quot;,system=&quot;DBName&quot;, username=&quot;UserName&quot;,
                      password=&quot;Password&quot;, driver=&quot;DriverName&quot;) as connect:

    #We can divide our huge_df to small chuncks. E.g. 100 churchs
    chunks_df = np.array_split(huge_df, 100)

    #Import chuncks to Teradata
    for i,_ in enumerate(chunks_df):

        data = [tuple(x) for x in chuncks_df[i].to_records(index=False)]
        connect.executemany(&quot;INSERT INTO DATABASE.TABLEWITH5COL values(?,?,?)&quot;,data,batch=True)


type in python for Column X: Int32
type in teradata for Column X: Integer

I can load the first line in with no issue at all but it is the second one that returns me an error message. Thank you all and any help would be appreciated!
",1
563,70885272,Gateway error when connecting Python to Teradata,"Can anyone share some information regarding the issue below? I have been able to connect to Teradata via Python using Teradata and Teradatasql libraries. I cannot connect using pyodbc and sqlalchemy.
Python code:
from sqlalchemy import create_engine

engine=create_engine('teradata://'+username+':'+password+'@'+server+'/'+database+'?driver='+driver)
conn = engine.connect()

Error:
DatabaseError: (teradata.api.DatabaseError) (135, '[HY000] [Teradata][ODBC Teradata Driver] (135) Neither TLS port nor Legacy Port has any response. Please check Teradata Database Gateway configurations., [Teradata][ODBC Teradata Driver] (135) Neither TLS port nor Legacy Port has any response. Please check Teradata Database Gateway configurations.')
(Background on this error at: http://sqlalche.me/e/14/4xp6)

",-1,-1,-1.0,"Can anyone share some information regarding the issue below? I have been able to connect to Teradata via Python using Teradata and Teradatasql libraries. I cannot connect using pyodbc and sqlalchemy.
Python code:
from sqlalchemy import create_engine

engine=create_engine('teradata://'+username+':'+password+'@'+server+'/'+database+'?driver='+driver)
conn = engine.connect()

Error:
DatabaseError: (teradata.api.DatabaseError) (135, '[HY000] [Teradata][ODBC Teradata Driver] (135) Neither TLS port nor Legacy Port has any response. Please check Teradata Database Gateway configurations., [Teradata][ODBC Teradata Driver] (135) Neither TLS port nor Legacy Port has any response. Please check Teradata Database Gateway configurations.')
(Background on this error at: http://sqlalche.me/e/14/4xp6)

",1
564,70945172,How to set up .net core teradata connection in c#?,"I am trying to connect to Teradata with dotnet core driver: Teradata.Client.Provider 17.10.2.
I have set-up a Teradata Vantage Express instance and I am able to connect to it via DBeaver (JDBC) with default credentials dbc/dbc.
Based on docs and this answer https://stackoverflow.com/a/21267030/3120219 I'm now trying to connect via a C# driver with the following code:
using System;
using Teradata.Client.Provider;
using Xunit;

namespace MyProject.Test
{
  public class TeradataConnectionTest
  {
    public void TestConnection()
    {
      var connectionStringBuilder = new TdConnectionStringBuilder
      {
        DataSource = &quot;localhost&quot;,
        Database = &quot;dbc&quot;,
        UserId = &quot;dbc&quot;,
        Password = &quot;dbc&quot;,
        AuthenticationMechanism = &quot;TD2&quot; // Tried also &quot;LDAP&quot; and &quot;TDNEGO&quot;
      };

      using TdConnection cn = new TdConnection();
      cn.ConnectionString = connectionStringBuilder.ConnectionString;
      cn.Open();  // exception here

      TdCommand cmd = cn.CreateCommand();
      cmd.CommandText = &quot;SELECT DATE&quot;;

      using (TdDataReader reader = cmd.ExecuteReader())
      {
        reader.Read();
        DateTime date = reader.GetDate(0);
      }
    }
  }
}

But I'm receiving this exception on connection open:
Teradata.Client.Provider.TdException
[.NET Data Provider for Teradata] [115057] The WebSocket handshake response is invalid. Details: Unexpected or invalid response received.
HTTP/1.1 302 Found
Cache-Control: public, no-store, max-age=0
X-Content-Type-Options: nosniff
X-Frame-Options: SAMEORIGIN
X-XSS-Protection: 1; mode=block
Content-Security-Policy: default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data:
Strict-Transport-Security: max-age=15768000; includeSubDomains
Location: /login.html
Transfer-Encoding: chunked
Date: Tue, 01 Feb 2022 17:30:07 GMT
Server: Teradata-Viewpoint

It seems that the client is trying to connect to the webserver instead of the data provider services.
What am I missing ?
",1,-1,-1.0,"I am trying to connect to Teradata with dotnet core driver: Teradata.Client.Provider 17.10.2.
I have set-up a Teradata Vantage Express instance and I am able to connect to it via DBeaver (JDBC) with default credentials dbc/dbc.
Based on docs and this answer https://stackoverflow.com/a/21267030/3120219 I'm now trying to connect via a C# driver with the following code:
using System;
using Teradata.Client.Provider;
using Xunit;

namespace MyProject.Test
{
  public class TeradataConnectionTest
  {
    public void TestConnection()
    {
      var connectionStringBuilder = new TdConnectionStringBuilder
      {
        DataSource = &quot;localhost&quot;,
        Database = &quot;dbc&quot;,
        UserId = &quot;dbc&quot;,
        Password = &quot;dbc&quot;,
        AuthenticationMechanism = &quot;TD2&quot; // Tried also &quot;LDAP&quot; and &quot;TDNEGO&quot;
      };

      using TdConnection cn = new TdConnection();
      cn.ConnectionString = connectionStringBuilder.ConnectionString;
      cn.Open();  // exception here

      TdCommand cmd = cn.CreateCommand();
      cmd.CommandText = &quot;SELECT DATE&quot;;

      using (TdDataReader reader = cmd.ExecuteReader())
      {
        reader.Read();
        DateTime date = reader.GetDate(0);
      }
    }
  }
}

But I'm receiving this exception on connection open:
Teradata.Client.Provider.TdException
[.NET Data Provider for Teradata] [115057] The WebSocket handshake response is invalid. Details: Unexpected or invalid response received.
HTTP/1.1 302 Found
Cache-Control: public, no-store, max-age=0
X-Content-Type-Options: nosniff
X-Frame-Options: SAMEORIGIN
X-XSS-Protection: 1; mode=block
Content-Security-Policy: default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data:
Strict-Transport-Security: max-age=15768000; includeSubDomains
Location: /login.html
Transfer-Encoding: chunked
Date: Tue, 01 Feb 2022 17:30:07 GMT
Server: Teradata-Viewpoint

It seems that the client is trying to connect to the webserver instead of the data provider services.
What am I missing ?
",1
565,70973241,pyodbc.Error for Teradata connection with Visual c++ redistributable,"I have some Python code (passed to me) that use pyodbc to connect to Teradata.
It gives me an error message
pyodbc.Error: ('HY000', '[HY000] [Teradata][ODBC Teradata Driver] (77)  Visual C++ Redistributable 2012 Update 4 required. (77) (SQLDriverConnect); [HY000] [Teradata][ODBC Teradata Driver] (77)  Visual C++ Redistributable 2012 Update 4 required. (77)')
I have windows 10 Enterprise, installed 4/2021, with 64bit operating system.
I checked the installed programs through the control panel, it listed many versions of Visual C++ Redistributable (including 2005, 2008, 2010, 2012, 2013, 2015, 2017), and I also installed the Microsoft Visual C++ 2015 Build Tools after getting this error.
I have Teradata Tools and Utilities -Base 16.20.29 installed.
In the ODBC Admin (32bit), I am able to set up the dsn for the Teradata DB, as the Teradata Database ODBC Driver 16.20 is listed in the drivers. With the configuration, I can click &quot;test&quot; and it is successful.
But in the ODBC Admin (64bit), I set up the dsn for the Teradata DB as well, as the Teradata Database ODBC Driver 16.20 is listed in the drivers. With the configuration, if I  click &quot;test&quot;, it says  &quot;Failed. [Teradata][ODBC Teradata Driver] (77)  Visual C++ Redistributable 2012 Update 4 required.&quot;
And I able to use the Teradata SQL Assistant to connect to the Teradata DB with the DSN I set up.
Why is this happening? What should I do to connect to Teradata with Python?
",-1,-1,-1.0,"I have some Python code (passed to me) that use pyodbc to connect to Teradata.
It gives me an error message
pyodbc.Error: ('HY000', '[HY000] [Teradata][ODBC Teradata Driver] (77)  Visual C++ Redistributable 2012 Update 4 required. (77) (SQLDriverConnect); [HY000] [Teradata][ODBC Teradata Driver] (77)  Visual C++ Redistributable 2012 Update 4 required. (77)')
I have windows 10 Enterprise, installed 4/2021, with 64bit operating system.
I checked the installed programs through the control panel, it listed many versions of Visual C++ Redistributable (including 2005, 2008, 2010, 2012, 2013, 2015, 2017), and I also installed the Microsoft Visual C++ 2015 Build Tools after getting this error.
I have Teradata Tools and Utilities -Base 16.20.29 installed.
In the ODBC Admin (32bit), I am able to set up the dsn for the Teradata DB, as the Teradata Database ODBC Driver 16.20 is listed in the drivers. With the configuration, I can click &quot;test&quot; and it is successful.
But in the ODBC Admin (64bit), I set up the dsn for the Teradata DB as well, as the Teradata Database ODBC Driver 16.20 is listed in the drivers. With the configuration, if I  click &quot;test&quot;, it says  &quot;Failed. [Teradata][ODBC Teradata Driver] (77)  Visual C++ Redistributable 2012 Update 4 required.&quot;
And I able to use the Teradata SQL Assistant to connect to the Teradata DB with the DSN I set up.
Why is this happening? What should I do to connect to Teradata with Python?
",1
566,71294041,Teradata: Average of date difference truncates decimal,"Teradata: I have Start_Timestamp and End_Timestamp and I need to find the average Avg_Duration for every ID.
Dataset looks like this:
--------------------------------------------------------------------
|ID     | Start_Timestamp             |End_Timestamp               |
--------------------------------------------------------------------
|111    |2021-08-25 19:37:51.327000   |2021-08-26 16:25:51.129600  |
--------------------------------------------------------------------
|111    |2021-07-16 06:17:23.124000   |2021-07-19 13:16:53.185350  |
--------------------------------------------------------------------
|111    |2021-06-22 10:11:21.754400   |2021-02-25 18:48:13.614650  |
--------------------------------------------------------------------

I ran the following code:
SELECT ID, AVG((End_Timestamp - Start_Timestamp) DAY ) AS Avg_Duration
FROM Table_A
Group By ID;

The result looks like this:
-----------------------
|ID     | Aug_Duration|
-----------------------
|111    |            2|
-----------------------

I expected Avg_Duration to be (1+3+3)/3=2.33.
I am aware that integer division gets truncated. So, I CAST the Timestamp difference before I took average to convert Integer into Decimal. My code for that is:
SELECT ID, AVG(CAST((End_Timestamp - Start_Timestamp) DAY AS DECIMAL(2,2))) AS Avg_Duration
FROM Table_A
Group By ID;

I expected 2.33, but, now TeraData window is unresponsive/idle. It neither runs nor I get any error message.
Can someone tell me where am I making a mistake and how can I get Avg_Duration with 2 decimal places and not a truncated one?
",-1,-1,-1.0,"Teradata: I have Start_Timestamp and End_Timestamp and I need to find the average Avg_Duration for every ID.
Dataset looks like this:
--------------------------------------------------------------------
|ID     | Start_Timestamp             |End_Timestamp               |
--------------------------------------------------------------------
|111    |2021-08-25 19:37:51.327000   |2021-08-26 16:25:51.129600  |
--------------------------------------------------------------------
|111    |2021-07-16 06:17:23.124000   |2021-07-19 13:16:53.185350  |
--------------------------------------------------------------------
|111    |2021-06-22 10:11:21.754400   |2021-02-25 18:48:13.614650  |
--------------------------------------------------------------------

I ran the following code:
SELECT ID, AVG((End_Timestamp - Start_Timestamp) DAY ) AS Avg_Duration
FROM Table_A
Group By ID;

The result looks like this:
-----------------------
|ID     | Aug_Duration|
-----------------------
|111    |            2|
-----------------------

I expected Avg_Duration to be (1+3+3)/3=2.33.
I am aware that integer division gets truncated. So, I CAST the Timestamp difference before I took average to convert Integer into Decimal. My code for that is:
SELECT ID, AVG(CAST((End_Timestamp - Start_Timestamp) DAY AS DECIMAL(2,2))) AS Avg_Duration
FROM Table_A
Group By ID;

I expected 2.33, but, now TeraData window is unresponsive/idle. It neither runs nor I get any error message.
Can someone tell me where am I making a mistake and how can I get Avg_Duration with 2 decimal places and not a truncated one?
",3
567,71500007,facing issue with python while connecting to teradata,"Traceback (most recent call last):
  File &quot;/Users/GA20081466/Desktop/LH_Imeitool/scripts/replication.py&quot;, line 93, in &lt;module&gt;
    with teradatasql.connect(host=&quot;xxxx&quot;, user=&quot;xxxx&quot;,
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/teradatasql/__init__.py&quot;, line 138, in __init__
    goside = ctypes.cdll.LoadLibrary(sLibPathName)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ctypes/__init__.py&quot;, line 452, in LoadLibrary
    return self._dlltype(name)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ctypes/__init__.py&quot;, line 374, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/teradatasql/teradatasql.dylib, 0x0006): tried: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/teradatasql/teradatasql.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/usr/lib/teradatasql.dylib' (no such file)

",-1,-1,-1.0,"Traceback (most recent call last):
  File &quot;/Users/GA20081466/Desktop/LH_Imeitool/scripts/replication.py&quot;, line 93, in &lt;module&gt;
    with teradatasql.connect(host=&quot;xxxx&quot;, user=&quot;xxxx&quot;,
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/teradatasql/__init__.py&quot;, line 138, in __init__
    goside = ctypes.cdll.LoadLibrary(sLibPathName)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ctypes/__init__.py&quot;, line 452, in LoadLibrary
    return self._dlltype(name)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ctypes/__init__.py&quot;, line 374, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/teradatasql/teradatasql.dylib, 0x0006): tried: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/teradatasql/teradatasql.dylib' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/usr/lib/teradatasql.dylib' (no such file)

",1
568,71531898,How to get difference of 2 dates in Toad Teradata SQL,"I'm trying to get the difference between a due date and the system date in Teradata SQL using Toad.
SELECT
RECORD_ID,
DUE_DATE,
(DUE_DATE - CURRENT_DATE) DaysDiff

FROM TABLENAME

It returns an error:
&quot;Invalid operation for DateTime or Interval.&quot;
",-1,-1,-1.0,"I'm trying to get the difference between a due date and the system date in Teradata SQL using Toad.
SELECT
RECORD_ID,
DUE_DATE,
(DUE_DATE - CURRENT_DATE) DaysDiff

FROM TABLENAME

It returns an error:
&quot;Invalid operation for DateTime or Interval.&quot;
",3
569,71204958,Storing a 55k+ string in Teradata using the .net client,"Here we are on day 3 of trying to figure out how to store a very large string into Teradata. I am not going to lie. Last night I took an extra melatonin pill hoping it would elevate me to seer level today but unfortunately, I haven't been able to figure out a solution to this issue other than splitting the string in half and storing it in 2 different fields -which I don't like-.
I am attempting to store a 55k+ length of a string in Teradata by using the .net class they have on nuget.
Scenario;
I call a webservice and I receive this very large string which is a base64 encoded image. I am supposed to store this string and this is my .net code / Tdcommand thus far;
cmd.CommandText = &quot;INSERT INTO PL1_STAGING.CRS_PHOTO_STG (... MAIN_PHOTO ...) VALUES (... &lt;base64string&gt;...);&quot;

Teradata responds with; 3738 String is longer than 31000 characters.
The Teradata documentation suggests; To insert longer strings, the user must have a USING clause and a DATA parcel that contain the characters.
I have attempted that but I haven't been able to import it successfully using a text file. While I am not an expert on Teradata, I have tried to use a Blob data type in my .net code but I haven't been able to convert the base64 string to blob in .net so that I can store it that way.
Thank you for the time and sorry for the introduction.
",-1,-1,-1.0,"Here we are on day 3 of trying to figure out how to store a very large string into Teradata. I am not going to lie. Last night I took an extra melatonin pill hoping it would elevate me to seer level today but unfortunately, I haven't been able to figure out a solution to this issue other than splitting the string in half and storing it in 2 different fields -which I don't like-.
I am attempting to store a 55k+ length of a string in Teradata by using the .net class they have on nuget.
Scenario;
I call a webservice and I receive this very large string which is a base64 encoded image. I am supposed to store this string and this is my .net code / Tdcommand thus far;
cmd.CommandText = &quot;INSERT INTO PL1_STAGING.CRS_PHOTO_STG (... MAIN_PHOTO ...) VALUES (... &lt;base64string&gt;...);&quot;

Teradata responds with; 3738 String is longer than 31000 characters.
The Teradata documentation suggests; To insert longer strings, the user must have a USING clause and a DATA parcel that contain the characters.
I have attempted that but I haven't been able to import it successfully using a text file. While I am not an expert on Teradata, I have tried to use a Blob data type in my .net code but I haven't been able to convert the base64 string to blob in .net so that I can store it that way.
Thank you for the time and sorry for the introduction.
",3
570,71137905,SAS SEQ+1 in Teradata,"I am converting following code in SAS to Teradata
DATA T_SEQ;
SET Macro_SEQ;
SEQ+1;
by Sec_key;
IF FIRST.Sec_key then SEQ=1;

My teradata script is
select *, Row_Number() over (partition by Sec_key Order by SEC_key) as SEQ
from T_SEQ

Problem is teradata script is not giving me the same ouput like SAS Script and I am getting different sequence number in teradata output as compare to SAS Script.
Basically I need this type of SEQ
---------  -------------------- ---------------  
Australia            1421810.92    1  
Canada               2604540.71    1  
Canada               1453719.46    2  
Central              3189418.36    1  
France               3121616.32    1

",-1,-1,-1.0,"I am converting following code in SAS to Teradata
DATA T_SEQ;
SET Macro_SEQ;
SEQ+1;
by Sec_key;
IF FIRST.Sec_key then SEQ=1;

My teradata script is
select *, Row_Number() over (partition by Sec_key Order by SEC_key) as SEQ
from T_SEQ

Problem is teradata script is not giving me the same ouput like SAS Script and I am getting different sequence number in teradata output as compare to SAS Script.
Basically I need this type of SEQ
---------  -------------------- ---------------  
Australia            1421810.92    1  
Canada               2604540.71    1  
Canada               1453719.46    2  
Central              3189418.36    1  
France               3121616.32    1

",3
571,71134933,Assigning Parent Number to Child Number in Teradata,"I've been trying to assign parent case number to child case number using Teradata SQL. I have the table
Patient Table




PatientID
Cal_Date
FirstVisitCreateTs
Location
VisitNum
VisitFlag
Comments




1234
2021-09-22
2021-09-22 10:30:23
Ground
101112
0
First Visit ID opened


1234
2021-09-23
2021-09-22 10:30:23
Ground
101112
0
First Visit ID opened


1234
2021-09-24
2021-09-24 11:42:41
Ground
111213
0
Second Visit ID opened before First getting closed


1234
2021-09-25
2021-09-24 11:42:41
Ground
111213
1
Second Visit ID closed


1234
2021-09-25
2021-09-22 10:30:23
Ground
101112
1
First Visit ID closed


1234
2021-10-02
2021-10-02 02:15:34
Ground
121314
0
Third Visit ID opened


1234
2021-10-03
2021-10-02 02:15:34
Ground
121314
1
Third Visit ID closed




And my desired output should be like
Output Table




PatientID
Cal_Date
FirstVisitCreateTs
Location
VisitNum
VisitFlag
ParentVisitNum
Comments




1234
2021-09-22
2021-09-22 10:30:23
Ground
101112
0
NULL
First Visit ID opened


1234
2021-09-23
2021-09-22 10:30:23
Ground
101112
0
NULL
First Visit ID opened


1234
2021-09-24
2021-09-24 11:42:41
Ground
111213
0
NULL
Second Visit ID opened before First getting closed


1234
2021-09-25
2021-09-24 11:42:41
Ground
111213
1
101112
Second Visit ID closed


1234
2021-09-25
2021-09-22 10:30:23
Ground
101112
1
101112
First Visit ID closed


1234
2021-10-02
2021-10-02 02:15:34
Ground
121314
0
NULL
Third Visit ID opened


1234
2021-10-03
2021-10-02 02:15:34
Ground
121314
1
121314
Third Visit ID closed




Key points:

ParentVisitNum is assigned to child when first VisitNum flag is not closed (0-Open,1-Closed).
Child is defined as any New VisitNum that is opened before the First VisitNum getting closed.

In this example: For the PatientID (1234), the first VisitNum is 101112 which is opened on 2021-09-22 and another VisitNum (111213) is opened on 2021-09-24 for the same PatientID (1234) before getting the first(parent) VisitNum closed (VisitFlag=1). In this case we will have to assign ParentVisitNum as 101112 to 111213 when it is closed(VisitNum=1).
I've tried many different TD SQL queries but found no luck. Any help would be greatly appreciated.
",-1,-1,-1.0,"I've been trying to assign parent case number to child case number using Teradata SQL. I have the table
Patient Table




PatientID
Cal_Date
FirstVisitCreateTs
Location
VisitNum
VisitFlag
Comments




1234
2021-09-22
2021-09-22 10:30:23
Ground
101112
0
First Visit ID opened


1234
2021-09-23
2021-09-22 10:30:23
Ground
101112
0
First Visit ID opened


1234
2021-09-24
2021-09-24 11:42:41
Ground
111213
0
Second Visit ID opened before First getting closed


1234
2021-09-25
2021-09-24 11:42:41
Ground
111213
1
Second Visit ID closed


1234
2021-09-25
2021-09-22 10:30:23
Ground
101112
1
First Visit ID closed


1234
2021-10-02
2021-10-02 02:15:34
Ground
121314
0
Third Visit ID opened


1234
2021-10-03
2021-10-02 02:15:34
Ground
121314
1
Third Visit ID closed




And my desired output should be like
Output Table




PatientID
Cal_Date
FirstVisitCreateTs
Location
VisitNum
VisitFlag
ParentVisitNum
Comments




1234
2021-09-22
2021-09-22 10:30:23
Ground
101112
0
NULL
First Visit ID opened


1234
2021-09-23
2021-09-22 10:30:23
Ground
101112
0
NULL
First Visit ID opened


1234
2021-09-24
2021-09-24 11:42:41
Ground
111213
0
NULL
Second Visit ID opened before First getting closed


1234
2021-09-25
2021-09-24 11:42:41
Ground
111213
1
101112
Second Visit ID closed


1234
2021-09-25
2021-09-22 10:30:23
Ground
101112
1
101112
First Visit ID closed


1234
2021-10-02
2021-10-02 02:15:34
Ground
121314
0
NULL
Third Visit ID opened


1234
2021-10-03
2021-10-02 02:15:34
Ground
121314
1
121314
Third Visit ID closed




Key points:

ParentVisitNum is assigned to child when first VisitNum flag is not closed (0-Open,1-Closed).
Child is defined as any New VisitNum that is opened before the First VisitNum getting closed.

In this example: For the PatientID (1234), the first VisitNum is 101112 which is opened on 2021-09-22 and another VisitNum (111213) is opened on 2021-09-24 for the same PatientID (1234) before getting the first(parent) VisitNum closed (VisitFlag=1). In this case we will have to assign ParentVisitNum as 101112 to 111213 when it is closed(VisitNum=1).
I've tried many different TD SQL queries but found no luck. Any help would be greatly appreciated.
",3
572,71570157,Read data from Teradata DB in Azure Spark,"I want to read and create a data frame of data from my Teradata Database, in azure spark. I have uploaded terajdbc4.jar in my spark cluster, and my code is as follows
val jdbcDF = sqlContext.load(&quot;jdbc&quot;, Map(
  &quot;url&quot; -&gt; &quot;jdbc:teradata://&lt;server_name&gt;, TMODE=TERA, user=my_user, password=*****&quot;,
  &quot;dbtable&quot; -&gt; &quot;schema.table_name&quot;,
  &quot;driver&quot; -&gt; &quot;com.teradata.jdbc.TeraDriver&quot;))

But I am getting errors as

&quot;Name or service not known at
java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)&quot;.

My server name is ecdwprod.na.ko.com, which is correct, But I don't know why I am facing this issue. Can anyone please help?
",-1,-1,-1.0,"I want to read and create a data frame of data from my Teradata Database, in azure spark. I have uploaded terajdbc4.jar in my spark cluster, and my code is as follows
val jdbcDF = sqlContext.load(&quot;jdbc&quot;, Map(
  &quot;url&quot; -&gt; &quot;jdbc:teradata://&lt;server_name&gt;, TMODE=TERA, user=my_user, password=*****&quot;,
  &quot;dbtable&quot; -&gt; &quot;schema.table_name&quot;,
  &quot;driver&quot; -&gt; &quot;com.teradata.jdbc.TeraDriver&quot;))

But I am getting errors as

&quot;Name or service not known at
java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)&quot;.

My server name is ecdwprod.na.ko.com, which is correct, But I don't know why I am facing this issue. Can anyone please help?
",3
573,71572692,Teradata Driver not available in Spark Context,"I am currently setting up a Package, where Spark Jobs can be tested and debugged locally before they are deployed to Databricks. For this I created a Docker Image with all the neccessary Pyspark libraries and within in the Docker Image it is possible to work with pyspark dataframes. It is also possible to get data from Databases where the Driver can be installed via Maven (I have tested it with the Kusto Database).
Now I would like to access data from the Teradata Database with pyspark. For Teradata there is no Maven package available, only a .jar file which you get from the Teradata webpage. In pyspark you can add those jars with:

    spark_conf = SparkConf()
    spark_conf.setAll([
                (&quot;spark.jars&quot;, &quot;/usr/app/terajdbc4.jar&quot;)
                ])
    
    # here a warning is send out
    #22/03/22 12:56:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your #platform... using builtin-java classes where applicable
    #Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    #Setting default log level to &quot;WARN&quot;.
    #To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    
    spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()
    
    ## here the error occurs
    spark_df = spark.read.format(&quot;jdbc&quot;) \
                .option(&quot;driver&quot;, &quot;com.teradata.jdbc.TeraDriver&quot;) \
                .option(&quot;url&quot;, f&quot;jdbc:teradata://{self.dbhostip}/LOGMECH=LDAP&quot;) \
                .option(&quot;dbtable&quot;, query) \
                .option(&quot;user&quot;, self.username) \
                .option(&quot;password&quot;, self.password) \
                .option(&quot;fetchsize&quot;, 10000) \
                .option(&quot;numPartitions&quot;, partitions) \
                .load()


However the spark.read runs into an error:
&gt; Traceback (most recent call last):
  File &quot;/databricks/python3/lib/python3.8/site-packages/pyspark/sql/utils.py&quot;, line 111, in deco
    return f(*a, **kw)
  File &quot;/databricks/python3/lib/python3.8/site-packages/py4j/protocol.py&quot;, line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o87.load.
: java.lang.NullPointerException
    at java.util.Hashtable.put(Hashtable.java:460)
    at java.util.Properties.setProperty(Properties.java:166)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$asProperties$1(JDBCOptions.scala:51)
    at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:234)
    at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:468)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:51)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:38)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)
    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)
    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)
    at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)
    at scala.Option.getOrElse(Option.scala:189)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)

The same jar is used in Databricks and is there installed via the Databricks Installation functionality. There the connection is working perfectly fine.
From the Docker I can also connect to the teradata with python code via some python package.
From Docker the connection with Kusto and Pyspark is also working fine:

    # Here no warning is send out (compare to .jar init)
    spark = SparkSession.builder.config(
            &quot;spark.jars.packages&quot;, &quot;com.microsoft.azure.kusto:kusto-spark_3.0_2.12:2.7.3&quot;
            ).getOrCreate()
    
    kusto_df = spark.read. \
            format(&quot;com.microsoft.kusto.spark.datasource&quot;). \
            option(&quot;kustoCluster&quot;, self.cluster). \
            option(&quot;kustoDatabase&quot;, self.database). \
            option(&quot;kustoQuery&quot;, query). \
            option(&quot;kustoAadAppId&quot;, self.client_id). \
            option(&quot;kustoAadAppSecret&quot;, self.client_secret). \
            option(&quot;kustoAadAuthorityID&quot;, self.authority_id). \
            load().repartition(partitions)


Now the big question is, why is it working on Databricks to use the Teradata driver and not locally?
",-1,-1,-1.0,"I am currently setting up a Package, where Spark Jobs can be tested and debugged locally before they are deployed to Databricks. For this I created a Docker Image with all the neccessary Pyspark libraries and within in the Docker Image it is possible to work with pyspark dataframes. It is also possible to get data from Databases where the Driver can be installed via Maven (I have tested it with the Kusto Database).
Now I would like to access data from the Teradata Database with pyspark. For Teradata there is no Maven package available, only a .jar file which you get from the Teradata webpage. In pyspark you can add those jars with:

    spark_conf = SparkConf()
    spark_conf.setAll([
                (&quot;spark.jars&quot;, &quot;/usr/app/terajdbc4.jar&quot;)
                ])
    
    # here a warning is send out
    #22/03/22 12:56:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your #platform... using builtin-java classes where applicable
    #Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    #Setting default log level to &quot;WARN&quot;.
    #To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    
    spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()
    
    ## here the error occurs
    spark_df = spark.read.format(&quot;jdbc&quot;) \
                .option(&quot;driver&quot;, &quot;com.teradata.jdbc.TeraDriver&quot;) \
                .option(&quot;url&quot;, f&quot;jdbc:teradata://{self.dbhostip}/LOGMECH=LDAP&quot;) \
                .option(&quot;dbtable&quot;, query) \
                .option(&quot;user&quot;, self.username) \
                .option(&quot;password&quot;, self.password) \
                .option(&quot;fetchsize&quot;, 10000) \
                .option(&quot;numPartitions&quot;, partitions) \
                .load()


However the spark.read runs into an error:
&gt; Traceback (most recent call last):
  File &quot;/databricks/python3/lib/python3.8/site-packages/pyspark/sql/utils.py&quot;, line 111, in deco
    return f(*a, **kw)
  File &quot;/databricks/python3/lib/python3.8/site-packages/py4j/protocol.py&quot;, line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o87.load.
: java.lang.NullPointerException
    at java.util.Hashtable.put(Hashtable.java:460)
    at java.util.Properties.setProperty(Properties.java:166)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$asProperties$1(JDBCOptions.scala:51)
    at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:234)
    at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:468)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:51)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:38)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)
    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)
    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)
    at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)
    at scala.Option.getOrElse(Option.scala:189)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748)

The same jar is used in Databricks and is there installed via the Databricks Installation functionality. There the connection is working perfectly fine.
From the Docker I can also connect to the teradata with python code via some python package.
From Docker the connection with Kusto and Pyspark is also working fine:

    # Here no warning is send out (compare to .jar init)
    spark = SparkSession.builder.config(
            &quot;spark.jars.packages&quot;, &quot;com.microsoft.azure.kusto:kusto-spark_3.0_2.12:2.7.3&quot;
            ).getOrCreate()
    
    kusto_df = spark.read. \
            format(&quot;com.microsoft.kusto.spark.datasource&quot;). \
            option(&quot;kustoCluster&quot;, self.cluster). \
            option(&quot;kustoDatabase&quot;, self.database). \
            option(&quot;kustoQuery&quot;, query). \
            option(&quot;kustoAadAppId&quot;, self.client_id). \
            option(&quot;kustoAadAppSecret&quot;, self.client_secret). \
            option(&quot;kustoAadAuthorityID&quot;, self.authority_id). \
            load().repartition(partitions)


Now the big question is, why is it working on Databricks to use the Teradata driver and not locally?
",0
574,71600258,How to get the list of teradata tables from a particular database using SAS?,"I am trying to get the tables using the following proc sql statement:
proc sql;
    Select tablename from dbc.tables;
run;

I have already defined the teradata and SQL server locations required. The error I get is &quot;File dbc.tables does not exist&quot;
If I run the query in Teradata SQL assistant, it works. Its just that I am unable to do the same using SAS.
",-1,-1,-1.0,"I am trying to get the tables using the following proc sql statement:
proc sql;
    Select tablename from dbc.tables;
run;

I have already defined the teradata and SQL server locations required. The error I get is &quot;File dbc.tables does not exist&quot;
If I run the query in Teradata SQL assistant, it works. Its just that I am unable to do the same using SAS.
",3
575,71607821,Getting a ANY or ALL Error in Teradata SQL Code,"I have a Teradata SQL code
select SUM(col_A)/COUNT(col_A)
from table_A 
where col_B in (select col_C from table_B
                where col_D in ('1','2','3')
                and col_E = 'N'
                and col_F = 'AC'
                and col_G = to_date('12/31/9999','MM/DD/YYYY')
                and col_H like ('P0','Q0')
                and col_I = 'Y'
                and col_J &gt; 0
                and col_K between (current_date-90) and (current_date)

The subquery with col_C runs fine on its own, however when I run the full code, I am getting the
[Error 3785] [SQLState 42000] Expected ANY or ALL is missing error.

I don't understand why the error shows as I don't see any parenthesized list specified as the right operand of a comparison predicate.
",-1,-1,-1.0,"I have a Teradata SQL code
select SUM(col_A)/COUNT(col_A)
from table_A 
where col_B in (select col_C from table_B
                where col_D in ('1','2','3')
                and col_E = 'N'
                and col_F = 'AC'
                and col_G = to_date('12/31/9999','MM/DD/YYYY')
                and col_H like ('P0','Q0')
                and col_I = 'Y'
                and col_J &gt; 0
                and col_K between (current_date-90) and (current_date)

The subquery with col_C runs fine on its own, however when I run the full code, I am getting the
[Error 3785] [SQLState 42000] Expected ANY or ALL is missing error.

I don't understand why the error shows as I don't see any parenthesized list specified as the right operand of a comparison predicate.
",3
576,71618135,How to combine three variables into a date (MM/DD/YYYY) in Teradata,"I have a table with the 3 below columns in Teradata and what to create a date that I can use to filter the table.
    Year Month Days
1   2,016   9   30
2   2,017   2   28
3   2,015   5   31

After the creation of a date from the 3 above columns, the 'date' table should look like so.
    Year Month Days  Date
1   2,016   9   30  9/30/2016
2   2,017   2   28  2/28/2017
3   2,015   5   31  5/31/2015

I have tried TO_DATE, different variations of Cast, etc. but it errors out.
",-1,0,-1.0,"I have a table with the 3 below columns in Teradata and what to create a date that I can use to filter the table.
    Year Month Days
1   2,016   9   30
2   2,017   2   28
3   2,015   5   31

After the creation of a date from the 3 above columns, the 'date' table should look like so.
    Year Month Days  Date
1   2,016   9   30  9/30/2016
2   2,017   2   28  2/28/2017
3   2,015   5   31  5/31/2015

I have tried TO_DATE, different variations of Cast, etc. but it errors out.
",3
577,71620303,How to use a SQL inside Teradata UDF?,"I want to convert a complex query into a reusable and parameterized function.
Thus I am trying to create an UDF in Teradata, but do not understand how to use SQL inside my UDF.
Below is the sample UDF from Oracle database. Can anyone please confirm if &amp; how I can create similar in Teradata ?

   CREATE FUNCTION test_ak (a IN NUMBER) 
   RETURN NUMBER 
   IS b NUMBER(10);
   BEGIN       
          SELECT a+1      
          INTO b
          FROM dual ;     
     RETURN(b);
   END;
   /

",-1,-1,-1.0,"I want to convert a complex query into a reusable and parameterized function.
Thus I am trying to create an UDF in Teradata, but do not understand how to use SQL inside my UDF.
Below is the sample UDF from Oracle database. Can anyone please confirm if &amp; how I can create similar in Teradata ?

   CREATE FUNCTION test_ak (a IN NUMBER) 
   RETURN NUMBER 
   IS b NUMBER(10);
   BEGIN       
          SELECT a+1      
          INTO b
          FROM dual ;     
     RETURN(b);
   END;
   /

",3
578,71707454,Problem to connect teradata by odbc linux driver,"Hello i'm trying to connect to teradata by odbc linux driver, but i have a mistake:
[IM002][unixODBC][Driver Manager]Data source name not found, and no default driver specified
More info:
the driver is : tdodbc1620-16.20.00.127-1.noarch.rpm
odbcinst -j
unixODBC 2.3.1
DRIVERS............: /etc/odbcinst.ini
SYSTEM DATA SOURCES: /etc/odbc.ini
FILE DATA SOURCES..: /etc/ODBCDataSources
USER DATA SOURCES..: /etc/odbc.ini
SQLULEN Size.......: 8
SQLLEN Size........: 8
SQLSETPOSIROW Size.: 8

odbcinst -q -d
[Teradata Database ODBC Driver 16.20]
odbcinst -q -s
[ODBC]
[Teradata_ODBC_DSN}

cat /etc/odbc.ini
[ODBC Data Sources]
Teradata_ODBC_DSN=Teradata Database ODBC Driver 16.20

[Teradata_ODBC_DSN]
Description=Teradata Database ODBC Driver 16.20
Driver=/opt/teradata/client/ODBC_64/lib/tdataodbc_sb64.so
DBCName=Ip
UID=
PWD=

cat /etc/odbcinst.ini
[Teradata Database ODBC Driver 16.20]
Description=Teradata Database ODBC Driver 16.20
Driver=/opt/teradata/client/ODBC_64/lib/tdataodbc_sb64.so
CPTimeout=240

LD_LIBRARY_PATH
:/usr/lib64/:/opt/teradata/client/ODBC_64/lib/

ODBCINI
/etc/odbc.ini

When i execute
isql -v Teradata_ODBC_DSN  (get the error)
[IM002][unixODBC][Driver Manager]Data source name not found, and no default driver 
specified

I don't understand why the datasource not found, let's see if anyone can give me any clues, thanks
",-1,-1,-1.0,"Hello i'm trying to connect to teradata by odbc linux driver, but i have a mistake:
[IM002][unixODBC][Driver Manager]Data source name not found, and no default driver specified
More info:
the driver is : tdodbc1620-16.20.00.127-1.noarch.rpm
odbcinst -j
unixODBC 2.3.1
DRIVERS............: /etc/odbcinst.ini
SYSTEM DATA SOURCES: /etc/odbc.ini
FILE DATA SOURCES..: /etc/ODBCDataSources
USER DATA SOURCES..: /etc/odbc.ini
SQLULEN Size.......: 8
SQLLEN Size........: 8
SQLSETPOSIROW Size.: 8

odbcinst -q -d
[Teradata Database ODBC Driver 16.20]
odbcinst -q -s
[ODBC]
[Teradata_ODBC_DSN}

cat /etc/odbc.ini
[ODBC Data Sources]
Teradata_ODBC_DSN=Teradata Database ODBC Driver 16.20

[Teradata_ODBC_DSN]
Description=Teradata Database ODBC Driver 16.20
Driver=/opt/teradata/client/ODBC_64/lib/tdataodbc_sb64.so
DBCName=Ip
UID=
PWD=

cat /etc/odbcinst.ini
[Teradata Database ODBC Driver 16.20]
Description=Teradata Database ODBC Driver 16.20
Driver=/opt/teradata/client/ODBC_64/lib/tdataodbc_sb64.so
CPTimeout=240

LD_LIBRARY_PATH
:/usr/lib64/:/opt/teradata/client/ODBC_64/lib/

ODBCINI
/etc/odbc.ini

When i execute
isql -v Teradata_ODBC_DSN  (get the error)
[IM002][unixODBC][Driver Manager]Data source name not found, and no default driver 
specified

I don't understand why the datasource not found, let's see if anyone can give me any clues, thanks
",1
579,71819296,Getting column parameter doesn't exist error in Teradata,"update t1
from 
table1 t1,
table2 t2
set
t1.active_ind=0,
t1.row_status_cd='L'
where
t2.col1
is NULL

Getting &quot;Column/ parameter table 1.t1 doesnt exists&quot; error in Teradata SQL though those fields are available in t1 . can anybody help me to resolve this please?
",-1,-1,-1.0,"update t1
from 
table1 t1,
table2 t2
set
t1.active_ind=0,
t1.row_status_cd='L'
where
t2.col1
is NULL

Getting &quot;Column/ parameter table 1.t1 doesnt exists&quot; error in Teradata SQL though those fields are available in t1 . can anybody help me to resolve this please?
",3
580,71824564,Insert timestamp from dataframe in teradata table,"I have a df that i want to move to a teradata table. I am using a framework that was discussed on this platform. However I am getting a error: (-6760) Invalid timestamp.


create table  name (
    time_st TIMESTAMP(0)
    )


current_time = now.strftime(""%m/%d/%Y %H:%M:%S"")
df = df.assign(time=current_time)
df['time'] = pd.to_datetime(df['time']  
print(df)

timestamp in df:
2022-04-11 10:38:44



Can someone help me with timestamp am i going wrong?
",-1,-1,-1.0,"I have a df that i want to move to a teradata table. I am using a framework that was discussed on this platform. However I am getting a error: (-6760) Invalid timestamp.


create table  name (
    time_st TIMESTAMP(0)
    )


current_time = now.strftime(""%m/%d/%Y %H:%M:%S"")
df = df.assign(time=current_time)
df['time'] = pd.to_datetime(df['time']  
print(df)

timestamp in df:
2022-04-11 10:38:44



Can someone help me with timestamp am i going wrong?
",1
581,71837345,Export in parquet file format in Teradata,"I am trying to export data using the TDload utility of Teradata and I need the exported file to be in parquet format.
The command I have used is:
tdload --SourceTdpid xxx.xxx.xxx.xxx --SourceUserName dbc 
--SourceUserPassword dbc --SourceTable DimAccount 
--TargetFilename DimAccount.parquet

But this does not export the data in parquet.
How to achieve it?
",-1,-1,-1.0,"I am trying to export data using the TDload utility of Teradata and I need the exported file to be in parquet format.
The command I have used is:
tdload --SourceTdpid xxx.xxx.xxx.xxx --SourceUserName dbc 
--SourceUserPassword dbc --SourceTable DimAccount 
--TargetFilename DimAccount.parquet

But this does not export the data in parquet.
How to achieve it?
",3
582,71881681,How to Get previous month and current year in Teradata/SQL,"I need a simple TERADATA/SQL statement which gives previous month and current year in the following format. In this case as '03 2022'.
I have tried to solve a similar request for current date in required format as '04/15/2022' by exec the below command

SELECT CURRENT_DATE(FORMAT 'mm/dd/yyyy') (CHAR(12)) as 'New_format'.

But now I am struggling to get exact value as 'mm yyyy' which gives previous month and current year.
I have tried several CAST and CONCAT, but none seems to be working. Please guide me.
Thank you in advance.
",1,-1,-1.0,"I need a simple TERADATA/SQL statement which gives previous month and current year in the following format. In this case as '03 2022'.
I have tried to solve a similar request for current date in required format as '04/15/2022' by exec the below command

SELECT CURRENT_DATE(FORMAT 'mm/dd/yyyy') (CHAR(12)) as 'New_format'.

But now I am struggling to get exact value as 'mm yyyy' which gives previous month and current year.
I have tried several CAST and CONCAT, but none seems to be working. Please guide me.
Thank you in advance.
",3
583,72121911,Python teradatasql lib How to combine use {fn teradata_write_csv} & {teradata_field_sep },"Python  teradatasql lib  How to combine use {fn teradata_write_csv} &amp; {teradata_field_sep }  ?  Thanks !
I use this like pic and show error , I cant find any example.

",-1,-1,-1.0,"Python  teradatasql lib  How to combine use {fn teradata_write_csv} &amp; {teradata_field_sep }  ?  Thanks !
I use this like pic and show error , I cant find any example.

",3
584,72273446,Data Transfer from Hive table to Teradata table,"I have a table (employee) in a Hive database (company) with a record count greater than 41 million. The table is partitioned on current_date column.
select count(*) from company.employee

The result of above query is Hive query editor is: 41,547,896.
Now the main task is to copy this data to a table (employee_td) in Teradata database (company_td).
Below is the code written in PySpark to transfer the data from Hive to Teradata.
# creating a dataframe
df = spark.sql(&quot;select * from company.employee&quot;)

# removing the duplicate records
df = df.distinct()

td_url = 'jdbc:teradata://*****/Database=company_td, LOGMECH=LDAP'

# writing the dataframe to Teradata
df.write.format('jdbc') \
        .option('url', td_url) \
        .option('user', db_user) \
        .option('password', password) \
        .option('dbtable', &quot;employee_td&quot;) \
        .option('driver','com.teradata.jdbc.TeraDriver') \
        .mode('append').save()

When the above code was executed I am facing the following error with some records copied to Teradata table. The number of records getting varied from execution to execution:
py4j.protocol.Py4JJavaError: An error occurred while calling o204.save.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 75 in stage 2.0 failed 4 times, most recent failure: Lost task 75.3 in stage 2.0 (TID 97, *******.***.******.com, executor 39): 
java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 16.10.00.05] [Error 1338] [SQLState HY000] A failure occurred while executing a PreparedStatement batch request. Details of the failure can be found in the exception chain that is accessible with getNextException.

As mentioned in the exception:
Details of the failure can be found in the exception chain that is accessible with getNextException

Can someone help me in raising a detailed exception chain for the above error.
",-1,-1,-1.0,"I have a table (employee) in a Hive database (company) with a record count greater than 41 million. The table is partitioned on current_date column.
select count(*) from company.employee

The result of above query is Hive query editor is: 41,547,896.
Now the main task is to copy this data to a table (employee_td) in Teradata database (company_td).
Below is the code written in PySpark to transfer the data from Hive to Teradata.
# creating a dataframe
df = spark.sql(&quot;select * from company.employee&quot;)

# removing the duplicate records
df = df.distinct()

td_url = 'jdbc:teradata://*****/Database=company_td, LOGMECH=LDAP'

# writing the dataframe to Teradata
df.write.format('jdbc') \
        .option('url', td_url) \
        .option('user', db_user) \
        .option('password', password) \
        .option('dbtable', &quot;employee_td&quot;) \
        .option('driver','com.teradata.jdbc.TeraDriver') \
        .mode('append').save()

When the above code was executed I am facing the following error with some records copied to Teradata table. The number of records getting varied from execution to execution:
py4j.protocol.Py4JJavaError: An error occurred while calling o204.save.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 75 in stage 2.0 failed 4 times, most recent failure: Lost task 75.3 in stage 2.0 (TID 97, *******.***.******.com, executor 39): 
java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 16.10.00.05] [Error 1338] [SQLState HY000] A failure occurred while executing a PreparedStatement batch request. Details of the failure can be found in the exception chain that is accessible with getNextException.

As mentioned in the exception:
Details of the failure can be found in the exception chain that is accessible with getNextException

Can someone help me in raising a detailed exception chain for the above error.
",0
585,72565321,How to load a timestamp variable from SQL Server in SSIS to pull from Teradata?,"Running into an issue where I am getting this error in an SSIS package:
[ODBC Source [52]] Error: Open Database Connectivity (ODBC) error occurred. state: '22008'. Native Error Code: -6760. [Teradata][ODBC Teradata Driver][Teradata Database](-6760)Invalid timestamp.

What I am doing is passing a variable into a Teradata query that is pulled from SQL server.  All housed within a sequence container.
select cast(dateadd(day,1, max(date))AS datetime) DateStart from [table]

the desired output should be:
2022-06-08 00:00:00.000

While this is the result I am passing to the variable, SSIS is kicking this back with the error above.
Pretty stuck here, would love some input from the community.
",-1,-1,-1.0,"Running into an issue where I am getting this error in an SSIS package:
[ODBC Source [52]] Error: Open Database Connectivity (ODBC) error occurred. state: '22008'. Native Error Code: -6760. [Teradata][ODBC Teradata Driver][Teradata Database](-6760)Invalid timestamp.

What I am doing is passing a variable into a Teradata query that is pulled from SQL server.  All housed within a sequence container.
select cast(dateadd(day,1, max(date))AS datetime) DateStart from [table]

the desired output should be:
2022-06-08 00:00:00.000

While this is the result I am passing to the variable, SSIS is kicking this back with the error above.
Pretty stuck here, would love some input from the community.
",3
586,72577973,Snowflake: Conversion error of an teradata query to snow sql,"I have a Teradata query (updated the fields with sample values):
select (case '-' when '-' then '-' ||'04' || ':' ||'00'
else '04' || ':' ||'00'
end (Interval hour to minute)) +
 (case
    '2400' when '2400' then 24
    else 0
end (interval hour));

output : -04:00 (varchar type)
select (case '-' when '-' then '-' ||'04' || ':' ||'00'
else '04' || ':' ||'00'
end (Interval hour to minute)) +
 (case
    '1835' when '2400' then 24
    else 0
end (interval hour));

output: 20:00 (varchar type)
Want to convert the same in snowflake, but the same output value was not able to insert in snowflake varchar column:
SELECT                             
(CASE SUBSTR(raw_data, 48, 1) 
 WHEN '-' THEN CONCAT('-' , SUBSTR(raw_data,49,2) , ':' , SUBSTR(raw_data,51,2))
 ELSE CONCAT(SUBSTR(raw_data,49,2) , ':' , SUBSTR(raw_data,51,2)) END) + 
 (CASE SUBSTR(raw_data,40,4)  WHEN '2400' THEN 24 ELSE 0  END)  
AS COLUMN_1
FROM
    (SELECT  temp_row.$1 as raw_data  from
           @JOB_MANAGEMENT.SNOWFALKE (file_format =&gt; 'DB.TBL_FILE_FORMAT',
            pattern=&gt;'.*/input_file.txt') temp_table) temp;

Sample:
SELECT                             
(CASE '-' WHEN '-' 
THEN CONCAT('-' , '04 , ':' , '00')
ELSE CONCAT('04' , ':' , '00') END)  + 
(CASE '1825'  WHEN '2400' THEN 24 ELSE 0  END)

Output : -04:00 (column type - varchar) -&gt; but throwing error in snowflake.
Numeric value '-04:00' is not recognized
",-1,-1,-1.0,"I have a Teradata query (updated the fields with sample values):
select (case '-' when '-' then '-' ||'04' || ':' ||'00'
else '04' || ':' ||'00'
end (Interval hour to minute)) +
 (case
    '2400' when '2400' then 24
    else 0
end (interval hour));

output : -04:00 (varchar type)
select (case '-' when '-' then '-' ||'04' || ':' ||'00'
else '04' || ':' ||'00'
end (Interval hour to minute)) +
 (case
    '1835' when '2400' then 24
    else 0
end (interval hour));

output: 20:00 (varchar type)
Want to convert the same in snowflake, but the same output value was not able to insert in snowflake varchar column:
SELECT                             
(CASE SUBSTR(raw_data, 48, 1) 
 WHEN '-' THEN CONCAT('-' , SUBSTR(raw_data,49,2) , ':' , SUBSTR(raw_data,51,2))
 ELSE CONCAT(SUBSTR(raw_data,49,2) , ':' , SUBSTR(raw_data,51,2)) END) + 
 (CASE SUBSTR(raw_data,40,4)  WHEN '2400' THEN 24 ELSE 0  END)  
AS COLUMN_1
FROM
    (SELECT  temp_row.$1 as raw_data  from
           @JOB_MANAGEMENT.SNOWFALKE (file_format =&gt; 'DB.TBL_FILE_FORMAT',
            pattern=&gt;'.*/input_file.txt') temp_table) temp;

Sample:
SELECT                             
(CASE '-' WHEN '-' 
THEN CONCAT('-' , '04 , ':' , '00')
ELSE CONCAT('04' , ':' , '00') END)  + 
(CASE '1825'  WHEN '2400' THEN 24 ELSE 0  END)

Output : -04:00 (column type - varchar) -&gt; but throwing error in snowflake.
Numeric value '-04:00' is not recognized
",3
587,72320894,Teradata: On doing PIVOT operation values of columns interchange,"I am having a Table (DB.TAB_UNPIVOTED) on TeraData having millions of rows, and 4 columns - ID, Week, Sales &amp; Profits. Here below is a small microcosm of the problem.
For sake of illustration DB.TAB_UNPIVOTED is like this:

I want to PIVOT this table. Following is my code:
SET SQL_STMT = ('CREATE TABLE DB.TAB_PIVOTED AS
                (   SELECT * FROM DB.TAB_UNPIVOTED
                    PIVOT
                    (   SUM(Sales) AS  Sales, SUM(Profits) AS Profits  
                        FOR KW_Prefix IN (
                                          'CW_1' AS CW_1,
                                          'CW_2' AS CW_2,
                                          'CW_3' AS CW_3
                                         )
                    ) AS dt 
               ) WITH DATA;'
              ) ;
EXECUTE IMMEDIATE SQL_STMT;

The strange thing is that, on PIVOTING, everytime I get a different table DB.TAB_PIVOTED, where values across different columns are interchanged. For eg; one time the output can be:

But, next time, it could be a dfferent result, with values interchanged, though Profits &amp; Sales maintain their pairing. In one output they could be under CW_1, where as on another run, it could be under CW_3:

This problem may not be reproduced on small data, but with my data in millions and CW varing from 1 to 52, I see it all the time.
Does anyone have an idea, where in the pivoting am I making the mistake?
Inputs would be very esteemed.
Update:
I tried running the code directly as well, instead of EXECUTE IMMEDIATE SQL_STMT,but same strange results.
CREATE TABLE DB.TAB_PIVOTED AS
                    (   SELECT * FROM DB.TAB_UNPIVOTED
                        PIVOT
                        (   SUM(Sales) AS  Sales, SUM(Profits) AS Profits  
                            FOR KW_Prefix IN (
                                              'CW_1' AS CW_1,
                                              'CW_2' AS CW_2,
                                              'CW_3' AS CW_3
                                             )
                        ) AS dt 
                   ) WITH DATA;

",-1,-1,-1.0,"I am having a Table (DB.TAB_UNPIVOTED) on TeraData having millions of rows, and 4 columns - ID, Week, Sales &amp; Profits. Here below is a small microcosm of the problem.
For sake of illustration DB.TAB_UNPIVOTED is like this:

I want to PIVOT this table. Following is my code:
SET SQL_STMT = ('CREATE TABLE DB.TAB_PIVOTED AS
                (   SELECT * FROM DB.TAB_UNPIVOTED
                    PIVOT
                    (   SUM(Sales) AS  Sales, SUM(Profits) AS Profits  
                        FOR KW_Prefix IN (
                                          'CW_1' AS CW_1,
                                          'CW_2' AS CW_2,
                                          'CW_3' AS CW_3
                                         )
                    ) AS dt 
               ) WITH DATA;'
              ) ;
EXECUTE IMMEDIATE SQL_STMT;

The strange thing is that, on PIVOTING, everytime I get a different table DB.TAB_PIVOTED, where values across different columns are interchanged. For eg; one time the output can be:

But, next time, it could be a dfferent result, with values interchanged, though Profits &amp; Sales maintain their pairing. In one output they could be under CW_1, where as on another run, it could be under CW_3:

This problem may not be reproduced on small data, but with my data in millions and CW varing from 1 to 52, I see it all the time.
Does anyone have an idea, where in the pivoting am I making the mistake?
Inputs would be very esteemed.
Update:
I tried running the code directly as well, instead of EXECUTE IMMEDIATE SQL_STMT,but same strange results.
CREATE TABLE DB.TAB_PIVOTED AS
                    (   SELECT * FROM DB.TAB_UNPIVOTED
                        PIVOT
                        (   SUM(Sales) AS  Sales, SUM(Profits) AS Profits  
                            FOR KW_Prefix IN (
                                              'CW_1' AS CW_1,
                                              'CW_2' AS CW_2,
                                              'CW_3' AS CW_3
                                             )
                        ) AS dt 
                   ) WITH DATA;

",3
588,72229510,Teradata: IN clause in Pivot can't take data from Table,"I wish to extract a few Calender Weeks from an yearly data. Once that's done, I want to pivot it, so that there is one row for each ID.
We have a table DB.MY_CWs having just one column CW containing the Calender Weeks we are interested in.
The following code extracts the relevant Calender Weeks.
CREATE TABLE DB.MY_TABLE  AS
(
    SELECT ID,
    WeekNumber_Of_Year(Sales_Date)) AS CW,
    AVG(Sales) AS Sales
    FROM DB.DataBase_XYZ
    WHERE CW IN (SELECT CW FROM DB.MY_CWs)
    GROUP BY ID,CW
) WITH DATA;

This Code gives us the output like this:

But, I would like to pivot it so that I get an output like this:

I took the help from code here and ran the following, but TeraData doesn't respond and there is no Error either.
CREATE TABLE DB.MY_TABLE2  AS
(
SELECT *
FROM DB.MY_TABLE
PIVOT
 (SUM(Sales) AS  Sales
  FOR CW IN (SELECT CW FROM DB.MY_CWs)
 ) AS dt 
) WITH DATA;

If instead of (SELECT CW FROM DB.MY_CWs) I would have used (15,16,17), then everything works fine and I would have got the pivoted Table, as shown above.
Can anyone suggest where I am making the mistake?
Many thanks.
",-1,-1,-1.0,"I wish to extract a few Calender Weeks from an yearly data. Once that's done, I want to pivot it, so that there is one row for each ID.
We have a table DB.MY_CWs having just one column CW containing the Calender Weeks we are interested in.
The following code extracts the relevant Calender Weeks.
CREATE TABLE DB.MY_TABLE  AS
(
    SELECT ID,
    WeekNumber_Of_Year(Sales_Date)) AS CW,
    AVG(Sales) AS Sales
    FROM DB.DataBase_XYZ
    WHERE CW IN (SELECT CW FROM DB.MY_CWs)
    GROUP BY ID,CW
) WITH DATA;

This Code gives us the output like this:

But, I would like to pivot it so that I get an output like this:

I took the help from code here and ran the following, but TeraData doesn't respond and there is no Error either.
CREATE TABLE DB.MY_TABLE2  AS
(
SELECT *
FROM DB.MY_TABLE
PIVOT
 (SUM(Sales) AS  Sales
  FOR CW IN (SELECT CW FROM DB.MY_CWs)
 ) AS dt 
) WITH DATA;

If instead of (SELECT CW FROM DB.MY_CWs) I would have used (15,16,17), then everything works fine and I would have got the pivoted Table, as shown above.
Can anyone suggest where I am making the mistake?
Many thanks.
",3
589,72226190,split int col in Teradata,"I was trying to split int column in Teradata but its showing error

*** Failure 9134 Unsupported type detected.

SELECT col1,strtok(col2,0 ,1) FROM table;

",-1,-1,-1.0,"I was trying to split int column in Teradata but its showing error

*** Failure 9134 Unsupported type detected.

SELECT col1,strtok(col2,0 ,1) FROM table;

",3
590,72205660,"Locking Row for Access in Presto SQL, udw in Teradata","Hi I'm trying to access teradata tables using Presto SQL and I get a Locking error. I can't use Locking Row for access in Presto SQL because it doesn't support it. So why is there a locking error in first place?

",-1,-1,-1.0,"Hi I'm trying to access teradata tables using Presto SQL and I get a Locking error. I can't use Locking Row for access in Presto SQL because it doesn't support it. So why is there a locking error in first place?

",3
591,72181000,Attribute error: module 'teradatasql' has no attribute 'connect' please solve this error,"import teradatasql as tds
import pandas as pd
with tds.connect(None,host='',user='',password='
') as connect:
cursor=connect.cursor ()
cursor.execute (&quot;create volatile table voltab (c1 integer, c2 varchar(100)) on commit preserve rows&quot;)
cursor.execute (&quot;insert into voltab (?, ?)&quot;, [
        [1, &quot;abc&quot;],
        [2, &quot;def&quot;],
        [3, &quot;ghi&quot;]])

cursor.execute (&quot;select * from voltab order by 1&quot;)
[ print (row) for row in cur.fetchall () ]
connect.commit()

Traceback (most recent call last):
File &quot;ttt.py&quot;, line 4, in 
with tds.connect(None,host='',user='',paasword='') as connect:
AttributeError: module 'teradatasql' has no attribute 'connect'
",0,-1,-1.0,"import teradatasql as tds
import pandas as pd
with tds.connect(None,host='',user='',password='
') as connect:
cursor=connect.cursor ()
cursor.execute (&quot;create volatile table voltab (c1 integer, c2 varchar(100)) on commit preserve rows&quot;)
cursor.execute (&quot;insert into voltab (?, ?)&quot;, [
        [1, &quot;abc&quot;],
        [2, &quot;def&quot;],
        [3, &quot;ghi&quot;]])

cursor.execute (&quot;select * from voltab order by 1&quot;)
[ print (row) for row in cur.fetchall () ]
connect.commit()

Traceback (most recent call last):
File &quot;ttt.py&quot;, line 4, in 
with tds.connect(None,host='',user='',paasword='') as connect:
AttributeError: module 'teradatasql' has no attribute 'connect'
",1
592,72180777,number pattern matching in Teradata,"I was trying to get the rows matched with a pattern by using like in Teradata
select * from table_name where col_name like '10%';

here I wanted to get all the rows with column values like 1000, 101, 109, 1048
this is showing me error:

*** Failure 3544 Partial string matching requires character operands.
Statement# 1, Info =0

",-1,-1,-1.0,"I was trying to get the rows matched with a pattern by using like in Teradata
select * from table_name where col_name like '10%';

here I wanted to get all the rows with column values like 1000, 101, 109, 1048
this is showing me error:

*** Failure 3544 Partial string matching requires character operands.
Statement# 1, Info =0

",3
593,72605108,Teradata sql assistant code completion aliases not working,"Teradata SQL assistant version 16.20.
Code completion(The drop down suggestion list after i write dots) is not working for me when I use aliases, does anyone know why?
",-1,-1,-1.0,"Teradata SQL assistant version 16.20.
Code completion(The drop down suggestion list after i write dots) is not working for me when I use aliases, does anyone know why?
",3
594,72650851,Teradata script is throwing error while executing it through Jenkins,"REPLACE MACRO Macroname
(
col1 TIMESTAMP(0)
)
AS
(  
    DELETE FROM
        tabe1
    WHERE td_colnm = 'Jenkins';
);

I am getting  the below error in Jenkins on the script execution.
&quot;Reason: liquibase.exception.DatabaseException: [Teradata Database] [TeraJDBC 17.00.00.03] [Error 3707] [SQLState 42000] Syntax error, expected something like ')' between ';' and the end of the request. [Failed SQL: (3707)&quot; REPLACE MACRO Macroname
(
col1 TIMESTAMP(0)
)
AS
(
DELETE FROM
tabe1
WHERE td_colnm = 'Jenkins']
Can anyone provide the solution please?
",-1,-1,-1.0,"REPLACE MACRO Macroname
(
col1 TIMESTAMP(0)
)
AS
(  
    DELETE FROM
        tabe1
    WHERE td_colnm = 'Jenkins';
);

I am getting  the below error in Jenkins on the script execution.
&quot;Reason: liquibase.exception.DatabaseException: [Teradata Database] [TeraJDBC 17.00.00.03] [Error 3707] [SQLState 42000] Syntax error, expected something like ')' between ';' and the end of the request. [Failed SQL: (3707)&quot; REPLACE MACRO Macroname
(
col1 TIMESTAMP(0)
)
AS
(
DELETE FROM
tabe1
WHERE td_colnm = 'Jenkins']
Can anyone provide the solution please?
",0
595,72810468,Can't download Teradata Vantage Express 16.20,"I can't do a simple download of any Teradata software such as Vantage Express 16.20.  I keep getting this message.
access denied
",0,-1,-1.0,"I can't do a simple download of any Teradata software such as Vantage Express 16.20.  I keep getting this message.
access denied
",1
596,72846782,WITH Clause syntax error getting in Teradata,"I am attempting to join multiple tables/views inside with clause but it failing. Please help me.
with a as (
    select
        *
    from
        db.a
),
b as (
    select
        *
    from
        db.b
),
c as (
    select
        * 
    from
        b
    where
        col = 'Hi'
),
d as (
    SELECT
        *
    FROM
        c P
    WHERE
        col = 'Y'
),
e as (
    SELECT
        *
    FROM
        b P
        LEFT JOIN (
            SELECT
                *
            FROM
                a
            WHERE
                col= 'Y'
        ) M ON p.col1= M.col1
    WHERE
         P.Date &gt; current_date - 5
)
select
    *
from
    e

Getting Error any thing wrong in it:

Teradata with clause help
Teradata with clause help
Teradata with clause help
Teradata with clause help
Teradata with clause help

",-1,-1,-1.0,"I am attempting to join multiple tables/views inside with clause but it failing. Please help me.
with a as (
    select
        *
    from
        db.a
),
b as (
    select
        *
    from
        db.b
),
c as (
    select
        * 
    from
        b
    where
        col = 'Hi'
),
d as (
    SELECT
        *
    FROM
        c P
    WHERE
        col = 'Y'
),
e as (
    SELECT
        *
    FROM
        b P
        LEFT JOIN (
            SELECT
                *
            FROM
                a
            WHERE
                col= 'Y'
        ) M ON p.col1= M.col1
    WHERE
         P.Date &gt; current_date - 5
)
select
    *
from
    e

Getting Error any thing wrong in it:

Teradata with clause help
Teradata with clause help
Teradata with clause help
Teradata with clause help
Teradata with clause help

",3
597,72967821,Implement Bucket Partitioning in Cloud Storage from a Teradata Data Source,"Has anyone successfully implemented partitioning in GCS where the source is Teradata? I'm using the WRITE_NOS functionality but I am unable to create the partitions in GCS because I cannot create a key-value pair for the partition name that GCS requires. Consider:
WRITE_NOS OUTPUT: gs://bucket/table/YYYY-MM-DD
GCS Expected: gs://bucket/table/dt=YYYY-MM-DD
I am unable to add the &quot;dt=&quot; before the date value. Can anyone help?
",0,-1,-1.0,"Has anyone successfully implemented partitioning in GCS where the source is Teradata? I'm using the WRITE_NOS functionality but I am unable to create the partitions in GCS because I cannot create a key-value pair for the partition name that GCS requires. Consider:
WRITE_NOS OUTPUT: gs://bucket/table/YYYY-MM-DD
GCS Expected: gs://bucket/table/dt=YYYY-MM-DD
I am unable to add the &quot;dt=&quot; before the date value. Can anyone help?
",3
598,73026519,Connect to Teradata in Python,"I have a script that uses a model and creates a big table for a client, and the last step is to save the result in a Teradata table. Currently I'm using teradatasql in this way: SubCat_p is the final dataframe that I have to save in a table that I've already created
import teradatasql as tsql
with tsql.connect(host=&quot;&lt;hostname&gt;&quot;, 
                    user=&quot;&lt;user_name&gt;&quot;,
                    password=&quot;&lt;user_password&gt;&quot;) as connect:
        with connect.cursor() as cur:
            cur.executemany(f&quot;&quot;&quot;
                    insert into &lt;TABLE_NAME&gt; (CustomerID, ItemSubCategoryID, Est, Comprador)
                    values (?,?,?,?)&quot;&quot;&quot;, SubCat_p.values.tolist())

The script works great with a lot of dataframes but when the result is a big dataframe (order of millons of rows) the script ends with an error
panic: runtime error: gobytes: length out of range

goroutine 17 [running, locked to thread]:
main._Cfunc_GoBytes(...)
        _cgo_gotypes.go:72
main.goCreateRows.func1(0x2?, 0x8924a486?)
        /tmp/13851/goside.go:566 +0x46
main.goCreateRows(0x0, 0x1, 0xc00012de20?, 0x7f98d52199b8?, 0xc00007c000?, 0x7f99740fe890, 0x7f987f8c4f10)
        /tmp/13851/goside.go:566 +0x98
Aborted (core dumped)

Is there an alternative to my method (or to teradatasql) to avoid this error? I found the module JayDeBeApi that works in a similar way but I failed to create the connection (I think it doesn't use a host name but a url that I don't know)
",-1,-1,-1.0,"I have a script that uses a model and creates a big table for a client, and the last step is to save the result in a Teradata table. Currently I'm using teradatasql in this way: SubCat_p is the final dataframe that I have to save in a table that I've already created
import teradatasql as tsql
with tsql.connect(host=&quot;&lt;hostname&gt;&quot;, 
                    user=&quot;&lt;user_name&gt;&quot;,
                    password=&quot;&lt;user_password&gt;&quot;) as connect:
        with connect.cursor() as cur:
            cur.executemany(f&quot;&quot;&quot;
                    insert into &lt;TABLE_NAME&gt; (CustomerID, ItemSubCategoryID, Est, Comprador)
                    values (?,?,?,?)&quot;&quot;&quot;, SubCat_p.values.tolist())

The script works great with a lot of dataframes but when the result is a big dataframe (order of millons of rows) the script ends with an error
panic: runtime error: gobytes: length out of range

goroutine 17 [running, locked to thread]:
main._Cfunc_GoBytes(...)
        _cgo_gotypes.go:72
main.goCreateRows.func1(0x2?, 0x8924a486?)
        /tmp/13851/goside.go:566 +0x46
main.goCreateRows(0x0, 0x1, 0xc00012de20?, 0x7f98d52199b8?, 0xc00007c000?, 0x7f99740fe890, 0x7f987f8c4f10)
        /tmp/13851/goside.go:566 +0x98
Aborted (core dumped)

Is there an alternative to my method (or to teradatasql) to avoid this error? I found the module JayDeBeApi that works in a similar way but I failed to create the connection (I think it doesn't use a host name but a url that I don't know)
",1
599,73169068,SQL Teradata- Syntax error while removing duplicates using row_number() over,"I need to delete duplicates from a table. I've tried to do this following exactly the code from this website as well as this
    WITH cte (Id, 
    Proname, 
    Cityname, 
    Companyname,
    ItemsNo,
    row_num) 
    AS (SELECT 
    Id, 
    Proname, 
    Cityname, 
    Companyname,
    ItemsNo,
    ROW_NUMBER() OVER 
    (PARTITION BY 
    Id, 
    Proname, 
    Cityname, 
    Companyname
    ORDER BY 
    Id, 
    Proname, 
    Cityname, 
    Companyname) AS row_num
    FROM dba.tabdupes)
    DELETE FROM cte
    WHERE row_num &gt; 1;

but every time I'm getting a syntax error in Teradata:
[3707]syntax error expected something like a 'SELECT' keyword or '(' or a 'TRANSACTION TIME' keyword or a 'VALIDRIME' keyword between ')' and the 'DELETE' keyword.
I've tried multiple solutions but can't get what is wrong here.
",-1,-1,-1.0,"I need to delete duplicates from a table. I've tried to do this following exactly the code from this website as well as this
    WITH cte (Id, 
    Proname, 
    Cityname, 
    Companyname,
    ItemsNo,
    row_num) 
    AS (SELECT 
    Id, 
    Proname, 
    Cityname, 
    Companyname,
    ItemsNo,
    ROW_NUMBER() OVER 
    (PARTITION BY 
    Id, 
    Proname, 
    Cityname, 
    Companyname
    ORDER BY 
    Id, 
    Proname, 
    Cityname, 
    Companyname) AS row_num
    FROM dba.tabdupes)
    DELETE FROM cte
    WHERE row_num &gt; 1;

but every time I'm getting a syntax error in Teradata:
[3707]syntax error expected something like a 'SELECT' keyword or '(' or a 'TRANSACTION TIME' keyword or a 'VALIDRIME' keyword between ')' and the 'DELETE' keyword.
I've tried multiple solutions but can't get what is wrong here.
",3
600,73311768,Teradata : Using Column reference with the Interval Parameter,"I have a problem with a query, I want to subtract an interval to a date. The interval parameter is a value of a column.
HINIC               | IREFT | IPERDC

2022-06-28 00:22:15 | DAY   | 3

2022-07-10 20:00:39 | MONTH | 1

The result I wanted would be
HINIC               | IREFT | IPERDC | RESULT

2022-06-28 00:22:15 | DAY   | 3 | 2022-06-25 00:22:15

2022-07-10 20:00:39 | MONTH | 1 | 2022-06-10 20:00:39

I tried to used &quot;interval&quot; but it looks like it doesn't accept columns as parameters.
When I use &quot;interval&quot; with a column, like this
Select 

CAST(HINIC AS DATE) - INTERVAL IPERDC DAY 

from table 

It returns this error

SQL Error [3707] [42000]: [Teradata Database] [TeraJDBC 16.20.00.06] [Error 3707] [SQLState 42000] Syntax error, expected something like a string or a Unicode character literal between the 'INTERVAL' keyword and the word IPERDC

Thank you.
",-1,-1,-1.0,"I have a problem with a query, I want to subtract an interval to a date. The interval parameter is a value of a column.
HINIC               | IREFT | IPERDC

2022-06-28 00:22:15 | DAY   | 3

2022-07-10 20:00:39 | MONTH | 1

The result I wanted would be
HINIC               | IREFT | IPERDC | RESULT

2022-06-28 00:22:15 | DAY   | 3 | 2022-06-25 00:22:15

2022-07-10 20:00:39 | MONTH | 1 | 2022-06-10 20:00:39

I tried to used &quot;interval&quot; but it looks like it doesn't accept columns as parameters.
When I use &quot;interval&quot; with a column, like this
Select 

CAST(HINIC AS DATE) - INTERVAL IPERDC DAY 

from table 

It returns this error

SQL Error [3707] [42000]: [Teradata Database] [TeraJDBC 16.20.00.06] [Error 3707] [SQLState 42000] Syntax error, expected something like a string or a Unicode character literal between the 'INTERVAL' keyword and the word IPERDC

Thank you.
",3
601,73782509,How to connect the teradata connection details in python,"I am trying to connect to Teradata using cx_Oracle  in Python.
import cx_Oracle
import psycopg2
connection = 'user/hostname/password'
con = cx_Oracle.connect(connection)
getting the error: net service name is incorrectly specified
",-1,-1,-1.0,"I am trying to connect to Teradata using cx_Oracle  in Python.
import cx_Oracle
import psycopg2
connection = 'user/hostname/password'
con = cx_Oracle.connect(connection)
getting the error: net service name is incorrectly specified
",1
602,73905577,Partition eliminations with multilevel partitions on teradata database,"I have a performance issue with multilevel partitioned tables on teradata database. It seems the partition elimination is not occurring if a table has a certain kind of partitioning structure.
As an example, consider the following table and query. With a single partitioning expression, the plan successfully eliminated all but one partition.
/* Table with single partitioning expression */
CREATE MULTISET TABLE table_single
     (
      id INTEGER,
      time_stamp INTEGER)
PRIMARY INDEX ( id ,time_stamp )
PARTITION BY RANGE_N(time_stamp  BETWEEN *,1651330800  AND 1656601199  EACH 3600 ,NO RANGE, UNKNOWN);

EXPLAIN SELECT count(*) FROM table_single WHERE time_stamp BETWEEN 1654041600 AND 1654041600 + 100;

// result
...
 3) We do an all-AMPs SUM step in TD_MAP1 to aggregate from 
    a single partition of table_single with a condition of
...

In contrast, when we add another partition, the plan became a full scan, which indicates partition elimination did not occur.
/* Example with multi level partition */
CREATE MULTISET TABLE table_mult
     (
      id INTEGER,
      time_stamp INTEGER)
PRIMARY INDEX ( id ,time_stamp )
PARTITION BY (
  RANGE_N(time_stamp  BETWEEN *,1651330800  AND 1656601199  EACH 3600 ,NO RANGE, UNKNOWN),
  RANGE_N(id  BETWEEN 1  AND 200  EACH 1 ,NO RANGE, UNKNOWN)
);

EXPLAIN SELECT count(*) FROM table_mult WHERE time_stamp BETWEEN 1654041600 AND 1654041600 + 100;

// result
...
  3) We do an all-AMPs SUM step in TD_MAP1 to aggregate from
     table_mult by way of an all-rows scan with a condition of
...

Further, when I change the partitioning expression slightly as below (removed the &quot;*&quot; from the first RANGE_N call), then the partition elimination is back.
/* Table with multilevel parition, v2 */
CREATE MULTISET TABLE table_mult2
     (
      id INTEGER,
      time_stamp INTEGER)
PRIMARY INDEX ( id ,time_stamp )
PARTITION BY (
  RANGE_N(time_stamp  BETWEEN 1651330800  AND 1656601199  EACH 3600 ,NO RANGE, UNKNOWN),
  RANGE_N(id  BETWEEN 1  AND 200  EACH 1 ,NO RANGE, UNKNOWN) 
);

EXPLAIN SELECT count(*) FROM table_mult2 WHERE time_stamp BETWEEN 1654041600 AND 1654041600 + 100;

// result
...
 3) We do an all-AMPs SUM step in TD_MAP1 to aggregate from 404
    partitions of table_mult2 with a condition of (
...

Not only the query plan changes, but also in practice we also observe that the query on the second table becomes slower as we have more data outside the target partitions, from which we believe the partition elimination is not functioning with that table.
We would like to understand the reasons for this behavior, in particular why the partition elimination does not occur for the second definition of the table.
",-1,-1,-1.0,"I have a performance issue with multilevel partitioned tables on teradata database. It seems the partition elimination is not occurring if a table has a certain kind of partitioning structure.
As an example, consider the following table and query. With a single partitioning expression, the plan successfully eliminated all but one partition.
/* Table with single partitioning expression */
CREATE MULTISET TABLE table_single
     (
      id INTEGER,
      time_stamp INTEGER)
PRIMARY INDEX ( id ,time_stamp )
PARTITION BY RANGE_N(time_stamp  BETWEEN *,1651330800  AND 1656601199  EACH 3600 ,NO RANGE, UNKNOWN);

EXPLAIN SELECT count(*) FROM table_single WHERE time_stamp BETWEEN 1654041600 AND 1654041600 + 100;

// result
...
 3) We do an all-AMPs SUM step in TD_MAP1 to aggregate from 
    a single partition of table_single with a condition of
...

In contrast, when we add another partition, the plan became a full scan, which indicates partition elimination did not occur.
/* Example with multi level partition */
CREATE MULTISET TABLE table_mult
     (
      id INTEGER,
      time_stamp INTEGER)
PRIMARY INDEX ( id ,time_stamp )
PARTITION BY (
  RANGE_N(time_stamp  BETWEEN *,1651330800  AND 1656601199  EACH 3600 ,NO RANGE, UNKNOWN),
  RANGE_N(id  BETWEEN 1  AND 200  EACH 1 ,NO RANGE, UNKNOWN)
);

EXPLAIN SELECT count(*) FROM table_mult WHERE time_stamp BETWEEN 1654041600 AND 1654041600 + 100;

// result
...
  3) We do an all-AMPs SUM step in TD_MAP1 to aggregate from
     table_mult by way of an all-rows scan with a condition of
...

Further, when I change the partitioning expression slightly as below (removed the &quot;*&quot; from the first RANGE_N call), then the partition elimination is back.
/* Table with multilevel parition, v2 */
CREATE MULTISET TABLE table_mult2
     (
      id INTEGER,
      time_stamp INTEGER)
PRIMARY INDEX ( id ,time_stamp )
PARTITION BY (
  RANGE_N(time_stamp  BETWEEN 1651330800  AND 1656601199  EACH 3600 ,NO RANGE, UNKNOWN),
  RANGE_N(id  BETWEEN 1  AND 200  EACH 1 ,NO RANGE, UNKNOWN) 
);

EXPLAIN SELECT count(*) FROM table_mult2 WHERE time_stamp BETWEEN 1654041600 AND 1654041600 + 100;

// result
...
 3) We do an all-AMPs SUM step in TD_MAP1 to aggregate from 404
    partitions of table_mult2 with a condition of (
...

Not only the query plan changes, but also in practice we also observe that the query on the second table becomes slower as we have more data outside the target partitions, from which we believe the partition elimination is not functioning with that table.
We would like to understand the reasons for this behavior, in particular why the partition elimination does not occur for the second definition of the table.
",4
603,74180319,Query timeout expired error when using teradata odbc driver,"I'm using following code in python to get a csv file contains a lot of data.


import teradata
import warnings
import pandas as pd
import pyodbc
from sqlalchemy import types, create_engine
warnings.filterwarnings(""ignore"")

udaExec = teradata.UdaExec (appName=""test"", version=""1.0"", logConsole=True)

with udaExec.connect(method=""odbc"",LoginTimeout=0,dsn='database') as connect:
    
    query_distance="""""" SELECT
      (CURRENT_DATE-0) AS COLLDATE,
     .
     .
     .
     .
)         
) SUM;
  """"""
data_flightno1 = pd.read_sql(query_distance,connect,chunksize=10000)
    
for chunk in data_flightno1:  
        
    data_flightno1.to_csv(""test2.csv"", sep="","", mode=""a"") 



I get the DatabaseError: (0, '[HYT00] [Teradata][ODBC Teradata Driver] Query timeout expired') when I run the code. What should I do to solve this error? Thanks in advance.
",-1,-1,-1.0,"I'm using following code in python to get a csv file contains a lot of data.


import teradata
import warnings
import pandas as pd
import pyodbc
from sqlalchemy import types, create_engine
warnings.filterwarnings(""ignore"")

udaExec = teradata.UdaExec (appName=""test"", version=""1.0"", logConsole=True)

with udaExec.connect(method=""odbc"",LoginTimeout=0,dsn='database') as connect:
    
    query_distance="""""" SELECT
      (CURRENT_DATE-0) AS COLLDATE,
     .
     .
     .
     .
)         
) SUM;
  """"""
data_flightno1 = pd.read_sql(query_distance,connect,chunksize=10000)
    
for chunk in data_flightno1:  
        
    data_flightno1.to_csv(""test2.csv"", sep="","", mode=""a"") 



I get the DatabaseError: (0, '[HYT00] [Teradata][ODBC Teradata Driver] Query timeout expired') when I run the code. What should I do to solve this error? Thanks in advance.
",1
604,74739651,how to convert xmlget and xmlelement from teradata to snowflake,"Below is the teradata code i want to convert to snowflake
    select XMLAGG (XMLELEMENT (n, col1
    || CHR (10))
   ORDER BY num
  ).EXTRACT ('//text()').getClobVal () AS abc

tried as below but not working. am new to both oracle and snowflake please help me with this
extract(listagg (parse_xml('&lt;n&gt; col1||  CHR (10) &lt;n&gt;')),'//text()')

",-1,-1,-1.0,"Below is the teradata code i want to convert to snowflake
    select XMLAGG (XMLELEMENT (n, col1
    || CHR (10))
   ORDER BY num
  ).EXTRACT ('//text()').getClobVal () AS abc

tried as below but not working. am new to both oracle and snowflake please help me with this
extract(listagg (parse_xml('&lt;n&gt; col1||  CHR (10) &lt;n&gt;')),'//text()')

",3
605,74472533,Load data to teradata from sas,"I am trying to load data to teradata from sas. The data should be ordered by a specific field.
There is an error because of the order by clause.
Does anyone know how I can do that?
Trying to upload ordered data to teradata
",-1,-1,-1.0,"I am trying to load data to teradata from sas. The data should be ordered by a specific field.
There is an error because of the order by clause.
Does anyone know how I can do that?
Trying to upload ordered data to teradata
",3
606,74297137,Automating excel to teradata table and number of rows and columns increasing in Excel,"I need to automate a excel table for Time value for Money Calculation to teradata table.
Table structure is like this - it has these columns:
Month Base_Rate 202201 202202 202203 202204....... 

and so on
I have attached image of sample data.
Same month is in row and column and I cannot change the structure of data in Excel.
What would be the best way to automate table creation and updating the records each month?
By automate I mean to create reusable script, that can be used every month to update data in table.
There is no definite period when the column gets added, roughly it is done nearly once a year which mean one whole year is added in columns at a time and then the same next year and so on.
And rows are added almost every month which mean for eg. Nov month details are added in Nov

I thought of truncate and load every single month, but this isn't the best option.
",1,1,-1.0,"I need to automate a excel table for Time value for Money Calculation to teradata table.
Table structure is like this - it has these columns:
Month Base_Rate 202201 202202 202203 202204....... 

and so on
I have attached image of sample data.
Same month is in row and column and I cannot change the structure of data in Excel.
What would be the best way to automate table creation and updating the records each month?
By automate I mean to create reusable script, that can be used every month to update data in table.
There is no definite period when the column gets added, roughly it is done nearly once a year which mean one whole year is added in columns at a time and then the same next year and so on.
And rows are added almost every month which mean for eg. Nov month details are added in Nov

I thought of truncate and load every single month, but this isn't the best option.
",3
607,74155244,Teradatasql teradata_write_csv file encoding?,"I am using the Teradatasql library to download data into a CSV file, using:
sql=&quot;{fn teradata_write_csv(&quot;+destination_path+&quot;)}SELECT DISTINCT...&quot;

How do I determine (or specify) what the file encoding is? Is there a default encoding, or does it use the encoding of the table? (There are multiple tables which may have different encodings)
According to the filesystem on Ubuntu the file is ASCII, according to Windows it is ANSI, but in both cases the file contains incorrect characters. When parsing the file in python I get:
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 5061: ordinal not in range(128)

",-1,-1,-1.0,"I am using the Teradatasql library to download data into a CSV file, using:
sql=&quot;{fn teradata_write_csv(&quot;+destination_path+&quot;)}SELECT DISTINCT...&quot;

How do I determine (or specify) what the file encoding is? Is there a default encoding, or does it use the encoding of the table? (There are multiple tables which may have different encodings)
According to the filesystem on Ubuntu the file is ASCII, according to Windows it is ANSI, but in both cases the file contains incorrect characters. When parsing the file in python I get:
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 5061: ordinal not in range(128)

",1
608,74851048,How to concat substrings with regexp_matches (as analogy regexp_substr in teradata) in Postgres 9.4,"I am having migration from teradata to greenplum and there is a string concatenation with regex_substr which is not existing at Postgres 9.4 version.
The part of the existing code is:
TRIM(COALESCE(REGEXP_SUBSTR('Office №9013/0713', '(?&lt;=№)[0-9]+', 1,1,'i), '-1')(INT))||'_'||TRIM(COALESCE(REGEXP_SUBSTR('Office №9013/0713', '(?&lt;=/)[0-9]+', 1,1,'i), '-1')(INT))
Results is follwing:
9013_713
I have tried using regexp_matches function as I checked lookbehind is not existing in 9.4.
Not sure if lookbehind '(?:№)([0-9]+)' is correct for 9.4.
regexp_matches('Office №9013/0713', '(?:№)([0-9]+)', 'gi') first part extraction after №.
regexp_matches('Office №9013/0713', '(?:\/)([0-9]+)', 'gi') for second part extraction after /.
The problem is that the result above provides array and I cannot concat to single string. Is there any easy approach to achieve 9013_713 with simple structure?
",-1,-1,-1.0,"I am having migration from teradata to greenplum and there is a string concatenation with regex_substr which is not existing at Postgres 9.4 version.
The part of the existing code is:
TRIM(COALESCE(REGEXP_SUBSTR('Office №9013/0713', '(?&lt;=№)[0-9]+', 1,1,'i), '-1')(INT))||'_'||TRIM(COALESCE(REGEXP_SUBSTR('Office №9013/0713', '(?&lt;=/)[0-9]+', 1,1,'i), '-1')(INT))
Results is follwing:
9013_713
I have tried using regexp_matches function as I checked lookbehind is not existing in 9.4.
Not sure if lookbehind '(?:№)([0-9]+)' is correct for 9.4.
regexp_matches('Office №9013/0713', '(?:№)([0-9]+)', 'gi') first part extraction after №.
regexp_matches('Office №9013/0713', '(?:\/)([0-9]+)', 'gi') for second part extraction after /.
The problem is that the result above provides array and I cannot concat to single string. Is there any easy approach to achieve 9013_713 with simple structure?
",2
609,75699268,teradata sql volatile table - case statement,"Create volatile table sample as

(select a.orig_name , a.sample_date,

CASE when a.orig_name = 'Apl' THEN 'APPLE'
    when a.orig_name = 'Orng' THEN 'ORANGE'
else 'other'

end as orig_name_grp

from tablename_fr)

with data primary index(sample_date) 
on commit preserve rows;

this code is throwing me error in teradata , can anyone help me fixing what the issue
how to use CASE statement with condition on itin volatile table
case statement is throwing error in teradata in create volatile table
",-1,-1,-1.0,"Create volatile table sample as

(select a.orig_name , a.sample_date,

CASE when a.orig_name = 'Apl' THEN 'APPLE'
    when a.orig_name = 'Orng' THEN 'ORANGE'
else 'other'

end as orig_name_grp

from tablename_fr)

with data primary index(sample_date) 
on commit preserve rows;

this code is throwing me error in teradata , can anyone help me fixing what the issue
how to use CASE statement with condition on itin volatile table
case statement is throwing error in teradata in create volatile table
",3
610,75877160,How to write a teradata SQL query case condition to check by ID and it's start date,"There are same IDs in multiple records with different startdate spans.
I need to write a teradata sql case condition to check by ID and it's start date.
In the case condition, I need to check two conditions: if the ID has a startdate as current month and it doesn't have a startdate span in last three months then &quot;yes it's a new member&quot; else &quot;not a new member&quot;.
Just FYI: below activerecord field is if the ID has a start date of current month it is shown as yes.
Example the table data is as below:
ID   startdate name.  activerecord 
101 03-01-2023 Lee     yes
101 02-01-2023 Lee     No
101 12-01-2022 Lee     No
102 03-01-2023 Mark.   yes
102 08-01-2022 Mark     No
102 07-01-2022 Mark.    No

I need the answer as below:
ID  startdate name  activerecord flag
101 03-01-2023 Lee   yes          No
101 02-01-2023 Lee   No.          No
101 12-01-2022 Lee   No.          No 
102 03-01-2023 Mark  Yes.         Yes
102 08-01-2022 Mark  No.          No
102 07-01-2022 Mark  No           No

Here 101 ID has a startdate as current month and  also has a start date span in last three months so this member is not a new member then print as 'No'.
102 ID has a startdate as current month but it doesn't have a date span in last three months so I need to consider this as a new member so print 'Yes'
I couldn't do this at all.
",-1,-1,-1.0,"There are same IDs in multiple records with different startdate spans.
I need to write a teradata sql case condition to check by ID and it's start date.
In the case condition, I need to check two conditions: if the ID has a startdate as current month and it doesn't have a startdate span in last three months then &quot;yes it's a new member&quot; else &quot;not a new member&quot;.
Just FYI: below activerecord field is if the ID has a start date of current month it is shown as yes.
Example the table data is as below:
ID   startdate name.  activerecord 
101 03-01-2023 Lee     yes
101 02-01-2023 Lee     No
101 12-01-2022 Lee     No
102 03-01-2023 Mark.   yes
102 08-01-2022 Mark     No
102 07-01-2022 Mark.    No

I need the answer as below:
ID  startdate name  activerecord flag
101 03-01-2023 Lee   yes          No
101 02-01-2023 Lee   No.          No
101 12-01-2022 Lee   No.          No 
102 03-01-2023 Mark  Yes.         Yes
102 08-01-2022 Mark  No.          No
102 07-01-2022 Mark  No           No

Here 101 ID has a startdate as current month and  also has a start date span in last three months so this member is not a new member then print as 'No'.
102 ID has a startdate as current month but it doesn't have a date span in last three months so I need to consider this as a new member so print 'Yes'
I couldn't do this at all.
",3
611,76056657,using python unable to push data from dataframe to teradata table,"using python unable to push data from dataframe to teradata table
I have created table and same list of columns in teradata which are present in dataframe
but when i try to push data there were few different types of errors
I have tried below like snippets
      import pandas as pd
      import teradatasql
      from google.cloud import bigquery

     client = bigquery.Client.from_service_account_json('path/to/service/account/key.json')
     query_job = client.query('SELECT * FROM bigquery_table')
     df = query_job.to_dataframe()
     con = teradatasql.connect(host='teradata_host', user='username', password='password')

     with con.cursor() as cur:
           cur.execute(&quot;CREATE TABLE teradata_table (col1 VARCHAR(100), col2 INTEGER, col3 FLOAT)&quot;)

     for index, row in df.iterrows():
         query = &quot;INSERT INTO teradata_table VALUES (?, ?, ?)&quot;
         cur.execute(query, tuple(row))
     con.commit() 

But am getting few different errors :
  OperationalError: [Version 17.20.0.19] [Session 31039764] [Teradata Database] [Error 3707] Syntax error, 
   expected something like a 'CHECK' keyword between ',' and the 'index' keyword.

and even I tried with to_sql as well but same issue
      df.to_sql('teradata_table',con=con,if_exists='append',index=False)

But am getting below error:
  OperationalError: [Version 17.20.0.19] [Session 31051994] [Teradata Database] 
 [Error 3707] Syntax error, expected something like '(' between the 'type' keyword and '='.

",-1,-1,-1.0,"using python unable to push data from dataframe to teradata table
I have created table and same list of columns in teradata which are present in dataframe
but when i try to push data there were few different types of errors
I have tried below like snippets
      import pandas as pd
      import teradatasql
      from google.cloud import bigquery

     client = bigquery.Client.from_service_account_json('path/to/service/account/key.json')
     query_job = client.query('SELECT * FROM bigquery_table')
     df = query_job.to_dataframe()
     con = teradatasql.connect(host='teradata_host', user='username', password='password')

     with con.cursor() as cur:
           cur.execute(&quot;CREATE TABLE teradata_table (col1 VARCHAR(100), col2 INTEGER, col3 FLOAT)&quot;)

     for index, row in df.iterrows():
         query = &quot;INSERT INTO teradata_table VALUES (?, ?, ?)&quot;
         cur.execute(query, tuple(row))
     con.commit() 

But am getting few different errors :
  OperationalError: [Version 17.20.0.19] [Session 31039764] [Teradata Database] [Error 3707] Syntax error, 
   expected something like a 'CHECK' keyword between ',' and the 'index' keyword.

and even I tried with to_sql as well but same issue
      df.to_sql('teradata_table',con=con,if_exists='append',index=False)

But am getting below error:
  OperationalError: [Version 17.20.0.19] [Session 31051994] [Teradata Database] 
 [Error 3707] Syntax error, expected something like '(' between the 'type' keyword and '='.

",1
612,76154845,Date format issue for Teradata input parakeets,"I am trying to give data format in teradata procedure for my input parameters.
REPLACE PROCEDURE UDMTOOL_STG_USER.TDPRC_SALES_ESL_INSERT_GLOBAL_APPLE_BRAND_QUERIES 
(
    IN  LV_COUNTRY_CD               VARCHAR(100),
    IN  LV_SEARCHED_KEYWORD_VAL     VARCHAR(300) CHARACTER SET UNICODE,
    IN  LV_SEARCHED_KEYWORD_VAL_ENG VARCHAR(300),
    IN  LV_EFFECTIVE_START_DT       DATE FORMAT 'YYYY-MM-DD',
    IN  LV_EFFECTIVE_END_DT         DATE FORMAT 'YYYY-MM-DD', 
    IN  LV_BRAND_NAME               VARCHAR(100),
    IN  LV_INSERT_TS                VARCHAR(19),
    IN  LV_UPDATED_ON               VARCHAR(19),
    IN  LV_UPDATED_BY               VARCHAR(100),
    IN  LN_REQUEST_ID               INTEGER,
    OUT LV_OUT_STATUS               VARCHAR(100) 
)

but when I deploy this procedure getting as this error
SPL1076:E(L20), The right parenthesis in parameter declaration is missing.
SPL1048:E(L20), Unexpected text ';' in place of SPL statement.

please tell me anything wrong here
",-1,-1,-1.0,"I am trying to give data format in teradata procedure for my input parameters.
REPLACE PROCEDURE UDMTOOL_STG_USER.TDPRC_SALES_ESL_INSERT_GLOBAL_APPLE_BRAND_QUERIES 
(
    IN  LV_COUNTRY_CD               VARCHAR(100),
    IN  LV_SEARCHED_KEYWORD_VAL     VARCHAR(300) CHARACTER SET UNICODE,
    IN  LV_SEARCHED_KEYWORD_VAL_ENG VARCHAR(300),
    IN  LV_EFFECTIVE_START_DT       DATE FORMAT 'YYYY-MM-DD',
    IN  LV_EFFECTIVE_END_DT         DATE FORMAT 'YYYY-MM-DD', 
    IN  LV_BRAND_NAME               VARCHAR(100),
    IN  LV_INSERT_TS                VARCHAR(19),
    IN  LV_UPDATED_ON               VARCHAR(19),
    IN  LV_UPDATED_BY               VARCHAR(100),
    IN  LN_REQUEST_ID               INTEGER,
    OUT LV_OUT_STATUS               VARCHAR(100) 
)

but when I deploy this procedure getting as this error
SPL1076:E(L20), The right parenthesis in parameter declaration is missing.
SPL1048:E(L20), Unexpected text ';' in place of SPL statement.

please tell me anything wrong here
",3
613,76165737,using python unable to insert pandas dataframe to teradata table,"I have to push data from pandas dataframe to teradata table using python
tried different methods teradatasql , pyodbc, sqlalchemy , teradata.UdaExec but am getting unique errors for each method
Below are the codes &amp; errors
  Code 1:
   host,user,password ='SMASUN.UPC','J3E186','admin123'
   database, table_name='Supply', 'AD_Products'
   con= teradatasql.connect(host=host, user=user, password=password, 
   database=database)
   sam=pd.read_csv(&quot;C:/Users/Desktop/products.csv&quot;)
   sam.to_sql(table_name, con=con, index=False, if_exists=&quot;replace&quot;)

 Error 1:
  OperationalError: [Version 17.20.0.19] [Session 34249729] [Teradata Database] [Error 3707] Syntax error, 
   expected something like a 'CHECK' keyword between '(' and the 'exp' keyword.
   at gosqldriver/teradatasql.formatError ErrorUtil.go:89
 

 Code 2:
       tera_engine= create_engine('teradata://'+ user +':' + password + '@'+ host + 
   ':22/')
      #tera_engine
     df='`SELECT * FROM `fucntional_.supply.products`' [this is from big query directly pushing to teradata]
     connection=tera_engine.connect()
     connection.execute(df)
     connection.close()

 Error 2:
  DatabaseError: (1207, '[HY000] [TPT][ODBC PostgreSQL Wire Protocol driver]Connection refused. Verify Host Name and Port Number.
 , [TPT][ODBC PostgreSQL Wire Protocol driver]Invalid attribute in connection string: DBCNAME.')

 Code 3:
      df=pd.read_csv(&quot;C:/Users/Desktop/products.csv&quot;)
      table_name = 'products'
     # Insert the dataframe into the table using executemany
      insert_query = f&quot;insert into {table_name} values (?, ?)&quot;
      values = [tuple(row) for row in df.values.tolist()]
      cursor.executemany(insert_query, values)
     con.commit()
     con.close()

 Error 3:
 InterfaceError: ('28000', '[28000] [Teradata][ODBC Teradata Driver][Teradata Database] (210) The UserId, 
Password or Account is invalid.FailCode = -8017 (210) (SQLDriverConnect); [28000] [Teradata][ODBC Teradata Driver][Teradata Database] (210) 
       The UserId, Password or Account is invalid. FailCode = -8017 (210)')

",-1,-1,-1.0,"I have to push data from pandas dataframe to teradata table using python
tried different methods teradatasql , pyodbc, sqlalchemy , teradata.UdaExec but am getting unique errors for each method
Below are the codes &amp; errors
  Code 1:
   host,user,password ='SMASUN.UPC','J3E186','admin123'
   database, table_name='Supply', 'AD_Products'
   con= teradatasql.connect(host=host, user=user, password=password, 
   database=database)
   sam=pd.read_csv(&quot;C:/Users/Desktop/products.csv&quot;)
   sam.to_sql(table_name, con=con, index=False, if_exists=&quot;replace&quot;)

 Error 1:
  OperationalError: [Version 17.20.0.19] [Session 34249729] [Teradata Database] [Error 3707] Syntax error, 
   expected something like a 'CHECK' keyword between '(' and the 'exp' keyword.
   at gosqldriver/teradatasql.formatError ErrorUtil.go:89
 

 Code 2:
       tera_engine= create_engine('teradata://'+ user +':' + password + '@'+ host + 
   ':22/')
      #tera_engine
     df='`SELECT * FROM `fucntional_.supply.products`' [this is from big query directly pushing to teradata]
     connection=tera_engine.connect()
     connection.execute(df)
     connection.close()

 Error 2:
  DatabaseError: (1207, '[HY000] [TPT][ODBC PostgreSQL Wire Protocol driver]Connection refused. Verify Host Name and Port Number.
 , [TPT][ODBC PostgreSQL Wire Protocol driver]Invalid attribute in connection string: DBCNAME.')

 Code 3:
      df=pd.read_csv(&quot;C:/Users/Desktop/products.csv&quot;)
      table_name = 'products'
     # Insert the dataframe into the table using executemany
      insert_query = f&quot;insert into {table_name} values (?, ?)&quot;
      values = [tuple(row) for row in df.values.tolist()]
      cursor.executemany(insert_query, values)
     con.commit()
     con.close()

 Error 3:
 InterfaceError: ('28000', '[28000] [Teradata][ODBC Teradata Driver][Teradata Database] (210) The UserId, 
Password or Account is invalid.FailCode = -8017 (210) (SQLDriverConnect); [28000] [Teradata][ODBC Teradata Driver][Teradata Database] (210) 
       The UserId, Password or Account is invalid. FailCode = -8017 (210)')

",1
614,76276689,How to drop table using pyspark jdbc connector to teradata?,"I can select from the teradata database, but I cannot drop using pyspark.
I have also used jaydebeapi to drop the table in the same spark session and that works.
Was hoping someone may have encountered the same issue.
drop_sql = &quot;&quot;&quot; (DROP TABLE &lt;DB_NAME&gt;.&lt;TABLENAME&gt;) &quot;&quot;&quot;


conn = spark.read \
.format(&quot;jdbc&quot;) \
.option(&quot;driver&quot;,&quot;com.teradata.jdbc.TeraDriver&quot;) \
.option(&quot;url&quot;,&quot;jdbc:teradata://&lt;IP_ADDRESS&gt;/DATABASE=. &lt;DB_NAME&gt;,TMODE=ANSI,CHARSET=UTF8,TYPE=FASTLOAD,LOGMECH=LDAP&quot;) \
.option(&quot;query&quot;, drop_sql) \
.option(&quot;user&quot;, user) \
.option(&quot;password&quot;,password)\
.option(&quot;fetchsize&quot;,10000).load()

ERROR:
Py4JJavaError: An error occurred while calling o265.load. : java.sql.SQLException: [Teradata Database] [TeraJDBC 17.20.00.15] [Error 3707] [SQLState 42000] Syntax error, expected something like a name or a Unicode delimited identifier or an 'UDFCALLNAME' keyword or a 'SELECT' keyword or '(' between '(' and the 'DROP' keyword.
",-1,-1,-1.0,"I can select from the teradata database, but I cannot drop using pyspark.
I have also used jaydebeapi to drop the table in the same spark session and that works.
Was hoping someone may have encountered the same issue.
drop_sql = &quot;&quot;&quot; (DROP TABLE &lt;DB_NAME&gt;.&lt;TABLENAME&gt;) &quot;&quot;&quot;


conn = spark.read \
.format(&quot;jdbc&quot;) \
.option(&quot;driver&quot;,&quot;com.teradata.jdbc.TeraDriver&quot;) \
.option(&quot;url&quot;,&quot;jdbc:teradata://&lt;IP_ADDRESS&gt;/DATABASE=. &lt;DB_NAME&gt;,TMODE=ANSI,CHARSET=UTF8,TYPE=FASTLOAD,LOGMECH=LDAP&quot;) \
.option(&quot;query&quot;, drop_sql) \
.option(&quot;user&quot;, user) \
.option(&quot;password&quot;,password)\
.option(&quot;fetchsize&quot;,10000).load()

ERROR:
Py4JJavaError: An error occurred while calling o265.load. : java.sql.SQLException: [Teradata Database] [TeraJDBC 17.20.00.15] [Error 3707] [SQLState 42000] Syntax error, expected something like a name or a Unicode delimited identifier or an 'UDFCALLNAME' keyword or a 'SELECT' keyword or '(' between '(' and the 'DROP' keyword.
",0
615,76311875,Teradata XMLPUBLISH_STREAM: Group multiple times?,"I'm trying to generate xml files from the database using the Teradata stored procedure XMLPUBLISH_STREAM. From the examples on the website, I can see, that there is the possibility of grouping via &quot;teradata_group&quot;.
I need to group on different columns in different parts of the xml.
Is this possible?
Example




ID
Name
Date




1
x
2012-01-01


1
x
2020-04-03


2
y
1999-01-01




My desired output should look like this:
&lt;xml&gt;
 &lt;elem id=&quot;1&quot; name=&quot;x&quot;&gt;
  &lt;date&gt;2012-01-01&lt;/date&gt;
  &lt;date&gt;2020-04-03&lt;/date&gt;
 &lt;/elem&gt;
 &lt;elem id=&quot;2&quot; name=&quot;y&quot;&gt;
  &lt;date&gt;1999-01-01&lt;/date&gt;
 &lt;/elem&gt;
&lt;/xml&gt;

Unfortunately I am not able to create these &quot;inner&quot; date-elements, because I use a &quot;teradata_group&quot; for the id and I am not able to use another &quot;group by&quot; or &quot;xsl:for-each&quot; inside this group. Since I never used this stored procedure before, my question is: is this even possible or can I only &quot;group once&quot;?
",-1,1,-1.0,"I'm trying to generate xml files from the database using the Teradata stored procedure XMLPUBLISH_STREAM. From the examples on the website, I can see, that there is the possibility of grouping via &quot;teradata_group&quot;.
I need to group on different columns in different parts of the xml.
Is this possible?
Example




ID
Name
Date




1
x
2012-01-01


1
x
2020-04-03


2
y
1999-01-01




My desired output should look like this:
&lt;xml&gt;
 &lt;elem id=&quot;1&quot; name=&quot;x&quot;&gt;
  &lt;date&gt;2012-01-01&lt;/date&gt;
  &lt;date&gt;2020-04-03&lt;/date&gt;
 &lt;/elem&gt;
 &lt;elem id=&quot;2&quot; name=&quot;y&quot;&gt;
  &lt;date&gt;1999-01-01&lt;/date&gt;
 &lt;/elem&gt;
&lt;/xml&gt;

Unfortunately I am not able to create these &quot;inner&quot; date-elements, because I use a &quot;teradata_group&quot; for the id and I am not able to use another &quot;group by&quot; or &quot;xsl:for-each&quot; inside this group. Since I never used this stored procedure before, my question is: is this even possible or can I only &quot;group once&quot;?
",3
616,76517599,Pulling data from Teradata using Python - specifically how to 'chunk' data so that hardware resources are not overrun,"and thank you in advance for any ideas/suggestions/solutions. I have basic knowledge of Python, so if I am misstating something, or missing something please let me know. I am absolutely trying to learn as I go.
So, onto my issue: Have a DB that I don't own (but have read access to) with data I need to pull into my DB for further parsing/joining/etc. Source DB is Teradata, destination DB is Vertica. I have the below script up and running, however the full pull is multiple 10s of millions of rows. I can return 1M rows no problem, so what I want to do is to 'chunk' the data into 1M rows of data, append to my destination DB table, and continue on until all data from the source table has been consumed. Thoughts?
# Import libraries
import teradatasql
import pandas as pd
from sqlalchemy import create_engine
from sqlalchemy.pool import NullPool
import sqlalchemy as sa


server_name = 'HostServerName'
domain_user = 'DomainAccount'
domain_password = 'DomainAccountPassword'
login_mechanism = 'LDAP'

Host3 = 'VerticaServerName'
UserName3 = 'VerticaUser'
Password3 = 'VerticaUserPassword'
Database3 = 'VerticaUserDB'

# Query against DB
query1 = 
'SELECT Column1,Column2,Column3,Column4 FROM HostServerName.Table1 WHERE Column3 IS NOT NULL'

#Connect to the HostServerName/Teradata DB and pull data
with teradatasql.connect(
  host=server_name, user=domain_user
, password=domain_password
, logmech=login_mechanism
, encryptdata='true'
) as connect:
 data = pd.read_sql(query1, connect
)
print(data)

# update the dataframe type, converts 'text'/string values to varchar
def updateType(data_para):
    dtypedict = {}  # create and empty dictionary
    for i,j in zip(data.columns,data.dtypes):
        if &quot;object&quot; in str(j):
            dtypedict.update({i: sa.types.VARCHAR})

    return dtypedict
updatedict = updateType(data)

#Connect to the Vertica DB and append data to table
engine = create_engine(f'vertica+vertica_python://{UserName3}
      :{Password3}@{Host3}/{Database3}'
     , pool_pre_ping=True, poolclass=NullPool
)
data.to_sql(
  'VerticaUSERDB.Table2'
, engine
, schema='VerticaSchema'
, if_exists='append'
, dtype=updatedict
, index=False
)

I did try to do this using some other answers on stackoverflow, however it errored out and I don't understand the errors I received.
",-1,1,-1.0,"and thank you in advance for any ideas/suggestions/solutions. I have basic knowledge of Python, so if I am misstating something, or missing something please let me know. I am absolutely trying to learn as I go.
So, onto my issue: Have a DB that I don't own (but have read access to) with data I need to pull into my DB for further parsing/joining/etc. Source DB is Teradata, destination DB is Vertica. I have the below script up and running, however the full pull is multiple 10s of millions of rows. I can return 1M rows no problem, so what I want to do is to 'chunk' the data into 1M rows of data, append to my destination DB table, and continue on until all data from the source table has been consumed. Thoughts?
# Import libraries
import teradatasql
import pandas as pd
from sqlalchemy import create_engine
from sqlalchemy.pool import NullPool
import sqlalchemy as sa


server_name = 'HostServerName'
domain_user = 'DomainAccount'
domain_password = 'DomainAccountPassword'
login_mechanism = 'LDAP'

Host3 = 'VerticaServerName'
UserName3 = 'VerticaUser'
Password3 = 'VerticaUserPassword'
Database3 = 'VerticaUserDB'

# Query against DB
query1 = 
'SELECT Column1,Column2,Column3,Column4 FROM HostServerName.Table1 WHERE Column3 IS NOT NULL'

#Connect to the HostServerName/Teradata DB and pull data
with teradatasql.connect(
  host=server_name, user=domain_user
, password=domain_password
, logmech=login_mechanism
, encryptdata='true'
) as connect:
 data = pd.read_sql(query1, connect
)
print(data)

# update the dataframe type, converts 'text'/string values to varchar
def updateType(data_para):
    dtypedict = {}  # create and empty dictionary
    for i,j in zip(data.columns,data.dtypes):
        if &quot;object&quot; in str(j):
            dtypedict.update({i: sa.types.VARCHAR})

    return dtypedict
updatedict = updateType(data)

#Connect to the Vertica DB and append data to table
engine = create_engine(f'vertica+vertica_python://{UserName3}
      :{Password3}@{Host3}/{Database3}'
     , pool_pre_ping=True, poolclass=NullPool
)
data.to_sql(
  'VerticaUSERDB.Table2'
, engine
, schema='VerticaSchema'
, if_exists='append'
, dtype=updatedict
, index=False
)

I did try to do this using some other answers on stackoverflow, however it errored out and I don't understand the errors I received.
",1
